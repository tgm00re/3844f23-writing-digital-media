"- From the table of descriptive statistics, we observe that mean and median values of **bmi** are very close to each other.\n- Hence, we will fill the missing values with the **mean values**."
# Exploratory Data Analysis
### Target Variable Visualization (stroke) : 
"- Clearly, the dataset is unbalanced in the favour of **no stroke**.\n- **19 : 1** ratio is observed for **No Stroke : Stroke!**\n- Thus, due to such heavy bias towards cases of **No Stroke**, predictions cannot be trusted!"
### Discrete Features :\n\n#### Distribution of Discrete Features :
"- Data distribution for **age** has dominant values around : **10**, **60** & **80**.\n- **avg_glucose_level** has 2 peaks of uneven heights present at values around : **100** & **200**.\n- **bmi** has a near about **normal distribution** but it has values in low numbers towards the right side! "
### Discrete Features w.r.t Target Variable (stroke) :
"- Because of too many unique data points in the **discrete_features**, it is difficult to gain any type of insight. Thus, we will convert these features into categorical features for visualizations.\n- We scale the data points of these features to a constant value that represents a range of values.(like mean)\n- Here, we divide the data points by a constant value and assign it's quotient value as the representative constant. The scaling constants are decided by looking into the data & intuition. "
- We remove the **stroke** feature from the list of categorical features as it is the target variable and we will treat it separately!
- All the categorical features are **Normally Distributed**.
### Categorical Features w.r.t Target Variable (stroke) :
"- All the graphs near about share the same pattern i.e displaying low number of **stroke** cases and no clear cut reason to point towards.  \n- **Female** population has recorded more cases of **stroke** than **male**.\n- Interestingly, people with **no hypertension** & **no heart disease** have displayed to be more prone to **suffering stroke** than people that have these medical conditions.\n- According to the dataset, people that have been **married** have **suffered stroke** more than those people who have never married.\n- When it comes to **smoking_status**, people that have **never smoked** have topped the numbers with **formerly smoked** people coming at the 2nd position to record **stroke** cases.\n- Not much info can be gained from **Residence_type** & **work_type**, however **Private** workers **suffered stroke** cases more than any other worker."
#### gender vs Discrete Features :
"- For both **male** & **female** population, **age** of those **suffering from stroke** is **60+**.\n- For majority of the **avg_glucose_level** values, both **gender** have recorded significant cases of **stroke**.\n- For **male** population, the lower limit of **bmi** values is slightly higher than the **female**. Overall, both the **gender** overlap the same **bmi** values for cases of **stroke**."
#### hypertension vs Discrete Features :
"- Wierdly, cases of **stroke** found in people having **hypertension** have a high lower limit of the **age 60+** than those who do not suffer from **hypertension**.\n- When it comes to **hypertension & avg_glucose_level**, cases of **stroke** & **no stroke** near about share the same values.\n- Due to **hypertension**, lower limits of **bmi** values are slightly reduced making people prone to **stroke**. "
#### heart_disease vs Discrete Features :
- Graphs of **hypertension** & **heart_disease** against discrete features are very similar with slight differences.\n- They share the same effects on **stroke**.
#### ever_married vs Discrete Features :
"- For **ever_married vs discrete features**, repeated insights can be found.\n- People that have been **married** have displayed cases of **stroke** for near about all the values of **avg_glucose_level**."
#### work_type vs Discrete Features :
"- Irrespective of the **work_type**, **stroke** cases have been found for **age of 60+** except for **children**.\n- Clearly, people that have worked to earn a living have suffered from **stroke**. \n- **Stroke** cases have been found more in people working in a job i.e **Govt_job** & **Private** than those who are **Self-employed**."
#### Residence_type vs Discrete Features :
- The graphs of **Rural Residence_type** & **Urban Residence_type** against discrete features w.r.t **stroke** are identical.\n- They cannot be separated from each other. They repeat the insights that have been highlighted uptill now.
#### smoking_status vs Discrete Features :
"- Irrespective of **smoking_status**, people **suffering from stroke** have been detected at **age around 60**.\n- Similar to **age**, same values of **avg_glucose_level** have been found in **stroke** cases irrespective of **smoking_status**.\n- However, because of the **smoking_status**, range of values for which cases of **stroke** differ. Range of values of people that **smokes** is slightly higher than everyone else."
### Discrete features vs Discrete features w.r.t Target variable (stroke) :
"- Due to the imbalance nature of the data, cases of **stroke** & **no stroke** cannot be separated.\n- No insights can be interpreted from the above graphs."
"- In order to visualize the correlation matrix, we create a new dataframe that contains values from **x_train** & **y_train**.\n- Thus, we reject anything outside the training data to avoid data leakage."
"- Clearly, we can see the difference in values between **Data Leakage** & **No Data Leakage**.\n- In the case of **No Data Leakage**, **age** displays a strong positive correlation with **stroke**. **avg_glucose_level** & **ever_married** display some kind of positive correlation. Opposite to positive correlation, **gender**, **Residence_type** & **work_type** have negative correlation with the **stroke**.\n- In the case of **Data Leakage**, none of the features display an extreme positive or negative correlation with **stroke**.\n- **age**, **heart_disease**, **avg_glucose_level**, **hypertension** & **ever_married** display some kind of positive correlation. Overall, all the features have a value very close to 0, displaying neutral correlation with **stroke**."
#### Mutual Information Test :
"- Mutual Information Score of **stroke** with categorical features display very low scores irrespective of **Data Leakage** or **No Data Leakage**.\n- According to the above scores, none of the features should be selected for modeling."
#### Chi Squared Test :
"- For **No Data Leakage**, we should reject the features that have low values. We will reject features with scores less than 20. Hence, we will not use : **smoking_status**, **heart_disease** & **hypertension**. This does contradict with the **Domain Information**.\n- For **Data Leakage**, **heart disease** & **hypertension** need to be selected for modeling and reject the other features due to low Chi Squared Score. "
#### ANOVA Test :
"- From the above ANOVA Scores, we ignore the features with values less than 20. Hence, we reject **bmi** for modeling irrespective of **Data Leakage** or **No Data Leakage**.\n- We ready the datasets for data scaling by dropping the features based on the above statistical tests.\n- We will ignore the **Domain Information**!"
- Selecting the features from the above conducted tests and splitting the data into **85 - 15 train - test** groups.
### 1] XGBoostClassifier :
# Load packages
## Parameters
The classes are equaly distributed in the train set (10% each). Let's check the same for the test set.    \nLet's also plot the class distribution.\n\n
### Test set images class distribution
Let's now plot the images.   \nThe labels are shown above each image.
### Test set images\n\nLet's plot now a selection of the test set images.  \nLabels are as well added (they are known).  
Let's check the class inbalance for the rsulted training set.
"And, as well, for the validation set."
"## Validation accuracy and loss\n\nLet's plot the train and validation accuracy and loss, from the train history."
The validation accuracy does not improve after few epochs and the validation loss is increasing after few epochs. This confirms our assumption that the model is overfitted. We will try to improve the model by adding Dropout layers.
# Visualize classified images\n\n## Correctly classified images\n\n\nWe visualize few images correctly classified.
## Incorrectly classified images\n\nLet's see also few images incorrectly classified.
"# 4. Choosing a better plot type\nI was not satisfied with a bar plot for this visualization. I wanted that could display the same information and allow comparing different countries. Adding multiple bar charts for multiple countries would not look good at all. Then I tryed a line chart, since adding multiple lines works better than multiple columns.\n"
"Much better... now lets try to add countries, instead of the aggregated of all of them. "
"Much better... now lets try to add countries, instead of the aggregated of all of them. "
Looking better than the bars... But still not good. My next idea was to transform the x axis into a radial.
Looking better than the bars... But still not good. My next idea was to transform the x axis into a radial.
This looks interesting! I think I can go forward with this chart type.
"# 5. Changing the data to be plotted from absolute to relative\nSome countries have much more absolute rows than others. This creates a distortion in the chart, so I will change the metric and plot the relative percentage for each job title in each country instead. This should allow us to compare them! "
Indeed! Now all countries are at the same scale. Comparing them proportionaly is much better than comparing absolute values.
"# 6. Closing the lines\nThis is a little bit trickier... To ""close"" the radar chart on Plotly, you need to add the first data point again to the end.[[](http://)](http://)"
"# 7. Creating a hierarchy in the axis titles\nIf you look at the axis they don't currently have an hierarchy or any specific order. I wan't to create one by keeping close togheter job titles that are similar, from the least technichal to the most technical. For me it makes sense to organize it in the following order:\n\n> Business Analyst **"
"# 8. Declutering \nThis is the most important part of any good data visualization. Remove any noise that is not directly contributing to how people are reading and interpreting data.\n\n## 8.1 Turning background to white\nNow that the background is white, we also need to change some other elements, such as grids, ticks and lines. Here my choices were:\n* Turn all text and chart elements to a light grey\n* Hide any chart grid and axis lines that are not essential. I only kept radial axis grid (the circles), because they are important for readers to understand the magnitude of the values displayed."
"It looks much better now, doesn't it?\n\n## 8.2 Smoothing all lines, and making they grey and thinner\nNow we will transform all lines to grey, reduce their width and also make them less sharp by applying some smoothing. "
"It looks much better now, doesn't it?\n\n## 8.2 Smoothing all lines, and making they grey and thinner\nNow we will transform all lines to grey, reduce their width and also make them less sharp by applying some smoothing. "
"This is SO MUCH BETTER! Because now we can actually ""see"" each line! But as they say, the devil is in the details. Let's keep improving it!\n\n## 8.3 Removing legends, adjusting grid and range\n* Now I thought that there where too much lines for grids (0, 10, 20, ..., 50) cluttering the chart. I don't like it, instead I will create only three lines (25% and 50%). \n* I also don't like that many values for legend, they dont help with anything other than making our visualization worse. Let's hide it!\n* There is one line that is almost over **Data Scientist** axis title, I will increase the range a little bit to avoid that."
"This is SO MUCH BETTER! Because now we can actually ""see"" each line! But as they say, the devil is in the details. Let's keep improving it!\n\n## 8.3 Removing legends, adjusting grid and range\n* Now I thought that there where too much lines for grids (0, 10, 20, ..., 50) cluttering the chart. I don't like it, instead I will create only three lines (25% and 50%). \n* I also don't like that many values for legend, they dont help with anything other than making our visualization worse. Let's hide it!\n* There is one line that is almost over **Data Scientist** axis title, I will increase the range a little bit to avoid that."
"# 9. Adjusting hover information\nWhen we hover the mouse over the datapoints we are seeing **r** and **theta** values. This is not informative for anyone that is viewing the chart. Let's adjust a few things there.\n\n* Add the country name (so it's easy to relate the line to the country without needing to have multiple colors or legends)\n* Add the data point (so it's easy to relate the value of each data point to each line)\n* Format the data point as percentage without decimal points (no need to have any decimal points, because the analysis is more qualitative) \n"
"# 9. Adjusting hover information\nWhen we hover the mouse over the datapoints we are seeing **r** and **theta** values. This is not informative for anyone that is viewing the chart. Let's adjust a few things there.\n\n* Add the country name (so it's easy to relate the line to the country without needing to have multiple colors or legends)\n* Add the data point (so it's easy to relate the value of each data point to each line)\n* Format the data point as percentage without decimal points (no need to have any decimal points, because the analysis is more qualitative) \n"
"# 10. Telling a story\nNow we will highlight some specific countries that we want to tell a story about. For this chart I'm choosing United States vs China. What we will do:\n\n* Add color to highlighted countries lines\n    * Choose color from flags colors (red for China, blue for United States)\n* Increase width of highlighted countries lines\n* Increase opacity of highlighted countries lines\n* Show legend only for highlighted countries"
"# 10. Telling a story\nNow we will highlight some specific countries that we want to tell a story about. For this chart I'm choosing United States vs China. What we will do:\n\n* Add color to highlighted countries lines\n    * Choose color from flags colors (red for China, blue for United States)\n* Increase width of highlighted countries lines\n* Increase opacity of highlighted countries lines\n* Show legend only for highlighted countries"
Now we are telling some story with this data! We are able to see the differences between China and United States! We can easily draw some conclusions from this chart!\n\n# 11. Adding a meaningful title\nNow we will add a meaninful title and the data source to the chart.\n\n* Bigger font size for title\n* Smaller font size for source\n* Both grey to not draw much attention
Now we are telling some story with this data! We are able to see the differences between China and United States! We can easily draw some conclusions from this chart!\n\n# 11. Adding a meaningful title\nNow we will add a meaninful title and the data source to the chart.\n\n* Bigger font size for title\n* Smaller font size for source\n* Both grey to not draw much attention
"# 12. Bringing all elements more close together\nUsing the standard autosize of Plotly is all elements too much separated from each other, let's bring them closer by defining the height and width size of the chart.\n\nAlso some minor adjustments to legend as well:\n* Changing color to grey\n* Change behaviour when click and doubleclick"
"# 12. Bringing all elements more close together\nUsing the standard autosize of Plotly is all elements too much separated from each other, let's bring them closer by defining the height and width size of the chart.\n\nAlso some minor adjustments to legend as well:\n* Changing color to grey\n* Change behaviour when click and doubleclick"
"# 13. More tips on how to do a great data storytelling\n\n## 13.1 Keep all your charts similar\nWhen showing multiple charts, stick to a few different chart types. The purpose of this is to make your readers lifes easier, as they will not need to learn how to read every new chart. After learning how to interpret one chart, reading all the other charts will be automatic.\n\n## 13.2 Stick to a few colors in every chart\nIf your highlighted data colors don't hold any meaning (unlike this example, where they encode the color of country flag) then you shouldn't use a lot of different colors. Keep them to a minimum.\n\n## 13.4 Tell just one story per chart\nIt might be tempting to highlight multiple things in a single chart. Instead create multiple charts to highlith different things, one single story per chart. "
"### Libraries\n\nMake a compact section where you put all the libraries. I will leave them unhidden in this notebook, but usually it's best to hide all the cells with a lot of code within an Analysis Competition, so they don't take away from the *story* that you want to tell."
"### ðŸ“Œ Color Scheme\n\nMantain a color scheme throughout the whole notebook. Some of the apps I am using are:\n* [HTML color codes](https://htmlcolorcodes.com/)\n* [Coolors](https://coolors.co/)\n* [ColorHunt](https://colorhunt.co/)\n* You can use [Image Color Picker](https://imagecolorpicker.com/en) to pick colors from an image\n* Google around, it sometimes helps for inspiration.\n\nFor this notebook I have chosen a rainbow color scheme, as gems have multiple colors, so a rainbow color scheme seemed fitted. You can also go ahead and use tones of purple and blues (like Sapphires), greend (Emeralds) or warmer tones like orange and red (like Amber or Ruby).\n\n\n\n### ðŸ“Œ Add COLOR to highligh\n\nWhen writing out your reseach, use color to emphasise something important. You can use different tones to emphasise what you mean (for example, red might be a disclaimer, or something bad, while green might represent something positive).\n\n\nHere is what you need to copy-paste in your markdown:\n\n```\nsearch for the treasure\n```\n\n* background - the highlight color\n* font weight - either normal, bold, italic etc.\n* color - the color of the font itself\n\n### ðŸ“Œ CSS Alerts\n\nMake use of alerts in order to conclude an important point or to raise attention from the reader onto certain aspects of the analysis.\n\nIn this notebook I am using a customized alert type that I have created a year ago in [my Treasure Hunt](https://www.kaggle.com/code/andradaolteanu/treasure-hunt-what-gives-to-be-really-good). These alerts have different colors and a shadow cast onto them. \n\n> You can go within the `alerts.css` file, download it, change to `.txt` format and customize the colors and shade however you want. When you're done, change it back to .css and .... you have **your own customized alerts to play with**!"
### I. Simple Showcase\n\nBelow we create a barplot to *show all kagglers that were mentioned more than once* within Martin's Hidden Gems series.\n\n**ðŸ’¡ Steps for easy plotting**:\n* we define the figure size and ax\n* we create the barplot using `seaborn`\n* we add a simple title for context
"### II. Adding a Twist ðŸ¸\n\n**ðŸ’¡ Additionat steps for an elevated plot**:\n* add a custom palette\n* show the title a bit of attention and love by changing the `size` and `weight`\n* show values on each bar instead of using the x axis (using `show_values_on_bars()` custom function)\n* add a *vertical line* to sepparate the most mentioned kaggler than the rest\n* add an *image* of the most favorite kaggler profile and an *arrow* to give more detail and context\n* use `sns.despine()` to remove the borders of the plot, so it appears more clean and easy to read"
"### II. Adding a Twist ðŸ¸\n\n**ðŸ’¡ Additionat steps for an elevated plot**:\n* add a custom palette\n* show the title a bit of attention and love by changing the `size` and `weight`\n* show values on each bar instead of using the x axis (using `show_values_on_bars()` custom function)\n* add a *vertical line* to sepparate the most mentioned kaggler than the rest\n* add an *image* of the most favorite kaggler profile and an *arrow* to give more detail and context\n* use `sns.despine()` to remove the borders of the plot, so it appears more clean and easy to read"
### ðŸ“Œ ðŸ W&B Plot Logging\n\nBelow is a simple example of how to log the `barplot` to you own W&B Dashboard. \n\n
"### ðŸ“Œ Other Plotting options\n\nSeaborn and Matplotlib are a great combo and they can both do a great job in creating a beautiful analysis. If you feel more adventurous, you can also try:\n* **Plotly**: Check out the work of [Schubert de Abreu](https://www.kaggle.com/spitfire2nd) and his use of plotly - https://www.kaggle.com/code/spitfire2nd/enthusiast-to-data-professional-what-changes\n* **D3.js**: If you are not familiar with Javascript, this could be a slow learning curve, but once you get the hang of it you can create almost anything! Check out [my notebook that uses D3 only](https://www.kaggle.com/code/andradaolteanu/how-are-the-ladies-and-the-gents-doing/notebook). I have also made a [discussion post](https://www.kaggle.com/discussions/general/293879) with some points to consider before starting learning D3.\n\n### Additional Questions to be asked\n* Does Martin influence how famous you are?\n* Does Martin make you a Grandmaster?\n* Do the notebooks have more versions than the average notebook on Kaggle?\n\n# 3. Let's help Martin with his reviews!\n\nLet's create a Text Generator using **PyTorch** that helps Martin create reviews based on his old ones!\n\n### ðŸ“Œ Create a Shaped Wordcloud\n\n* Get the text from `reviews` and create an single string out of it\n* Create the `WordCloud` object\n* Add the size of the figure and a title"
### ðŸ“Œ Wordcloud with a Twist ðŸ¸\n\n* Make a custom shape of the cloud: In order for the wordcloud to take the shape of the image you should input a `.jpg` image with **white background** (NOT black and NOT transparent - because the function will interpret the transparent background as black).\n* More custom fonts like I used below can be found here: https://www.dafont.com/\n* Create a customized palette using `similar_color_func()`\n* Give the title some love\n* Increase the size of the plot and remove axis
### ðŸ“Œ Wordcloud with a Twist ðŸ¸\n\n* Make a custom shape of the cloud: In order for the wordcloud to take the shape of the image you should input a `.jpg` image with **white background** (NOT black and NOT transparent - because the function will interpret the transparent background as black).\n* More custom fonts like I used below can be found here: https://www.dafont.com/\n* Create a customized palette using `similar_color_func()`\n* Give the title some love\n* Increase the size of the plot and remove axis
"\n  ðŸ’¡ Note: The following code is heavily inspired by this notebook: Beginners Guide to Text Generation(Pytorch)\n\n\n## I. Text Cleaning\n\n### ðŸ“Œ How to make Schemas\n\nI use [Google Slides](https://www.google.com/slides/about/) for all my schemas, no matter if they are just an explanation of the data, a view of the machine learning pipeline or a deep dive into a new concept.\n\nI LOVE schemas: they are such a easy way to express something that might be hard to understand by using just words. I am also **lazy sometimes** and I don't want to read, so by looking at pictures I get the overall information ^^. Is this a good habbit? Of course not, but I do it anyways ðŸ˜…\n\n> A schema that shows the cleaning process of the text (before applying the model).\n\n\n\n### ðŸ“Œ Comment your code\n\nðŸ’¡ You know what you're doing, **but maybe others don't**! Try your best to *comment your code and explain everything that you're doing*, *printing* your results occasonaly, so the readers can stay on track."
> Let's take a look at the Average Weekly Sales per Year and find out if there are another holiday peak sales that were not considered by 'IsHoliday' field.
"> As we can see, there is one important Holiday not included in 'IsHoliday'. It's the Easter Day. It is always in a Sunday, but can fall on different weeks. \n    >* In 2010 is in Week 13\n    >* In 2011, Week 16\n    >* Week 14 in 2012\n    >* and, finally, Week 13 in 2013 for Test set\n\n> So, we can change to 'True' these Weeks in each Year."
"> The same chart, but showing also the median of the Sales and not divided by Year:"
"> Just as an observation, the mean and the median are very different, suggesting that some stores/departments might sell much more than others."
### Average Sales per Store and Department
"> Yeah, there are Sales difference between the Stores."
"> Yeah, there are Sales difference between the Stores."
"> And there are Sales difference between the Departments too. Also some Depts are not in the list, like number '15', for example."
"> Correlation Metrics:\n    >* 0: no correlation at all\n    >* 0-0.3: weak correlation \n    >* 0.3-0.7: moderate correlaton\n    >* 0.7-1: strong correlation\n\n> Positive Correlation indicates that when one variable increase, the other also does. Negative is the opposite."
"> 'MarkDown' 1 to 5 are not strong correlated to 'Weekly_Sales' and they have a lot of null values, then we can drop them.\n\n> Also, 'Fuel_Price' is strong correlated to 'Year'. One of them must be dropped else they would carry similar information to the model. 'Year' will not be dropped, because it differentiate same Weeks for 'Store'+'Dept'.\n\n> Other variables that have weak correlation with 'Weekly_Sales' can be analyzed to see if they are useful."
### Pclass - Survived
### Embarked - Survived
### Embarked - Survived
### SibSp - Survived
### SibSp - Survived
### Parch - Survived
### Parch - Survived
### Age - Survived
### Age - Survived
### Fare - Survived
### Fare - Survived
\n## 2-Correlation Between Features
\n## 2-Correlation Between Features
"**Outcome**    \n\n* Sex, Pclass, Fare and Embarked are associated with Survived. \n\n\n"
### Correlation Matrix
"\n##Â 2- Drop Features\n* Ticket, Cabin, Name, PassengerId and Age are deleted according to the result of the correlation matrix."
Let's Explore the target feature i.e SalesPrice.
**Red line in histogram indicates the mean of the SalePrice and the Green line indicates the median**
"From the above plots, we can observe that the Sales Price is not normally distributed. From the boxplot we can observe that the dataset have a number of outliers.\n\nFor the probability plot, the red line represents those points which would have been plotted for y-axis points **if those were normally distributed**. But the blue points represents the actual scenario. We can see that there is a lot of deviation on the both the ends i.e on the top right and bottom left."
"Skewness refers to the amount of asymmetry in the given feature or in other words amount of distortions from the normal distribution. \n\nHere we can observe that the value of skewness is quite high which means that there is a large amount of asymmetry. \n\nThe peak of the histogram represents the mode i.e the price for which maximum number of houses were sold.\n\nWhat kind of skewness is present in the given case?\n\nAs the mean of the feature is greater than the median which is greater than the mode and the line is flat towards the right in the histogram, the given feature is **Positively Skewed**. Most of the houses were sold less than the average price.\n"
There is an amazing library called missingno which helps us to visualize the number of Null values present in each feature.
Just by going through the plot we can see that there are a lot of NULL values
#### GrLivArea
We can observe that there is almost **a linear relationship between Living Area and the Sale Price**. If the area is huge then price should also have also been high but the two rightmost points suggest something else. **These two points are outliers**. We will drop both of them below.
#### Garage Area
#### Total Basement Area
#### Total Basement Area
#### 1st Floor Area
#### 1st Floor Area
"Before we move forward we need to understand the assumptions of linear regression:\n* Linearity\n* Homoscedasticity\n* No or little Multicollinearity\n* Independence of Error\n\nSince we fit a linear model, we assume that the relationship is linear, and the errors, or residuals, are pure random fluctuations around the true line. We expect that the variability in the dependent variable doesn't increase as the value of the independent increases, which is the assumptions of equal variance, also known as Homoscedasticity. We also assume that the observations are independent of one another(No Multicollinearity), and a correlation between sequential observations or auto-correlation is not there.\n\nNow, these assumptions are prone to happen altogether. In other words, if we see one of these assumptions in the dataset, it's more likely that we may come across with others mentioned above. Therefore, we can find and fix various assumptions with a few unique techniques.\n\nIn order to discover the linearity let's plot scatter plots for GrLivArea and MasVnrArea"
"Before we move forward we need to understand the assumptions of linear regression:\n* Linearity\n* Homoscedasticity\n* No or little Multicollinearity\n* Independence of Error\n\nSince we fit a linear model, we assume that the relationship is linear, and the errors, or residuals, are pure random fluctuations around the true line. We expect that the variability in the dependent variable doesn't increase as the value of the independent increases, which is the assumptions of equal variance, also known as Homoscedasticity. We also assume that the observations are independent of one another(No Multicollinearity), and a correlation between sequential observations or auto-correlation is not there.\n\nNow, these assumptions are prone to happen altogether. In other words, if we see one of these assumptions in the dataset, it's more likely that we may come across with others mentioned above. Therefore, we can find and fix various assumptions with a few unique techniques.\n\nIn order to discover the linearity let's plot scatter plots for GrLivArea and MasVnrArea"
We can observe that the relationship between Sales Price and GrLivArea is much more linear than the relationship between Sales Price and MasVnrArea.\n\nLet's look at the residual plot for independent variable GrLivArea and our target variable SalePrice. \n\nA residual value is a measure of how much a regression line vertically misses a data point. Regression lines are the best fit of a set of data. You can think of the lines as averages; a few data points will fit the line and others will miss. A residual plot has the Residual Values on the vertical axis; the horizontal axis displays the independent variable.
We can observe that the relationship between Sales Price and GrLivArea is much more linear than the relationship between Sales Price and MasVnrArea.\n\nLet's look at the residual plot for independent variable GrLivArea and our target variable SalePrice. \n\nA residual value is a measure of how much a regression line vertically misses a data point. Regression lines are the best fit of a set of data. You can think of the lines as averages; a few data points will fit the line and others will miss. A residual plot has the Residual Values on the vertical axis; the horizontal axis displays the independent variable.
"Ideally, if the assumptions are met, the residuals will be randomly scattered around the centerline of zero with no apparent pattern. The residual will look like an unstructured cloud of points centered around zero. However, our residual plot is anything but an unstructured cloud of points. Even though it seems like there is a linear relationship between the response variable and predictor variable, the residual plot looks more like a funnel. \n\nThe error plot shows that as GrLivArea value increases, the variance also increases, which is the characteristics known as Heteroscedasticity. The linear regression analysis requires the dependent variable to be multivariate normally distributed. A histogram, box plot, or a Q-Q-Plot can check if the target variable is normally distributed. \n\nLet's plot the three graphs again for the target feature."
We can observe that the SalesPrice feature now follows a normal distibution as the Histogram resembles bell-shape and the QQ-plot also overlaps with the red line. Let's again plot the scatter plot for GrLivArea and SalePrice to see whether Heteroscedasticity was removed.
"We can see that the scatterplot on the left had heteroscedasticity(funnel like shape) but after applying log transformation to the feature, it was removed and now it's Homoscedastic."
"Linear Regression indicates significant relationships between the dependent variable and the independent variable. It assumes that there is a linear relationship between the independent variables and the dependent variable but this is not always the case in real life. There are very less scenarios where this assumption holds true. Multiple Regression also suffers from multi-collinearity, auto-correlation, and heteroskedasticity.\n\nThe interpretation of the linear coefficient is that it represents the mean change in the dependent variable for 1 unit change in the independent variable when all the other independent variables are held constant.\n\nNow suppose that there are two correlated independent variables (A and B) and we are aiming to find the right coefficient for these independent variables. When the coefficient for A is calculated, will that coefficient be accurate? No, it wonâ€™t be right because changes in A are associated with shifts in B, and as we have already discussed that all the other variables need to be held constant, B canâ€™t be held as a constant because A and B are correlated. This is known as Multicollinearity and it is one of the disadvantages of Linear Regression.\n\nIn order to solve this problem, we use various regularization techniques (l1, l2 etc) or other kind of regression techniuques like Elastic Net Regression, Lasso and Ridge Regression which automatically takes care of multicollinearity.\n\nCheck this [blog](https://medium.com/gdg-vit/overcoming-the-drawbacks-of-linear-regression-497fffcdd2d8) for better understanding"
"As the quality increases, price of the houses also increase"
**Heatmap**. The missingno correlation heatmap measures nullity correlation: how strongly the presence or absence of one variable affects the presence of another:
"**Dendrogram**.The dendrogram allows you to more fully correlate variable completion, revealing trends deeper than the pairwise ones visible in the correlation heatmap:"
"**Dendrogram**.The dendrogram allows you to more fully correlate variable completion, revealing trends deeper than the pairwise ones visible in the correlation heatmap:"
\n# Column Types\n[Next](#t2_5)\n\nLet's look at the number of columns of each data type. int64 and float64 are numeric variables [(which can be either discrete or continuous)](https://stats.stackexchange.com/questions/206/what-is-the-difference-between-discrete-data-and-continuous-data). object columns contain strings and are [categorical features](https://stats.stackexchange.com/questions/206/what-is-the-difference-between-discrete-data-and-continuous-data). 
Correlations clustermap\n
\n# Visualizations\n\n[Back to Contents](#top)\n\n[Next](#t4)\n
Patient age quantile by sars cov2 exam result
"% of SARS COV2 exam result (0=Negative, 1=Positive)"
"% of SARS COV2 exam result (0=Negative, 1=Positive)"
"\n# Encoding Variables\n\n[Back to Contents](#top)\n\n[Next](#t5)\n\nBefore we go any further, we need to deal with pesky categorical variables. A machine learning model unfortunately cannot deal with categorical variables (except for some models such as [LightGBM](Before we go any further, we need to deal with pesky categorical variables. A machine learning model unfortunately cannot deal with categorical variables (except for some models such as LightGBM. Therefore, we have to find a way to encode (represent) these variables as numbers before handing them off to the model. There are two main ways to carry out this process). Therefore, we have to find a way to encode (represent) these variables as numbers before handing them off to the model. There are two main ways to carry out this process.\nYou can see this kaggle course: [Intermediate Machine Learning Home Page](https://www.kaggle.com/alexisbcook/categorical-variables)"
\n# Confusion Matrix function\n\n[Back to Contents](#top)\n\n[Next](#t6_2)\n
\n# Gradient Boosting Model function\n\n[Back to Contents](#top)\n\n[Next](#t6_3)\n
\n# Gradient Boosting Model function\n\n[Back to Contents](#top)\n\n[Next](#t6_3)\n
\n# XGBoost\n\n[Back to Contents](#top)\n\n[Next](#t6_4)\n
"- We can note the twin peak is associated with the lower `Reynolds` number, which is much higher `spl` than the lower `Reynolds` peak. \n- Lower `Reynolds` number cases are also associated with higher `spl` noise, in general.  \n- It's not quite straightforward to differentiate the two possible sources, low & high `Reynolds` numbers are also quite subjective. \n- What we can note that the higher `Reynolds` have two distrinctive peaks, which tend to tranform into a levelled out `plateau` at higher `freq` as the `Reynolds` number is reduced. \n- If there is a clear location a model might stumble on it would be the twin peak at `u_inf=71.3` which is quite probably associated with the low `Reynolds` associated `LBL-VS` noise.\n\n\n    3.5 | HIGHER FLOW SEPARATION CASES\n\n\n- Whilst `aoa=0` can still have flow separation, most notably at very high `u_inf`, the flow separation is very minor \n- Larger flow separation is associated with higher aoa values.\n- Let's group all the higher aoa case we have & see if we can note anything associated to `spl` as `aoa` is increased."
"- An increase in `aoa` tends to move the spectrum diagonally towards lower `freq` and `spl` simultaneously\n- The higher the `aoa` becomes, the more notable the low `freq` peaks become, quite often generating a higher local `spl` maximum at a very narrow `freq` range.\n- Medium sized `l_chord` & `aoa` tend to be associated with more freqent localised `spl` variation in the mid frequency range."
"- Looks like adding the `Reynolds` feature improved the misspredictions, especially the lower `l_chord` cases, looks quite neatly aligned now.\n\n\n    5.2 | HIGH FLOW SEPARATION MODEL\n\n\nTaking a look at the relation model prediction vs `spl` again:"
"- High flow separation cases are predicted much better than the lower flow separation case when using the same hyperparameter optimisation approach. \n- Some minor variation for higher `l_chord` cases can be noted. Let's see if the addition of `Reynolds` number can help increase the accuracy of the higher `l_chord` cases.\n- `Reynolds` feature also has a positive effect on model accuracy for the higher `aoa` model.\n\n\n    5.3 | GENERAL MODEL (BOTH CASES)\n\n\nThe Overall model should have similar mispredictions to both previous models, let's take a look:"
"- High flow separation cases are predicted much better than the lower flow separation case when using the same hyperparameter optimisation approach. \n- Some minor variation for higher `l_chord` cases can be noted. Let's see if the addition of `Reynolds` number can help increase the accuracy of the higher `l_chord` cases.\n- `Reynolds` feature also has a positive effect on model accuracy for the higher `aoa` model.\n\n\n    5.3 | GENERAL MODEL (BOTH CASES)\n\n\nThe Overall model should have similar mispredictions to both previous models, let's take a look:"
"- Looks like we have very similar misspredictions for the overall model (both lower and higher `l_chord` cases.\n- After add the additional feature, by the looks of it, we have made quite an accurate model for the prediction of `spl`.\n- The Gaussian based model knows can be a little too accurate for many problems, in that case you can simply adjust the hyperparameters, especially the matrix diagonal term. In this problem, we assumed that the noise hyperparameter, `sigma_n` is equal to 0.01 here.\n\n# 6 | CONCLUSIVE REMARKS\n\n- In this problem, we aimed to create a model that would predict the target variable `spl`, given a set of features. \n- A brief EDA was conducted, some interesting relations were noted. Each individual spectrum has its own unique relation to `spl`, it's not exactly possible to pinpoint exact spectrum noise sources, given we don't have anything visual to go by. \n- More notably was the `Reynolds` number relation that was noted in the experiment noise sources. This feature actually helped improve the model which was quite nice, \n- In the end, our customly created Gaussian Process Regression model was able to quite precisely predict the `spl`, with the addition of the new feature."
"## 5.1. Checking some patients\nPyMC3 comes with a very powerful visualization tool called [ArviZ](https://arviz-devs.github.io/arviz/index.html). However, I didn't figure out how to use yet... Let's use Matplotlib and Seaborn."
"Here I plotted 100 out of the 4000 personalized models each patient has! In green we can see the fitted regression line, in yellow the standard deviation. Let's ensemble all that!"
#### - Insights\n- It is quite a big dataframe (the train.csv file is more than 2GB large). It has more than 13 millions rows and a few features.\n- The test dataframe has an extra columns called `session_level` which will be used to create the submission file.\n\nLet's see if all features are relevant by analysing them.\n## ðŸ’¡ Missing values
"### - Insights\n- There are a lot of missing values in some columns.\n- We will drop the `fullscreen`, `hq` and `music` features at least, and maybe the ones with more than 80% of missing values. To make this decision about keeping or not the columns with high missing ratio, we will have to evaluate the relevance of them.\n- The train and test dataframes have similar ratios of missing values which are good news.\n\n## ðŸ’¡ Analysis of the features\nLet's analyse all the features in order to understand what they mean.\n### 'Session id' and 'index'\n> The ID of the session the event took place in\n\n> The index of the event for the session"
"### - Insights\n- There are a lot of missing values in some columns.\n- We will drop the `fullscreen`, `hq` and `music` features at least, and maybe the ones with more than 80% of missing values. To make this decision about keeping or not the columns with high missing ratio, we will have to evaluate the relevance of them.\n- The train and test dataframes have similar ratios of missing values which are good news.\n\n## ðŸ’¡ Analysis of the features\nLet's analyse all the features in order to understand what they mean.\n### 'Session id' and 'index'\n> The ID of the session the event took place in\n\n> The index of the event for the session"
"#### - Insights\n- **Train set:**\n - A session typically encompasses anywhere between several hundred to a few thousand individual events.\n - Both the median and the mean are slightly over 1000.\n - The distribution appears to be normal with a slight positive skew.\n - Above the 90th percentile, the values for the number of events are very large. I set a x limit for visualization but the number of events go very far. Should we consider them as outliers?\n- **Test set:**\n - Only 3 sessions are represented in the test set having a fair amount of events.\n - The session with 1501 events is in the tail of the gaussian distribution but still in a reasonable area.\n - Do not forget that the available test set only represents the half of the final one.\n- **Events vs Level:**\n - Currently, we have only analyzed the number of events that occur during the sessions.\n - Keep in mind that a session is divided into question levels, and we aim to predict their success. However, the number of events can vary for each question level. It's possible that the number of actions (clicks) during a question may be related to a correct or incorrect answer. It would be interesting to inspect that later.\n \n### 'Elapsed time' and 'index'\n> How much time has passed (in milliseconds) between the start of the session and when the event was recorded.\n\nThe elapsed time should be correlated with the event index. As more time is spent, it is expected that a higher number of events will occur. This is why I have decided to calculate the mean for each event index and plot it."
"#### - Insights\n- **Train set:**\n - A session typically encompasses anywhere between several hundred to a few thousand individual events.\n - Both the median and the mean are slightly over 1000.\n - The distribution appears to be normal with a slight positive skew.\n - Above the 90th percentile, the values for the number of events are very large. I set a x limit for visualization but the number of events go very far. Should we consider them as outliers?\n- **Test set:**\n - Only 3 sessions are represented in the test set having a fair amount of events.\n - The session with 1501 events is in the tail of the gaussian distribution but still in a reasonable area.\n - Do not forget that the available test set only represents the half of the final one.\n- **Events vs Level:**\n - Currently, we have only analyzed the number of events that occur during the sessions.\n - Keep in mind that a session is divided into question levels, and we aim to predict their success. However, the number of events can vary for each question level. It's possible that the number of actions (clicks) during a question may be related to a correct or incorrect answer. It would be interesting to inspect that later.\n \n### 'Elapsed time' and 'index'\n> How much time has passed (in milliseconds) between the start of the session and when the event was recorded.\n\nThe elapsed time should be correlated with the event index. As more time is spent, it is expected that a higher number of events will occur. This is why I have decided to calculate the mean for each event index and plot it."
"#### - Insights\n- As expected, the average elapsed time increases with the increase in event index, but this trend only holds until around index 2800. After that, the behavior becomes erratic with a peak and then drops and remains constant until much later.\n- This phenomenon can be explained by the fact that as the number of events increases, there are fewer and fewer examples, and the average starts to represent unique cases which are likely outliers. These outliers may include individuals who did ""spam clicks"" to achieve a high number of events in a shorter amount of time or, in the case of the peak, individuals who were inactive for an extended period.\n\nLet's now check the elapsed times for individual cases and see if this trend remains the same."
"#### - Insights\n- Almost all of the examples shown exhibit gaps, indicating brief pauses in activity.\n- Some examples, however, exhibit huge gaps due to extended periods of inactivity.\n- The maximum elapsed time in the test set appears to be more reasonable than the outliers observed in the training set. It may be beneficial to get rid of sessions in which the individual was inactive for an extended period of time.\n\n### 'Event name' and 'name'\n> The name of the event type.\n\n> The event name (e.g. identifies whether a notebook_click is is opening or closing the notebook)\n\nThese 2 columns are quite similar since they describe the action of the user. Let's display there distributions independently and in a pivot table. The data are from the train set."
"#### - Insights\n- The events mainly consist of clicks and occasionally hovers. I am uncertain about the distinction between the various types of clicks.\n- Certain ""names"" only occur with specific ""event_names"". Not all combinations are possible.\n- I have also checked the distribution in the test set, and it is comparable to the distribution in the training set.\n\n### 'Index' and 'level'\n> What level of the game the event occurred in (0 to 22)"
"#### - Insights\n- A greater number of events take place during specific periods. For instance, there is a heightened level of event activity observed at level 18.\n- This pattern can also be observed in various individual cases as examples. However, each one remains unique.\n\n### 'Page'\n>  The page number of the event (only for notebook-related events)"
#### - Insights\n- A lot of values are still missing in this columns (almost 98%) because a page is only indicated when the event is notebook-related.\n- I don't know if this information is relevant or not. Is it worth it to keep it and how should we fill in the missing values? With zeros?\n- The test set has a slightly different distribution from the train set for the page number.\n\n### 'Hover duration'\n> How long (in milliseconds) the hover happened for (only for hover events)
#### - Insights\n- A lot of values are still missing in this columns (almost 98%) because a page is only indicated when the event is notebook-related.\n- I don't know if this information is relevant or not. Is it worth it to keep it and how should we fill in the missing values? With zeros?\n- The test set has a slightly different distribution from the train set for the page number.\n\n### 'Hover duration'\n> How long (in milliseconds) the hover happened for (only for hover events)
"#### - Insights\n- Similar to the page feature, the hover duration is indicated only for a few rows during an hover event. Typically, the duration lasts anywhere from a few milliseconds to a few seconds, with rare instances lasting more than 4 seconds.\n- In the training set, we can observe that the mean is significantly higher than the median, which is due to the presence of outliers where the user remained on hover for an extended period. This type of examples should be avoided.\n\n### 'Text'\n> The text the player sees during this event"
"How we propose a forecasting problem using other sciences such as classical statistics, probability theory and mathematical economics?  \nIn statistics, we are limited to methods such as descriptive statistics, histograms, means, variance which one cant give us appropriate results and inference.   \n_fact: Usual the most data science courses end education after statistics._"
"[back to top](#table-of-contents)\n\n# **3. ðŸ’¹ Introduction to econometrics** \nLike machine learning, the difficulty of econometric modeling lies in the fact that most processes cannot be accurately estimated. We often have to choose one or the other. So let's stick to one golden rule:\n> Â«Models should be as simple as possible, but not simplerÂ» F. Engels  \n\nI think these are the words of Einstein...\n\n**It is possible to single out the main classes of econometric models:**\n1. Regression Models\n2. Systems of simultaneous equations or regression models\n3. Single Equation Time Series Models\n4. Time series models with multiple equations. Vector autoregression.\n\n**Basic data types in econometric models:**\n1. Time Series\n2. Spatial or cross-sectional data\n3. Panel data"
"General view of this regression model:\n$y = w_{0} + w_{1} \cdot x_{1} + \varepsilon$\n* $y - \text{response variable}$\n* $w_{0} - \text{constant}$\n* $w_{1} - \text{slope and regression coefficient of } x_1 $\n* $x_1 - \text{independent variable, regressor}$\n* $\varepsilon - \text{residual}$\n\nI gonna show you how work this coefficients\n\nLeets se whats going on while we changing $w_0$:\n$y = w_{0} + w_{1} \cdot x_{1}$  \nif $w_{1} = 0$, $y = w_{0} + 0 \cdot x_{1} = w_{0}$  \nif $y \rightarrow n$ then $ w_{0} \rightarrow n $  \nAs we can see $w_{0}$ is responsible for level of response variable"
Leets see whats going on while we changing $w_1$:\n\n$y = w_0 + w_1 \cdot x_{1}$  \nif $w_0 = 0$ then $y= w_1 \cdot x_{1} \Rightarrow \frac{y}{x_1} = w_1 $\n\nAnd we can see that  $a_1$ is slope of regressor and response variable
Leets see whats going on while we changing $w_1$:\n\n$y = w_0 + w_1 \cdot x_{1}$  \nif $w_0 = 0$ then $y= w_1 \cdot x_{1} \Rightarrow \frac{y}{x_1} = w_1 $\n\nAnd we can see that  $a_1$ is slope of regressor and response variable
"[back to top](#table-of-contents)\n\nâ€‹\n# **3.2. ðŸ§¨ Ordinary Least Squares**\nThe most popular method optimization linear regression - OLS. First of all it should be said that optimization regression split of two methods:  __Precise Analytical Method__ and __Approximate Numerical Method__. And in this topic we will talk more about first type. But now let's figure it out what is OLS and why you need to know this. As we know that regression finction look like $ f_{w}(x_i) = \langle x, w \rangle + w_{0}$ and you can check out that im told about w_0 is constant, but why? \n\nOften we can ignore $w_0$, because we can achieve the same result if make new feature $x_i$ equal 1. Then new created feature will be have weight equal $w_0$.\n\n$$ \begin{pmatrix}x_{i1},\dotsc,x_{iD}\end{pmatrix} \cdot \begin{pmatrix} w_{1}\\\vdots\\x_{D}\\\end{pmatrix} + w_{0} = \begin{pmatrix} 1, x_{i1},\dotsc,x_{iD}\end{pmatrix} \cdot \begin{pmatrix} w_{0}\\ w_{1}\\\vdots\\x_{D}\\\end{pmatrix}$$\n\nWe want that our regression model as better as possible approximate our dependent."
Lets look at the class field and check how many fraudulent transactions we have in this data 
Fraudulent transactions provided here contributes mere **0.17%** which indicates\n**we have a highly imbalanced data to work on.** \nIf I can summarize what Andrew Ng has mentioned in his lecture on Anomaly detection is \nSupervised Classification technique is not the perfect candidate for highly imbalanced data. In this case it is \n 0.172% (near to 0)\n\nIf We think from the persepctive of building the model to find out the anomalous data which is not seen very frequently \nWe should go for Anomaly detection technique using Gaussian Distribution.  \n
"Below is the most crucial function used to detect how well we are doing with our subset (Cross validation subset) .\nI have decided values for Epsilon for detecting the fradulent transactions from the Subsets.  \n**(Tip :- Ideally you should provide range of epsilon values, due to time constraint on running this kernel i have provided few values here for demonstration purpose)**\n\n **For now remember Epsilon value is the threshold value below which we will mark transaction as Anomalous.**\n           ----\n\nRewriting above sentense again \nP(x) for X if less than the epsilon value then mark that transaction as anomalous transaction. \n\nWe need to maintain healthy balance between the Recall and Precision . We may get Recall value above 0.80 and close to 0.90 here but at the expense of reducing our precision which is not advisable.\n"
**Lets visualize which features are not much of help in detecting the anamoly **
Lets Visualize our predictions in below scatter plot \n         -------
From the above result we can see that we are able to maintain the balance between Recall and Precision. \n\nPrecision of around 87% with Recall of 68% is not bad at all when we have such highly unbalanced data. \nThese numbers are not fixed and can vary . \n \n These numbers were different for Cross validation dataset and we shortlisted our Epsilon value by comparing the results of F1 Score.\n\nI will show you the result we achieved on Cross validation dataset again.
"If we compare the individual values, we actually see that they are fairly close together when we consider the entire search grid! \n\n## Distribution of Scores\n\nLet's plot the distribution of scores for both models in a kernel density estimate plot."
"Bayesian optimization did not produce the highest individual score, but it did tend to spend more time evaluating ""better"" values of hyperparameters. __Random search got lucky and found the best values but Bayesian optimization tended to ""concentrate"" on better-scoring values__. That's pretty much what we expect: random search does a good job of exploring the search space which means it will probably happen upon a high-scoring set of values (if the space is not extremely high-dimensional) while Bayesian optimization will tend to focus on a set of values that yield higher scores. __If all you wanted was the conclusion, then you're probably good to go. If you really enjoy making plots and doing exploratory data analysis and want to gain a better understanding of how these methods work, then read on!__ In the next few sections, we will thoroughly explore these results.\n\nOur plan for going through the results is as follows:\n\n* Distribution of scores\n    * Overall distribution\n    * Score versus the iteration (did scores improve as search progressed)\n* Distribution of hyperparameters\n    * Overall distribution including the hyperparameter grid for a reference\n    * Hyperparameters versus iteration to look at _evolution_ of values\n* Hyperparameter values versus the score\n    * Do scores improve with certain values of hyperparameters (correlations)\n    * 3D plots looking at effects of 2 hyperparameters at a time on the score\n* Additional Plots\n    * Time to run each evaluation for Bayesian optimization\n    * Correlation heatmaps of hyperparameters with score\n    \nThere will be all sorts of plots: heatmaps, 3D scatterplots, density plots, bar charts (hey even bar charts can be helpful!)\n\nAfter going through the results, we will do a little meta-machine learning, and implement the best model on the full set of features."
"## Score versus Iteration\n\nNow, to see if either method improves over the course of the search, we need to plot the score as a function of the iteration. "
"Again keeping in mind that Bayesian optimization has not yet finished, we can see a clear upward trend for this method and no trend whatsoever for random search. \n\n### Linear Regression of Scores versus Iteration\n\nTo show that Bayesian optimization improves over time, we can regress the score by the iteration. Then, we can use this to extrapolate into the future, __a wildly inappropriate technique in this case, but fun nonetheless!__\n\nHere we use `np.polyfit` with a degree of 1 for the linear regression (you can compare the results with `LinearRegression`  from `sklearn.linear_model`."
"In both search methods, the `gbdt` (gradient boosted decision tree) and `dart` (dropout meets additive regression tree) do much better than `goss` (gradient based one-sided sampling). `gbdt` does the best on average (and for the max), so it might make sense to use that method in the future! Let's view the results as a barchart:"
__`gbdt` (or `dart`) it should be! Notice that random search tried `gbdt` about the same number of times as the other two (since it selected with no reasoning) while Bayesian optimization tried `gbdt` much more often. __\n\nSince `gbdt` supports `subsample` (using on a sample of the observations to train on in every tree) we can plot the distribution of `subsample` where `boosting_type=='gbdt'`. We also show the reference distribution.
__`gbdt` (or `dart`) it should be! Notice that random search tried `gbdt` about the same number of times as the other two (since it selected with no reasoning) while Bayesian optimization tried `gbdt` much more often. __\n\nSince `gbdt` supports `subsample` (using on a sample of the observations to train on in every tree) we can plot the distribution of `subsample` where `boosting_type=='gbdt'`. We also show the reference distribution.
There is a significant disagreement between the two methods on the optimal value for `subsample`. Perhaps we would want to leave this as a wide distribution in any further searches (although some subsampling does look to be beneficial).
"__The only clear distinction is that the score decreases as the learning rate increases.__ Of course, we cannot say whether that is due to the learning rate itself, or some other factor (we will look at the interplay between the learning rate and the number of esimators shortly). The learning rate domain was on a logarithmic scale, so it's most accurate for the plot to be as well (unfortunately I cannot get this to work yet)."
Now for the next four hyperparameters versus the score.
Now for the next four hyperparameters versus the score.
"There are not any strong trends here. Next we will try to look at two hyperparameters simultaneously versus the score in a 3-dimensional plot. This makes sense for hyperparameters that work in concert, such as the learning rate and the number of esimators or the two regularization values."
"## 3D Plots \n\nTo try and examine the simultaneous effects of hyperparameters, we can make 3D plots with 2 hyperparameters and the score. A truly accurate plot would be 10-D (one for each hyperparameter) but in this case we will stick to 3 dimensions. 3D plots can be made in matplotlib by import `Axes3D` and specifying the `3d` projection in a call to `.add_subplot`"
First up is `reg_alpha` and `reg_lambda`. These control the amount of regularization on each decision tree and help to prevent overfitting to the training data.
The next plot is learning rate and number of estimators versus the score. __Remember that the number of estimators was selected using early stopping for 100 rounds with 5-fold cross validation__. The number of estimators __was not__ a hyperparameter in the grid that we searched over. Early stopping is a more efficient method of finding the best number of estimators than including it in a search (based on my limited experience)!
Here there appears to be a clear trend: a lower learning rate leads to higher values! What does the plot of just learning rate versus number of estimators look like?
Here there appears to be a clear trend: a lower learning rate leads to higher values! What does the plot of just learning rate versus number of estimators look like?
"This plot is very easy to interpret: the lower the learning rate, the more estimators that will be trained. From our knowledge of the model, this makes sense: each individual decision trees contribution is lessened as the learning rate is decreased leading to a need for more decision trees in the ensemble. Moreover, from the previous graphs, it appears that decreasing the learning rate increases the model score."
"### Function for 3D plotting\n\nAny time you write code more than twice, it should be encoded into a function! That's what the next code block is for: putting this code into a function that we can use many times! This function can be used for __any__ 3d plotting needs."
The bayesian optimization results are close in trend to those from random search: lower learning rate leads to higher cross validation scores.
"## Correlation Heatmap\n\nNow we can make a heatmap of the correlations. I enjoy heatmaps and thankfully, they are not very difficult to make in `seaborn`."
That's a lot of plot for not very much code! We can see that the number of estimators and the learning rate have the greatest magnitude correlation (ignoring subsample which is influenced by the boosting type).
That's a lot of plot for not very much code! We can see that the number of estimators and the learning rate have the greatest magnitude correlation (ignoring subsample which is influenced by the boosting type).
Feel free to use this code for your own heatmaps! (Also send me color recommendations because I am not great at picking out a palette).
- **No null values** present in the data!
"- Average customer **Age** in the dataset is in the late 30s i.e **38.85**. \n- Average **Annual Income (k\\$)** of the customers is **60.56** i.e just short of the 2018 median income of USA citizen, 63k$.\n- **Spending Score (1-100)** average of the mall customer is in the center with **50.20**."
### Distribution of Categorical and Numerical Features :
"- **Gender**, the only categorical feature, data displays a **normally distribution**.\n- Distribution of **Age** and **Annual Income (k\\$)** is **positively or righly skewed**.\n- **Spending Score (1-100)** data distribution is similar to the **Head and Shoulder** pattern observed in stock charts. \n- It displays a stock's price rising to a peak and then declines back to the base of the prior up-move. Something similar can be observed with the **2 shoulders** forming around the values **20 & 80** with **head** being centered in between **40 - 60**.\n- We will drop the **CustomerID** feature as it is just a number that is tagged to a customer."
### Categorical Features :
"- For the above dataset, **female** customers just edge out the **male** customers. "
### Numerical Features vs Categorical Features :
"- **Age** range of **female** customers is from **30 to just below 50** whereas **male** customers **Age** ranges from **just below 30 to 50**. \n- For both **Genders**, a bulge at the age of **30 - 35** can be observed. Median **Age** of **male** is slightly more than those of **female**.\n- For **Annual Income (k\\$)**, **female** customer's income starts from **40k** whereas **male** customer's is above this value. \n- Median **Annual Income (k\\$)** for both **Genders** is near about **60k**. **Annual Income (k\\$)** of **male** customers tapers very sharply at the apex with some outliers as compared to **female** customers.\n- Median **Spending Score (1-100)** of both **Genders** is same despite having different starting points of **just below 40** & **just above 20** for **female** and **male** customers respectively.\n- Both the **Genders** display a strong bulge at the median value especially **female** customers. However, **male** customers display a small but significant bulge in the range of **0 - 20** as well."
### Numerical Features vs Numerical Features w.r.t Categorical Feature :
"- **Annual Income (k\\$)** datapoints are present throughout the all the **Age** values, **Gender** does not provide any significant information. \n- For **Spending Score (1-100)**, it can be clearly observed that **Age** ranges **20 - 30** display very high spending habits. \n- **Age** group **30 - 40** highlights both the extremes of spending habits in the customer. **40 - 70 Age** group customer displays the other end of spending habits with low values.    \n- From the **Annual Income (k\\$) vs Spending Score (1-100)**, distinct 5 groups can be observed. For **Annual Income (k\\$)** values between **0 - 40**, data highlights 2 groups of customers with **Spending Score (1-100)** between **0 - 40** and **60 - 100**.\n- After displaying this 1 extreme, data highlights the middle group of customers that have an **Annual Income (k\\$)** between **40 - 70k** and **Spending Score (1-100)** between **40 - 60**.\n- **Annual Income (k\\$)** values between **70 - 140k** define the other extreme that is divided into 2 groups based on **Spending Score (1-100)** values of **0 - 40** and **60 - 100**."
### Correlation Matrix :
"- Both the matrix displayed are same! It is only done for visualization purpose. This trick can be used when the dataset has too many features to look into! \n- **CustomerID** displays a very high positive correlation with **Annual Income (k\\$)** as the customers are arranged in ascending order according to their **Annual Income (k\\$)**. We are not going to include **CustomerID** for modeling purpose.\n- **Gender** does not display any relation with other features. It is pretty much neutral with all the values sticking to 0.\n- **Spending Score (1-100)** and **Age** display a negative correlation i.e if value of one feature increases, then another feature's value decreases and vice-versa is true!\n- This information is gained from the **EDA** section the correlation matrix values further back the evidence. \n- We will now move to the modeling section by creating combinations of these features and finding different ways in which the mall customers can be segmented!"
### Original Dataset :\n\n#### Elbow Method & Silhouette Score Method :
"- From the results of the above 2 methods, we select :\n    - **k : Clusters = 4**"
"- From the results of the above 2 methods, we select :\n    - **k : Clusters = 4**"
### Normalized Dataset :\n\n#### Elbow Method & Silhouette Score Method :
### Normalized Dataset :\n\n#### Elbow Method & Silhouette Score Method :
"- From the results of the above 2 methods, we select :\n    - **k : Clusters = 3**"
"- From the results of the above 2 methods, we select :\n    - **k : Clusters = 3**"
### Age - Spending Score (1-100)
### Original Dataset :\n\n#### Elbow Method & Silhouette Score Method :
"- From the results of the above 2 methods, we select :\n    - **k : Clusters = 4**"
"- From the results of the above 2 methods, we select :\n    - **k : Clusters = 4**"
### Normalized Dataset :\n\n#### Elbow Method & Silhouette Score Method :
### Normalized Dataset :\n\n#### Elbow Method & Silhouette Score Method :
"- From the results of the above 2 methods, we select :\n    - **k : Clusters = 6**"
"- From the results of the above 2 methods, we select :\n    - **k : Clusters = 6**"
### Annual Income (k\$) - Spending Score (1-100)
### Original Dataset :\n\n#### Elbow Method & Silhouette Score Method :
"- From the results of the above 2 methods, we select :\n    - **k : Clusters = 5**"
"- From the results of the above 2 methods, we select :\n    - **k : Clusters = 5**"
### Normalized Dataset :\n\n#### Elbow Method & Silhouette Score Method :
### Normalized Dataset :\n\n#### Elbow Method & Silhouette Score Method :
"- From the results of the above 2 methods, we select :\n    - **k : Clusters = 5**"
"- From the results of the above 2 methods, we select :\n    - **k : Clusters = 5**"
### Age - Annual Income (k\$) - Spending Score (1-100)
### Original Dataset :\n\n#### Elbow Method & Silhouette Score Method :
"- From the results of the above 2 methods, we select :\n    - **k : Clusters = 6**"
"- From the results of the above 2 methods, we select :\n    - **k : Clusters = 6**"
### Normalized Dataset\n\n#### Elbow Method & Silhouette Score Method :
### Normalized Dataset\n\n#### Elbow Method & Silhouette Score Method :
"- From the results of the above 2 methods, we select :\n    - **k : Clusters = 6**"
"- From the results of the above 2 methods, we select :\n    - **k : Clusters = 6**"
### Results Table\n\n#### Original Dataset :\n\n|Sr. No.|Feature Combination|Number of Clusters|\n|-|-|-|\n|1.|Age - Annual Income (k\$)|4|\n|2.|Age - Spending Score (1-100)|4|\n|3.|Annual Income (k\$) - Spending Score (1-100)|5|\n|4.|Age - Annual Income (k\$) - Spending Score (1-100)|6|\n\n#### Normalized Dataset :\n\n|Sr. No.|Feature Combination|Number of Clusters|\n|-|-|-|\n|1.|Age - Annual Income (k\$)|3|\n|2.|Age - Spending Score (1-100)|6|\n|3.|Annual Income (k\$) - Spending Score (1-100)|5|\n|4.|Age - Annual Income (k\$) - Spending Score (1-100)|6|
# Importing Libraries 
# Loding Dataset
# Takeaways\n1. Plotting good visual for Eda\n2. Dyanic visual plots\n3. Plotly.express library
# Box Plots And Voilin plots
### Boxplot
### Violin Plot
### Violin Plot
### Histogram
### Histogram
## Multivaiate  plot of Age vs Categorical columns
#  Bubble Chart
# 3D Scatter Plots
# 3D Scatter Plots
# Multivarite Numerical Plot
# Cantour Plots
# Scatter Geo plot
"# Predicting Molecular Properties\nCan you measure the magnetic interactions between a pair of atoms?\n\nIn this competition, you will develop an algorithm that can predict the magnetic interaction between two atoms in a molecule (i.e., the scalar coupling constant).\n\n![](http://www.chem.ucalgary.ca/courses/350/Carey5th/Ch13/coupling04.gif)\n\n**NOTE : Some (but not all) of the text in this kernel was taken from the competition details. I do this to show exactly what the description and rules are for the competition alongside some exploritory code. Be sure to read the competition details yourself directly from the website here: https://www.kaggle.com/c/champs-scalar-coupling/overview/description. **"
"# The Data\n\nIn this competition, you will be predicting the scalar_coupling_constant between atom pairs in molecules, given the two atom types (e.g., C and H), the coupling type (e.g., 2JHC), and any features you are able to create from the molecule structure (xyz) files.\n\nFor this competition, you will not be predicting all the atom pairs in each molecule rather, you will only need to predict the pairs that are explicitly listed in the train and test files. For example, some molecules contain Fluorine (F), but you will not be predicting the scalar coupling constant for any pair that includes F.\n\nThe training and test splits are by molecule, so that no molecule in the training data is found in the test data."
These plots are beautiful. It's a shame we don't have this data for the test set.
## Target vs. Atom Count
"When we look at the target `scalar_coupling_constant` in relation to the `atom_count` - there visually appears to be a relationship. We notice the gap in coupling constant values, between ~25 and ~75. It is rare to see a value within this range. Could this be a good case for a classification problem between the two clusters?"
# Super Simple Baseline Model [1.239 Public LB]\nThe simplest thing we can do as a model is predict that the target is the **average** value that we observe for that **type** in the training set!
# Super Simple Baseline Model [1.239 Public LB]\nThe simplest thing we can do as a model is predict that the target is the **average** value that we observe for that **type** in the training set!
"# Evaluation Metric\n\nSubmissions are evaluated on the Log of the Mean Absolute Error, calculated for each scalar coupling type, and then averaged across types, so that a 1% decrease in MAE for one type provides the same improvement in score as a 1% decrease for another type.\n\n![Eval Metric](https://i.imgur.com/AK6z3Dn.png)\n\nWhere:\n\n- `T` is the number of scalar coupling types\n- `nt` is the number of observations of type t\n- `yi` is the actual scalar coupling constant for the observation\n- `yi^` is the predicted scalar coupling constant for the observation\n\nFor this metric, the MAE for any group has a floor of 1e-9, so that the minimum (best) possible score for perfect predictions is approximately -20.7232."
Evaluation metric is important to understand as it determines how your model will be scored. Ideally we will set the loss function of our machine learning algorithm to use this metric so we can minimize the specific type of error.\n\nCheck out this kernel by `@abhishek` with code for the evaluation metric: https://www.kaggle.com/abhishek/competition-metric\n
# Feature Creation\nThis feature was found from `@inversion` 's kernel here: https://www.kaggle.com/inversion/atomic-distance-benchmark/output\nThe code was then made faster by `@seriousran` here: https://www.kaggle.com/seriousran/just-speed-up-calculate-distance-from-benchmark
"\n    It can be seen that dataset has successfully imported. In the dataset, there are 31 columns with 119040 observations. The details of each variables also can be seen above. After examining columns and column data types, the next step will inspect null values in each column.\n\n"
"\n    Since most of the columns have many null values, and this notebook only uses a few columns, the unused columns will be dropped.\n\n"
"\n    Based on the results above, it can be concluded that:\n    \n        \n            From the results of the average CV score, it can be said that the model's performance is good. In addition, from CV standard deviation value, it can be concluded that the model has very minimum variance between each CV scores.\n            From evaluation metrics score, it can be seen that RMSE and MAE value is close to 0. In addition, the R squared value also reveals that 77% of the variability observed in the target variable is explained by the regression model. Based on those results, it can be concluded that the model has good performance.\n            Based on the intercept and coefficients result, the linear regression formula will be:\n            $$ .: y = 10.6635 + 0.9205x :. $$\n            with:\n                \n                    $ x $ = independent variable (MinTemp)\n                    $ y $ = dependent variable (MaxTemp)\n                \n        \n    \n\n"
"\n    The following plot will compare the actual and predicted temperatures. Due to large amount of data, the following plot will use the first 15 data as samples to compare actual and predicted temperatures.\n\n"
"\n    The following plot will compare the actual and predicted temperatures. Due to large amount of data, the following plot will use the first 15 data as samples to compare actual and predicted temperatures.\n\n"
## 6.2 | Evaluating Assumptions ðŸ”\n## 6.2.1 | Linearity Assumption\n
## 6.2 | Evaluating Assumptions ðŸ”\n## 6.2.1 | Linearity Assumption\n
"\n    The scatter plot above shows that it is not a perfect linear relationship since the predicted line barely fits the diagonal line. The predictions are skewed towards lower values (between -10 and 0) and especially towards higher values (above 20). It is advised to apply nonlinear transformations or add polynomial terms to some predictors. If those fail to reflect the relationship between the predictors and the label, consider using more variables.\n    \n        . : . Linearity assumption not satisfied . : .\n    \n\n"
## 6.2.2 | Normality Assumption\n
"\n    The distribution is right-skewed since the median value is smaller than the mean value. In addition, the p-value distribution shows that it is smaller than 0.05, which indicates that the distribution is not normal, which may affect the confidence interval. It is suggested to perform non-linear transformations on variables.\n    \n        . : . Normality assumption not satisfied . : .\n    \n\n"
## 6.2.5 | Homoscedasticity Assumption\n
"\n    There is no uniform variance across the residuals, which is potentially problematic. This can be solved by either using weighted least squares regression instead of the standard OLS or transforming the dependent or highly skewed variables. A log transformation on the dependent variable is also a good starting point.\n    \n        . : . Homoscedasticity assumption not satisfied . : .\n    \n\n"
# Check sales for holidays
# The average holiday sales are equivalent to Saturday and Sunday sales.
" Observations: \nThe following columns are the one's that show the greatest correlation with our diagnosis column. There are two things that can be done. \n* We can either use only the columns which have greatest correlation, or we can continue to use all the columns.\n* I will be using all these columns to predict our result\n* You can eliminate a few and see if the accuracy improves!"
" Observations: \nLooks wonderful, isn't it! \n* There are only a handful of columns that show negative correlation with the 'diagnosis column'\n* Around half of our columns are more than 50% positively correlated to diagnosis column.\n\nWe have to select which of the attributes we want to use in building our model!\n"
"# **W & B Artifacts**\n\nAn artifact as a versioned folder of data.Entire datasets can be directly stored as artifacts .\n\nW&B Artifacts are used for dataset versioning, model versioning . They are also used for tracking dependencies and results across machine learning pipelines.Artifact references can be used to point to data in other systems like S3, GCP, or your own system.\n\nYou can learn more about W&B artifacts [here](https://docs.wandb.ai/guides/artifacts)\n\n![](https://drive.google.com/uc?id=1JYSaIMXuEVBheP15xxuaex-32yzxgglV)"
# **Basic Statistics of Features**
# **Target Variable Distribution**
# **Target Class Balance**
# **Target Class Balance**
# **Distribution of features**
# **Distribution of features**
\n# **Logging to W & B environment**
\n### 2. Data Loading and Description
\n#### 2.1  Loading the data files 
\n#### 3.3 Dealing with Missing values
\n#### 3.4 Transforming Sex
\n#### 4.2 Gender and Survived\n
This bar plot above shows the distribution of female and male survived. The x_label shows gender and the y_label shows % of passenger survived. This bar plot shows that 74% female passenger survived while only ~19% male passenger survived.
This bar plot above shows the distribution of female and male survived. The x_label shows gender and the y_label shows % of passenger survived. This bar plot shows that 74% female passenger survived while only ~19% male passenger survived.
This count plot shows the actual distribution of male and female passengers that survived and did not survive. It shows that among all the females ~ 230 survived and ~ 70 did not survive. While among male passengers ~110 survived and ~480 did not survive.
\n#### 4.3 Pclass and Survived
"So it clearly seems that,The survival of the people belong to 3rd class is very least.\nIt looks like ...\n-  63% first class passenger survived titanic tragedy, while\n-  48% second class and\n-  only 24% third class passenger survived."
"So it clearly seems that,The survival of the people belong to 3rd class is very least.\nIt looks like ...\n-  63% first class passenger survived titanic tragedy, while\n-  48% second class and\n-  only 24% third class passenger survived."
"This kde plot is pretty self explanatory with all the labels and colors. Something I have noticed that some readers might find questionable is that in, the plot; the third class passengers have survived more than second class passnegers. It is true since there were a lot more third class passengers than first and second.\n\n"
\n#### 4.5 Age and Survived
"There is nothing out of the ordinary of about this plot, except the very left part of the distribution. It shows that\n\nchildren and infants were the priority."
\n#### 6.1 Classifier Comparision\n\nBy Classifier Comparison we choose which model best for the given data.
"#### Important Note:\n\n- If you're facing an error to see the result of below code cell, It's because there is a lot of __calcualation__ is taking place as multiple algorithms are running so for that I suggest get this notebook to edit mode and try to run cell by cell.\n\n- I am commenting two of the below code cells as they are taking a lot of notebook space, but i want you guys try to run them from your side. "
"## References\n\ní•´ë‹¹ ì»¤ë„ì€ ì•„ëž˜ì˜ ë©‹ì§„ ë¶„ì„ ì»¤ë„ì„ ì°¸ê³ í•˜ì˜€ìŠµë‹ˆë‹¤.  \në©‹ì§„ ë¶„ì„ì„ ì œê³µí•´ì¤€ ëª¨ë“  ë¶„ë“¤ê»˜ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤. í•­ìƒ ë§Žì€ ë„ì›€ì´ ë˜ê³  ìžˆìŠµë‹ˆë‹¤.\n\n* [colab, ì „íƒœê· ë‹˜ì˜ íƒ€ì´íƒ€ë‹‰ ë¶„ì„](https://colab.research.google.com/drive/1cqv5yD9uLHHrVFL-TGM9NPSD1ZyF4AC1#scrollTo=jwBNmHcw3w91)\n* [ë¸”ë¡œê·¸, ì´ìœ í•œë‹˜ì˜ íƒ€ì´íƒ€ë‹‰ ë¶„ì„ íŠœí† ë¦¬ì–¼](http://kaggle-kr.tistory.com/17?category=821486)\n* [ê³µê°œ ì»¤ë„, Kerasë¥¼ ì´ìš©í•œ ê°„ë‹¨í•œ ëª¨ë¸ êµ¬í˜„!](https://www.kaggle.com/everystep/keras)"
## 1. ë°ì´í„° ì…‹ í™•ì¸\n---\nì¼ë‹¨ í•„ìš”í•œ íŒ¨í‚¤ì§€(ë¼ì´ë¸ŒëŸ¬ë¦¬)ë¥¼ import í•©ë‹ˆë‹¤.\n
"train setê³¼ test setì— ë¹„ìŠ·í•˜ê²Œ `Age` í”¼ì³ì— ì•½ 20%, `Cabin` í”¼ì³ì— ì•½ 80%ì˜  \nê²°ì¸¡ì¹˜ê°€ ìžˆìŒì„ í™•ì¸í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.\n\n### **1.2 Target Label, `Survived` í™•ì¸**\n\ntarget label ì´ ì–´ë–¤ distribution ì„ ê°€ì§€ê³  ìžˆëŠ” ì§€ í™•ì¸í•´ë´ì•¼ í•©ë‹ˆë‹¤.  \nì§€ê¸ˆ ê°™ì€ binary classification ë¬¸ì œì˜ ê²½ìš°ì—ì„œ,  \n1ê³¼ 0ì˜ ë¶„í¬ê°€ ì–´ë– ëƒì— ë”°ë¼ ëª¨ë¸ì˜ í‰ê°€ ë°©ë²•ì´ ë‹¬ë¼ ì§ˆ ìˆ˜ ìžˆìŠµë‹ˆë‹¤."
"target label ì˜ ë¶„í¬ê°€ ì œë²• ê· ì¼(balanced)í•©ë‹ˆë‹¤.  \në¶ˆê· ì¼í•œ ê²½ìš°, ì˜ˆë¥¼ ë“¤ì–´ì„œ 100ì¤‘ 1ì´ 99, 0ì´ 1ê°œì¸ ê²½ìš°ì—ëŠ”  \në§Œì•½ ëª¨ë¸ì´ ëª¨ë“ ê²ƒì„ 1ì´ë¼ í•´ë„ ì •í™•ë„ê°€ 99%ê°€ ë‚˜ì˜¤ê²Œ ë©ë‹ˆë‹¤.  \n0ì„ ì°¾ëŠ” ë¬¸ì œë¼ë©´ ì´ ëª¨ë¸ì€ ì›í•˜ëŠ” ê²°ê³¼ë¥¼ ì¤„ ìˆ˜ ì—†ê²Œ ë©ë‹ˆë‹¤.  \nì§€ê¸ˆ ë¬¸ì œì—ì„œëŠ” ê·¸ë ‡ì§€ ì•Šìœ¼ë‹ˆ ê³„ì† ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤."
Let's check our information for gaps.
"There are gaps in our data. Since the gaps are insignificant, we will fill them in using information from open sources."
Let's look at the leaders on individual grounds.
"Conclusions from the constructed visualization:\n* Obviously, cities located closer to the South Pole will have more sunny days per year :)\n* The most expensive cities are in Europe. This is due to the high standard of living, as well as high wages in these countries.\n* The highest life expectancy - in the cities of Europe and Japan. It is also associated with a high standard of living.\n* The dirtiest air and longest working hours are in developing countries, as well as in China. This is due to the low requirements at the level of legislation of the countries."
"Let's build scatterplots, take the level of happiness as the target variable."
There is a positive and inverse correlation with some features.\nHypothesis: There is multicollinearity in the data.
This kernel mainly focuses on what parameters are important for a student to get into a graduate school.\n\nBy the end of this kernel it will be clear of what are the scores required for different tests to have better admission chances and get into a good graduate school.
# 2.Understanding Data\n\n        Back to the table of contents
# 3.Data Analysis\n\n        Back to the table of contents
"Here we can see that the chance of admit is highly correlated with CGPA, GRE and TOEFEL scores are also correlated."
"Here we can see that the chance of admit is highly correlated with CGPA, GRE and TOEFEL scores are also correlated."
Inferences from the above pairplot:\n* GRE score TOEFL score and CGPA all are linearly related to each other \n* Research Students tend to Score higher by all means
Inferences from the above pairplot:\n* GRE score TOEFL score and CGPA all are linearly related to each other \n* Research Students tend to Score higher by all means
From the above 2 graphs its clear that people tend to score above 310 in GRE and above 100 in TOEFL
From the above 2 graphs its clear that people tend to score above 310 in GRE and above 100 in TOEFL
Ratings of university increase with the increase in the CGPA
Loading libraries...
Loading datasets...
Outlier detection
Distribution of cat features....
# **Import of Libraries**
![Kaggle TPS '21-11 1.png](attachment:45f45d9e-997a-4ea6-9307-95d4cb912f2a.png)
"> **""id"" and ""target"" columns are int64.** We do not really need ""id"" column. We do not really need ""target"" as int64.\nMoreover, we can downscale float64 for the sake of faster computation. We will work on it later on. \n\n> **Now let's plot our target values:**"
"> **Good news, our classes are well balanced!**\n\n> **Now we are going to plot some features (we go with 12) against its target values.**\n>\n> Additionally to the above, let's get 30000 sample for a faster run time."
"\n# 3. Exploratory Data Analysis ðŸ“Š \n\n> **The problem statement of this competition says that the features are anonymized.** In reality, there are not many things we can observe and make decisions on. Let's take a closer look at the distribution of the features:"
"> Special thanks to **EDUARDO GUTIERREZ** and his notebook for .describe method representation and styling. [**Eduardo's notebook**](https://www.kaggle.com/eduardogutierrez/tps-nov-21-exploratory-data-analysis). \n>\n> **What is the distribution of the features?**\n>* Features **""f2""**, **""f35""** and **""f44""** are of a higher magnitude comparing to other features;\n>* The standard deviation is in the range of .05 to 1.78 if we exclude above mentioned features;\n>* Some features are almost identical in terms of their scale and magnitude."
> **Let's take a closer look at the distribution of the features:**
"> **We have plotted Probability Density Function estimation for each feature. What does it tell us?**\n>* The features are distributed differently;\n>* More than half of the features are candle-like distributed (looks like Pearson Type 6 or even Poison distribution);\n>* The rest of the features are bell-shaped-like (e.g., Gaussian distribution);\n>* It might be a good idea to transform some of the features."
> **Let's us take a look at features correlation matrix**:
"> If we wish to label the strength of the features association, for absolute values of correlation, **0-0.19** is regarded as very weak (our example is even weaker: **0-0.10**). "
"> **We've been able to significantly shrink the magnitude of ""f2"".**\n> \n> Let's plot the Probability Density Function Estimation for the transformed features:"
"> The distributions of the transformed features looks more bell-shaped-like but still...\n> A good idea is to transform features separately, adding them into a model and monitor the model performance. It might be a very time consuming process taking into consideration not only log transformations. We will leave it as an exercise for the reader."
 \n## Notebook  Content\n1. [Memory issues](#1)\n1. [EDA-briefly](#2)\n1. [Feature Engineering (to be continued) ](#3)\n
 \n#  1-Memory issues
Let us proceede with further interesting **EDA**
This time around we plotted the independently but on the same graph and the conclusion is the same. Right before the problem happens signal increases. Ok what wcan we now say regarding the univariate distribution of the columns themselves?
This time around we plotted the independently but on the same graph and the conclusion is the same. Right before the problem happens signal increases. Ok what wcan we now say regarding the univariate distribution of the columns themselves?
Very intersting (again for the first 150 000 observations) it seems that (yellow=accoustic_data) follows normal distribution with some outliers. On the other hand time to failure takes on 2(?) very distinct values...
 \n# Import Libraries
"\n#### Exploratory Data Analysis\n- Loading Data \n- Data Disribution\n- Data Visualization\n\nSetup all the param, which we will use in model\n"
"\n# Data Visualization and EDA\n> Data distrubution per class\n\nas per below bar chart, it clearly showing that data set is quite imbalance. And even it's expected in medical domain."
"Histogram is clearing showing that training data is Imbalanced. Because in class â€˜No DRâ€™ records are approx. 1750 while in class â€˜Severeâ€™ very less. So, may be for balancing data set, we would be requiring data augmentation. \n\nThere are couple of ways to do image data augmentation. We will see down in this kernel.\n"
\n### Train and Test dataset \n- We will use pie chart for showing the size of dataset.
It's showing train and testing data are in 2:1 ratio. Both are quite small data set.
"\n### GrayScale Images\nConverting the Ratina Images into Grayscale. So, we can usnderstand the regin or intest ."
"It's clearly showing, that the image [0,1] has give regin  black around the EYE ball. Which is ust noise, that will not add any value fo model. We need to remove this black area. in my next iteration will work on that to crop black are from image. "
\n# Data Pre-Processing\n\n #### Croping Images\n\nhttps://stackoverflow.com/questions/13538748/crop-black-edges-with-opencv
- Croping Images randomly for resizing.
\n # Visualization Test Result\n- this section will visualize the predicted classes of test data.
References:\n\n1. https://medium.com/@vijayabhaskar96/tutorial-on-keras-imagedatagenerator-with-flow-from-dataframe-8bd5776e45c1\n1. https://medium.com/@vijayabhaskar96/tutorial-on-keras-flow-from-dataframe-1fd4493d237c\n1. https://www.pyimagesearch.com/2017/03/20/imagenet-vggnet-resnet-inception-xception-keras/\n1. https://jkjung-avt.github.io/keras-image-cropping/\n1. https://www.kaggle.com/aleksandradeis/aptos2019-blindness-detection-eda
\n# Import libraries\n[Back to Table of Contents](#ToC)
\n# Define useful classes\nNOTE: I use the neural network in [*Siwei Xu's tutorial\n*](https://towardsdatascience.com/deep-reinforcement-learning-build-a-deep-q-network-dqn-to-play-cartpole-with-tensorflow-2-and-gym-8e105744b998) with some proper modifications to adapt to the problem in ConnectX competition and be able to save trained and load pre-trained models.\n\n[Back to Table of Contents](#ToC)
\n# Define useful classes\nNOTE: I use the neural network in [*Siwei Xu's tutorial\n*](https://towardsdatascience.com/deep-reinforcement-learning-build-a-deep-q-network-dqn-to-play-cartpole-with-tensorflow-2-and-gym-8e105744b998) with some proper modifications to adapt to the problem in ConnectX competition and be able to save trained and load pre-trained models.\n\n[Back to Table of Contents](#ToC)
\n# Define helper-functions\n[Back to Table of Contents](#ToC)
\n# Define helper-functions\n[Back to Table of Contents](#ToC)
\n# Create ConnectX environment\n[Back to Table of Contents](#ToC)
"Our response variable, **diagnosis**, is categorical and has two classes,  'B' (Benign) and 'M' (Malignant). All explanatory variables are numerical, so we can skip data type conversion.\n\nLet's now take a closer look at our response variable, since it is the main focus of our analysis. We begin by checking out the distribution of its classes."
"Out of the 569 observations, 357 (or 62.7%) have been labeled malignant, while the rest 212 (or 37.3%) have been labeled benign. Later when we develop a predictive model and test it on unseen data, we should expect to see a similar proportion of labels.\n\nAlthough our dataset has 30 columns excluding the **id** and the **diagnosis** columns, they are all in fact very closely related since they all contain information on the same 10 key attributes but only differ in terms of their perspectives (i.e., the mean, standard errors, and the mean of the three largest values denoted as ""worst""). \n\nIn this sense, we could attempt to dig out some quick insights by analyzing the data in only one of the three perspectives. For instance, we could choose to check out the relationship between the 10 key attributes and the **diagnosis** variable by only choosing the ""mean"" columns.\n\nLet's quickly scan for any interesting patterns between our 10 ""mean"" columns and the response variable by generating a scatter plot matrix as shown below:"
"Out of the 569 observations, 357 (or 62.7%) have been labeled malignant, while the rest 212 (or 37.3%) have been labeled benign. Later when we develop a predictive model and test it on unseen data, we should expect to see a similar proportion of labels.\n\nAlthough our dataset has 30 columns excluding the **id** and the **diagnosis** columns, they are all in fact very closely related since they all contain information on the same 10 key attributes but only differ in terms of their perspectives (i.e., the mean, standard errors, and the mean of the three largest values denoted as ""worst""). \n\nIn this sense, we could attempt to dig out some quick insights by analyzing the data in only one of the three perspectives. For instance, we could choose to check out the relationship between the 10 key attributes and the **diagnosis** variable by only choosing the ""mean"" columns.\n\nLet's quickly scan for any interesting patterns between our 10 ""mean"" columns and the response variable by generating a scatter plot matrix as shown below:"
"There are some interesting patterns visible. For instance, the almost perfectly linear patterns between the **radius**, **perimeter** and **area** attributes are hinting at the presence of multicollinearity between these variables. Another set of variables that possibly imply multicollinearity are the **concavity**, **concave_points** and **compactness**. \n\nIn the coming up section, we will generate a matrix similar to the one above, but this time displaying the correlations between the variables instead of a scatter plot. Let's find out if our hypothesis about the multicollinearity has any statistical support. "
"# 2. The Variables\n---\nAs said earlier, let's take a look at the correlations between our variables. This time however, we will create a correlation matrix with all variables (i.e., the ""mean"" columns, the ""standard errors"" columns, as well as the ""worst"" columns)."
"Looking at the matrix, we can immediately verify the presence of multicollinearity between some of our variables. For instance, the **radius_mean** column has a correlation of 1 and 0.99 with **perimeter_mean** and **area_mean** columns, respectively. This is probably because the three columns essentially contain the same information, which is the physical size of the observation (the cell). Therefore we should only pick one of the three columns when we go into further analysis. \n\nAnother place where multicollienartiy is apparent is between the ""mean"" columns and the ""worst"" column. For instance, the **radius_mean** column has a correlation of 0.97 with the **radius_worst** column. In fact, each of the 10 key attributes display very high (from 0.7 up to 0.97) correlations between its ""mean"" and ""worst"" columns. This is somewhat inevitable, because the ""worst"" columns are essentially just a subset of the ""mean"" columns; the ""worst"" columns are also the ""mean"" of some values (the three largest values among all observations). Therefore, I think we should discard the ""worst"" columns from our analysis and only focus on the ""mean"" columns. \n\nIn short, we will drop all ""worst"" columns from our dataset, then pick only one of the three attributes that describe the size of cells. But which one should be pick?\n\nLet's quickly go back to 6th grade and review some geometry. If we think of a cell as roughly taking a form of a circle, then the formula for its radius is, well, its radius,  *r*. The formulae for its perimeter and area are then **\\(2\pi r\\) ** and **\\(\pi r^2\\) **, respectively. As we can see, a cell's **radius** is the basic building block of its size. Therefore, I think it is reasonable to choose **radius** as our attribute to represent the size of a cell. \n\nSimilarly, it seems like there is multicollinearity between the attributes **compactness**, **concavity**, and **concave points**. Just like what we did with the size attributes, we should pick only one of these three attributes that contain information on the shape of the cell. I think **compactness** is an attribute name that is straightforward, so I will remove the other two attributes. \n\nWe will now go head and drop all unnecessary columns. "
"Are we all set now?\n\nLet's take a look at the correlation matrix once again, this time created with our trimmed-down set of variables."
Looks great! Now let's move on to our model.
## Dependencies 
## Evaluation metric 
[Go to Contents Menu](#0.)\n\n# **Visualizer class** 
[Go to Contents Menu](#0.)\n\n# **'ObjectOrientedTitanic' class**  
- Let's import libraries
- Read csv and see the top 5 instances of the data
### **Univariate Analysis**
- Our features have very close to normal distribution. \n- Solids have slightly right skewness.
"As we have seen,\n\n* ph : 14.98 %\n* Sulfate : 23.84 %\n* Trihalomethanes : 4.94 %"
- Another method to show the missing values by using **[missingno library](https://github.com/ResidentMario/missingno)** \n
- Another method to show the missing values by using **[missingno library](https://github.com/ResidentMario/missingno)** \n
- **Missingno bar** gives us bar cahrt version of the completeness & nullity of the variables.
"------------------------------------------------------------------\n## What is Autoencoder?\n\n![](https://lilianweng.github.io/lil-log/assets/images/autoencoder-architecture.png)\n\nPicture Credit: https://lilianweng.github.io\n\nThe auto encoder learns to produce the same output as the input as much as possible. Through this learning process, the representation of the input can be effectively compressed in the latent space. In other words, it compresses the dimensions and stores the representation of the input in the latent space."
> This dataset contains the ECG readings of patients.\n> Each row corresponds to a single complete ECG of a patient. Every single ECG is composed of 140 data points(readings).\n>  \n> 1. Columns 0-139 contain the ECG data point for a particular patient. These are floating point numbers.\n> 2. The label which shows whether the ECG is normal or abnormal. It is a categorical variable with value either 0 or 1.
## Checking Target Imblance
**OK! Target distribution is balanced.**
Plot the normal ECG.
Plot the abnormal ECG.
Plot the abnormal ECG.
-----------------------------------\n# Modeling
"If the reconstruction error is greater than the threshold(blue-dotted line), classify the ECG as abnormal."
"Looking at the picture above, it seems that the threshold was set by considering the Normal and Anomaly distributions well. In particular, it seems that recall is more important than precision for abnomaly detection. In actual application, it may be necessary to consider fine-tuning the threshold."
Check the dataset for gaps in the data
There are no missing values!
Let's look at the distribution of the target variable
"We are dealing with an unbalanced sample, where the majority of people are healthy."
Let's analyze numerical variables.
"The analysis showed that only the BMI variable is close to the normal distribution, the rest are close to bimodal."
Let's look at the distribution of the number of people with heart disease from various factors
I propose to look at the distribution of categorical variables depending on gender
Let's carry out the Shapiro-Wilk test to check for the normality of the distribution of numerical variables.
"The test for normality has not been passed, therefore, to find the relationship of our variables, it is necessary to use non-parametric tests. Since we mostly have categorical variables, we use Pearson's Chi-square test to check."
"The purpose of the test is to determine if two variables are related to each other.\n\nNull hypothesis:\nWe start by defining the null hypothesis ( H0 ), which states that there is no relationship between the variables.\nAn alternative hypothesis could argue that there is a significant relationship between the two."
"In our dataset, there is a statistically significant relationship between the target variable and categorical features. This means that you should try to make a small prediction of the target feature."
# **Target variable distribution**
## **Numerical Feature Distribution**
## **Numerical Feature Distribution**
## **Categorical Feature Distribution**
## **Categorical Feature Distribution**
## **Correlation of Features**
## **Correlation of Features**
## **Logging Plots to Weights and Biases**
## **Logging Plots to Weights and Biases**
# **Preprocessing**
## How to compare Regression Models
![winner2.jpg](attachment:6803c413-577f-493f-886d-22179bb4b3e3.jpg)
## 3. 1 Import Libraries
## 3.2 Defining function for regression metrics
## 4.1 Avocado Prices
## 4.2 Boston House Prices
## 4.2 Boston House Prices
# 5. Data pre-processing
## Import Libraries
## Import the Data
Now we should create our own network and train it. First we'll want to define the criterion (something like nn.CrossEntropyLoss or nn.NLLLoss) and the optimizer (typically optim.SGD or optim.Adam).\n* Make a forward pass through the network\n* Use the network output to calculate the loss\n* Perform a backward pass through the network with loss.backward() to calculate the gradients\n* Take a step with the optimizer to update the weights
"Oh what just happended there? I'll explain not to worry.\n\n#### Training: \n* I'm looping over the train loader, pulling out the images and labels.\n* Note that I have a line of code optimizer.zero_grad(). When you do multiple backwards passes with the same parameters, the gradients are accumulated. This means that you need to zero the gradients on each training pass or you'll retain gradients from previous training batches.\n* I have named the next variable log_ps because our model gives us back logs of class probabilities, you can take exponent to convert it to normal probabilities which I've done down below for validation. \n* We calculate the loss. Then backpropagate through the network. We then make one optimizer step. Which brings us closer and closer to the global optimum.\n\n#### Validation\n* We turn off the gradients for validation as it is not needed and saves a lot of memory and computation. Note that we should turn it back on after each step of validation.\n* We loop over the test_loader and essentially repeat some steps we have done above. Since it's validation we don't need to backpropagate. \n* The next step - With the probabilities, we can get the most likely class using the ps.topk method. This returns the $k$ highest values. Since we just want the most likely class, we can use ps.topk(1). This returns a tuple of the top-$k$ values and the top-$k$ indices. If the highest value is the fifth element, we'll get back 4 as the index.\n* Then we check if the predicted value is equal to the actual value. \n* We then calculate the percentage of correct predictions, which indeed is using the mean of our top predictions. But you cannot just use torch.mean because topk returns a byte tensor but we need a float tensor to perform torch.mean we do that in the next step.\n\nThe same process is repeated over and over again. The results are printed on each step. With this simple model we're able to get about 98% accuracy on validation which is awesome, isn't it?\n\nHope that made sense. "
## Visualizing Model Performance
This graph looks decent to me. We're doing fairly well for our first model in PyTorch.
"# About the dataset\n\nContext\nOur world population is expected to grow from 7.3 billion today to 9.7 billion in the year 2050. Finding solutions for feeding the growing world population has become a hot topic for food and agriculture organizations, entrepreneurs and philanthropists. These solutions range from changing the way we grow our food to changing the way we eat. To make things harder, the world's climate is changing and it is both affecting and affected by the way we grow our food â€“ agriculture. This dataset provides an insight on our worldwide food production - focusing on a comparison between food produced for human consumption and feed produced for animals.\n\nContent\nThe Food and Agriculture Organization of the United Nations provides free access to food and agriculture data for over 245 countries and territories, from the year 1961 to the most recent update (depends on the dataset). One dataset from the FAO's database is the Food Balance Sheets. It presents a comprehensive picture of the pattern of a country's food supply during a specified reference period, the last time an update was loaded to the FAO database was in 2013. The food balance sheet shows for each food item the sources of supply and its utilization. This chunk of the dataset is focused on two utilizations of each food item available:\n\nFood - refers to the total amount of the food item available as human food during the reference period.\nFeed - refers to the quantity of the food item available for feeding to the livestock and poultry during the reference period.\nDataset's attributes:\n\nArea code - Country name abbreviation\nArea - County name\nItem - Food item\nElement - Food or Feed\nLatitude - geographic coordinate that specifies the northâ€“south position of a point on the Earth's surface\nLongitude - geographic coordinate that specifies the east-west position of a point on the Earth's surface\nProduction per year - Amount of food item produced in 1000 tonnes\n\nThis is a simple exploratory notebook that heavily expolits pandas and seaborn"
Let's see what the data looks like...
# Plot for annual produce of different countries with quantity in y-axis and years in x-axis
"Clearly, China, India and US stand out here. So, these are the countries with most food and feed production.\n\nNow, let's have a close look at their food and feed data\n\n# Food and feed plot for the whole dataset"
"Clearly, China, India and US stand out here. So, these are the countries with most food and feed production.\n\nNow, let's have a close look at their food and feed data\n\n# Food and feed plot for the whole dataset"
"So, there is a huge difference in food and feed production. Now, we have obvious assumptions about the following plots after looking at this huge difference.\n\n# Food and feed plot for the largest producers(India, USA, China)"
"So, there is a huge difference in food and feed production. Now, we have obvious assumptions about the following plots after looking at this huge difference.\n\n# Food and feed plot for the largest producers(India, USA, China)"
"Though, there is a huge difference between feed and food production, these countries' total production and their ranks depend on feed production."
"So, cereals, fruits and maize are the most produced items in the last 50 years\n\n# Food and feed plot for most produced items "
"# Now, we plot a heatmap of correlation of produce in difference years"
"# Now, we plot a heatmap of correlation of produce in difference years"
"So, we gather that a given year's production is more similar to its immediate previous and immediate following years."
"So, we gather that a given year's production is more similar to its immediate previous and immediate following years."
# Heatmap of production of food items over years\n\nThis will detect the items whose production has drastically increased over the years
# Heatmap of production of food items over years\n\nThis will detect the items whose production has drastically increased over the years
"There is considerable growth in production of Palmkernel oil, Meat/Aquatic animals, ricebran oil, cottonseed, seafood, offals, roots, poultry meat, mutton, bear, cocoa, coffee and soyabean oil.\nThere has been exceptional growth in production of onions, cream, sugar crops, treenuts, butter/ghee and to some extent starchy roots."
"# Elbow method to select number of clusters\nThis method looks at the percentage of variance explained as a function of the number of clusters: One should choose a number of clusters so that adding another cluster doesn't give much better modeling of the data. More precisely, if one plots the percentage of variance explained by the clusters against the number of clusters, the first clusters will add much information (explain a lot of variance), but at some point the marginal gain will drop, giving an angle in the graph. The number of clusters is chosen at this point, hence the ""elbow criterion"". This ""elbow"" cannot always be unambiguously identified. Percentage of variance explained is the ratio of the between-group variance to the total variance, also known as an F-test. A slight variation of this method plots the curvature of the within group variance.\n# Basically, number of clusters = the x-axis value of the point that is the corner of the ""elbow""(the plot looks often looks like an elbow)"
"As the elbow corner coincides with x=2, we will have to form **2 clusters**. Personally, I would have liked to select 3 to 4 clusters. But trust me, only selecting 2 clusters can lead to best results.\nNow, we apply k-means algorithm."
"Now, let's visualize the results."
"So, the blue cluster represents China(Mainland), USA and India while the red cluster represents all the other countries.\nThis result was highly probable. Just take a look at the plot of cell 3 above. See how China, USA and India stand out. That has been observed here in clustering too.\n\nYou should try this algorithm for 3 or 4 clusters. Looking at the distribution, you will realise why 2 clusters is the best choice for the given data"
"\n\nWeights & Biases (W&B) is a set of machine learning tools that helps you build better models faster. **Kaggle competitions require fast-paced model development and evaluation**. There are a lot of components: exploring the training data, training different models, combining trained models in different combinations (ensembling), and so on.\n\n> â³ Lots of components = lots of places to go wrong = lots of time spent debugging \n\nW&B can be useful for Kaggle competition with it's lightweight and interoperable tools:\n\n* quickly track experiments,\n* version and iterate on datasets, \n* evaluate model performance,\n* reproduce models,\n* visualize results and spot regressions,\n* and share findings with colleagues.\n\nTo learn more about Weights and Biases check out this [kernel](https://www.kaggle.com/ayuraj/experiment-tracking-with-weights-and-biases)."
"> You can learn more about using W&B in this introduction kernel, [Experiment Tracking with Weights and Biases](https://www.kaggle.com/ayuraj/experiment-tracking-with-weights-and-biases)."
### Let's quickly test the function to read the DICOM file.
"> Each MRI image has a resolution of 512x512 pixels. \n\n> There are some images without any useful content, i.e, there's no brainy stuff in the image."
Let's look at gaps
"Great, no missing information!"
Let's look at the quantitative distribution of the target variable
Lets look at continuous columns
Lets look at continuous columns
\nSTATISTICAL TESTS
# Exploratory Data Analisys\nIn the following we're gonna see some data analysis on the corpus. \n\nSpecifically:\n- General dataset infos\n    - Number of samples\n    - Class Label Distributiom\n- Text analysis (Done both on 'text' and 'selected_text' for trainin and on 'text' for the test set)\n    - Number of characters in tweets \n    - Number of words in a tweet\n    - Average word lenght in a tweet\n    - Word distribution\n    - Number of unique words\n    - Top Bi-grams and Tri-grams\n    \n
"## Insights\n\nFrom the above analysis, we can say the following:\n1. Both training and test set have a majority of **netrual samples** and a (more or less) equal amount of positive and negative samples. The sentiment distribution of the test set follows the one of the training set.\n2. As expected, positive words like ""good"", ""love"", ""happy"" are in the top of the **most frequent words** for both text and selected_text for the positive class, also the test set seems to follow the same line.\n3. **Negative and Neutral posts** seems to be pretty **high corelated** w.r.t to word, bi-gram and tri-gram analysis. \n4. The **selected_text** seems to have a very good bi-gram, tri-gram **corelation** w.r.t the **text**. Maybe a simple approach based on these features can work.\n5. Neutral posts seem to be longer."
Evolution of Mobile Phone\n![](https://s3b.cashify.in/blog/wp-content/uploads/2018/05/mobile-phone-evolution.jpg)
"In this kernel, I analyzed mobile phone features with price range. Also I build a model using support vector machine algortihm.\n\n### CONTENTS:\n\n1. [Read Data](#1)\n\n\n2. [Data Analysis](#2):\n     2. 1.  [Data Visualization](#3) :\n            * [PRICE RANGE AND RAM](#4)\n            * [BATTERY POWER- RAM AND PRICE RANGE](#5)\n            * [INTERNAL MEMORY IN GIGABYTE - RAM AND PRICE RANGE](#6)\n            * [TOUCH SCREEN-RAM AND PRICE RANGE](#7)\n            * [3G-RAM and Price Range](#8)\n            * [4G-RAM and Price Range](#9)\n            * [NUMBER OF CORES OF PROCESOR(n_cores)- RAM and PRICE RANGE](#10)\n            * [Primary Camera mega pixels(pc) - RAM and Price Range](#11)\n            * [Front Camera mega pixels(fc) - RAM and Price Range](#12)\n            * [Weight of mobile phone(mobile_wt)- RAM and Price Range](#13)\n            * [Mobile Depth in cm (m_dep), Pixel Resolution Height(px_height), Pixel Resolution Width(px_width), Clock Speed(clock_speed) , RAM and Price Range](#14)\n            * [Longest time that a single battery charge will last when you are(talk_time),RAM and Price Range](#15)\n            * [WIFI-RAM and PRICE RANGE](#16)\n            * [DUAL SIM- RAM AND PRICE RANGE](#17)\n            * [Screen Height of mobile(sc_h) AND Screen Width of mobile(sc_w)- RAM and Price Range](#18)\n            \n\n3. [Support Vector Machine Algorithm](#19):\n     * [Class Balance Visualization](#20)\n     * [First Model](#21)\n     * [Feature Selection](#22)\n     * [Model with GridSearchCV](#23)\n     * [CV SCORES](#24)\n     * [Building Model](#25)\n     \n4. [Prediction Visualization](#26)"
"In the following table, first 10 entries are shown. "
Now let's check if there are any missing values in the dataset.
Now let's check if there are any missing values in the dataset.
 \n## 2- DATA ANALYSIS 
Following heatmap shows correlation values between features.
"As we can see our target price range has highly positive correlation between ram. \n\nAlso \n\n* 3G and 4G\n* pc(Primary Camera mega pixels) and fc(Front Camera mega pixels)\n* px_weight(Pixel Resolution Width) and px_height(Pixel Resolution Height)\n* sc_w(Screen Width of mobile in cm) and sc_h(Screen Height of mobile in cm)\n\nfeatures have highly positive correlation. For example 	as long as sc_w (screen width of mobile) increase, sc_h(screen height of mobile) is increasing.\n"
"Following scatter stripplot shows touch screen or not and ram values according to price range.\n\n0= not touch screen, \n1= has touch screen"
 \n#### 5- 3G-RAM and Price Range
"Following boxplots show 3G or not and ram values according to price range.\n\n0= not 3G, \n1= has 3G"
 \n#### 6- 4G-RAM- PRICE RANGE
"Following boxplots show 4G or not and ram values according to price range.\n\n0= not 4G, \n1= has 4G"
 \n#### 7- NUMBER OF CORES OF PROCESOR(n_cores)- RAM and PRICE RANGE
"Following violin plots shows wifi or not and ram values according to price range.\n\n0- has not wifi, \n1- has wifi"
 \n#### 14- DUAL SIM- RAM AND PRICE RANGE
"Following distibution plots show dual sim or not and ram values according to price range.\n\n0- has not dual , \n1- has wifi"
 \n#### 15- Screen Height of mobile(sc_h) AND Screen Width of mobile(sc_w)- RAM and Price Range
I used f_classif method to determine best features
"In the graph above, we can see 4 or 5 features gives higher train accuracies. "
**StratifiedKFold:** Provides train/test indices to split data in train/test sets.\n\nThis cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class.\n\nFollowing barchart shows Cross Validation Scores for SVC with 10 fold. And average mean score is 0.957\n
 \n#### BUILDING MODEL
"Kaggle offers very nice interface to look at the data. Basically what we have here is:\n\n\nsurvival Survival 0 = No, 1 = Yes\n\npclass: Ticket class 1 = 1st, 2 = 2nd, 3 = 3rd\n\nsex: Male or Female\n\nAge: Age in years\n\nsibsp: # of siblings / spouses aboard the Titanic\n\nparch: # of parents / children aboard the Titanic\n\nticket: Ticket number\n\nfare: Passenger fare\n\ncabin: Cabin number\n\nembarked: Port of Embarkation C = Cherbourg, Q = Queenstown, S = Southampton\n\n**Missing Values**\nLet us define a function that will find the missing values and also plot the frequency of them. With this information we can impute the values accordingly.  OR as in the case of Cabin feature delete it, since imputing does not make much sense.\n"
"There is a possibility that test and train datasets do not have sama NaN values, especially if they are small. Let us check:\n"
"**Data imputation**\n\nFirst of all we need to think is it instincitive to do it? For example Cabin variable should be excluded. \nAge variable can be imputed, but which value? First things that come to mind are mean and median imputation. But one should note that they are not automatically correct and that imputing with these two method can lead to more damage than leaving the data untouched. We are going to focus on these two simple method of imputing numerical data, but for more on imputation please consult [](https://en.wikipedia.org/wiki/Imputation_(statistics))\n\n\nChoosing between mean or median imputation we should look for outliers. If there are signs of them then median is the choice since mean will be heavily affected by them. Depending on the context (and that is general rule!) mean could also be unfavorable because is produces(possibly) value in data set that did not exist beforehand. Let us plot our Age variable"
"There are couple of outliers here, we can procede bothways. Here we can learn another lesson regarding modeling. When in doubt try both methods and see what works better.\n"
Before proceeding let us have a look at how our variables interact with each other. (not the correlation/heat matrix)\n
"We are slowly getting our dataset ready. What variables are useless, drop them! and can we create new features?\n"
"One last thing before we start with modeling. We have a lot of variables, but question is, is there a lot of correlation between them. Because if there is some of them are redundant, i.e. using onge of them is waste of resources. (Highly positive or negative correlated).  In the following we are going to show-case heat-map of correlation, to get a view of the correlation. Pearson correlation gives quantifies the relationships.\n"
One possibility is to pair all of the varibales that have coefficient larger or smaller than 0.5 and -0.5 respectively. To keep things simple we are going to skip this step.
Let's check the data for information gaps.
there are big omissions of information in age as well as cabin number. Minor in the port of loading.
there are big omissions of information in age as well as cabin number. Minor in the port of loading.
"Large omissions of information were found in the age, as well as the cabin number."
"Hypotheses:\n* The distribution of age is close to normal, the value of ticket prices has a long ""tail"".\n* Boxplots of both features indicate the presence of possible outliers for these variables (separate patterns in the data are possible)."
Let's visualize categorical variables depending on the survival of passengers.
"We use statistical tests to test hypotheses. First, let's test the hypotheses about the normality of the distribution of numerical variables."
"The hypotheses about the normal distribution were confirmed. For the effect of these variables on the target, 2 different tests should be used. For the Fare variable, we use the Mann-Whitney U-test, for the Age variable, we use the Student's T-test."
"Next, we will conduct statistical tests of the influence of categorical variables on the target categorical variable. To do this, we use the Chi-square-Pearson test."
All variables have an effect on the target variable. You need to add them to the forecast building model. Gender and ticket class have the most influence.
\n\n\n\n0Â Â IMPORTSÂ Â Â Â â¤’
\n\n\n\n1Â Â BACKGROUND INFORMATIONÂ Â Â Â â¤’\n\n---\n
\n\n\n\n\n\n\n    3Â Â HELPER FUNCTION & CLASSESÂ Â Â Â â¤’\n\n\n---
"**The following functions are for feature identification, matrix manipulations, etc**"
"**The following functions are for feature identification, matrix manipulations, etc**"
\n\n\n\n\n\n\n    4Â Â DATASET EXPLORATION & PREPROCESSINGÂ Â Â Â â¤’\n\n\n---\n\nLet's investigate the provided information and then plot some images for various things
"\n\n4.4 LET'S LOOK INTO THE CAMERA EXTRINSICS\n\n---\n\nREFERENCE --> https://ksimek.github.io/2012/08/22/extrinsic/\n \nAs discussed previously, the camera's **extrinsic matrix** describes the **camera's location in the world (3D), and what direction it's pointing (orientation)**. It has two components: \n* **a rotation matrix, $\mathbf{R}$ (or $\theta$)**\n* **a translation vector $\mathbf{t}$**\n\nHowever, shocker, these don't exactly correspond to the camera's rotation and translation. \n\nThe extrinsic matrix takes the form of a rigid transformation matrix: \n* a $3\times 3$ rotation matrix in the left-block\n* a $3\times 1$ translation column-vector in the right\n\n$$\n\left [\n    \begin{array}{c|c}\nR & t\n    \end{array}\n\right] = \n\left [\n    \begin{array}{ccc|c}\nr_{1,1} & r_{1,2} & r_{1,3} & t_1 \\\nr_{2,1} & r_{2,2} & r_{2,3} & t_2 \\\nr_{3,1} & r_{3,2} & r_{3,3} & t_3\n    \end{array}\n\right]\n$$\n\n\n\nIt's common to see a version of this matrix with extra row of $(0,0,0,1)$ added to the bottom. This makes the matrix square, which allows us to further decompose this matrix into a rotation followed by translation\n\n\n\n$$\n\begin{align}\n    \left [\n        \begin{array}{c|c} \n            R & \boldsymbol{t} \\\n            \hline\n            \boldsymbol{0} & 1 \n        \end{array}\n    \right ] &= \n    \left [\n        \begin{array}{c|c} \n            I & \boldsymbol{t} \\\n            \hline\n            \boldsymbol{0} & 1 \n        \end{array}\n    \right ] \n    \times\n    \left [\n        \begin{array}{c|c} \n            R & \boldsymbol{0} \\\n            \hline\n            \boldsymbol{0} & 1 \n        \end{array}\n    \right ] \\\n        &=\n\left[ \begin{array}{ccc|c} \n1 & 0 & 0 & t_1 \\\n0 & 1 & 0 & t_2 \\\n0 & 0 & 1 & t_3 \\\n  \hline\n0 & 0 & 0 & 1\n\end{array} \right] \times\n\left[ \begin{array}{ccc|c} \nr_{1,1} & r_{1,2} & r_{1,3} & 0  \\\nr_{2,1} & r_{2,2} & r_{2,3} & 0 \\\nr_{3,1} & r_{3,2} & r_{3,3} & 0 \\\n  \hline\n0 & 0 & 0 & 1\n\end{array} \right] \n\end{align}\n$$\n\n\n\nThis matrix describes how to transform points in world coordinates to camera coordinates. \n* The vector $t$ can be interpreted as the position of the world origin in camera coordinates\n* The columns of $R$ represent represent the directions of the world-axes in camera coordinates.\n\n**The important thing to remember about the extrinsic matrix is that it describes how the world is transformed relative to the camera.** This is often counter-intuitive, because we usually want to specify how the camera is transformed relative to the world. \n\n---\n\nLet's briefly look into each matrix type to get a grasp of what is happening in each in laymens terms**\n\nIn 3D space, rotation can occur about the x, y, or z-axis. Such a type of rotation that occurs about any one of the axis is known as a basic or elementary rotation. Given below are the rotation matrices that can rotate a vector through an angle about any particular axis.\n\n\n\n$$\n\begin{align}\n    R_x = \left [\n        \begin{array}{ccc} \n            1 & 0 & 0 \\\n            0 & \cos(\boldsymbol{\gamma}) & -\sin(\boldsymbol{\gamma}) \\\n            0 & \sin(\boldsymbol{\gamma}) & \cos(\boldsymbol{\gamma}) \n        \end{array}\n    \right ] \n\end{align} \qquad\text{This is also known as ROLL}\quad\text{(CC-rotation of }\gamma\text{ about the x-axis)}\n$$ \n\n\n\n$$\n\begin{align}\n    R_y = \left [\n        \begin{array}{ccc} \n            \cos(\boldsymbol{\beta}) & 0 & \sin(\boldsymbol{\beta}) \\\n            0 & 1 & 0 \\\n            -\sin(\boldsymbol{\beta}) & 0 & \cos(\boldsymbol{\beta})\n        \end{array}\n    \right ]\n\end{align} \qquad\text{This is also known as PITCH}\quad\text{(CC-rotation of }\beta\text{ about the y-axis)}\n$$\n\n\n\n$$\n\begin{align}\n    R_z = \left [\n        \begin{array}{ccc} \n            \cos(\boldsymbol{\alpha}) & -\sin(\boldsymbol{\alpha}) & 0 \\\n            \sin(\boldsymbol{\alpha}) & \cos(\boldsymbol{\alpha}) & 0 \\\n            0 & 0 & 1 \n        \end{array}\n    \right ]\n\end{align} \qquad\text{This is also known as YAW}\quad\text{(CC-rotation of }\alpha\text{ about the z-axis)}\n$$"
\n\n4.5 LOOK AT A SINGLE EXAMPLE W/ COVISIBILITY & CALIBRATION INFORMATION\n\n---\n
"# Visualize the data\n\nNow that our data has been easily loaded in, the next step is to visualize our images. This helps us understand what is being used as an input for our model. It also serves as a check to see if our images have been loaded in correctly."
"# Feature Engineering\n\nBecause we are working with categorical and noncontinuous data, we want to convert our model into one-hot encodings. One-hot encodings are a way for the model to understand that we're looking at categorial instead of continuous data. Transforming features so that they'll be more understandable is called feature engineering. Learn more about feature engineering [here](https://developers.google.com/machine-learning/crash-course/representation/feature-engineering)."
"# Visualize Model Metrics\n\nLet's graph the ROC AUC metric and loss after each epoch for the training and validation data. Although we didn't use a random seed for our notebook, the results may slightly vary, generally the scores for the validataion data is similar, if not better, than the training dataset."
"# Evaluate the Model\n\nAlthough we used the validatation dataset to continually evaluate the model, we also have a separate testing dataset. Let's prepare the testing dataset."
# 2. | Importing Libraries ðŸ“š\n\n    ðŸ‘‰ Importing libraries that will be used in this notebook.\n
# 3. | Color Palletes ðŸŽ¨\n\n    ðŸ‘‰ This section will create some color palettes that will be used in this notebook.\n
# 3. | Color Palletes ðŸŽ¨\n\n    ðŸ‘‰ This section will create some color palettes that will be used in this notebook.\n
"# 4. | Reading Dataset ðŸ‘“\n\n    ðŸ‘‰ After importing libraries, the dataset that will be used will be imported.\n"
## 5.1 | Categorical Variable ðŸ” \n\n    ðŸ‘‰ The first type of variable that will be explored is categorical variable.\n\n
## 5.2 | Continuous Variable ðŸ”¢\n\n    ðŸ‘‰ The second type of variable that will be explored is continuous variable.\n\n
#### 5.2.2.1 | Sepal Length (cm)
"\n    ðŸ‘‰ From the histogram and boxenplot, it can be seen that this column is normally distributed. This also proven by skewness value (0.32) of this column.\n    ðŸ‘‰ In this column, the kurtosis value is -0.55, which indicates that the column is platikurtic.\n    ðŸ‘‰ From the Q-Q plot, the data values tend to closely follow the 45-degree, which means the data is likely normally distributed (as stated previously).\n    ðŸ“Œ If skewness is less than -1 or greater than 1, the distribution is highly skewed. If skewness is between -1 and -0.5 or between 0.5 and 1, the distribution is moderately skewed. If skewness is between -0.5 and 0.5, the distribution is approximately symmetric.\n    \n    ðŸ“Œ Kurtosis values used to show tailedness of a column. The value of normal distribution (mesokurtotic) should be equal to 3. If kurtosis value is more than 3, it is called leptokurtic. Meanwhile, if kurtosis value is less than 3, then it is called platikurtic.\n"
#### 5.2.2.2 | Sepal Width (cm)
"\n    ðŸ‘‰ From the histogram and boxenplot, it can be seen that this column is normally distributed. This also proven by skewness value (0.33) of this column.\n    ðŸ‘‰ It also can be seen that the median value is closer too Q1 (can be seen from the boxen plot).\n    ðŸ‘‰ In this column, the kurtosis value is 0.29, which indicates that the column is platikurtic.\n    ðŸ‘‰ From the Q-Q plot, the data values tend to closely follow the 45-degree, which means the data is likely normally distributed (as stated previously).\n"
#### 5.2.2.3 | Petal Length (cm)
"\n    ðŸ‘‰ From the histogram and boxenplot, it can be seen that this column is normally distributed. This also proven by skewness value (-1.4) of this column.\n    ðŸ‘‰ It also can be seen that the median value is closer too Q3 (can be seen from the boxen plot).\n    ðŸ‘‰ In this column, the kurtosis value is -1.4, which indicates that the column is platikurtic.\n    ðŸ‘‰ There is a gap between 45-degree line with the upper and bottom part of Q-Q plot.\n"
#### 5.2.2.4 | Petal Width (cm)
"\n    ðŸ‘‰ From the histogram and boxenplot, it can be seen that this column is normally distributed. This also proven by skewness value (-0.1) of this column.\n    ðŸ‘‰ It also can be seen that the median value is closer too Q3 (can be seen from the boxen plot).\n    ðŸ‘‰ In this column, the kurtosis value is -1.3, which indicates that the column is platikurtic.\n    ðŸ‘‰ There is a gap between 45-degree line with the upper and bottom part of Q-Q plot.\n"
## 6.1 | Pairplot of Numerical Variables ðŸ“\n
## 6.2 | Jointplot between Sepal Length and Sepal Width ðŸŒ¹\n
## 6.2 | Jointplot between Sepal Length and Sepal Width ðŸŒ¹\n
## 6.3 | Distribution of Numerical Distributions ðŸ“Š\n
## 6.3 | Distribution of Numerical Distributions ðŸ“Š\n
\n    ðŸ‘‰ Iris-setosa is having smaller feature and less distributed. Iris-versicolor is average distributed and has average features. Iris-virginica is highly distributed with large number of values and features.\n
## 6.4 | Heatmap ðŸ”¥\n
# 7. | Dataset Pre-processing âš™\n\n    ðŸ‘‰ This section will prepare the dataset before building the machine learning models.\n
\n\n## 2| IMPORT NECESSARY LIBRARIES
\n\n## 3|LOAD DATASET
\n\n#### 5.1 | Catplot and Barplot
"\n\nChart report  \n    \n**From this graph, it is clear that the number of men in the ""sex"" variable in the dataset is much more than the number of women**"
"\n\nChart report  \n    \n**From this graph, it is clear that the number of men in the ""sex"" variable in the dataset is much more than the number of women**"
\n\nChart report  \n    \n\n\n**It is clear from this graph that the presence of anemia is less than the absence**
\n\nChart report  \n    \n\n\n**It is clear from this graph that the presence of anemia is less than the absence**
"\n\nChart report  \n    \n\n\n**Looking at the values in the graph, it is known that the risk of death of men due to heart attack is 2 times higher than that of women.**"
"\n\nChart report  \n    \n\n\n**Looking at the values in the graph, it is known that the risk of death of men due to heart attack is 2 times higher than that of women.**"
"\n\nChart report  \n    \n\n\n**Looking at the values in the graph, it becomes clear that the mortality rate of smokers is much lower than that of non-smokers. But here you need to be careful. Because it is known to everyone that under normal circumstances, the death rate of people who smoke is high. The problem here is with our dataset. Because the number of smokers in our dataset is high and the number of deaths is low. Hence the inconsistency.**"
\n\n#### 5.2 |Distplot
"\n\nChart report  \n    \n\n\n**From the first graph, we can observe that the values of the ""age"" variable are mostly distributed as values of 50, 60 and 65 - 70. In addition, there are many peaks in the distribution graph of the ""age"" variable. This is an indicator that the values of the variable are not normally distributed. Looking at the second graph, we can observe that the values of the ""platelets"" variable are mainly distributed between 200000 and 400000. At the same time, the graph does not have many peaks, skewness and kurtosis are low. It can be said that the values of this variable are normally distributed.**"
"\n\nChart report  \n    \n\n\n**From the first graph, we can observe that the values of the ""age"" variable are mostly distributed as values of 50, 60 and 65 - 70. In addition, there are many peaks in the distribution graph of the ""age"" variable. This is an indicator that the values of the variable are not normally distributed. Looking at the second graph, we can observe that the values of the ""platelets"" variable are mainly distributed between 200000 and 400000. At the same time, the graph does not have many peaks, skewness and kurtosis are low. It can be said that the values of this variable are normally distributed.**"
\n\n#### 5.3 |Boxplot
\n\n#### 5.3 |Boxplot
\n\n#### 5.4 |Pairplot
\n\n#### 5.4 |Pairplot
"\n\nChart report  \n    \n\n\n**From the graph above, we can observe whether there is a correlation between the numerical variables, as well as the distribution of those variables**"
\n\n#### 5.5 |Scatterplot
\n\nChart report  \n    \n\n**As it can be seen there is not any correlation between the variables**
\n\nChart report  \n    \n\n**As it can be seen there is not any correlation between the variables**
"\n\nChart report  \n    \n\n**When we look at the graphs, we do not see a significant difference between the classes of the 'sex' variable according to the variables mentioned.**"
"\n\nChart report  \n    \n\n**When we look at the graphs, we do not see a significant difference between the classes of the 'sex' variable according to the variables mentioned.**"
"\n\nChart report  \n\n    \n**When we look at the graphs, we do not see a significant difference between the classes of the 'diabetes' variable according to the variables mentioned.**"
"\n\nChart report  \n\n    \n**When we look at the graphs, we do not see a significant difference between the classes of the 'diabetes' variable according to the variables mentioned.**"
\n\n#### 5.6 |Smooth Kernel Density with Marginal Histograms
\n\n#### 5.7 |Regplot and heatmap
"\n\nChart report  \n\n**From the graphs here, we do not observe any correlation between the variables**"
"\n\nChart report  \n\n**From the graphs here, we do not observe any correlation between the variables**"
"\n\nChart report  \n\n**From the heatmap above, we do not observe any correlation between the variables**"
\n\n#### 5.8 |Lmplot
\n\n#### 5.9 |Hexagonal binning plot
"**A hexagonal bin plot is created by covering the data range with a regular array of hexagons and coloring each hexagon according to the number of observations it covers. As with all bin plots, the hex-binned plots are good for visualizing large data sets for which a scatter plot would suffer from overplotting.**\n\n**Reference:** https://blogs.sas.com/content/iml/2014/09/02/hexagonal-bin-plot.html#:~:text=A%20hexagonal%20bin%20plot%20is,plot%20would%20suffer%20from%20overplotting."
\n\n#### 5.10 |2D Histogram
\n\n#### 5.11 |Pandas Crosstab
\n\n#### 5.12 |Lineplot
\n\n#### 5.12 |Lineplot
\n\n## 6|DATASET PREPROCESSING
\n\n#### 7.6 |Importance Levels of the Variables
\n\n#### 7.7 |Classification Report
\n\n#### 7.8 |ROC AUC - Random Forests
\n\n#### 7.9 |Confusion Matrix Visualization
\n\n#### 7.12 |ROC AUC - Logistic Regression
![](http://t3.ftcdn.net/jpg/02/92/36/76/360_F_292367633_TwzHFo2XLSSbcihgxESm2sKcQ0NlwrIG.jpg)
### 0.0 Load modules
### 0.1 Load data
"I notice that Statten Island and the Bronx are highly underrepresented in this dataset. For Statten Island, the reason is that the population of the island is small. However, this can't be the case for the Bronx which has a population comparable (~1.4mln) to Manhattan (~1.6mln) or for for Brooklyn /Queens with their populations of ~2.5mln and ~2.4mln, respectively. \n\nThis makes sense: Queens, the Bronx  and, to a fair extent Brooklyn, are residential neighborhoods unlike Manhattan which is a business centre as well as a tourist destination."
### 1.4.3 Longitude and latitude
Longitude and latitude are somewhat correlated with each other. This is because the locations of properties tend to come from clusters.
### 1.4.4 Room type
"As far as room types, this dataset is balanced away from 'Shared room' properties. The proportions of private room and entire home/apt rentals are close, with entire home/apt dominating prive room by <10%."
### 1.4.5 Minimum nights
### 1.4.6 Reviews per month
"The distribution of the number of reviews per month is highly skewed however way we cut it. This is because there is a large weight on small numbers: there are a lot of properties which only get a few reviews and a rather fat tail of properties which get a lot of reviews. \n\nOne explanation would be that the properties which are available a larger fraction of the year get more reviews. However, a scatter plot of reviews_per_month and availability_365 variables shows no evidence of a relationship so that explanation would appear to not be valid."
"This distribution is highly skewed towards the low and high end. The dataset contains a hiuge number of properties that are available only for a couple of days each year, and a decent number that are available for > 300 days. "
### Feature engineering
### 1.5.0 Pearson correlation matrix
"There don't appear to exist obvious, strong correlations between these variables. \n\nHowever, the number of reviews per month is fairly (40%) correlated with the total number of reviews and the the total number of reviews is correlated (at 30%) with the availability of the property. Both of these correlations make sense.\n\nIt's also interesting that the longitude is anticorrelated (at 20%) with the price. That also makes sense - property in the Bronx and in Queens is cheaper than Manhattan and Brooklyn."
### 1.5.1 PairPlot
## 1.6 Encoding categorical features
### 1. Color Palettes
### 2.Look at how these colors look on a plot
### 2.Look at how these colors look on a plot
### 3. Change the color palette
### 3. Change the color palette
### 4. Impact on the plot
### 4. Impact on the plot
### 5. seaborn palettes
### 5. seaborn palettes
### 6. matplotlib colormaps as color palettes
### 6. matplotlib colormaps as color palettes
### 7. Let's set the palette to a matplotlib colormap
### 7. Let's set the palette to a matplotlib colormap
### 8. Impact on the plot
### 8. Impact on the plot
### 9. Building custom color palettes
### 9. Building custom color palettes
### 10. Let's see how the plot has changed
### 10. Let's see how the plot has changed
"# 22.Controlling plot aesthetics \n---\n[**Go To TOP**](#00)\n\n![](https://tgmstat.files.wordpress.com/2013/11/tips1.png)\n\n### In this section, you can learn\n\n1. First plot with seaborn\n2. Changing the plot style with set_style\n	1. Set plot background to a white grid\n	1. Set the plot background to dark\n	1. Set the background to white\n	1. Adding 'ticks\n3. Customizing the styles\n	1. Style parameters\n4. Plotting Context Presets\n	1. Plotting Context Preset - paper\n	1. Plotting Preset - talk\n	1. Plotting Preset - poster"
### 2. Changing the plot style with set_style\n#### 1. Set plot background to a white grid
#### 2. Set the plot background to dark
#### 2. Set the plot background to dark
#### 3.Set the background to white
#### 3.Set the background to white
#### 4.Adding 'ticks
#### 4.Adding 'ticks
### 3.Customizing the styles\n#### 1.Style parameters
### 4.Plotting Context Presets\n#### 1.Plotting Context Preset - paper
#### 2.Plotting Preset - talk
#### 2.Plotting Preset - talk
#### 3.Plotting Preset - poster
#### 3.Plotting Preset - poster
"# 23.Plotting categorical data \n---\n[**Go To TOP**](#00)\n\n![](https://i.stack.imgur.com/IsxzL.png)\n\n### In this section, you can learn\n\n1. Scatterplots\n2. Swarmplot\n3. Boxplot\n4. Violinplot\n5. Barplot\n6. Countplot\n7. Wide form plots"
#### 2.Swarmplot
#### 3.Boxplot
#### 3.Boxplot
#### 4.Violinplot
#### 4.Violinplot
#### 5.Barplot
#### 5.Barplot
#### 6.Count Plot
#### 6.Count Plot
#### 7.Wide form plot
### 3.Plot it with PairGrid()
### 4.Plot it with PairGrid()
### 4.Plot it with PairGrid()
### Thanks for Reading this notebook...ðŸ™...ðŸ™...ðŸ™!!!
## Loading Modules
## Loading Datasets\n\nLoading train and test dataset
"### Pclass, Sex & Embarked vs. Survival"
"From the above plot, it can be seen that:\n- Almost all females from Pclass 1 and 2 survived.\n- Females dying were mostly from 3rd Pclass.\n- Males from Pclass 1 only have slightly higher survival chance than Pclass 2 and 3."
### Age vs. Survival
"From *Pclass* violinplot, we can see that:\n- 1st Pclass has very few children as compared to other two classes.\n- 1st Plcass has more old people as compared to other two classes.\n- Almost all children (between age 0 to 10) of 2nd Pclass survived.\n- Most children of 3rd Pclass survived.\n- Younger people of 1st Pclass survived as compared to its older people.\n\nFrom *Sex* violinplot, we can see that:\n- Most male children (between age 0 to 14) survived.\n- Females with age between 18 to 40 have better survival chance."
"From *Pclass* violinplot, we can see that:\n- 1st Pclass has very few children as compared to other two classes.\n- 1st Plcass has more old people as compared to other two classes.\n- Almost all children (between age 0 to 10) of 2nd Pclass survived.\n- Most children of 3rd Pclass survived.\n- Younger people of 1st Pclass survived as compared to its older people.\n\nFrom *Sex* violinplot, we can see that:\n- Most male children (between age 0 to 14) survived.\n- Females with age between 18 to 40 have better survival chance."
"From the above figures, we can see that:\n- Combining both male and female, we can see that children with age between 0 to 5 have better chance of survival.\n- Females with age between ""18 to 40"" and ""50 and above"" have higher chance of survival.\n- Males with age between 0 to 14 have better chance of survival."
"Heatmap of Correlation between different features:\n\n>Positive numbers = Positive correlation, i.e. increase in one feature will increase the other feature & vice-versa.\n>\n>Negative numbers = Negative correlation, i.e. increase in one feature will decrease the other feature & vice-versa.\n\nIn our case, we focus on which features have strong positive or negative correlation with the *Survived* feature."
"## Feature Extraction\n\nIn this section, we select the appropriate features to train our classifier. Here, we create new features based on existing features. We also convert categorical features into numeric form."
"In the example code below, we plot a confusion matrix for the prediction of ***Random Forest Classifier*** on our training dataset. This shows how many entries are correctly and incorrectly predicted by our classifer."
## Comparing Models\n\nLet's compare the accuracy score of all the classifier models used above.
### Required Libraries
## Dataset
---------------------------------------------------------------------\n# Loading library and dataset
The effect of each dimension reduction is identified using the MNIST dataset. 
"__________________________________________________________________\n# PCA\nPCA is the most representative method of dimensionality reduction. This is a method of re-axis of multidimensional data in the direction of large variance. The greater the dependence between variables, the smaller the principal component can represent the original data.\nHowever, since it is assumed that each feature follows a normal distribution, it is not appropriate to apply a variable with a distorted distribution to PCA."
"# Truncated SVD\nTruncated SVD is a method of extracting and decomposing only the upper part of the diagonal elements in the sigma matrix, that is, the upper part of the singular values.\nWith this decomposition, the original matrix cannot be accurately restored because it artificially decomposes $Uâˆ‘V^T$ of smaller dimensions.\nHowever, despite the data information being compressed and decomposed, it is possible to approximate the original matrix to a considerable degree."
"# Truncated SVD\nTruncated SVD is a method of extracting and decomposing only the upper part of the diagonal elements in the sigma matrix, that is, the upper part of the singular values.\nWith this decomposition, the original matrix cannot be accurately restored because it artificially decomposes $Uâˆ‘V^T$ of smaller dimensions.\nHowever, despite the data information being compressed and decomposed, it is possible to approximate the original matrix to a considerable degree."
"-------------------------------------------------------------------\n# NMF\n\nNMF is a variant of the Low-Rank Approximation method like SVD. However, it must be guaranteed that the values â€‹â€‹of all elements in the source matrix are positive. NMF decomposes a matrix into W and H matrices. The W matrix indicates how well the values â€‹â€‹of the latent elements correspond to the source matrix. The H matrix represents how this latent element is composed of sour features."
"-------------------------------------------------------------------\n# NMF\n\nNMF is a variant of the Low-Rank Approximation method like SVD. However, it must be guaranteed that the values â€‹â€‹of all elements in the source matrix are positive. NMF decomposes a matrix into W and H matrices. The W matrix indicates how well the values â€‹â€‹of the latent elements correspond to the source matrix. The H matrix represents how this latent element is composed of sour features."
"------------------------------------------\n# LDA\nLDA is a method of dimensionality reduction in the classification problem of supervised learning. It finds a low-dimensional feature space that can classify the training data well, and reduces the dimensionality by projecting the original features into that space."
"------------------------------------------\n# LDA\nLDA is a method of dimensionality reduction in the classification problem of supervised learning. It finds a low-dimensional feature space that can classify the training data well, and reduces the dimensionality by projecting the original features into that space."
"---------------------------------------------\n# t-SNE\nt-SNE is often used for visualization purposes by compressing data on a two-dimensional plane. Points that are close to the original feature space are also expressed in a two-dimensional plane after compression. Since the nonlinear relationship can be identified, the model performance can be improved by adding the compression results expressed by these t-SNEs to the original features. However, since the computation cost is high, it is not suitable for compression exceeding two or three dimensions."
"---------------------------------------------\n# t-SNE\nt-SNE is often used for visualization purposes by compressing data on a two-dimensional plane. Points that are close to the original feature space are also expressed in a two-dimensional plane after compression. Since the nonlinear relationship can be identified, the model performance can be improved by adding the compression results expressed by these t-SNEs to the original features. However, since the computation cost is high, it is not suitable for compression exceeding two or three dimensions."
"-------------------------------------------------------\n# UMAP\nUMAP (Uniform Manifold Approximation and Projection), which is faster than t-SNE and separates the data space well, has been proposed for nonlinear dimensionality reduction. In other words, it can process very large datasets quickly and is suitable for sparse matrix data. Furthermore, compared to t-SNE, it has the advantage of being able to embed immediately when new data comes in from other machine learning models."
"-------------------------------------------------------\n# UMAP\nUMAP (Uniform Manifold Approximation and Projection), which is faster than t-SNE and separates the data space well, has been proposed for nonlinear dimensionality reduction. In other words, it can process very large datasets quickly and is suitable for sparse matrix data. Furthermore, compared to t-SNE, it has the advantage of being able to embed immediately when new data comes in from other machine learning models."
"## UMAP connectivity plot\n\n> UMAP works by constructing an intermediate topological representation of the approximate manifold the data may have been sampled from. In practice this structure can be simplified down to a weighted graph. Sometimes it can be beneficial to see how that graph (representing connectivity in the manifold) looks with respect to the resulting embedding. It can be used to better understand the embedding, and for diagnostic purposes.\n\nRef: https://umap-learn.readthedocs.io"
------------------------------------------------------------\n# UMAP 3D plot
"The figure above was drawn by reducing the dimension to 3D with UMAP.\n\nIf you compare it with the previous two-dimensional plot, you can see that it is visually complex and the points are distributed sparsely in space.\n**If the dimension is increased further, the above phenomenon will become more severe.**\n\n**It's a small experiment, but we've experienced the curse of a dimension.**"
### 4.1.1 Count of Null values in features \n\nzoom in to see the hidden features
There are too many features with null values right now we will just fill those values with mean values.
### 4.4.1 Utility Score vs action with 80% accuracy for train data
We can see that there is positive linear relation ship between action and score which is obvious given the linear metrics\n\n**Note: this graph is just to see relationship of action vs score.\nWe can only use values 0 or 1 for the action.**
### 4.4.2 plot utility score vs Accuracy
You will only get a utility score if your prediction accuracy is higher than 50%.
### 4.5 Heatmap of features.
Threre are many features which are highly correlated\nI think we should look into some highly correlated features to see their distribution and relation.\n\nI used plotly so that you can zoom into the feature.\nYou can see that most of high correlated feature are near the center diagonal line\nMost important patter is in the bottom right corner if you zoom into it you will see that\nThere is alternate high and low correlations\n\nAnother interesing one is 18 to 36 feature\n\nBut there is no feature which is highly correlated to the resp value.
### 4.6 Pairplot for highly related features (18 to 39)
"ðŸ˜†ðŸ˜†ðŸ˜†\nOh man EDA is fun.\nI mean look at it, It looks like those optical illusion images"
## 4.9 Distribution of Resps
## 4.10 Distplot of some features\n\nI am going to plot distribution plot of those 10 features which has highest difference in mean\nof data with negative resps and positive resps.
## 4.11 Count of neg and pos resps in train_data
Number of datapoints in negative and positive resps are almost same which is a good news\nwe do not have to worry about training on unbalanced dataset. 
## 4.13 Mean Value of some features over time
## 4.14 Mean Value of resp features over time
## 4.14 Mean Value of resp features over time
## 4.15 Rolling mean of Mean Value of resp features over time
## 4.15 Rolling mean of Mean Value of resp features over time
## 4.16 Value of resp over 10 random days
## 4.16 Value of resp over 10 random days
## 4.17 White Noise Time Series Analysis\n\n**What is white noise ? **\nwhite noise in time series is random number which can not be predicted.\nWe can make model for the data if data is not white noise.\n\nEvery signal consist of some form of white noise.\ny(t) = signal + noise.\n\nWhite noise are series of variable which are independent and they have same variance.\nthat means the value in present has no relation with value of past or future.\n\nIf your signal is just white noise it means you can not model it they are just random numbers.\n\nWhen a model makes prediction all the errors made by models are just white noise which means.\nmodel has used all the signal available to make the prediction and all that left is white noise.\n\nTime series is not white noise if following conditions are satisfied.\n1. Mean is not-zero or near zero.\n2. Mean change over time\n3. Variance change over time\n4. Values correlate with lag values\n
### 4.17.1 Correlation with lag values.\n\nTo check that the values are correlated with lag values we will use pandas autocorrelation function.
Acctually looking at it. I looks like white noise.\nstraight grey line is 95% confidence bar.\ndashed grey line is 99% confidence bar.\n\nThere are few points which are crossing that line.\nSo is the data just white noise or did I missed something.
# 1) Import important libraries and packages
# 2) Load and clean dataset
# 2. Loading Libraries\n## Importing Libraries
# 2.2 Reading Data\n# Loading Data
# 4.1 Top 5 States With Maximum Number of Donor Cities 
# 4.2 Locality of schools with their counts 
# 4.2 Locality of schools with their counts 
# 4.3 Top Donor State Which Donated Highest Money
# 4.3 Top Donor State Which Donated Highest Money
### Top 5 Donor States\n* **California (46+ Million)**\n* **New York (24+ Million)**\n* **Texas (17+ Million)**\n* **Illinois (14+ Million)**\n* **Florida (13+ Million)**
# 4.5 Average amount funded by top 5 states in terms of number of projects
# 4.6 Percentage of donors as teachers
# 4.6 Percentage of donors as teachers
# 4.7 Top Donor Checked Out carts
# 4.7 Top Donor Checked Out carts
# 4.8 Average Percentage of free lunch based on Metro Type
# 4.10 Top 5 projects with their count
## Merging schools and projects to derive insights from them
### Visualize Projects count according to School Metro Type
#### Inference:- A lot of projects are allocated to urban areas. 
### Import Libraries
### Reading Data
"This notebook presents two ideas for target enginnering:\n- Training on **cross-sectional daily bins** of `weight` x `resp`\n- Training on **two** targets simultaneiously\n\n# Why Target Engineering?\n\nIn low signal-to-noise prediction problems, it is very difficult to build a robust model. One way to aid in robustness is to engineer the target. In fact, most everyone in this competition is already doing this with something like\n\n```y = (train_data['resp'] > 0).astype(int)```\n\n\nto make a binary target for use in a classifier. Can we do ""better"" than this?\n\nAs noted in the great notebook [The Most Important Model Parameter](https://www.kaggle.com/gkoundry/the-most-important-model-parameter), there is a drift in the mean daily return, `resp`, and this drift is different when  you account for the `weight`:"
"How do we account for this drift in the modeling process? The referenced notebook indicated that one could tune the classifier decision boundary, noting that this is likely just overfitting to the public leaderboard."
"## Cross Sectional Daily Bins\n\nSince `weight` is strictly positive, it doesn't help to engineer the `resp` target simply by multiplying by `weight`. However, one approach to engineer the target and ""solve"" this is to quantile the `resp x weight` per day and then set the target to be the top N bins. This approach focuses the classifer to train on high return **and** high weight trades. It removes, in training, any drift in the dataset."
The weights are log-normally distributed. A simple log transform shows this.
"The Gender Gap\n\nI wanted to start with something simple and important at the same time. So I questioned myself: over the past three years, did the proportion of Men and Women change? I knew there was a gap, but I would like to see it decreasing (and a lot) in the recent years.\n    "
"\n    Unfortunately, there is a considerable gap in professionals participation in Kaggle: 84% of men against 16% of women.\n\nAnd what is worse than that is that women participation did not increase over the past three years. I saw some other notebooks that showed an increase in female students. Maybe this will cause an increase in data professionals next year, but need a lot more women to close this gap.\n\nMaybe Kaggle could host women only competitions, in a bid to attract more of them to the platform (and to Data Science).\n\n\nIf we zoom-in using the same chart, we can see which countries are getting more women into Data over the past few years."
"\n    Unfortunately, there is a considerable gap in professionals participation in Kaggle: 84% of men against 16% of women.\n\nAnd what is worse than that is that women participation did not increase over the past three years. I saw some other notebooks that showed an increase in female students. Maybe this will cause an increase in data professionals next year, but need a lot more women to close this gap.\n\nMaybe Kaggle could host women only competitions, in a bid to attract more of them to the platform (and to Data Science).\n\n\nIf we zoom-in using the same chart, we can see which countries are getting more women into Data over the past few years."
"\n    While most countries increased the gender gap in 2020, India is the country that is closing the gap faster. But remember that we are still talking about 18.5% women against 80.3% of men in India.\n    "
"\n    Ok, there is a huge gap in participation, but the pay gap must be closing, no? We are past 2020 after all.\n\nTo analyse the pay gap I decided to break down the average anual compensation of women and men for education levels. I was hoping to see the gap closing with higher degrees. \n    "
"\n    I have to confess that seeing this chart was very disappointing. First the distance between women and men salaries increased in 2020 for most education levels. The pandemic could partially explain the sharp drop in 2020, with women having to leave their jobs or reduce hours to take care of children at home, for example. However, the gap also slightly increased in 2019, we are in an alarming trend.\n\nAnd the worst news is that  even though the gap closes a little bit for Bachelor's and Master's degrees, it increases again for PhDs (Doctoral)! This was something that I did not expect, and I feel sorry for all women that despite all effort  to achieve the highest education title are still treated unequally to men.\n\nLet's do something to close the gap? Give more opportunities for women to ingress data careers even if they don't have all the required experience. And please, pay women the same you pay men for consistent education level and experience.\n\n    "
"\nHere we are seeing how many persons answered None to cloud platforms (meaning that they don't use a cloud platform on a regular basis). And it is decreasing over time! So... Cloud adoption is increasing amongst professionals with Modern Data Scientists being the ones that use cloud services the most. This is very good news, meaning that everyone is having more access to the best Data Science tools and they are also getting closer to productionizing Data Science!\n\nNow there is one thing I think it's curious... I would expect ML Veterans to have a lot of experience with cloud, but they don't. Are they too cool for using the cloud?\n\nHey Kaggle! This a good question for next years survey: How many years of experience with cloud platforms?\n\nNow how about we have a look at cloud adoption per provider?\n\n"
"\nNo big surprises in the cloud providers adoption. Google Cloud and Microsoft are increasing marketshare due to discounts and policies for both startups and large corporations. AWS is the biggest provider and usually adopted by business that were ""cloud first"" a few years ago.\n"
"### Plot for a sanity check\n\nTo make sure the conversion went as planned, let's make a plot showing the distribution of loan lengths."
"It looks as if there are a number of loans that are unreasonably long. Reading through the discussions, other people had noticed this as well. At this point, we will just leave in the outliers. We also will drop the time offset columns."
#### Bureau Balance\n\nThe bureau balance dataframe has a `MONTHS_BALANCE` column that we can use as a months offset. The resulting column of dates can be used as a `time_index`.
#### Previous Applications\n\nThe `previous` dataframe holds previous applications at Home Credit. There are a number of time offset columns in this dataset:\n\n* `DAYS_DECISION`: number of days before current application at Home Credit that decision was made about previous application. This will be the `time_index` of the data.\n* `DAYS_FIRST_DRAWING`: number of days before current application at Home Credit that first disbursement was made\n* `DAYS_FIRST_DUE`: number of days before current application at Home Credit that first due was suppoed to be\n* `DAYS_LAST_DUE_1ST_VERSION`: number of days before current application at Home Credit that first was??\n* `DAYS_LAST_DUE`: number of days before current application at Home Credit of last due date of previous application\n* `DAYS_TERMINATION`: number of days before current application at Home Credit of expected termination\n\nLet's convert all these into timedeltas in a loop and then make time columns.
#### Previous Applications\n\nThe `previous` dataframe holds previous applications at Home Credit. There are a number of time offset columns in this dataset:\n\n* `DAYS_DECISION`: number of days before current application at Home Credit that decision was made about previous application. This will be the `time_index` of the data.\n* `DAYS_FIRST_DRAWING`: number of days before current application at Home Credit that first disbursement was made\n* `DAYS_FIRST_DUE`: number of days before current application at Home Credit that first due was suppoed to be\n* `DAYS_LAST_DUE_1ST_VERSION`: number of days before current application at Home Credit that first was??\n* `DAYS_LAST_DUE`: number of days before current application at Home Credit of last due date of previous application\n* `DAYS_TERMINATION`: number of days before current application at Home Credit of expected termination\n\nLet's convert all these into timedeltas in a loop and then make time columns.
#### Previous Credit and Cash\n\nThe `credit_card_balance` and `POS_CASH_balance` each have a `MONTHS_BALANCE` column with the month offset. This is the number of months before the current application at Home Credit of the previous application record. These will represent the `time_index` of the data. 
#### Previous Credit and Cash\n\nThe `credit_card_balance` and `POS_CASH_balance` each have a `MONTHS_BALANCE` column with the month offset. This is the number of months before the current application at Home Credit of the previous application record. These will represent the `time_index` of the data. 
#### Installments Payments \n\nThe `installments_payments` data contains information on each payment made on the previous loans at Home Credit. It has two date offset columns:\n\n* `DAYS_INSTALMENT`: number of days before current application at Home Credit that previous installment was supposed to be paid\n* `DAYS_ENTRY_PAYMENT`: number of days before current application at Home Credit that previous installment was actually paid\n\nBy now the process should be familiar: convert to timedeltas and then make time columns. The DAYS_INSTALMENT will serve as the `time_index`. 
#### Installments Payments \n\nThe `installments_payments` data contains information on each payment made on the previous loans at Home Credit. It has two date offset columns:\n\n* `DAYS_INSTALMENT`: number of days before current application at Home Credit that previous installment was supposed to be paid\n* `DAYS_ENTRY_PAYMENT`: number of days before current application at Home Credit that previous installment was actually paid\n\nBy now the process should be familiar: convert to timedeltas and then make time columns. The DAYS_INSTALMENT will serve as the `time_index`. 
"# Applying Featuretools\n\nWe can now start making features using the time columns. We will create an entityset named clients much as before, but now we have time variables that we can use. "
## Step 1 - Load Data ##\n\nThis next hidden cell will import some libraries and set up our data pipeline. We have a training split called `ds_train` and a validation split called `ds_valid`.
Let's take a look at a few examples from the training set.
Let's take a look at a few examples from the training set.
"## Step 2 - Define Pretrained Base ##\n\nThe most commonly used dataset for pretraining is [*ImageNet*](http://image-net.org/about-overview), a large dataset of many kind of natural images. Keras includes a variety models pretrained on ImageNet in its [`applications` module](https://www.tensorflow.org/api_docs/python/tf/keras/applications). The pretrained model we'll use is called **VGG16**."
"When training a neural network, it's always a good idea to examine the loss and metric plots. The `history` object contains this information in a dictionary `history.history`. We can use Pandas to convert this dictionary to a dataframe and plot it with a built-in method."
"# Conclusion #\n\nIn this lesson, we learned about the structure of a convnet classifier: a **head** to act as a classifier atop of a **base** which performs the feature extraction.\n\nThe head, essentially, is an ordinary classifier like you learned about in the introductory course. For features, it uses those features extracted by the base. This is the basic idea behind convolutional classifiers: that we can attach a unit that performs feature engineering to the classifier itself.\n\nThis is one of the big advantages deep neural networks have over traditional machine learning models: given the right network structure, the deep neural net can learn how to engineer the features it needs to solve its problem.\n\nFor the next few lessons, we'll take a look at how the convolutional base accomplishes the feature extraction. Then, you'll learn how to apply these ideas and design some classifiers of your own."
 Prerequisites 
Please Upvote my kernel and keep it in your favourite section if you think it is helpful.
\n4.2 Finding LR\n
\n4.3 Train Model\n
Just a quick look on variables we are dealing with.
"#### A citate from a movie: 'Children and women first'. \n* Sex: Survival chances of women are higher.\n* Pclass: Having a first class ticket is beneficial for the survival.\n* SibSp and Parch: middle size families had higher survival rate than the people who travelled alone or big families. The reasoning might be that alone people would want to sacrifice themselves to help others. Regarding the big families I would explain that it is hard to manage the whole family and therefore people would search for the family members insetad of getting on the boat.\n* Embarked C has a higher survival rate. It would be interesting to see if, for instance, the majority of Pclass 1 went on board in embarked C."
## 1.2 Survival by Sex and Age
"* Survival rate of boys is higher than of the adult men. However, the same fact does not hold for the girls. and between 13 and 30 is lower. Take it into consideration while engineering the variable: we could specify a categorical variable as young and adult.\n* For women the survival chances are higher between 14 and 40 age. For men of the same age the survival chances are flipped."
## 1.3.1 Survival by Class and Embarked
"* As noticed already before, the class 1 passangers had a higher survival rate.\n* All women who died were from the 3rd class. \n* Embarked in Q as a 3rd class gave you slighly better survival chances than embarked in S for the same class.\n* In fact, there is a very high variation in survival rate in embarked Q among 1st and 2nd class. The third class had the same survival rate as the 3rd class embarked C. We will exclude this variable embarked Q. From crosstab we see that there were only 5 passengers in embarked Q with the 1st and 2nd class. That explains large variation in survival rate and a perfect separation of men and women in Q."
"* As noticed already before, the class 1 passangers had a higher survival rate.\n* All women who died were from the 3rd class. \n* Embarked in Q as a 3rd class gave you slighly better survival chances than embarked in S for the same class.\n* In fact, there is a very high variation in survival rate in embarked Q among 1st and 2nd class. The third class had the same survival rate as the 3rd class embarked C. We will exclude this variable embarked Q. From crosstab we see that there were only 5 passengers in embarked Q with the 1st and 2nd class. That explains large variation in survival rate and a perfect separation of men and women in Q."
## 1.3.2 Fare and class distribution
## 1.3.2 Fare and class distribution
* It appears that the higher the fare was in the first class the higher survival chances a person from the 1st had.
## 1.3.3 Class and age distribution
"* Interesting note that Age decreases proportionally with the Pclass, meaning most old passangers are from 1st class. We will construct a new feature Age*Class to intefere the this findig. \n* The younger people from 1st had higher survival chanches than older from the same class.\n* Majority (from the 3rd class) and most children from the 2nd class survived."
## 1.4 Survival rate regarding the family members
"Assumption: the less people was in your family the faster you were to get to the boat. The more people they are the more managment is required. However, if you had no family members you might wanted to help others and therefore sacrifice.\n\n* The females traveling with up to 2 more family members had a higher chance to survive. However, a high variation of survival rate appears once family size exceeds 4 as mothers/daughters would search longer for the members and therefore the chanes for survival decrease.\n* Alone men might want to sacrifice and help other people to survive. "
"## 1.6 Survival rate by cabin\nCabin is supposed to be less distingushing, also taking into consideration that most of the values are missing."
"## 1.7 Correlation of the variables\n* Pclass is slightly correlated with Fare as logically, 3rd class ticket would cost less than the 1st class.\n* Pclass is also slightly correlated with Survived\n* SibSp and Parch are weakly correlated as basically they show how big the family size is.\n"
"## 1.7 Correlation of the variables\n* Pclass is slightly correlated with Fare as logically, 3rd class ticket would cost less than the 1st class.\n* Pclass is also slightly correlated with Survived\n* SibSp and Parch are weakly correlated as basically they show how big the family size is.\n"
"## 2. FEATURE SELECTION AND ENGINEERING\n## 2.1 Impute values\nNB: The calculation of values to impute should only be done on train set. For example, you want to impute the mean of age in the mussing values in test set. The mean of age should only be calculated on train set to avoid data leakage.\n\nFirst, we check how many nas there is in general. If there is only small amount then we can just exclude those individuals. Considering that there are 891 training samples, 708 do not have missing values. 183 samples have na values. It is better to impute. There are different techniques one can impute the values."
Check if we disrupted the distribution somehow.
## 2.2 ENGENEER VALUES
"## 1. Random Forest\nThere are many categorical features, so I have chosen random forest to do the classification."
## 2. Logistic Regression\n
"## Setup program\n\n### Configure imports and eager execution\n\nImport the required Python modulesâ€”including TensorFlowâ€”and enable eager execution for this program. Eager execution makes TensorFlow evaluate operations immediately, returning concrete values instead of creating a [computational graph](https://www.tensorflow.org/guide/graphs) that is executed later. If you are used to a REPL or the `python` interactive console, this feels familiar. Eager execution is available in [Tensorlow >=1.8](https://www.tensorflow.org/install/).\n\nOnce eager execution is enabled, it *cannot* be disabled within the same program. See the [eager execution guide](https://www.tensorflow.org/guide/eager) for more details."
"## The Iris classification problem\n\nImagine you are a botanist seeking an automated way to categorize each Iris flower you find. Machine learning provides many algorithms to classify flowers statistically. For instance, a sophisticated machine learning program could classify flowers based on photographs. Our ambitions are more modestâ€”we're going to classify Iris flowers based on the length and width measurements of their [sepals](https://en.wikipedia.org/wiki/Sepal) and [petals](https://en.wikipedia.org/wiki/Petal).\n\nThe Iris genus entails about 300 species, but our program will only classify the following three:\n\n* Iris setosa\n* Iris virginica\n* Iris versicolor\n\n\n  \n    \n  \n  \n    Figure 1. Iris setosa (by Radomil, CC BY-SA 3.0), Iris versicolor, (by Dlanglois, CC BY-SA 3.0), and Iris virginica (by Frank Mayfield, CC BY-SA 2.0).Â \n  \n\n\nFortunately, someone has already created a [data set of 120 Iris flowers](https://en.wikipedia.org/wiki/Iris_flower_data_set) with the sepal and petal measurements. This is a classic dataset that is popular for beginner machine learning classification problems."
"Notice that like-features are grouped together, or *batched*. Each example row's fields are appended to the corresponding feature array. Change the `batch_size` to set the number of examples stored in these feature arrays.\n\nYou can start to see some clusters by plotting a few features from the batch:"
"To simplify the model building step, create a function to repackage the features dictionary into a single array with shape: `(batch_size, num_features)`.\n\nThis function uses the [tf.stack](https://www.tensorflow.org/api_docs/python/tf/stack) method which takes values from a list of tensors and creates a combined tensor at the specified dimension."
"### Visualize the loss function over time\n\nWhile it's helpful to print out the model's training progress, it's often *more* helpful to see this progress. [TensorBoard](https://www.tensorflow.org/guide/summaries_and_tensorboard) is a nice visualization tool that is packaged with TensorFlow, but we can create basic charts using the `matplotlib` module.\n\nInterpreting these charts takes some experience, but you really want to see the *loss* go down and the *accuracy* go up."
"## Evaluate the model's effectiveness\n\nNow that the model is trained, we can get some statistics on its performance.\n\n*Evaluating* means determining how effectively the model makes predictions. To determine the model's effectiveness at Iris classification, pass some sepal and petal measurements to the model and ask the model to predict what Iris species they represent. Then compare the model's prediction against the actual label.  For example, a model that picked the correct species on half the input examples has an *[accuracy](https://developers.google.com/machine-learning/glossary/#accuracy)* of `0.5`. Figure 4 shows a slightly more effective model, getting 4 out of 5 predictions correct at 80% accuracy:\n\n\n  \n    \n    \n    \n  \n  \n    Example features\n    Label\n    Model prediction\n  \n  \n    5.93.04.31.511\n  \n  \n    6.93.15.42.122\n  \n  \n    5.13.31.70.500\n  \n  \n    6.0 3.4 4.5 1.6 12\n  \n  \n    5.52.54.01.311\n  \n  \n    Figure 4. An Iris classifier that is 80% accurate.Â \n  \n"
"![Alt Text](https://sa.kapamilya.com/absnews/abscbnnews/media/ancx/culture/2019/57/bannerburnout.jpg?ext=.jpg)\n\n About this Kernel \n\nIn this kernel we'll try to understand what are the factos contributing the mental health of a person. This dataset is from a 2014 survey that measures attitudes towards mental health and frequency of mental health disorders in the tech workplace. This kernel is going to be different from my other kernels as together we are going to understand mental health at workplace in a systematic manner. So, what exactly is the first step? The first step towards this is getting the domain knowledge. Lets begin!\n\n\n\nThere are always a lot of great kernels regarding different ways of solving the problems but only a few handful address the problems of domain knowledge and getting started. In this notebook , I will start with complete explanation of everything you need know related to Prostate Cancer and its detection and I will built on that to explain the dataset,perform EDA and then Build a baseline model\n\n Domain Knowledge #101 \n\n> **Q1 What exactly do we mean by Mental Health at workplace?**\n\nMental health affects your emotional, psychological and social well-being. It affects how we think, feel, and act. It also helps determine how we handle stress, relate to others, and make choices. In the workplace, communication and inclusion are keys skills for successful high performing teams or employees. The impact of mental health to an organization can mean an increase of absent days from work and a decrease in productivity and engagement. In the United States, approximately 70% of adults with depression are in the workforce. Employees with depression will miss an estimated 35 million workdays a year due mental illness. Those workers experiencing unresolved depression are estimated to encounter a 35% drop in their productivity, costing employers $105 billion dollars each year.\n\n\n![Alt Text](https://exudeinc-wpengine.netdna-ssl.com/wp-content/uploads/2018/07/mental-health.jpg)\n\n\n> **Q2 What can your employer do about this?**\n\nSo what can employers do? Itâ€™s called Mental Health First Aid.\n\nMental Health First Aid teaches participants how to notice and support an individual who may be experiencing a mental health or substance use concern or crisis and connect them with the appropriate employee resources. It teaches employees critical communication and support skills that can influence your organizations bottom line.\n\nResearch shows that employees who go through Mental Health First Aid have an increased awareness of mental health among themselves and their co-workers. It allows them to recognize the signs of someone who maybe struggling and teaches them the skills to know when to reach out and what resources are available. Which in turn creates beneficial intervention that increases engagement and creates an environment of inclusion and support.\n\nEmployers can also offer robust benefit packages to support employees who go through mental health issues. That includes Employee Assistance Programs, Wellness programs that focus on mental and physical health, Health and Disability Insurance or flexible working schedules or time off policies.\n\nOrganizations that incorporate mental health awareness help to create a healthy and productive work environment that reduces the stigma associated with mental illness, increases the organizations mental health literacy and teaches the skills to safely and responsibly respond to a co-workers mental health concern.\n\nIncorporating mental health awareness in the workplace can help lead the way for mental health issues throughout your community by equipping people with the tools they need to start a dialogue so that more people can get the help they need.\n\n Initial Understanding of the data #101 \n"
*1. Loading the data*\n\n*2. Checking the head of the data*\n\n*3. Looking out for the null values*
" To be Noted: \n\nUnknowingly, we have stumbled upon the fact that the number of males in the dataset are 4 times the number of females. Thus, we must keep this in mind and avoid making any faulty assumptions that males are more susceptible to mental health issues etc. \n\nAlternatively, we may conclude that the number of males in the tech industry are much more as compared to the number of females (This research was conducted specifically for the tech industry.).\n"
"\n* There's only one column which is **'work_interfere'** remaining that contains null values. For now we will proceed without any imputation. \n* Actually, there's another column, **'self_employed'** which contains around 18 null values which we failed to notice at first.\n\n> Now let us move forward and perform EDA"
> Let us begin by understanding the target data!
" Inferences: \n\n\nThis is the respondents result of question, **'Have you sought treatment for a mental health condition?'**.\n\n\nThis is our target variable.\nLooking at the first graph, we see that the percentage of respondents who want to get treatment is exactly 50%. Workplaces that promote mental health and support people with mental disorders are more likely to  have increased productivity, reduce absenteeism, and benefit from associated economic gains. If employees enjoy good mental health, employees can:\n\n* Be more productive\n* Take active participation in employee engagement activities and make better relations; both at workplace and personal life.\n* Be more joyous and make people around them happy.\n"
"> After analysing the target variable, we will try to explore the individual columns and what they mean."
"This is respondent's answer to the question, '**Are you self-employed?**'.\n\nWe see that the number of people who are self employed are around 10%. Most of the people who responded to the survey belonged to working class. We also see that though there is a vast difference between people who are self employed or not, the number of people who seek treatment in both the categories is more or less similar. \n\n> Thus, we may conclude that whether a person is self employed or not, does not largely affect whether he may be seeking mental treatment or not. "
"This is respondent's answer to the question, '**Are you self-employed?**'.\n\nWe see that the number of people who are self employed are around 10%. Most of the people who responded to the survey belonged to working class. We also see that though there is a vast difference between people who are self employed or not, the number of people who seek treatment in both the categories is more or less similar. \n\n> Thus, we may conclude that whether a person is self employed or not, does not largely affect whether he may be seeking mental treatment or not. "
" Inference: \n\nThis is the respondents answer to the question, **'Do you have a family history of mental illness?'**.\n\nFrom close to 40% of the respondents who say that they have a family history of mental illness, the plot shows that they significantly want to get treatment rather than without a family history. This is acceptable, remember the fact that people with a family history pay more attention to mental illness. Family history is a significant risk factor for many mental health disorders. \n\n> Thus, this is an important factor that has to be taken under consideration as it influences the behaviour of the employees to a significant extent.\n\n"
" Inference: \n\nThis is the respondents answer to the question, **'Do you have a family history of mental illness?'**.\n\nFrom close to 40% of the respondents who say that they have a family history of mental illness, the plot shows that they significantly want to get treatment rather than without a family history. This is acceptable, remember the fact that people with a family history pay more attention to mental illness. Family history is a significant risk factor for many mental health disorders. \n\n> Thus, this is an important factor that has to be taken under consideration as it influences the behaviour of the employees to a significant extent.\n\n"
" Inference: \n\n\nThis was the respondent's answer to the question, **'If you have a mental health condition, do you feel that it interferes with your work?'.**\n\n* On seeing the first graph we conclude that around 48% of people say that sometimes work interefers with their mental health. Now **'Sometimes'** is a really vague response to a question, and more often than not these are the people who actually face a condition but are too shy/reluctant to choose the extreme category.\n* Coming to our second graph, we see that the people who chose **'Sometimes'** had the highest number of people who actually had a mental condition. Similar pattern was shown for the people who belonged to the **'Often category'*.\n* But what is more surprising to know is that even for people whose mental health **'Never'** has interfered at work, there is a little group that still want to get treatment before it become a job stress. It can be triggered a variety of reasons like the requirements of the job do not match the capabilities, resources or needs of the worker.\n\n\n\n> We will be leaving the 'number_of_employees' category and move forward with the next column which is 'remote_work'.\n"
" Inference: \n\n\nThis was the respondent's answer to the question, **'If you have a mental health condition, do you feel that it interferes with your work?'.**\n\n* On seeing the first graph we conclude that around 48% of people say that sometimes work interefers with their mental health. Now **'Sometimes'** is a really vague response to a question, and more often than not these are the people who actually face a condition but are too shy/reluctant to choose the extreme category.\n* Coming to our second graph, we see that the people who chose **'Sometimes'** had the highest number of people who actually had a mental condition. Similar pattern was shown for the people who belonged to the **'Often category'*.\n* But what is more surprising to know is that even for people whose mental health **'Never'** has interfered at work, there is a little group that still want to get treatment before it become a job stress. It can be triggered a variety of reasons like the requirements of the job do not match the capabilities, resources or needs of the worker.\n\n\n\n> We will be leaving the 'number_of_employees' category and move forward with the next column which is 'remote_work'.\n"
" Inference: \n\nThis was the respondent's answer to the question, **'Do you work remotely (outside of an office) at least 50% of the time?'.**\n\nAround 70% of respondents don't work remotely, which means the biggest factor of mental health disorder came up triggered on the workplace. On the other side, it has slightly different between an employee that want to get treatment and don't want to get a treatment. \nThe number of people who seek treatment in both the categories is more or less similar and it does not affect our target variable. \n\n> Let's move forward with our next variable which is 'tech_company'."
" Inference: \n\nThis was the respondent's answer to the question, **'Do you work remotely (outside of an office) at least 50% of the time?'.**\n\nAround 70% of respondents don't work remotely, which means the biggest factor of mental health disorder came up triggered on the workplace. On the other side, it has slightly different between an employee that want to get treatment and don't want to get a treatment. \nThe number of people who seek treatment in both the categories is more or less similar and it does not affect our target variable. \n\n> Let's move forward with our next variable which is 'tech_company'."
"\nThis is the respondents answer to the question, **'Is your employer primarily a tech company/organization?'.**\n\n* Although the survey was specifically designed to be conducted in the tech field, there are close to 18% of the companies belonginf to the non tech field. However, looking at the second graph, one may conclude that whether a person belongs to the tech field or not, mental health still becomes a big problem.\n\n* However, on a deeper look we find that the number of employees in the tech sector who want to get treatment is slightly lower than the one's who don't. But in the non-tech field the situation gets reversed."
> The next category that we'll be looking into is **benefits**!
" Inference: \n\nThis was the respondent's answer to the question, **'Does your employer provide mental health benefits?'.**\n\n* We see that around 38% of the respondents said that their employer provided them mental health benefits, whereas a significant number ( 32% ) of them didn't even know whether they were provided this benefit.\n* Coming to the second graph, we see that for the people who **YES** said to mental health benefits, around 63% of them said that they were seeking medical help. \n* Surprisingly, the people who said **NO** for the mental health benefits provided by the company, close to 45% of them who want to seek mental health treatment.   "
" Inference: \n\nThis was the respondent's answer to the question, **'Does your employer provide mental health benefits?'.**\n\n* We see that around 38% of the respondents said that their employer provided them mental health benefits, whereas a significant number ( 32% ) of them didn't even know whether they were provided this benefit.\n* Coming to the second graph, we see that for the people who **YES** said to mental health benefits, around 63% of them said that they were seeking medical help. \n* Surprisingly, the people who said **NO** for the mental health benefits provided by the company, close to 45% of them who want to seek mental health treatment.   "
"This was the respondent's answer to the question, **'Do you know the options for mental health care your employer provides?'.**\n\nSince this graph is more or less similar to the benefits one, we won't be discussing it in more detail. \n\n> Moving forward, the next category is wellness program. Lets try understanding that! \n"
"This was the respondent's answer to the question, **'Do you know the options for mental health care your employer provides?'.**\n\nSince this graph is more or less similar to the benefits one, we won't be discussing it in more detail. \n\n> Moving forward, the next category is wellness program. Lets try understanding that! \n"
" Inference: \n\nThis is the respondents answer to the question, **'Has your employer ever discussed mental health as part of an employee wellness program?'.**\n\n* About 19% of the repondents say **YES** about becoming a part of the employee wellness program and out of those 60% of employee want to get treatment. \n* One shocking revealation is that more than 65% of respondents say that there aren't any wellness programs provided by their company. But close to half of those respondents want to get treatment, which means the company needs to fulfil its duty and provide it soon. \n \n> The next category is **seek_help**, we will be leaving it as it is more or less similar to care_options, benefits and wellness_program. Our next category is anonymity."
" Inference: \n\nThis is the respondents answer to the question, **'Has your employer ever discussed mental health as part of an employee wellness program?'.**\n\n* About 19% of the repondents say **YES** about becoming a part of the employee wellness program and out of those 60% of employee want to get treatment. \n* One shocking revealation is that more than 65% of respondents say that there aren't any wellness programs provided by their company. But close to half of those respondents want to get treatment, which means the company needs to fulfil its duty and provide it soon. \n \n> The next category is **seek_help**, we will be leaving it as it is more or less similar to care_options, benefits and wellness_program. Our next category is anonymity."
" Inference: \n\nThis is the respondent's answer to the question, '**Is your anonymity protected if you choose to take advantage of mental health or substance abuse treatment resources?**'.\n\n* Around 65% of the people were not aware whether anonymity was provided to them and 30% said yes to the provision of anonymity by the company.\n* Looking at the second graph, we see that out of the people who answered yes to the provision of anonymity, around 60% of them were seeking help regarding their mental condition. Possible reasoning for this may be that the employee feels that the company has protected his/her privacy and can be trusted with knowing the mental health condition of it's workers. The most basic reason behind hiding this from the fellow workers can be the social stigma attached to mental health.\n\n> The next factor that we will be discussing is '**leave.**'\n"
" Inference: \n\nThis is the respondent's answer to the question, '**Is your anonymity protected if you choose to take advantage of mental health or substance abuse treatment resources?**'.\n\n* Around 65% of the people were not aware whether anonymity was provided to them and 30% said yes to the provision of anonymity by the company.\n* Looking at the second graph, we see that out of the people who answered yes to the provision of anonymity, around 60% of them were seeking help regarding their mental condition. Possible reasoning for this may be that the employee feels that the company has protected his/her privacy and can be trusted with knowing the mental health condition of it's workers. The most basic reason behind hiding this from the fellow workers can be the social stigma attached to mental health.\n\n> The next factor that we will be discussing is '**leave.**'\n"
"This is the respondent's answer to the question, '**How easy is it for you to take medical leave for a mental health condition?**'\n\n* While close to 50% of the people answered that they did not know about it, suprisingly around 45% of those people sought help for their condition.\n* A small percent of people ( around 8% ) said that it was very difficult for them to get leave for mental health and out of those, 75% of them sought for help.\n* Employees who said it was 'somewhat easy' or 'very easy' to get leave had almost 50% people seeking medical help.\n\n> The next category that we'd be looking into is **mental health consequence.**\n\n\n"
"This is the respondent's answer to the question, '**How easy is it for you to take medical leave for a mental health condition?**'\n\n* While close to 50% of the people answered that they did not know about it, suprisingly around 45% of those people sought help for their condition.\n* A small percent of people ( around 8% ) said that it was very difficult for them to get leave for mental health and out of those, 75% of them sought for help.\n* Employees who said it was 'somewhat easy' or 'very easy' to get leave had almost 50% people seeking medical help.\n\n> The next category that we'd be looking into is **mental health consequence.**\n\n\n"
" Inference: \n\nThis is the respondent's answer to the question, '**Do you think that discussing a mental health issue with your employer would have negative consequences?**'.\n\n* Around same number of people ( around 40% each ) answered **Maybe** as well as **No** for the negative impact of discussing mental health consequences with the employer and about 23% said **Yes** to it.\n* 23% is a significant number who feel that discussing their mental health might create a negative impact on their employer. This may be because of the stigma, decreased productivity, impact on promotions or any other preconcieved notion.\n* It is nice to know that out of the people who answered No, there were only around 40% of the people who actually sought after help, whereas in both the other categories, it is more than 50%.\n\n> The next factor that we are going to discuss is **physical health consequence.** It will be interesting to compare both of these two together."
" Inference: \n\nThis is the respondent's answer to the question, '**Do you think that discussing a mental health issue with your employer would have negative consequences?**'.\n\n* Around same number of people ( around 40% each ) answered **Maybe** as well as **No** for the negative impact of discussing mental health consequences with the employer and about 23% said **Yes** to it.\n* 23% is a significant number who feel that discussing their mental health might create a negative impact on their employer. This may be because of the stigma, decreased productivity, impact on promotions or any other preconcieved notion.\n* It is nice to know that out of the people who answered No, there were only around 40% of the people who actually sought after help, whereas in both the other categories, it is more than 50%.\n\n> The next factor that we are going to discuss is **physical health consequence.** It will be interesting to compare both of these two together."
" Inference: \n\nThis is the respondent's answer to the question, '**Do you think that discussing a physical health issue with your employer would have negative consequences?**'\n\n* There is a starking difference between the reponses for the same question regarding mental and physical health. More than 70% of the employees believe that their physical health does not create a negative impact on their employer and only 5% of them believes that it does. \n* While it maybe incorrect for us to draw any conclusions about whether they seek mental help on the basis of their physical condition, because it is more or less same for all the three categories, we must keep in mind about **how differently mental and physical health are treated as a whole.**"
" Inference: \n\nThis is the respondent's answer to the question, '**Do you think that discussing a physical health issue with your employer would have negative consequences?**'\n\n* There is a starking difference between the reponses for the same question regarding mental and physical health. More than 70% of the employees believe that their physical health does not create a negative impact on their employer and only 5% of them believes that it does. \n* While it maybe incorrect for us to draw any conclusions about whether they seek mental help on the basis of their physical condition, because it is more or less same for all the three categories, we must keep in mind about **how differently mental and physical health are treated as a whole.**"
"This is the respondent's answer to the question, '**Would you be willing to discuss a mental health issue with your coworkers?**'\n\n* Around 62% of the employees said that they might be comfortable discussing some type of mental problems with their coworkers, and out of them around 50% actually sought for medical help.\n* 20% of the employees believed that discussing mental health with their coworkers wasn't a good option for them.\n\n> The next category is **supervisor.** Lets find out whether the employees are comfortable sharing their mental health with their supervisor."
"This is the respondent's answer to the question, '**Would you be willing to discuss a mental health issue with your coworkers?**'\n\n* Around 62% of the employees said that they might be comfortable discussing some type of mental problems with their coworkers, and out of them around 50% actually sought for medical help.\n* 20% of the employees believed that discussing mental health with their coworkers wasn't a good option for them.\n\n> The next category is **supervisor.** Lets find out whether the employees are comfortable sharing their mental health with their supervisor."
" Inference: \n\nThis is the respondent's answer to the question, **'Would you be willing to discuss a mental health issue with your direct supervisor(s)?'**.\n\n* This graph is quite different from the one of the coworker. Here, around 40% of the workers believe in sharing their mental health with their supervisors. This may have something to do with their performance etc.\n* Looking at the second graph, employees who actually sought for help regarding their mental health was more or less similar for all the three categories.\n\n> This has become really tiring now! Anyway, just 2-3 categories more left for analysis. Let's move forward with our next  variable, which is **'mental_health_interview'** "
" Inference: \n\nThis is the respondent's answer to the question, **'Would you be willing to discuss a mental health issue with your direct supervisor(s)?'**.\n\n* This graph is quite different from the one of the coworker. Here, around 40% of the workers believe in sharing their mental health with their supervisors. This may have something to do with their performance etc.\n* Looking at the second graph, employees who actually sought for help regarding their mental health was more or less similar for all the three categories.\n\n> This has become really tiring now! Anyway, just 2-3 categories more left for analysis. Let's move forward with our next  variable, which is **'mental_health_interview'** "
"This is the respondent's answer to the question, '**Do you think that discussing a mental health issue with your employer would have negative consequences?**'.\n\n* As our intution might suggest us, 80% of the respondents believe that it is a good option to discuss your mental health with the future employer. This is actually a good thing! This might not have been the case 15 years ago.\n* While around 15% of the candidates seem confused about whether they should be discussing their mental conditions with the future employer or not, less than 5% think that it may not be a good option discussing it.\n\n> The next category is **physical_health_interview**. Let's see if there's any difference in the respondent's answer for this one with the previous one."
"This is the respondent's answer to the question, '**Do you think that discussing a mental health issue with your employer would have negative consequences?**'.\n\n* As our intution might suggest us, 80% of the respondents believe that it is a good option to discuss your mental health with the future employer. This is actually a good thing! This might not have been the case 15 years ago.\n* While around 15% of the candidates seem confused about whether they should be discussing their mental conditions with the future employer or not, less than 5% think that it may not be a good option discussing it.\n\n> The next category is **physical_health_interview**. Let's see if there's any difference in the respondent's answer for this one with the previous one."
" Inference: \n\nThis is the respondent's answer to the question, **'Would you bring up a physical health issue with a potential employer in an interview?'**.\n\n* While a majority of the people are still dubious about discussing their physical health condition with the future employer, however, close to 17% believe that there is no issue in discussing their physical health conditions.\n* Around 50% of the people still remain confused about whether it is a good option to discuss their condition or not.\n\n> Coming to the last but one, **mental_vs_physical**. Let's see what insights can be drawn from this category!"
" Inference: \n\nThis is the respondent's answer to the question, **'Would you bring up a physical health issue with a potential employer in an interview?'**.\n\n* While a majority of the people are still dubious about discussing their physical health condition with the future employer, however, close to 17% believe that there is no issue in discussing their physical health conditions.\n* Around 50% of the people still remain confused about whether it is a good option to discuss their condition or not.\n\n> Coming to the last but one, **mental_vs_physical**. Let's see what insights can be drawn from this category!"
"This was the respondent's answer to the question, **'Do you feel that your employer takes mental health as seriously as physical health?'**\n\n* While close to 50% people said that they didn't know, the number of people who answered **Yes** as well as **No** were completely equal. \n* For the people who answered Yes as well as the ones who answered No, more than 505 of them sought after medical help for their mental health, whereas it was not the case for the one's belonging to the 'Don't know' category.\n\n> Coming to the last column, we have finally reached to **obs_consequence**. This definitely calls for a meme!\n\n![Alt_Text](https://www.awesomeinventions.com/wp-content/uploads/2016/02/Not-Tired1.jpg)"
"This was the respondent's answer to the question, **'Do you feel that your employer takes mental health as seriously as physical health?'**\n\n* While close to 50% people said that they didn't know, the number of people who answered **Yes** as well as **No** were completely equal. \n* For the people who answered Yes as well as the ones who answered No, more than 505 of them sought after medical help for their mental health, whereas it was not the case for the one's belonging to the 'Don't know' category.\n\n> Coming to the last column, we have finally reached to **obs_consequence**. This definitely calls for a meme!\n\n![Alt_Text](https://www.awesomeinventions.com/wp-content/uploads/2016/02/Not-Tired1.jpg)"
" Inference: \n\nThis was the respondent's answer to the question, **'Have you heard of or observed negative consequences for coworkers with mental health conditions in your workplace?'**\n\n* Majority ( 85% ) of the people, answered **No** to this question. This is quite important to note  that IT being an organised sector, follows strict guidelines of employee satisfaction etc. Thus, we didn't come across any major issue regarding the employer behavior as such!\n\n> Anyway, I think we're done with the EDA. Let me know if we left something out and I will try top cover that as well!"
" Inference: \n\n* We can see that the target column, i.e **'treatment'** has almost equal values for both the categories. This means that we do not have to perform undersampling or oversampling.\n* Now let us make a heatmap and try to understand the correlation of various features with the target variable."
  Evaluating Models \n
0Â Â IMPORTS
1Â Â BACKGROUND INFORMATION
3Â Â HELPER FUNCTIONS
4Â Â INFERENCE LOOP
"# Introduction\n\nThe Future Sales competition is the final assesment in the 'How to win a Data Science' course in the Advanced Machine Learning specialisation from HSE University, Moscow. The aim is to predict the monthly sales of items in specific shops, given historical data. The sale counts are clipped between 0 and 20."
# Load Data
# Remove outliers
"We'll remove the obvious outliers in the dataset - the items that sold more than 1000 in one day and the item with price greater than 300,000."
# Cleaning Item Data
Clean item names.
### Import Required Packges
### Importing the Data
Drama looks to be the most popular genre followed by Comedy.
Now lets generate a list 'genreList' with all possible unique genres mentioned in the dataset.
"# Score Predictor\n\nSo now when we have everything in place, we will now build the score predictor. The main function working under the hood will be the Similarity function, which will calculate the similarity between movies, and will find 10 most similar movies. These 10 movies will help in predicting the score for our desired movie. We will take the average of the scores of the similar movies and find the score for the desired movie.\n\nNow the similarity between the movies will depend on our newly created columns containing binary lists. We know that features like the director or the cast will play a very important role in the movie's success. We always assume that movies from David Fincher or Chris Nolan will fare very well. Also if they work with their favorite actors, who always fetch them success and also work on their favorite genres, then the chances of success are even higher. Using these phenomena, lets try building our score predictor."
#### Let's check our prediction for Godfather
"# Outline\n1. Stochastic gradient descent and online learning\n    - 1.1. SGD\n    - 1.2. Online approach to learning\n2. Categorical data processing: Label Encoding, One-Hot Encoding, Hashing trick\n    - 2.1. Label Encoding\n    - 2.2. One-Hot Encoding\n    - 2.3. Hashing trick\n3. Vowpal Wabbit\n    - 3.1. News. Binary classification\n    - 3.2. News. Multiclass classification\n    - 3.3. IMDB reviews\n    - 3.4. Classifying gigabytes of StackOverflow questions\n4. VW and Spooky Author Identification "
"# 1. Stochastic gradient descent and online learning\n##  1.1. Stochastic gradient descent\n\nDespite the fact that gradient descent is one of the first things learned in machine learning and optimization courses, it is hard to overrate one of it's modifications, namely, Stochastic Gradient Descent (SGD).\n\nLets recap that the very idea of gradient descent is to minimize some function by making small steps in the direction of fastest function decreasing. The method was named due to the following fact from calculus: vector $\nabla f = (\frac{\partial f}{\partial x_1}, \ldots \frac{\partial f}{\partial x_n})^T$ of partial derivatives of the function $f(x) = f(x_1, \ldots x_n)$ points to the direction of the fastest function growth. It means that by moving in the opposite direction (antigradient) it is possible to decrease the function value with the fastest rate.\n\n\n\nHere is a snowboarder (me) in Sheregesh, Russian most popular winter resort. I highly recommended it if you like skiing or snowboarding. We place this picture not only for a good view but also for picturing the idea of gradient descent. If you have an aim to ride as fast as possible, you need to choose the way with steepest descent (as long as you stay alive). Calculating antigradient can be seen as evaluating the slope in each particular point."
Lets look at the first document from this collection:
"Now we convert the data into something Vowpal Wabbit can understand, and we throw away words shorter than of 3 symbols. Here we skip many important NLP stages (stemming and lemmatization, for example), however, we will later see that VW solves the problem even without these steps."
"Now we load predictions, compute AUC and plot the ROC curve:"
AUC value we get states that here we've achieved high classification quality.
### ***What platforms did Data analysist find to be most helpful when they first started studying data-science?***
"- This graphs represents Online courses platforms for learning to take the road map of Data Analysts. This graphs show Online vidoe platforms like Coursera, EDX are best platforms for learning data analysts courses form begnning. "
### ***Programming Languages Data Analyst Use On a Regular Basis:***
- Python and SQL are the Necessary languages for completing data analytics course. This graph shows python and SQL as top rated language.
### ***Do Data Analysts use any of the following hosted notebook products?***
- Noted Books are used for coding practices or projects. these notebooks are also available online for team work or for save or run your coding work in better way. according to this graph colab note books are on the top.
### ***Top 10 Cloud Platforms Used By Data Analyst:***
"- Cloud services are often used for saving your projects, models or creating virtual machines, This graph represent Google cloud platform as best rated.***"
### ***Top 10 Pre-trainned Model Used by Data Analyst:***
- All job roles are mostly depending on Kaggle data sets except for MLops they are using Tensorlfow hub a lot.
### ***Following integrated development environments (IDE's) do Data Analysts use on a regular basis?:***
- Most of the job roles are using VScode and Jupyter except for developer advocate they are using Vscode as well as not using any IDE in parallel.***
### ***Top 10 Libraries Used For Visualization By Data Analysts:***
- Most of the analystsb roles are using Matplotlib and Seaborn except for Statitician they are using ggplot.\n- So here we need to learn matplot or seaborn for detailed or impressive visualization of data .
### ***Which of the following machine learning frameworks Data Analysts use on a regular basis?***
- Scikit learn is mostly used all job roles. But MLop use tensorflow keras and Sklearn simultanously more than anyone else.***
### ***Which of the following ML algorithms Data Analyst use on a regular basis?***
\nMost commanly use of ML algorithms by data analysts are linear/ logistic or decision tree algorothms. 
### ***Which categories of computer vision methods does Data Analysts use on a regular basis?***
- According to graph mostly data analysts does not need of computer vision but if in any case of analyzation image classification or general purpose network are on the top position.***
### ***Which of the following natural language processing (NLP) methods Data Analysts use on a regular basis?***
- NLP represents unstructed data or this language word embedding or word to vectors is often use.***
### ***Business Intelligence Tools Used By Data Analysts:***\n
***Data Architect and Data Analyst are using BI tools more than any one else. and this graph shows microsoft power Bi tableau on top rated.***
### ***Which of the following ML model hubs/repositories do Data Analysts use most often?***
- Kaggle Data_sets are mostly used by data analysts
"### ***Do Data analysts use any of the following data products (relational databases, data warehouses, data lakes,or similar)?***"
- Most commanly use of data base in the role of data analysts is My SQL data base then came others databases.
### ***Do Data Analysts use any of the following automated machine learning tools?***
- Acording to this graph max data analysts are not preffered to use automated machine learning tools but if they are consider this then the top name of automated tool of google cloud autoML.
### ***Favorite Media Sources for Data_analysts***
- Data Analysts job roles are using Youtube and Kaggle and blogs except for data administrtatpr and managers they don't use any media source.
### ***What products or platforms did Data Architect find to be most helpful when they first started studying datascience?***
"Online Platforms Which can help to study about Data architect job tittle. Through this graph we described some online platforms in which included or on top list platforms are Coursera, Edx, Youtube etc. then comes other some social media platforms."
### ***Which Programming Languages Use by Data Architect***
This graph describe the most frequantly use of language in Data_Architect field. According to graph description there are two languages Python or SQL are most commanly used in that field.
### ***Which of the following integrated development environments (IDE's) do Data_Architect use on a regular basis?***\n
"IDE stands for Integrated Development Environment. Itâ€™s a coding tool which allows you to write, test, and debug your code in an easier way, as they typically offer code completion or code insight by highlighting, resource management, debugging tools. According to this graph presentations three IDEs are used most frequantly by Data Architects. VS code, Pycharm or Motepad."
### ***Do you use any of the following hosted notebook products?***
"Notebooks have been commonplace in data science for most of the last 10 years and every data scientist has worked with one. They allow data scientists to rapidly experiment and share insights through quick environment creation, interactive output, and code snippets that can be executed in any order. According to this graph's results the kaggle notebook or colab notebook are on top position."
### ***Data Visualization Libraries Used by Data_Architect on regular basis***
These are one of the most popular data visualization libraries being used by developers across the globe and is used to manipulate documents based on data. Data architects are not excessive deal with visualization libraries but if they needed to use of any library then this graph defined some top libraries that are used by data architects. in which matplotlib/seaborn or plotly are on top level.
### ***Which of the following ML algorithms Data_Architect use on a regular basis?***
"A machine learning algorithm is the method by which the AI system conducts its task, generally predicting output values from given input data. This graph defines that algorithms that are mostly used by Data architects in which linear or logistic regression on the top."
### ***Which of the following machine learning frameworks Data Architect use on a regular basis***
A machine learning framework is an interface that allows developers to build and deploy machine learning models faster and easier. A tool like this allows enterprises to scale their machine learning efforts securely while maintaining a healthy ML lifecycle. According to this graphical representations the library of sk-learn is most comanly used by data architects.
### ***Do Data_Architects download pre-trained model weights from any of the serivces***
"A pre-trained model is a saved network that was previously trained on a large dataset, typically on a large-scale image-classification task. You either use the pretrained model as is or use transfer learning to customize this model to a given task. Accoding to this servey most of people in the field of Data Architect are not preffer to use of any pre-trainned, and if it is necessary then they preffer tensorFlow hub, Pycharm, ONNX models or kaggle data_sets."
### ***Which of the following ML model hubs/repositories do Data_Architects use most often?***
"According to this graphical presentation kaggle data_set, Tensor-flow hub and then other google drives are most commonly used by Data architect for ml models hubs and repositories."
### ***Which categories of computer vision methods does Data_Architect use on a regular basis?***
"Computer vision is a field of artificial intelligence that trains computers to interpret and understand the visual world. Using digital images from cameras and videos and deep learning models, machines can accurately identify and classify objects. according to this graphical presentation if some computer vision metheds needed by data architects that are General purpose image/videos tools."
### ***Which of the following natural language processing (NLP) methods Data Architects use on a regular basis?***
"Natural language processing (NLP) refers to the branch of computer scienceâ€”and more specifically, the branch of artificial intelligence or AIâ€”concerned with giving computers the ability to understand text and spoken words in much the same way human beings can. Word embeddings/vectors, transfomer languages models, BERT, XLnet, etc are on top priorities by Data architects."
### ***Which of the following cloud computing platforms do Data_Architects use?***
"the practice of using a network of remote servers hosted on the internet to store, manage, and process data, rather than a local server or a personal computer. Most of Data Architects used the services of AWS, microsofrt Azure or google cloud platforms according to this graphical result."
### ***Do Data_Architects use any of the following business intelligence tools?***
"Business intelligence (BI) tools are types of application software which collect and process large amounts of unstructured data from internal and external systems, including books, journals, documents, health records, images, files, email, video and other business sources. According to this graphical results most Data Architects are preffered to use of microsoft power BI And on 2nd option is Tableau."
### ***Do Data_Architect use any of the following managed machine learning products?***
Acoording to this graph most of Data Architect did not use ML products and if they are needed then they preffer to use of amazon sageMaker/Azure ML studio or Google cloud Vertex Al.
### ***Do Data_Architect use any of the following automated machine learning tools?***
Automated machine learning (AutoML) is the process of automating the tasks of applying machine learning to real-world problems. AutoML potentially includes every stage from beginning with a raw dataset to building a machine learning model ready for deployment. According to this server the results of this graph describes that most of data architects are not used ML automated tools and if they need to useing that tools they preffer amazon sagemaker autopilot.
### ***Do Data_Architect use any of the following products to serve your machine learning models?***
According to this interpretation Machine learning flow then tensorflow Exterded are likely to popular products to serve your machine learning models.
### ***Who/what are data_Architects favorite media sources that report on data science topics?***
"Favorite media sources from which anybody learn and then apply that learning into a practical way. Data Architect is also a skill and for this some social media platforms might be helpfull for this type of learning. According to this interpretation youtube, kaggle, journal publications are most commamly usable platforms for the learning of any kind of skill or specially Data Architect."
##### ***What products or platforms did Data Scientist find to be most helpful when they first started studying datascience?***
"- Most of the data sicentist are using a combination of Online courses, kaggle(comptions and notebooks) and youtube for learning data science."
##### ***What programming languages Data Scientist use on a regular basis?***
Python with SQL are the best languages for completing data analytics course. This graph shows python as top rated language.
##### ***Which of the following integrated development environments (IDE's) do Data Scientists use on a regular basis?***
- Jupyter notebooks with VS code are the most used IDE's by data scientists.
##### ***Do you use any of the following hosted notebook products?*** 
Noted Books are used for coding practices or projects. these notebooks are also available online for team work or for save or run your coding work in better way. according to this graph colab note books are on the top.
##### ***Data Visualization Libraries Used by Data Scientist on regular basis***
"- matplotlib, seaborn and plotly are the most used libraries for data visualization."
##### ***Which of the following machine learning frameworks Data Scientist use on a regular basis?***\n
"- Scikit learn, XG boost and Tensorflow are the most used frameworks for data science."
##### ***Which of the following ML algorithms Data Sicentist use on a regular basis?***
"- Linear or Logistic Regression, Decision TreesRandom Forest and Boosting algorithms are the most used algorithms for data science."
##### ***Which categories of computer vision methods does Data Scientists use on a regular basis?***
- Most of the Data Scientists are not using computer vision methods but if they are using then they are using Image classfication and general purpose image/video tools.
##### ***Which of the following natural language processing (NLP) methods Data Scientists use on a regular basis?***
- Word embeddings/vectors and Transformer Langugage models are the most used NLP methods by data scientists.
##### ***Do Data Scientists download pre-trained model weights from any of the serivces?***
- Most of the data scientists are not using pre-trained model weights but if they are using then they are using Kaggle and Hugging Face.
##### ***Which of the following ML model hubs/repositories do Data Scientist use most often?***
"- Most of the Data Scientst are using Kaggle, Hugging Face and Tensorflow hub for ML model hubs/repositories."
##### ***Which of the following cloud computing platforms do Data Scientist use?***
"- Most of the Data Scientists are using AWS, GCP and Azure for cloud computing platforms and a big number of data scientists are using none of these platforms."
"##### ***Of the cloud platforms that you are familiar with, which has the best developer experience (mostenjoyable to use)?***"
- Most of the data Scientists have best experience with AWS and GCP.
##### ***Do Data Scientist use any of the following cloud computing products?***
"- Amazon Elastic Compute Cloud (EC2) and Google Compute Engine are the most used cloud computing products by data scientists, but big number of the data scientists are not using any of these products."
##### ***Do Data Scientists use any of the following data storage products?***
"- Most of the data scientists are using Amazon Simple Storage Service (S3) and Google Cloud Storage (GCS) for data storage products, but a big number of data scientists are not using any of these products."
"##### ***Do Data Scientist use any of the following data products (relational databases, data warehouses, data lakes,or similar)?***"
"- Most of the data scientists are using MySQL and PostgreSQL for data products, but a big number of data scientists are not using any of these products."
##### ***Do Data Scientist use any of the following business intelligence tools?***
- Most of the Data Scientist are not using any business intelligence tools but if they are using then they are using Tableau and Power BI.
##### ***Do Data Scientist use any of the following managed machine learning products?***
- Most of the Data Scientist are not using any managed machine learning products but if they are using then they are using Amazon SageMaker and Google Cloud AI Platform.
##### ***Do Data Scientist use any of the following automated machine learning tools?***
- Most of the Data Scientist are not using any automated machine learning Models but if they are using then they are using Google Cloud AutoML and Azure Automated Machine Learning.
##### ***Do Data Scientist use any of the following products to serve your machine learning models?***
- Most of the Data Scientist are not using any products to serve their machine learning models but if they are using then they are MLFLow and TFX
##### ***Do Data Scientist use any tools to help monitor your machine learning models and/or experiments?***
- Most of the Data Scientist are not using any tools to help monitor their machine learning models and/or experiments but if they are using then they are using ML flow and TensorBoard
##### ***Do Data Scientists use any of the following responsible or ethical AI products in your machine learning practices?***
- Most of the Data Scientist are not using any responsible or ethical AI products in their machine learning practices but if they are using then they are using Google Responsible AI Toolkit and Microsoft AI Resources.
##### ***Do Data Scientists use any of the following types of specialized hardware when training machine learning models?***
- Most of the Data Scientist are using GPUs and TPUs for training machine learning models but a big number of data scientists are not using any of these products.
##### ***Who/what are data scientists favorite media sources that report on data science topics?***
"- Most of the data scientists favourite sources are Blogs, kaggle and Youtube for media sources that report on data science topics."
##### ***Mostly use products or platforms find to be most helpful when they first started studying***
"#### ***Discription :***\n if you want to presue your in field of data science as Manager. As you see in the graph, mostly people starting learning from online courses, video plateforms and kaggle."
##### ***Most use programming languages***
"#### ***Discription :***\nAs you see in the graph, Every Manager mainly use python for data analysis other langauges are you but in common.  "
##### ***Mostly use integrated development environments (IDE's)***
#### ***Discription :***\nIDE are very nessory component for progrmming where we write and debug our program. Graph show that mostly manager use jupyter notebook and visual studio code(VSCode).
##### ***Mostly use hosted notebooks*** 
"#### ***Discription :***\nNotebook are use for online code writing and debugging. As you see in graph, Colab and Kaggle Notebook are commonly in manager. "
##### ***Mostly use Data Visualization Libraries***
"#### ***Discription :***\nAs you see in the graph, Mostly data visulization libraries use by manager are Matplotlib and seaborn"
##### ***Mostly machine learning frameworks***\n
#### ***Discription :***\nMachine Learning Libraries or frameworks presents in above graph that shows Scikit-Learn and TensorFlow are widely use in manager 
##### ***Mostly use ML Algorithms***
"#### ***Discription :***\nAs you see in the graph,Mostly use Machine Learning algorthim is Linear or Logistic Regression by Managers "
##### ***Mostly use categories of computer vision methods***
"#### ***Discription :***\nAs you see in the graph,In Computer Vision Image classification and other general purpose networks are widely use. "
##### ***Mostly use natural language processing (NLP) methods***
"#### ***Discription :***\nIn Natural Language processing methods, Transformer language models are most populer in the managers"
##### ***Most Download pre-trained model weights from any of the serivces?***
"#### ***Discription :***\nAs you see in the graph, Kaggle datasets in group of pre-trained models weight is the most populer in the managers"
##### ***Mostly use ML model hubs/repositories***
"#### ***Discription :***\nAs you see in the graph, Kaggle datasets in group of hub/Repositories is the most populer in the managers because moslty data managers use kaggle which are easy tu use and save and compile the data very effciently"
##### ***Mostly use cloud computing platforms***
"#### ***Discription :***\nCloud plateform is very common in data related field because of high performence computing. So, graph showes that more the 50% of Manager use amazon Web services(AWS) for high performence computing and large data storge."
"##### ***The cloud platforms are familiar with, which has the best developer experience (mostenjoyable to use)***"
#### ***Discription :***\nwhen we see the previous graph mostly managers use AWS for our work and now we see this graph also shows AWS have provide friendly and enjoyable enviroment for their customers. 
##### ***Mostly use the following cloud computing products***
"#### ***Discription :***\nIn previous Two Graphs AWS is familier with manager, So, this graph shows that Amzaon Elastic Computing Cloud(EC2) is also very known in Manager because managers run high computer task in AWS"
##### ***Mostly use the following data storage products***
"#### ***Discription :***\nData Storage products is very common in Manager. So, Amazon Simple Storage Servies is widely use in Data storage."
"##### ***Mostly use the following data products (relational databases, data warehouses, data lakes,or similar)***"
#### ***Discription :***\nData Storage products work with data products. mean how you want to storge the data. My SQL is vwidely use in Managers. 
##### ***Mostly Use Following business intelligence tools***
#### ***Discription :***\nMS Power BI is very populer in Managers Community.
##### ***Following managed machine learning products***
"#### ***Discription :***\nGraph shows that, In Managed Machine Learning products Amazon SageMarker is very common in managers."
##### ***Following automated machine learning tools***
#### ***Discription :***\nGoogle Cloud Auto ML is widely use in Manager.
##### ***Following products to serve your machine learning models***
#### ***Discription :***\nML Flow is the product that serve Machine Learning models for the managers
##### ***Tools to help monitor your machine learning models and/or experiments***
#### ***Discription :***\nTensorboard is most populer tools to monitor your machine learning models. 
##### ***following are the responsible or ethical AI products in your machine learning practices***
##### ***following types of specialized hardware when training machine learning models***
##### ***following types of specialized hardware when training machine learning models***
#### ***Discription :***\nGPU is the specilized hardware which are mostly use in managers 
##### ***favorite media sources that report on data science topics***
"#### ***Discription :***\nAs you see in the graph, kaggle and youtube are use very famous media plateform in the managers."
##### ***Mostly use products or platforms find to be most helpful when they first started studying***
"#### ***Discription :***\nOnline Courses plateform and Video Plateform are very common in newbie of every field. So, if you want to research scientist you want to learn from these plateform "
##### ***Most use programming languages***
#### ***Discription :***\nEvery Research Scientist which is connected to Data science domain it is nessory to learn the python as primary programming Language.
##### ***Mostly use integrated development environments (IDE's)***
#### ***Discription :***\nif you want to talk about IDE jupyter notebook is most populer bcause of python almost 80% use this.
##### ***Mostly use hosted notebooks*** 
"#### ***Discription :***\nNotebook are use for online code writing and debugging. As you see in graph, Colab and Kaggle Notebook are common. "
##### ***Mostly use Data Visualization Libraries***
"#### ***Discription :***\nAs you see in the graph, Mostly data visulization libraries use by Research Scientist are Matplotlib and seaborn for visulized your Research."
##### ***Mostly machine learning frameworks***\n
#### ***Discription :***\nMachine Learning Libraries or frameworks presents in above graph that shows Scikit-Learn and TensorFlow are widely use Research Field
##### ***Mostly use ML Algorithms***
"#### ***Discription :***\nAs you see in the graph,Mostly use Machine Learning algorthim is Linear or Logistic Regression by Managers "
##### ***Mostly use categories of computer vision methods***
##### ***Mostly use natural language processing (NLP) methods***
##### ***Mostly use natural language processing (NLP) methods***
"#### ***Discription :***\nIn Natural Language processing methods, Transformer language models are most populer"
##### ***Most Download pre-trained model weights from any of the serivces?***
"#### ***Discription :***\nAs you see in the graph, Kaggle datasets in group of hub/Repositories is the most populer because moslty data managers use kaggle which are easy tu use and save and compile the data very effciently"
##### ***Mostly use ML model hubs/repositories***
"#### ***Discription :***\nAs you see in the graph, Kaggle datasets in group of hub/Repositories is the most populer because moslty data managers use kaggle which are easy tu use and save and compile the data very effciently"
##### ***Mostly use cloud computing platforms***
"#### ***Discription :***\nCloud plateform is very common in data related field because of high performence computing. So, graph showes that more the 30% use amazon Web services(AWS) for high performence computing and large data storge."
"##### ***The cloud platforms are familiar with, which has the best developer experience (mostenjoyable to use)***"
#### ***Discription :***\n we see this graph shows Google cloud plateform have provide friendly and enjoyable enviroment for their customers. 
##### ***Mostly use the following cloud computing products***
"#### ***Discription :***\nIn previous Two Graphs GCC is familier in Research Scienist, So, this graph shows that Google cloud Compute Engine is also very known "
##### ***Mostly use the following data storage products***
#### ***Discription :***\nGoogle Cloud Storage Servies is widely use in Data storage.
"##### ***Mostly use the following data products (relational databases, data warehouses, data lakes,or similar)***"
#### ***Discription :***\nData Storage products work with data products. mean how you want to storge the data. My SQL is vwidely use in Research. 
##### ***Mostly Use Following business intelligence tools***
#### ***Discription :***\nMS Power BI is very populer in Community.
##### ***Following managed machine learning products***
"#### ***Discription :***\nGraph shows that, In Managed Machine Learning products Amazon SageMarker is very common in managers."
##### ***Following automated machine learning tools***
#### ***Discription :***\nGoogle Cloud Auto ML is widely use in Manager.
##### ***Following products to serve your machine learning models***
#### ***Discription :***\nML Flow is the product that serve Machine Learning models
##### ***Tools to help monitor your machine learning models and/or experiments***
#### ***Discription :***\nTensorboard is most populer tools to monitor your machine learning models. 
##### ***following responsible or ethical AI products in your machine learning practices***
##### ***following types of specialized hardware when training machine learning models***
##### ***following types of specialized hardware when training machine learning models***
#### ***Discription :***\nGPU is the specilized hardware which are mostly use in research Scientist
##### ***favorite media sources that report on data science topics***
"#### ***Discription :***\nAs you see in the graph, Resarh Scientist have the most interest on th journal Publication and other type of publication."
##### ***What products or platforms did Machine Learning/ MLops Engineer find to be most helpful when they first started studying datascience?***
"- The most helpful platform for Machine Learning/ MLops Engineer is Kaggle, Youtube, and Coursera."
##### ***What programming languages Machine Learning/ MLops Engineer use on a regular basis?***
- The most used programming language by Machine Learning/ MLops Engineer is Python.
##### ***Which of the following integrated development environments (IDE's) do Machine Learning/ MLops Engineer use on a regular basis?***
- the most used IDE by Machine Learning/ MLops Engineer is Jupyter Notebook
##### ***Do Machine Learning/ MLops Engineer use any of the following hosted notebook products?*** 
- Most of the Machine Learning/ MLops Engineer use Kaggle Notebook and Google Colab.
##### ***Data Visualization Libraries Used by Machine Learning/ MLops Engineer on regular basis***
"- The most used Data Visualization Libraries by Machine Learning/ MLops Engineer is Matplotlib,Seaborn and plotly"
##### ***Which of the following machine learning frameworks Machine Learning/ MLops Engineer use on a regular basis?***\n
"- Scikit learnm Tensorflow, Keras, Pytorch are the most used machine learning frameworks by Machine Learning/ MLops Engineer."
##### ***Which of the following ML algorithms Machine Learning/ MLops Engineer use on a regular basis?***
"- Linear or Logistic Regression, Decision Trees or Random Forests, Gradient Boosting Machines (GBM's), Convolutional Neural Networks (CNN's) are the most used ML algorithms by Machine Learning/ MLops Engineer."
##### ***Which categories of computer vision methods does Machine Learning/ MLops Engineer use on a regular basis?***
"- Most of the Machine Learning/MLops are not familiar with computer vision methods, but those who are familiar with it use Image classification and other general purpose networks."
##### ***Which of the following natural language processing (NLP) methods Machine Learning/ MLops Engineer use on a regular basis?***
"- Most of the Machine Learning/MLops are usinf word embeddings and encoder-decoder models, but a large portion of them are not familiar with NLP methods."
##### ***Do Machine Learning/ MLops Engineer download pre-trained model weights from any of the serivces***
"- Most of the MLops are not familiar with pre-trained model weights, but those who are familiar with it use Hugging face model and Tensroflow Hub."
##### ***Which of the following ML model hubs/repositories do Machine Learning/ MLops Engineer use most often?***
"- Hugging face model, Kaggle Dataset and Tensroflow Hub are the most used ML model hubs/repositories by Machine Learning/ MLops Engineer."
##### ***Which of the following cloud computing platforms do Machine Learning/ MLops Engineer use?***
"- Amazon Web Services (AWS), Google Cloud Platform (GCP), Microsoft Azure are the most used cloud computing platforms by Machine Learning/ MLops Engineer."
"##### ***Of the cloud platforms that you are familiar with, which has the best developer experience (mostenjoyable to use)?***"
- AWS has the best developer experience for Machine Learning/ MLops Engineer.
##### ***Do Machine Learning/ MLops Engineer use any of the following cloud computing products?***
"- Most of the Machine Learning/ MLops Engineer use Amazon Elastic Compute Cloud (EC2), Google Cloud Compute Engine and Microsoft Azure Virtual Machines."
##### ***Do Machine Learning/ MLops Engineer use any of the following data storage products?***
"- Most of the Machine Learning/ MLops Engineer use Amazon S3, Google Cloud Storage and Microsoft Azure Blob Storage"
"##### ***Do Machine Learning/ MLops Engineer use any of the following data products (relational databases, data warehouses, data lakes,or similar)?***"
"- Most of MLops are not using any of the following data products, but those who are using it use MySQL, PostgreSQL, Google BigQuery and Microsoft SQL Server."
##### ***Do Machine Learning/ MLops Engineer use any of the following business intelligence tools?***
##### ***Do Machine Learning/ MLops Engineer use any of the following managed machine learning products?***
##### ***Do Machine Learning/ MLops Engineer use any of the following managed machine learning products?***
"- Most of the MLops dont use any managed Ml products but thosem who are using  uses Amazon SageMaker, Google Cloud AI Platform and Microsoft Azure Machine Learning Studio."
##### ***Do Machine Learning/ MLops Engineer use any of the following automated machine learning tools?***
##### ***Do Machine Learning/ MLops Engineer use any of the following products to serve your machine learning models?***
##### ***Do Machine Learning/ MLops Engineer use any of the following products to serve your machine learning models?***
- Most of MLops Dont use any of the product but those who are using they uses MLflow and TFX
##### ***Do Machine Learning/ MLops Engineer use any tools to help monitor your machine learning models and/or experiments?***
"- Most of MLops Dont use any of the tools but those who are using they uses TensorBoard, Ml flow and Weights & Biases "
##### ***Do Machine Learning/ MLops Engineer use any of the following responsible or ethical AI products in your machine learning practices?***
"- Most of the Mlops engineer dont use any of the responsible or ethical AI products but those who are using they uses Google Responsible AI, Amazon AI ethics and Microsoft AI."
##### ***Do Machine Learning/ MLops Engineer use any of the following types of specialized hardware when training machine learning models?***
- Most of the Mlops engineer uses GPu's and TPu's
##### ***Who/what are Machine Learning/ MLops Engineer favorite media sources that report on data science topics?***
"- Most of the Mlops engineer uses Kaggle, Youtube and Blogs."
##### ***What products or platforms did Statistician find to be most helpful when they first started studying Statistics?***
##### ***What programming languages Statistician use on a regular basis?***
##### ***What programming languages Statistician use on a regular basis?***
##### ***Which of the following integrated development environments (IDE's) do Statistician use on a regular basis?***
##### ***Which of the following integrated development environments (IDE's) do Statistician use on a regular basis?***
##### ***Do Statistician use any of the following hosted notebook products?*** 
##### ***Do Statistician use any of the following hosted notebook products?*** 
##### ***Data Visualization Libraries Used by Statistician on regular basis***
##### ***Data Visualization Libraries Used by Statistician on regular basis***
##### ***Which of the following machine learning frameworks Statistician use on a regular basis?***\n
##### ***Which of the following machine learning frameworks Statistician use on a regular basis?***\n
##### ***Which of the following ML algorithms Statistician use on a regular basis?***
##### ***Which of the following ML algorithms Statistician use on a regular basis?***
##### ***Which categories of computer vision methods does Statistician use on a regular basis?***
##### ***Which categories of computer vision methods does Statistician use on a regular basis?***
##### ***Which of the following natural language processing (NLP) methods Statistician use on a regular basis?***
##### ***Which of the following natural language processing (NLP) methods Statistician use on a regular basis?***
##### ***Do Statistician download pre-trained model weights from any of the serivces***
##### ***Do Statistician download pre-trained model weights from any of the serivces***
##### ***Which of the following ML model hubs/repositories do Statistician use most often?***
##### ***Which of the following ML model hubs/repositories do Statistician use most often?***
##### ***Which of the following cloud computing platforms do Statistician use?***
##### ***Which of the following cloud computing platforms do Statistician use?***
"##### ***Of the cloud platforms that Statistician are familiar with, which has the best developer experience (mostenjoyable to use)?***"
"##### ***Of the cloud platforms that Statistician are familiar with, which has the best developer experience (mostenjoyable to use)?***"
##### ***Do Statistician use any of the following cloud computing products?***
##### ***Do Statistician use any of the following cloud computing products?***
##### ***Do Statistician use any of the following data storage products?***
##### ***Do Statistician use any of the following data storage products?***
"##### ***Do Statistician use any of the following data products (relational databases, data warehouses, data lakes,or similar)?***"
"##### ***Do Statistician use any of the following data products (relational databases, data warehouses, data lakes,or similar)?***"
##### ***Do Statistician use any of the following business intelligence tools?***
##### ***Do Statistician use any of the following business intelligence tools?***
##### ***Do Statistician use any of the following managed machine learning products?***
##### ***Do Statistician use any of the following managed machine learning products?***
##### ***Do Statistician use any of the following automated machine learning tools?***
##### ***Do Statistician use any of the following automated machine learning tools?***
##### ***Do Statistician use any of the following products to serve your machine learning models?***
##### ***Do Statistician use any of the following products to serve your machine learning models?***
##### ***Do Statistician use any tools to help monitor your machine learning models and/or experiments?***
##### ***Do Statistician use any tools to help monitor your machine learning models and/or experiments?***
##### ***Do Statistician use any of the following responsible or ethical AI products in your machine learning practices?***
##### ***Do Statistician use any of the following responsible or ethical AI products in your machine learning practices?***
##### ***Do Statistician use any of the following types of specialized hardware when training machine learning models?***
##### ***Do Statistician use any of the following types of specialized hardware when training machine learning models?***
##### ***Who/what are Statistician favorite media sources that report on data science topics?***
##### ***What products or platforms did Software Engineer find to be most helpful when they first started studying Software Engineering?***
##### ***What programming languages Software Engineer use on a regular basis?***
##### ***What programming languages Software Engineer use on a regular basis?***
##### ***Which of the following integrated development environments (IDE's) do Software Engineer use on a regular basis?***
##### ***Which of the following integrated development environments (IDE's) do Software Engineer use on a regular basis?***
##### ***Do you use any of the following hosted notebook products?*** 
##### ***Do you use any of the following hosted notebook products?*** 
##### ***Data Visualization Libraries Used by Software Engineer on regular basis***
##### ***Data Visualization Libraries Used by Software Engineer on regular basis***
##### ***Which of the following machine learning frameworks Software Engineer use on a regular basis?***\n
##### ***Which of the following machine learning frameworks Software Engineer use on a regular basis?***\n
##### ***Which of the following ML algorithms Software Engineer use on a regular basis?***
##### ***Which of the following ML algorithms Software Engineer use on a regular basis?***
##### ***Which categories of computer vision methods does Software Engineer use on a regular basis?***
##### ***Which categories of computer vision methods does Software Engineer use on a regular basis?***
##### ***Which of the following natural language processing (NLP) methods Software Engineer use on a regular basis?***
##### ***Which of the following natural language processing (NLP) methods Software Engineer use on a regular basis?***
##### ***Do Software Engineer download pre-trained model weights from any of the serivces***
##### ***Do Software Engineer download pre-trained model weights from any of the serivces***
##### ***Which of the following ML model hubs/repositories do Software Engineer use most often?***
##### ***Which of the following ML model hubs/repositories do Software Engineer use most often?***
##### ***Which of the following cloud computing platforms do Software Engineer use?***
##### ***Which of the following cloud computing platforms do Software Engineer use?***
"##### ***Of the cloud platforms that you are familiar with, which has the best developer experience (mostenjoyable to use)?***"
"##### ***Of the cloud platforms that you are familiar with, which has the best developer experience (mostenjoyable to use)?***"
##### ***Do Software Engineer use any of the following cloud computing products?***
##### ***Do Software Engineer use any of the following cloud computing products?***
##### ***Do Software Engineer use any of the following data storage products?***
##### ***Do Software Engineer use any of the following data storage products?***
"##### ***Do Software Engineer use any of the following data products (relational databases, data warehouses, data lakes,or similar)?***"
"##### ***Do Software Engineer use any of the following data products (relational databases, data warehouses, data lakes,or similar)?***"
##### ***Do Software Engineer use any of the following business intelligence tools?***
##### ***Do Software Engineer use any of the following business intelligence tools?***
##### ***Do Software Engineer use any of the following managed machine learning products?***
##### ***Do Software Engineer use any of the following managed machine learning products?***
##### ***Do Software Engineer use any of the following automated machine learning tools?***
##### ***Do Software Engineer use any of the following automated machine learning tools?***
##### ***Do Software Engineer use any of the following products to serve your machine learning models?***
##### ***Do Software Engineer use any of the following products to serve your machine learning models?***
##### ***Do Software Engineer use any tools to help monitor your machine learning models and/or experiments?***
##### ***Do Software Engineer use any tools to help monitor your machine learning models and/or experiments?***
##### ***Do Software Engineer use any of the following responsible or ethical AI products in your machine learning practices?***
##### ***Do Software Engineer use any of the following responsible or ethical AI products in your machine learning practices?***
##### ***Do Software Engineer use any of the following types of specialized hardware when training machine learning models?***
##### ***Do Software Engineer use any of the following types of specialized hardware when training machine learning models?***
##### ***Who/what are Software Engineer favorite media sources that report on data science topics?***
##### ***What products or platforms did Engineer (Non-software) find to be most helpful when they first started studying Engineering (Non-software)?***
##### ***What programming languages Engineer (Non-software) use on a regular basis?***
##### ***What programming languages Engineer (Non-software) use on a regular basis?***
##### ***Which of the following integrated development environments (IDE's) do Engineer (Non-software) use on a regular basis?***
##### ***Which of the following integrated development environments (IDE's) do Engineer (Non-software) use on a regular basis?***
##### ***Do Engineer (Non-software) use any of the following hosted notebook products?*** 
##### ***Do Engineer (Non-software) use any of the following hosted notebook products?*** 
##### ***Data Visualization Libraries Used by Engineer (Non-software) on regular basis***
##### ***Data Visualization Libraries Used by Engineer (Non-software) on regular basis***
##### ***Which of the following machine learning frameworks Engineer (Non-software) use on a regular basis?***\n
##### ***Which of the following machine learning frameworks Engineer (Non-software) use on a regular basis?***\n
##### ***Which of the following ML algorithms Engineer (Non-software) use on a regular basis?***
##### ***Which of the following ML algorithms Engineer (Non-software) use on a regular basis?***
##### ***Which categories of computer vision methods does Engineer (Non-software) use on a regular basis?***
##### ***Which categories of computer vision methods does Engineer (Non-software) use on a regular basis?***
##### ***Which of the following natural language processing (NLP) methods Engineer (Non-software) use on a regular basis?***
##### ***Which of the following natural language processing (NLP) methods Engineer (Non-software) use on a regular basis?***
##### ***Do Engineer (Non-software) download pre-trained model weights from any of the serivces***
##### ***Do Engineer (Non-software) download pre-trained model weights from any of the serivces***
##### ***Which of the following ML model hubs/repositories do Engineer (Non-software) use most often?***
##### ***Which of the following ML model hubs/repositories do Engineer (Non-software) use most often?***
##### ***Which of the following cloud computing platforms do Engineer (Non-software) use?***
##### ***Which of the following cloud computing platforms do Engineer (Non-software) use?***
"##### ***Of the cloud platforms that Engineer (Non-software) are familiar with, which has the best developer experience (mostenjoyable to use)?***"
"##### ***Of the cloud platforms that Engineer (Non-software) are familiar with, which has the best developer experience (mostenjoyable to use)?***"
##### ***Do Engineer (Non-software) use any of the following cloud computing products?***
##### ***Do Engineer (Non-software) use any of the following cloud computing products?***
##### ***Do Engineer (Non-software) use any of the following data storage products?***
##### ***Do Engineer (Non-software) use any of the following data storage products?***
"##### ***Do Engineer (Non-software) use any of the following data products (relational databases, data warehouses, data lakes,or similar)?***"
"##### ***Do Engineer (Non-software) use any of the following data products (relational databases, data warehouses, data lakes,or similar)?***"
##### ***Do Engineer (Non-software) use any of the following business intelligence tools?***
##### ***Do Engineer (Non-software) use any of the following business intelligence tools?***
##### ***Do Engineer (Non-software) use any of the following managed machine learning products?***
##### ***Do Engineer (Non-software) use any of the following managed machine learning products?***
##### ***Do Engineer (Non-software) use any of the following automated machine learning tools?***
##### ***Do Engineer (Non-software) use any of the following automated machine learning tools?***
##### ***Do Engineer (Non-software) use any of the following products to serve your machine learning models?***
##### ***Do Engineer (Non-software) use any of the following products to serve your machine learning models?***
##### ***Do Engineer (Non-software) use any tools to help monitor your machine learning models and/or experiments?***
##### ***Do Engineer (Non-software) use any tools to help monitor your machine learning models and/or experiments?***
##### ***Do Engineer (Non-software) use any of the following responsible or ethical AI products in your machine learning practices?***
##### ***Do Engineer (Non-software) use any of the following responsible or ethical AI products in your machine learning practices?***
##### ***Do Engineer (Non-software) use any of the following types of specialized hardware when training machine learning models?***
##### ***Do Engineer (Non-software) use any of the following types of specialized hardware when training machine learning models?***
##### ***Who/what are Engineer (Non-software) favorite media sources that report on data science topics?***
##### ***What products or platforms did data administrator find to be most helpful when they first started studying data administrator?***
##### ***What programming languages data administrator use on a regular basis?***
##### ***What programming languages data administrator use on a regular basis?***
##### ***Which of the following integrated development environments (IDE's) do data administrator use on a regular basis?***
##### ***Which of the following integrated development environments (IDE's) do data administrator use on a regular basis?***
##### ***Do you use any of the following hosted notebook products?*** 
##### ***Do you use any of the following hosted notebook products?*** 
##### ***Data Visualization Libraries Used by data administrator on regular basis***
##### ***Data Visualization Libraries Used by data administrator on regular basis***
##### ***Which of the following machine learning frameworks data administrator use on a regular basis?***\n
##### ***Which of the following machine learning frameworks data administrator use on a regular basis?***\n
##### ***Which of the following ML algorithms data administrator use on a regular basis?***
##### ***Which of the following ML algorithms data administrator use on a regular basis?***
##### ***Which categories of computer vision methods does data administrator use on a regular basis?***
##### ***Which categories of computer vision methods does data administrator use on a regular basis?***
##### ***Which of the following natural language processing (NLP) methods data administrator use on a regular basis?***
##### ***Which of the following natural language processing (NLP) methods data administrator use on a regular basis?***
##### ***Do data administrator download pre-trained model weights from any of the serivces***
##### ***Do data administrator download pre-trained model weights from any of the serivces***
##### ***Which of the following ML model hubs/repositories do data administrator use most often?***
##### ***Which of the following ML model hubs/repositories do data administrator use most often?***
##### ***Which of the following cloud computing platforms do data administrator use?***
##### ***Which of the following cloud computing platforms do data administrator use?***
"##### ***Of the cloud platforms that you are familiar with, which has the best developer experience (mostenjoyable to use)?***"
"##### ***Of the cloud platforms that you are familiar with, which has the best developer experience (mostenjoyable to use)?***"
##### ***Do data administrator use any of the following cloud computing products?***
##### ***Do data administrator use any of the following cloud computing products?***
##### ***Do data administrator use any of the following data storage products?***
##### ***Do data administrator use any of the following data storage products?***
"##### ***Do data administrator use any of the following data products (relational databases, data warehouses, data lakes,or similar)?***"
"##### ***Do data administrator use any of the following data products (relational databases, data warehouses, data lakes,or similar)?***"
##### ***Do data administrator use any of the following business intelligence tools?***
##### ***Do data administrator use any of the following business intelligence tools?***
##### ***Do data administrator use any of the following managed machine learning products?***
##### ***Do data administrator use any of the following managed machine learning products?***
##### ***Do data administrator use any of the following automated machine learning tools?***
##### ***Do data administrator use any of the following automated machine learning tools?***
##### ***Do data administrator use any of the following products to serve your machine learning models?***
##### ***Do data administrator use any of the following products to serve your machine learning models?***
##### ***Do data administrator use any tools to help monitor your machine learning models and/or experiments?***
##### ***Do data administrator use any tools to help monitor your machine learning models and/or experiments?***
##### ***Do data administrator use any of the following responsible or ethical AI products in your machine learning practices?***
##### ***Do data administrator use any of the following responsible or ethical AI products in your machine learning practices?***
##### ***Do data administrator use any of the following types of specialized hardware when training machine learning models?***
##### ***Do data administrator use any of the following types of specialized hardware when training machine learning models?***
##### ***Who/what are data administrator favorite media sources that report on data administrator topics?***
##### ***What products or platforms did Data Engineer find to be most helpful when they first started studying Data Engineer?***
##### ***What programming languages Data Engineer use on a regular basis?***
##### ***What programming languages Data Engineer use on a regular basis?***
##### ***Which of the following integrated development environments (IDE's) do Data Engineer use on a regular basis?***
##### ***Which of the following integrated development environments (IDE's) do Data Engineer use on a regular basis?***
##### ***Do you use any of the following hosted notebook products?*** 
##### ***Do you use any of the following hosted notebook products?*** 
##### ***Data Visualization Libraries Used by Data Engineer on regular basis***
##### ***Data Visualization Libraries Used by Data Engineer on regular basis***
##### ***Which of the following machine learning frameworks Data Engineer use on a regular basis?***\n
##### ***Which of the following machine learning frameworks Data Engineer use on a regular basis?***\n
##### ***Which of the following ML algorithms Data Engineer use on a regular basis?***
##### ***Which of the following ML algorithms Data Engineer use on a regular basis?***
##### ***Which categories of computer vision methods does Data Engineer use on a regular basis?***
##### ***Which categories of computer vision methods does Data Engineer use on a regular basis?***
##### ***Which of the following natural language processing (NLP) methods Data Engineer use on a regular basis?***
##### ***Which of the following natural language processing (NLP) methods Data Engineer use on a regular basis?***
##### ***Do Data Engineer download pre-trained model weights from any of the serivces***
##### ***Do Data Engineer download pre-trained model weights from any of the serivces***
##### ***Which of the following ML model hubs/repositories do Data Engineer use most often?***
##### ***Which of the following ML model hubs/repositories do Data Engineer use most often?***
##### ***Which of the following cloud computing platforms do Data Engineer use?***
##### ***Which of the following cloud computing platforms do Data Engineer use?***
"##### ***Of the cloud platforms that you are familiar with, which has the best developer experience (mostenjoyable to use)?***"
"##### ***Of the cloud platforms that you are familiar with, which has the best developer experience (mostenjoyable to use)?***"
##### ***Do Data Engineer use any of the following cloud computing products?***
##### ***Do Data Engineer use any of the following cloud computing products?***
##### ***Do Data Engineer use any of the following data storage products?***
##### ***Do Data Engineer use any of the following data storage products?***
"##### ***Do Data Engineer use any of the following data products (relational databases, data warehouses, data lakes,or similar)?***"
"##### ***Do Data Engineer use any of the following data products (relational databases, data warehouses, data lakes,or similar)?***"
##### ***Do Data Engineer use any of the following business intelligence tools?***
##### ***Do Data Engineer use any of the following business intelligence tools?***
##### ***Do Data Engineer use any of the following managed machine learning products?***
##### ***Do Data Engineer use any of the following managed machine learning products?***
##### ***Do Data Engineer use any of the following automated machine learning tools?***
##### ***Do Data Engineer use any of the following automated machine learning tools?***
##### ***Do Data Engineer use any of the following products to serve your machine learning models?***
##### ***Do Data Engineer use any of the following products to serve your machine learning models?***
##### ***Do Data Engineer use any tools to help monitor your machine learning models and/or experiments?***
##### ***Do Data Engineer use any tools to help monitor your machine learning models and/or experiments?***
##### ***Do Data Engineer use any of the following responsible or ethical AI products in your machine learning practices?***
##### ***Do Data Engineer use any of the following responsible or ethical AI products in your machine learning practices?***
##### ***Do Data Engineer use any of the following types of specialized hardware when training machine learning models?***
##### ***Do Data Engineer use any of the following types of specialized hardware when training machine learning models?***
##### ***Who/what are Data Engineer favorite media sources that report on data science topics?***
##### ***What products or platforms did Teacher / professor find to be most helpful when they first started studying Teacher / professor?***
##### ***What programming languages Teacher / professor use on a regular basis?***
##### ***What programming languages Teacher / professor use on a regular basis?***
##### ***Which of the following integrated development environments (IDE's) do Teacher / professor use on a regular basis?***
##### ***Which of the following integrated development environments (IDE's) do Teacher / professor use on a regular basis?***
##### ***Do you use any of the following hosted notebook products?*** 
##### ***Do you use any of the following hosted notebook products?*** 
##### ***Data Visualization Libraries Used by Teacher / professor on regular basis***
##### ***Data Visualization Libraries Used by Teacher / professor on regular basis***
##### ***Which of the following machine learning frameworks Teacher / professor use on a regular basis?***\n
##### ***Which of the following machine learning frameworks Teacher / professor use on a regular basis?***\n
##### ***Which of the following ML algorithms Teacher / professor use on a regular basis?***
##### ***Which of the following ML algorithms Teacher / professor use on a regular basis?***
##### ***Which categories of computer vision methods does Teacher / professor use on a regular basis?***
##### ***Which categories of computer vision methods does Teacher / professor use on a regular basis?***
##### ***Which of the following natural language processing (NLP) methods Teacher / professor use on a regular basis?***
##### ***Which of the following natural language processing (NLP) methods Teacher / professor use on a regular basis?***
##### ***Do Teacher / professor download pre-trained model weights from any of the serivces***
##### ***Do Teacher / professor download pre-trained model weights from any of the serivces***
##### ***Which of the following ML model hubs/repositories do Teacher / professor use most often?***
##### ***Which of the following ML model hubs/repositories do Teacher / professor use most often?***
##### ***Which of the following cloud computing platforms do Teacher / professor use?***
##### ***Which of the following cloud computing platforms do Teacher / professor use?***
"##### ***Of the cloud platforms that you are familiar with, which has the best developer experience (mostenjoyable to use)?***"
"##### ***Of the cloud platforms that you are familiar with, which has the best developer experience (mostenjoyable to use)?***"
##### ***Do Teacher / professor use any of the following cloud computing products?***
##### ***Do Teacher / professor use any of the following cloud computing products?***
##### ***Do Teacher / professor use any of the following data storage products?***
##### ***Do Teacher / professor use any of the following data storage products?***
"##### ***Do Teacher / professor use any of the following data products (relational databases, data warehouses, data lakes,or similar)?***"
"##### ***Do Teacher / professor use any of the following data products (relational databases, data warehouses, data lakes,or similar)?***"
##### ***Do Teacher / professor use any of the following business intelligence tools?***
##### ***Do Teacher / professor use any of the following business intelligence tools?***
##### ***Do Teacher / professor use any of the following managed machine learning products?***
##### ***Do Teacher / professor use any of the following managed machine learning products?***
##### ***Do Teacher / professor use any of the following automated machine learning tools?***
##### ***Do Teacher / professor use any of the following automated machine learning tools?***
##### ***Do Teacher / professor use any of the following products to serve your machine learning models?***
##### ***Do Teacher / professor use any of the following products to serve your machine learning models?***
##### ***Do Teacher / professor use any tools to help monitor your machine learning models and/or experiments?***
##### ***Do Teacher / professor use any tools to help monitor your machine learning models and/or experiments?***
##### ***Do Teacher / professor use any of the following responsible or ethical AI products in your machine learning practices?***
##### ***Do Teacher / professor use any of the following responsible or ethical AI products in your machine learning practices?***
##### ***Do Teacher / professor use any of the following types of specialized hardware when training machine learning models?***
##### ***Do Teacher / professor use any of the following types of specialized hardware when training machine learning models?***
##### ***Who/what are Teacher / professor favorite media sources that report on Teacher / professor topics?***
##### 2.	How many people required for data science workload?
## Leading Platforms ðŸ¤¦â€â™‚ï¸
##### Most tool for monitoring machine learning models
The Plot shows that 460+ people use TensorBoard as Tool for monitoring machine Learning models 
##### Most Used automated machine learning tools
The Plot shows that 340+ people use MLflow as Automated Tool for Machine Learning
##### Most used Product based on Machine learning models
The Plot shows that 250+ people use Google cloud auto ML as Product based on Machine Learning models
##### Most used product management tools for machine learning
The Plot shows that 300+ people use Amazon SageMaker as Product Mangement Tool for Machine Learning
##### Most used ML model hubs/repositories
The Plot shows that 1600+ people use kaggle dataset as Machine Learning model hubs/repositorie
##### Most used pre-trained model weights
The Plot shows that 2700+ people use kaggle dataset as Machine Learning pre trained Models 
##### Most used machine learning frameworks
The Plot shows that 2200+ people Scikit-Learn as Machine Learning Plateform  
## Database
"### **Report for Business Analytics**\n  In this survey we analyze the business aspect and impact of having applying Machine Learning and Data Science in the coo-operate or IT sectors and what are the benefits we get from it.\n  \n  We concluded the following results in survey.\n## Scope of the Industry\n1. According to the survey analysis on average 1 out of 4 users are spending their money on Machine Learning & Cloud Computing services at home or at work.\n  \n2. According to the survey analysis on average 8% of the businesses are still using Machine Learning method and average 5% of the businesses recently start using it.\n   \n3. According to the survey analysis on average minimum 1 to 4 or more data scientist required based on the organization to implement the Machine Learning Methods.\n## Machine Learning\n1. According to the survey analysis, Tensor Board becomes the most popular tool for monitoring machine learning models its capture more than 1/3rd of the Market.\n   \n2. According to the survey analysis, ML flow and Tensor flow Extended (TFX) the most popular automated machine learning tools has approx. 60% of the collective user of the total users.\n   \n3. According to the survey analysis, the most popular product based on Machine learning models is Google Cloud AutoML has a market share of average 28% users.\n   \n4. According to the survey analysis, Amazon Sage Maker the most popular product management tools for machine learning has average 30% of the users used it.\n   \n5. According to the survey analysis, the most popular ML model hubs/repositories are Kaggle Datasets capture approx. 1/2 of market users.\n   \n6. According to the survey analysis, the Kaggle Dataset has no competitor in users to beat in pre-trained model weights.\n  \n7. According to the survey analysis, the most popular cloud computing services are AWS, Azure, and Google Cloud Platform.\n   \n8. According to the survey analysis, the Scikit-Learn are the most popular Machine Learning Frame Work as approx. 30% of the userâ€™s shares. "
"Let's get to know our data by performing a preliminary data analysis.\n\n#  Part 1. Preliminary data analysis\n\nFirst, we will initialize the environment:"
"You should use the `seaborn` library for visual analysis, so let's set it up too:"
"You should use the `seaborn` library for visual analysis, so let's set it up too:"
"To make it simple, we will work only with the training part of the dataset:"
It would be instructive to peek into the values of our variables.\n \nLet's convert the data into *long* format and depict the value counts of the categorical features using [`factorplot()`](https://seaborn.pydata.org/generated/seaborn.factorplot.html).
We can see that the target classes are balanced. That's great!\n\nLet's split the dataset by target values: sometimes you can immediately spot the most significant feature on the plot.
We can see that the target classes are balanced. That's great!\n\nLet's split the dataset by target values: sometimes you can immediately spot the most significant feature on the plot.
"You can see that the target variable greatly affects the distribution of cholesterol and glucose levels. Is this a coincidence?\n\nNow, let's calculate some statistics for the feature unique values:"
"# Part 2. Visual data analysis\n\n## 2.1. Correlation matrix visualization\n\nTo understand the features better, you can create a matrix of the correlation coefficients between the features. Use the initial dataset (non-filtered).\n\n### Task:\n\nPlot a correlation matrix using [`heatmap()`](http://seaborn.pydata.org/generated/seaborn.heatmap.html). You can create the matrix using the standard `pandas` tools with the default parameters.\n\n### Solution:"
"**Question 2.1. (1 point).** Which pair of features has the strongest Pearson's correlation with the *gender* feature?\n\n1. Cardio, Cholesterol\n2. Height, Smoke\n3. Smoke, Alco\n4. Height, Weight"
"## 2.2. Height distribution of men and women\n\nFrom our exploration of the unique values we know that the gender is encoded by the values *1* and *2*. And though you don't have a specification of how these values correspond to men and women, you can figure that out graphically by looking at the mean values of height and weight for each value of the *gender* feature.\n\n### Task:\n\nCreate a violin plot for the height and gender using [`violinplot()`](https://seaborn.pydata.org/generated/seaborn.violinplot.html). Use the parameters:\n- `hue` to split by gender;\n- `scale` to evaluate the number of records for each gender.\n\nIn order for the plot to render correctly, you need to convert your `DataFrame` to *long* format using the `melt()` function from `pandas`. Here is [another example](https://stackoverflow.com/a/41575149/3338479) of this.\n\n### Solution:"
"### Task:\n\nCreate two [`kdeplot`](https://seaborn.pydata.org/generated/seaborn.kdeplot.html)s of the *height* feature for each gender on the same chart. You will see the difference between the genders more clearly, but you will be unable to evaluate the number of records in each of them.\n\n### Solution:"
"### Task:\n\nCreate two [`kdeplot`](https://seaborn.pydata.org/generated/seaborn.kdeplot.html)s of the *height* feature for each gender on the same chart. You will see the difference between the genders more clearly, but you will be unable to evaluate the number of records in each of them.\n\n### Solution:"
"## 2.3. Rank correlation\n\nIn most cases, *the Pearson coefficient of linear correlation* is more than enough to discover patterns in data. \nBut let's go a little further and calculate a [rank correlation](https://en.wikipedia.org/wiki/Rank_correlation). It will help us to identify such feature pairs in which the lower rank in the variational series of one feature always precedes the higher rank in the another one (and we have the opposite in the case of negative correlation).\n\n### Task:\n\nCalculate and plot a correlation matrix using the [Spearman's rank correlation coefficient](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient).\n\n### Solution:"
"## 2.3. Rank correlation\n\nIn most cases, *the Pearson coefficient of linear correlation* is more than enough to discover patterns in data. \nBut let's go a little further and calculate a [rank correlation](https://en.wikipedia.org/wiki/Rank_correlation). It will help us to identify such feature pairs in which the lower rank in the variational series of one feature always precedes the higher rank in the another one (and we have the opposite in the case of negative correlation).\n\n### Task:\n\nCalculate and plot a correlation matrix using the [Spearman's rank correlation coefficient](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient).\n\n### Solution:"
"**Question 2.2. (1 point).** Which pair of features has the strongest Spearman correlation?\n\n1. Height, Weight\n2. Age, Weight\n3. Cholesterol, Gluc\n4. Cardio, Cholesterol\n5. Ap_hi, Ap_lo\n6. Smoke, Alco"
"### Task:\n\nCreate a *count plot* using [`countplot()`](http://seaborn.pydata.org/generated/seaborn.countplot.html), with the age on the *X* axis and the number of people on the *Y* axis. Each value of the age should have two columns corresponding to the numbers of people of this age for each *cardio* class.\n\n### Solution:"
**Question 2.4. (1 point).** At what age does the number of people with CVD outnumber the number of people without CVD for the first time?\n\n1. 44\n2. 55\n3. 64\n4. 70
"# Introduction: Taxi Fare Prediction\n\nWelcome to another Kaggle challenge. In this contest, the aim is to predict the fare of a taxi ride given the starting time, the starting and ending latitude / longitude, and the number of passengers. This is a __supervised regression__ machine learning task.\n\nIn this notebook, I'll provide you with a solid foundation and leave you with the challenge to better the score. Although the dataset is large, this is an approachable problem and as usual with Kaggle competitions, provides realistic practice for building a machine learning solution. The best way to learn is by doing, so let's work through a complete machine learning problem!\n\nGreat resources for Kaggle competitions are the [discussion forums](https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/discussion) and the [kernels](https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/kernels) completed by other data scientists. I recommend using, adapting, and building on others' code, especially when you are getting started."
"## Read in 5 million rows and examine data\n\nThroughout this notebook, we will work with only 5 million rows (out of 55 million). The first point for improvement might therefore be to use more data!\n\n* __Potential improvement 1: use more data__\n\nGenerally, performance of a machine learning model increases as the amount of training data increases. However, there might be diminishing returns, and I sample the data here in order to train faster. The data file is randomly sorted by date, so taking the first 5 million rows is a random sample in terms of time. \n\nWhen we read in the data, we tell pandas to treat the `pickup_datetime` as a date. We will also drop the `key` since it is a unique identifier and does not tell us anything about the taxi trip. After reading in the data we'll remove any rows with `nan`  (observations with missing entries). "
"For visualization purposes, I'll create a binned version of the fare. This divides the variable into a number of bins, turning a continuous variable into a discrete, categorical variable."
### Empirical Cumulative Distribution Function Plot\n\nAnother plot for showing the distribution of a single variable is the [empirical cumulative distribution function](https://en.wikipedia.org/wiki/Empirical_distribution_function). This shows the percentile on the y-axis and the variable on the x-axis and gets around some of the issues associated with binning data for histograms or the kernel width of the KDE.\n
"This shows the distribution is heavily right skewed. Most of the fares are below \$20, with a heavy right tail of larger fares.\n\n#### Other Outliers\n\nWe can also remove observations based on outliers in other columns. First we'll make a graph of the passenger counts which seemed to have some suspicious values."
"Based on this graph, we'll remove any passenger counts greater than 6."
Now we can graph the `latitude` and `longitude` columns to see the distribution. We'll just sample 10000 values so the plot doesn't take too long to generate.
"### Rides on Map of NYC\n\nFor a more contextualized representation, we can plot the pickup and dropoff on top of a map of New York. The following code is taken directly from https://www.kaggle.com/breemen/nyc-taxi-fare-data-exploration by Kaggle user [breeman](https://www.kaggle.com/breemen). All credit goes to him and please check out the rest of his kernel for more excellent work! \n\nThe map was extracted from OpenStreetMaps (https://www.openstreetmap.org/export#map=12/40.7250/-73.8999)"
Another plot we can make is the passenger count distribution colored by the fare bin.
"There does not appear to be much difference between the number of passengers. To get a more accurate picture, we can calculate the actual stats."
"The test distribution seems to be similar to the training distribution.\n\nAs a final step, we can find the correlations between distances and fares."
"All the measures of distance have a _positive_ linear correlation with the fare, indicating that as they increase, the fare tends to increase as well. \n\nThe correlation coefficient measures the strength and direction of a linear relationship. Because the linear relationship with the target variable is so strong, we may be able to just use a linear model (regression) to accurately predict the fares. "
"As a sanity check, we can plot the predictions."
"The predicted distribution appears reasonable. Because the competition uses root mean squared error as the metric, any predictions that are far off will have an outsized effect on the error. Let's look at predictions that were greater than \$100."
"Using this one more feature improved our score slightly. Here's another chance for improvement using the same model:\n\n* __Potential Improvement 3: find an optimal set of features or construct more features__. This can involve [feature selection](http://scikit-learn.org/stable/modules/feature_selection.html) or trying different combinations of features and evaluating them on the validation data. You can build additional features by looking at others' work or researching the problem.\n\n#### Collinear Features\n\nOne thing we do want to be careful about is highly correlated, known as [collinear](https://en.wikipedia.org/wiki/Multicollinearity), features. These can decrease the generalization performance of the model and lead to less interpretable models. Many of our features are already highly correlated as shown in the heatmap below. This plots the Pearson Correlation Coefficient for each pair of variables."
You might not want to use two variables that are very highly correlated with each other (such as `euclidean` and `haversine`) because of issues with interpretability and performance.
"The random forest does much better than the simple linear regression. This indicates that the problem is not completely linear, or at least is not linear in terms of the features we have constructed. From here going forward, we'll use the same random forest model because of the increased performance. \n\n#### Overfitting\n\nGiven the gap between the training and the validation score, we can see that our model is __overfitting__ to the training data. This is one of the most common problems in machine learning and is usually addressed either by training with more data, or adjusting the hyperparameters of the model. This leads to another recommendation for improvement:\n\n* __Potential Improvement 4: Try searching for better random forest model hyperparameters__. You may find Scikit-Learn's `RandomizedSearchCV` a useful tool.\n\nI'll provide some starter code for hyperparameter optimization later in the notebook. \n\nNext we can make predictions with the random forest for uploading to the competition."
This time we don't see any extreme predictions as we saw with the first linear regression. The random forest tends to not produce outlying predictions because the voting of the trees means that any single tree that estimates an extreme value will be balanced by the other predictions. \n\nLet's look at the 3 predictions the original simple linear regression estimated as over \$100.
"It appears that using more features helps the model! We can look at the feature importances to see which the model considers ""most relevant"".\n\n#### Feature Importances"
"The `haversine` distance is by far the most important with the other features showing considerably less relevance to the model. This suggests that distance is key, and we might want to find a more accurate way of calculating distances."
"The `haversine` distance is by far the most important with the other features showing considerably less relevance to the model. This suggests that distance is key, and we might want to find a more accurate way of calculating distances."
"# Additional Feature Engineering\n\nWe saw that adding more features improves the performance of the model. A natural progression is therefore to use even more features! We have not made any use of the `pickup_datetime` which provides the precise moment of pickup and that's where we'll turn our attention to next. \n\n## Extract Datetime Information\n\nWe can write a simple function that extracts as much date and time information from a datetime as possible. This is adapted from the excellent fast.ai library, in particular, the structured module (available at https://github.com/fastai/fastai/blob/master/fastai/structured.py). I have made a few changes based on what's worked best for me in the past on time-series problems."
"## Explore Time Variables\n\nWe now have a ton of time-variables to explore! First, let's ask the question if fares have increased over time. To do this, we can plot the `time_elapsed` versus the fare."
There appears to be a minor increase in prices over time which might be expected taking into account inflation. Let's look at the average fare amount by the hour of day.
There appears to be a minor increase in prices over time which might be expected taking into account inflation. Let's look at the average fare amount by the hour of day.
We can make the same plot by day of the week.
We can make the same plot by day of the week.
"Both of these plots do not seem to show much difference between the different times. \n\n### Fractional Time Plots\n\nAs a final exploration of the time variables, we can plot the fare amount versus the fractional time. "
"Both of these plots do not seem to show much difference between the different times. \n\n### Fractional Time Plots\n\nAs a final exploration of the time variables, we can plot the fare amount versus the fractional time. "
"None of these graphs are very decisive. One interesting thing to note is the horizontal bars at different fare amounts. This suggests there may be certain routes that always have the same fare amount. We explored the fare distribution earlier, but it might be a good idea to revisit the abnormalities in the fares."
#### Correlations with Target\n\nAgain we can show the correlations of all features with the target. 
It seems the most useful time variables may be the `Year` or `Elapsed` because most of the time features have a small correlation with the target. The `Elapsed` correlation is positive indicating that fares have tended to increase over time.
It seems that the new features helped both the random forest and the linear regression. Let's take a look at the random forest feature importances.
"Once again, the `haversine` distance dominates the importance. The time elapsed since the first record seems to be relatively important although the other time features do not seem to be of much use. "
"The best model from random search exhibits less overfitting, but also does not do as well on the validation data. There are probably further benefits from more hyperparameter tuning using additional data. "
"## Better Model\n\nAlthough the random forest has high performance, it does not always work the best for every problem. There are still more [models to try](http://scikit-learn.org/stable/supervised_learning.html):\n\n* __Potential improvement 5: try more models.__ You might find the Gradient Boosting Machine or Deep Neural Networks to be capable learners.\n\n# Your Challenge\n\nFrom here, we can still engineer more features, perform feature selection, or we can try upgrading the model. At this point, I'm going to leave it up to you! I've given you a decent start and some recommendations for improvement so I'll hand things over to you. __Try to build a model that outperforms those in this notebook on the validation data.__\n\nThe best performance of the models is as follows:\n\n| Model         | Train RMSE | Validation RMSE |\n|---------------|------------|-----------------|\n| Linear        | 4.87       | 4.86            |\n| Random Forest | 2.62       | 3.38            |\n\nIt's important to try and beat the __validation score__ as opposed to the training score because you can always get a better training score by training a model with greater capacity (more flexibility to fit the data). However, this won't improve the generalization performance, which is the goal of machine learning: build a model that generalizes to new data! You can also submit the predictions from this notebook and try to better the testing predictions. I look forward to seeing what you can come up with.\n\nI'm working on some more notebooks with additional performance gains, but see if you can take these methods and improve! To wrap up, here are my recommendations:\n\n1. Train with more data\n2. Experiment with different methods for filtering outliers\n3. Find an optimal set of features or build more features\n4. Hyperparameter tuning of the random forest\n5. Try more complex models such as the gradient boosting machine\n\nI have to add that these are all __potential__ improvements because of course we don't know if they will work until we try! I wish you the best of luck, and I'll be back in another kernel trying some improvements of my own. \n\nBest,\n\nWill"
\n\n\n\n\n\n4Â Â PREPARE THE DATASETÂ Â Â Â â¤’\n\nIn this section we prepare the **`tf.data.Datasets`** we will use for training and validation
"4.1 READ TFRECORD FILES - CREATE THE RAW DATASET(S)\n\n---\n\nHere we will leverage **`tf.data.TFRecordDataset`** to read the TFRecord files.\n* The simplest way is to specify a list of filenames (paths) of TFRecord files.\n* It is a subclass of **`tf.data.Dataset`**.\n\nThis newly created raw dataset contains **`tf.train.Example`** messages, and when iterated over it, we get scalar string tensors."
"5.3 CREATE A LEARNING RATE SCHEDULER\n\n---\n\nAdapting the learning rate for your stochastic gradient descent optimization procedure can increase performance and reduce training time. Sometimes this is called learning rate annealing or adaptive learning rates. Here we will call this approach a learning rate schedule. See [**this article**](https://machinelearningmastery.com/using-learning-rate-schedules-deep-learning-models-python-keras/) for a basic tutorial on learning rade schedules.\n\n\nWe will utilize a basic step function following a warmup phase. Warmup is commonly used in learning rate schedule where we start training a model with a much smaller learning rate and increase it during the first few epochs/steps until the initial learning rate is used.\n\nIntuitively, this method will allow a model to adjust itself less before it becomes more familiar with the dataset. This usually prevents breaking pretrained weights. For adaptive optimisers like Adam, warmup also allows the optimizers to compute bettere statistics of the gradients."
5.4 WRAP THE CONFIGURATION DETAILS IN A CLASS OBJECT FOR EASY ACCESS\n\n---\n
6.4 CUSTOM TRAIN LOOP\n\n---\n\nINFORMATION
6.5 JUST-IN-CASE SAVE\n\n---\n\nINFORMATION
6.6 VIEW PREDICTIONS & DISTRIBUTION OF LEVENSHTEIN DISTANCE FOR VAL DATASET\n\n---\n\nINFORMATION
\n\n\n\n\n\n7Â Â INFER ON TEST DATAÂ Â Â Â â¤’\n\nIn this section we will use our trained model to generate the predictions we will use to submit to the competition
"## Motivation\n\nAt a high level, one can observe there is a periodicity to the number of transactions. They vary both over a one day period and potentially over a week.\n\nBelow, we show a histogram of the number of transactions per time interval to display this periodicity."
"Next we use our 'make_day_feature' function to create our new feature. We use the offset parameter to change what we define as the 'start' of the day. \n\nAs we can see, there is not a significant correlation on the day of the week and whether a transaction is fraudulent. This feature on is unlikely to be useful."
"Next we use our 'make_day_feature' function to create our new feature. We use the offset parameter to change what we define as the 'start' of the day. \n\nAs we can see, there is not a significant correlation on the day of the week and whether a transaction is fraudulent. This feature on is unlikely to be useful."
Next we create a feature which encodes the (relative) hour of the day: 
Next we create a feature which encodes the (relative) hour of the day: 
"Wow! Clearly the time of day has a strong dependence on whether the transaction is fraudulent and will likely make a good feature for our machine learning models.\n\nPhysically, the higher fraction of fraudulent transactions coincides with when there are a low number of transactions. This likely reflects international fraud: e.g., card details being used by a third-party in a fraudulent way in a country different to where the card was issued.  \n\nGoing forward, this is a feature you can use in your models and it will likely be a good predictor of the target."
"\nNext, we define a function to visualize the percentage of missing values in each feature of a DataFrame in the form of a barplot. In this way, depending on the percentage of missing values in each feature, we can decide how to deal with missing values:"
We call â€â€the __missing_percent_plot__ function on the training data:
\n# Step 10 | Correlation Analysis\n\nâ¬†ï¸ [Tabel of Contents](#contents_tabel)
\n\nConclusion:\n\n* Target has the highest correlation first with `Sex` and then with `Pclass`.
"\n\nNow we define a function that provides a complete report of the model's performance on the training and test data, plus the confusin_matrix of the model on the test data and the summary of the model performance using the above function:"
\n    \nLet's evaluate our XGBoost classifier using the above function:
"The average age of male customers is lightly higher than female ones (39.8 versus 38.1). Distribution of male age is more uniform than females, where we can observe that the biggest age group is 30-35 years old. Kolgomorov-Smirnov test shows that the differences between these two groups are statistically insignificant."
There are slightly more female customers than male ones (112 vs. 87). Females are 56% of total customers.
There is a negligible correlation between age and annual income of customers for both sex groups.
There are week negative correlations (<0.5) between age and spending score for both sex groups.
There are week negative correlations (<0.5) between age and spending score for both sex groups.
There is a negligible correlation between annual income and spending score of customers for both sex groups.
"In order to find an appropriate number of clusters, the elbow method will be used. In this method for this case, the inertia for a number of clusters between 2 and 10 will be calculated. The rule is to choose the number of clusters where you see a kink or ""an elbow"" in the graph."
"The graph above shows the reduction of a distortion score as the number of clusters increases. However, there is no clear ""elbow"" visible. The underlying algorithm suggests 5 clusters. A choice of 5 or 6 clusters seems to be fair. \n\nAnother way to choose the best number of clusters is to plot the silhuette score in a function of number of clusters. Let's see the results."
"The graph above shows the reduction of a distortion score as the number of clusters increases. However, there is no clear ""elbow"" visible. The underlying algorithm suggests 5 clusters. A choice of 5 or 6 clusters seems to be fair. \n\nAnother way to choose the best number of clusters is to plot the silhuette score in a function of number of clusters. Let's see the results."
Silhouette score method indicates the best options would be 5 or 6 clusters. Let's compare both.
"The biggest cluster is a cluster number 1 with 79 observations (""medium-medium"" clients). There are two the smallest ones each containing 23 observations (cluster 3 ""high-high"" and cluster 0 ""low-high"" clients). Below there is a 3D projection of 5 generated clusters. It is not very helpful in terms of a visualisation in a static mode but if you run the code in an interactive environment (e.g. Spyder) you can rotate it!"
Below a Plotly version:
Below a Plotly version:
To check the quality of each cluster we can examine the Silhuette plot.
To check the quality of each cluster we can examine the Silhuette plot.
**6 CLUSTERS**
A heatplot below shows how many clusters were generated by the DBSCAN algorithm for the respective parameters combinations.
"The heatplot above shows, the number of clusters vary from 17 to 4. However, most of the combinations gives 4-7 clusters.\nTo decide which combination to choose I will use a metric - a silhuette score and I will plot it as a heatmap again."
"The heatplot above shows, the number of clusters vary from 17 to 4. However, most of the combinations gives 4-7 clusters.\nTo decide which combination to choose I will use a metric - a silhuette score and I will plot it as a heatmap again."
Global maximum is 0.26 for `eps`=12.5 and `min_samples`=4.
DBSCAN created 5 clusters plus outliers cluster (-1). Sizes of clusters 0-4 vary significantly - some have only 4 or 8 observations. There are 18 outliers.
The graph above shows that there are some outliers - these points do not meet distance and minimum samples requirements to be recognised as a cluster.
Number of faults in the system according to their Fault_Type\n
  \n\n
"\n    \nVoltage or Current graph, where there is large fluctuation in the graph, there faults have occurred\n    \n"
 Separating Faults into different Categories
  
2. Faulty System with Line A to Ground Fault\n
\nLogistic Regression\n
\nDecision Tree Classifier\n
\nRandom Forest Classifier\n
\nXGB Classifier\n
\nXGB Classifier\n
\nSupport Vector Machines\n
\nSupport Vector Machines\n
 Comparing Different Models 
#  | GRAPH â„–1
"Ð¡onclusions from the graph â„–1:\n- Between 1990 and 2000 Ñhanges are insignificantly\n- Between 2014 and 2020,  military spending around the world did not change  \n- Sharp increase is noted in the period from 2002 to 2012"
### Import Libraries
In this kernel I speed up feature importance using scikit-learn-intelex on different datasets:
Let's look at the achieved acceleration:
*Please upvote if you liked it.*
V \n## 2- Import
 \n## 3- Estimator
\n# 1. Import of Libraries
"\n# 2. Data visualization\n\nSpecial thanks and reference: [SIDDHESH PUJARI](https://www.kaggle.com/siddheshpujari) and his [notebook](https://www.kaggle.com/siddheshpujari/forest-cover-eda#Dataset-Info).\n>The dataset includes four wilderness areas located in the Roosevelt National Forest in northern Colorado.\n>The Roosevelt National Forest is a National Forest, located in north central Colorado.\n>Each observation is a 30m x 30m patch.\n>\n>* **Elevation** - Elevation in meters.\n>* **Aspect** - Aspect in degrees azimuth.\n>To study how aspect works , please refer the following website [link](https://pro.arcgis.com/en/pro-app/tool-reference/3d-analyst/how-aspect-works.htm) that explains how it works.\n>\n>* **Slope** - Slope in degrees. To study how the slope works, please refer the following website [link](https://pro.arcgis.com/en/pro-app/tool-reference/3d-analyst/slope.htm).\n>* **Horizontal_Distance_To_Hydrology** - Horz Dist to nearest surface water features.\n>* **Vertical_Distance_To_Hydrology** - Vert Dist to nearest surface water features.\n>* **Horizontal_Distance_To_Roadways** - Horz Dist to nearest roadway.\n>* **Hillshade_9am (0 to 255 index)** - Hillshade index at 9am, summer solstice.\n>To study how aspect works , please refer the following website [link](https://pro.arcgis.com/en/pro-app/tool-reference/3d-analyst/hillshade.htm)\n>\n>* **Hillshade_Noon (0 to 255 index)** - Hillshade index at noon, summer solstice.\n>* **Hillshade_3pm (0 to 255 index)** - Hillshade index at 3pm, summer solstice.\n>* **Horizontal_Distance_To_Fire_Points** - Horz Dist to nearest wildfire ignition points.\n>* **Wilderness_Area** (4 binary columns, 0 = absence or 1 = presence) - Wilderness area designation.\n>* **Soil_Type** (40 binary columns, 0 = absence or 1 = presence) - Soil Type designation.\n> Cover_Type (7 types, integers 1 to 7) - Forest Cover Type designation.\n>\n> **Seven Types of Forest Cover**:\n>* 1 - Spruce/Fir.\n>* 2 - Lodgepole Pine.\n>* 3 - Ponderosa Pine.\n>* 4 - Cottonwood/Willow.\n>* 5 - Aspen.\n>* 6 - Douglas-fir.\n>* 7 - Krummholz.\n>\n> **Let's read the data first** (I strongly recommend using 'datatable' to for faster data reading):"
> **Now let's plot our target values**:
"> Well **`df_train`** is **extremely class-imbalanced**. The classes 4, 5, 6 and 7 combined form only 1% of all observations.\n>\n> Assumption: If we are to choose folds for training, we are better going with Kfolds.\n>\n> **Next**, let's get 30000 samples and plot it:"
\n# 4. Exploratory Data Analysis\n\n> Let's take a closer look at the distribution of the features:
"> **We have plotted Probability Density Function estimation for each feature. What does it tell us?**\n>* The features are distributed differently;\n>* The data is not perfectly symmetrical. The most of the features either right or left skewed.\n>* Only **Hillshade_3pm** is bell-shaped-like (e.g., Gaussian distribution);\n>* Let's plot the skewness:"
"> **We have plotted Probability Density Function estimation for each feature. What does it tell us?**\n>* The features are distributed differently;\n>* The data is not perfectly symmetrical. The most of the features either right or left skewed.\n>* Only **Hillshade_3pm** is bell-shaped-like (e.g., Gaussian distribution);\n>* Let's plot the skewness:"
"> Alright, the most of the features are skewed right. \n>\n> Let's see what is the total representation of 'continuous' and 'categorical' features::"
"> Alright, the most of the features are skewed right. \n>\n> Let's see what is the total representation of 'continuous' and 'categorical' features::"
> Let's us take a look at features correlation matrix:
> Let's us take a look at features correlation matrix:
"> If we wish to label the strength of the features association, for absolute values of correlation, 0-0.19 is regarded as very weak (the most of our examples are: -0.20-0.20). **""Elevation""** has a positive correlation with **""Wilderness_Area4""**. **""Wilderness_Area1""** and **""Wilderness_Area3""** are also correlated.\n"
\n## ___Train and valid loops with tqdm bar
\n## ___CFG
"# Analysis Time!\n\nOk the short inspection at the beginning give us some hints how should we move from here. I'm going to play with the data we have while analysing the data at the same time. With this way I hope we can get the data in better shape while digging deeper into it.\n\nThese inspections are going to be some sort of intuitive but we can create new features depending on our assertions.\n\nWe're going to start with basic correlation table here. I dropped the top part since it's just mirror of the other part below. With this table we can understand some linear relations between different features.\n\n#### Observations:\n- There's strong relation between overall quality of the houses and their sale prices.\n- Again above grade living area seems strong indicator for sale price.\n- Garage features, number of baths and rooms, how old the building is etc. also having effect on the price on various levels too.\n- There are some obvious relations we gonna pass like total square feet affecting how many rooms there are or how many cars can fit into a garage vs. garage area etc.\n- Overall condition of the house seems less important on the pricing, it's interesting and worth digging.\n"
- **We're going to merge the datasets here before we start editing it so we don't have to do these operations twice. Let's call it features since it has features only. So our data has 2919 observations and 79 features to begin with...**
"- **That's quite a lot! No need to panic though we got this. If you look at the data description given to us we can see that most of these missing data actually not missing, it's just means house doesn't have that specific feature, we can fix that easily...**"
"### **Ok this is how we gonna fix most of the missing data:**\n\n1. First we fill the NaN's in the columns where they mean 'None' so we gonna replace them with that,\n2. Then we fill numerical columns where missing values indicating there is no parent feature to measure, so we replace them with 0's.\n3. Even with these there are some actual missing data, by checking general trends of these features we can fill them with most frequent value(with mode).\n4. MSZoning part is little bit tricky I choose to fill them with most common type of the related MSSubClass type. It's not perfect but at least we decrease randomness a little bit.\n5. Again we fill the Lot Frontage with similar approach."
"Now for a little practice! We want to solve the problem of binary classification of IMDB movie reviews. We have a training set with marked reviews, 12500 reviews marked as good, another 12500 bad. Here, it's not easy to get started with machine learning right away because we don't have the matrix $X$; we need to prepare it. We will use a simple approach: bag of words model. Features of the review will be represented by indicators of the presence of each word from the whole corpus in this review. The corpus is the set of all user reviews. The idea is illustrated by a picture\n\n"
"**To get started, we automatically download the dataset from [here](http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz) and unarchive it along with the rest of datasets in the data folder. The dataset is briefly described [here](http://ai.stanford.edu/~amaas/data/sentiment/). There are 12.5k of good and bad reviews in the test and training sets.**"
## 1. ëª¨ë“ˆ ë¶ˆëŸ¬ì˜¤ê¸° (Import module)\n \n#### ì œê°€ ê°€ìž¥ ìžì£¼ ì“°ëŠ” ëª¨ë“ˆë“¤ì„ ë¶ˆëŸ¬ì˜¬ ì˜ˆì •ìž…ë‹ˆë‹¤.\n#### It will load all the modules I use the most.
## 2. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°(Read Dataset)
\n\n\n\n0Â Â IMPORTSÂ Â Â Â â¤’
\n\n\n\n1Â Â SETUPÂ Â Â Â â¤’\n\n---\n
\n\n4.2 TRAINING THE MODEL\n\n[REF]\n\n---
\n\n4.3 VALIDATE AND VISUALIZE\n\n---
\n\n4.3 VALIDATE AND VISUALIZE\n\n---
\n\n\n\n\n\n\n    5Â Â MODEL INFERENCEÂ Â Â Â â¤’\n\n\n---\n\n**Only run this when things are done as masks will be deleted**
## 1. Import libraries \n\n[Back to Table of Contents](#0.1)
## 2. Download datasets \n\n[Back to Table of Contents](#0.1)
"Thanks to https://www.kaggle.com/startupsci/titanic-data-science-solutions\n\nNow we are ready to train a model and predict the required solution. There are 60+ predictive modelling algorithms to choose from. We must understand the type of problem and solution requirement to narrow down to a select few models which we can evaluate. Our problem is a classification problem. We want to identify relationship between output (Survived or not) with other variables or features (Gender, Age, Port...). We are also perfoming a category of machine learning which is called supervised learning as we are training our model with a given dataset. With these two criteria - Supervised Learning, we can narrow down our choice of models to a few. These include:\n\n- Linear Regression, Logistic Regression\n- Naive Bayes \n- k-Nearest Neighbors algorithm\n- Perceptron\n- Support Vector Machines and Linear SVR\n- Stochastic Gradient Descent, GradientBoostingRegressor, RidgeCV, BaggingRegressor\n- Decision Tree Classifier, Random Forest, AdaBoostClassifier, XGBRegressor, LGBM, ExtraTreesRegressor \n- Gaussian Process Classification\n- MLPRegressor (Deep Learning)\n- Voting Classifier\n\nEach model is built using cross-validation (except LGBM). The parameters of the model are selected to ensure the maximum matching of accuracy on the training and validation data. A plot is being built for this purpose with [learning_curve](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html?highlight=learning_curve#sklearn.model_selection.learning_curve) from sklearn library."
### 5.1 Linear Regression \n\n[Back to Table of Contents](#0.1)
"In today's heavily overparameterized models, the value of the training loss provides few guarantees on model generalization ability. Indeed, optimizing only the training loss value, as is commonly done, can easily lead to suboptimal model quality. Motivated by the connection between geometry of the loss landscape and generalization -- including a generalization bound that we prove here -- we introduce a novel, effective procedure for instead simultaneously minimizing loss value and loss sharpness. In particular, our procedure, Sharpness-Aware Minimization (SAM), seeks parameters that lie in neighborhoods having uniformly low loss; this formulation results in a min-max optimization problem on which gradient descent can be performed efficiently. We present empirical results showing that SAM improves model generalization across a variety of benchmark datasets (e.g., CIFAR-{10, 100}, ImageNet, finetuning tasks) and models, yielding novel state-of-the-art performance for several. Additionally, we find that **SAM natively provides robustness to label noise on par with that provided by state-of-the-art procedures that specifically target learning with noisy labels.**\n\n*Sharpness-Aware Minimization for Efficiently Improving\nGeneralization* : https://arxiv.org/abs/2010.01412\n\n![](https://github.com/davda54/sam/raw/main/img/loss_landscape.png)"
Helper Function\n\nConverts the activation function for the entire network
Visualize Training & Validation Metrics
Training Loss vs Validation Loss
Training Loss vs Validation Loss
Training Accuracy vs Validation Accuracy
Training Accuracy vs Validation Accuracy
![Upvote!](https://img.shields.io/badge/Upvote-If%20you%20like%20my%20work-07b3c8?style=for-the-badge&logo=kaggle)
# Import libraries ðŸ“š
"\n\nI will be integrating W&B for visualizations and logging artifacts!\n\n> [Kaggle ML & DS Survey W&B Dashboard](https://wandb.ai/ruchi798/kaggle-survey?workspace=user-ruchi798)ðŸ‹ï¸â€â™€ï¸\n>\n> - To get the API key, an account is to be created on the [website](https://wandb.ai/home) first.\n> - Next, use secrets to use API Keys more securely ðŸ¤«"
"# CommonLit Readability ðŸ“ A complete Analysis\n\n![nlp-header.png](attachment:4a916c95-e05d-4636-98fa-3e82cb1066ce.png)\n\n**Natural Language Processing or NLP** is a branch of Artificial Intelligence which deal with bridging the machines understanding humans in their Natural Language. Natural Language can be in form of text or sound, which are used for humans to communicate each other. NLP can enable humans to communicate to machines in a natural way.\n\n**Text Classification** is a process involved in Sentiment Analysis. It is classification of peoples opinion or expressions into different sentiments. Sentiments include Positive, Neutral, and Negative, Review Ratings and Happy, Sad. Sentiment Analysis can be done on different consumer centered industries to analyse people's opinion on a particular product or subject.\n\nNatural language processing has its roots in the 1950s. Already in 1950, Alan Turing published an article titled ""Computing Machinery and Intelligence"" which proposed what is now called the Turing test as a criterion of intelligence, a task that involves the automated interpretation and generation of natural language, but at the time not articulated as a problem separate from artificial intelligence."
\n# 1. Loading Data ðŸ’Ž\n\nJust load the dataset and global variables for colors and so on.
"\n## 2.1 Missing values\n\nAs we can see, the only missing values are in: `url_legal` and `license`. For now, we are going to do an analysis based on the `excerpt` text so we can go ahead."
\n## 2.2 Target and Std_err Distributions ðŸ“¸\n
\n## 4.2 WordCloud ðŸŒŸ
"back to table of content\n\n# 5. Baseline Model and Comparison\n\nCurrently, we have the messages as lists of tokens (also known as lemmas) and now we need to convert each of those messages into a vector the SciKit Learn's algorithm models can work with.\n\nWe'll do that in three steps using the bag-of-words model:\n\n1. Count how many times does a word occur in each message (Known as term frequency)\n2. Weigh the counts, so that frequent tokens get lower weight (inverse document frequency)\n3. Normalize the vectors to unit length, to abstract from the original text length (L2 norm)\n\nLet's begin the first step:\n\nEach vector will have as many dimensions as there are unique words in the SMS corpus. We will first use SciKit Learn's **CountVectorizer**. This model will convert a collection of text documents to a matrix of token counts.\n\nWe can imagine this as a 2-Dimensional matrix. Where the 1-dimension is the entire vocabulary (1 row per word) and the other dimension are the actual documents, in this case a column per text message.\n\n![vectorizer.png](attachment:fe0ac5b9-f924-4a7c-b8e3-306485322784.png)"
"# 3. Visualize data\nOne of the biggest parts of the notebook. Here we can look through some variables and see some dependencies. Firstly, let's check the **dependency of the oil from the date**:"
"As we have so much rows in out dataset, it will be easier to group data, as example, by week or month. The aggregation will be made by **mean**."
"# 3.1. Linear Regression\nAfter that, we can build some more plots. **Linear regression** is widely used in practice and adapts naturally to even complex forecasting tasks. The linear regression algorithm learns how to make a weighted sum from its input features."
"# 3.2 Lag feature\nTo make a lag feature we shift the observations of the target series so that they appear to have occured later in time. Here we've created a 1-step lag feature, though shifting by multiple steps is possible too. So, firstly, we should **add lag** to our data:"
"So lag features let us fit curves to lag plots where each observation in a series is plotted against the previous observation. Let's build same plots, but with ***'lag'*** feature:"
"# 3.3 Some more statistics & visualizations\nIn this block we are going to explore data. Firstly, let's count for each category in each dataset, ***value_counts()***:"
"# 3.3 Some more statistics & visualizations\nIn this block we are going to explore data. Firstly, let's count for each category in each dataset, ***value_counts()***:"
Here we can see stats for **df_holidays**:
Here we can see stats for **df_holidays**:
Here we count values for some columns of **df_stores**:
Here we count values for some columns of **df_stores**:
Let's plot **pie chart** for ***'family'*** of **df_train**:
Let's plot **pie chart** for ***'family'*** of **df_train**:
"# 3.4 BoxPlot\nIn addition, we can build some **boxplots**: for **df_oil** & **df_trans**."
"# 3.6. Trend. Forecasting Trend\nWe'll use a function from the **statsmodels** library called **DeterministicProcess**. Using this function will help us avoid some tricky failure cases that can arise with time series and linear regression. The order argument refers to polynomial order: 1 for linear, 2 for quadratic, 3 for cubic, and so on."
"Here we can see **Linear Trend** & **Linear Trend Forecast** for **Transactions** (plots 1,2) and **Sales** (plots 3,4)."
"Here we can see **Linear Trend** & **Linear Trend Forecast** for **Transactions** (plots 1,2) and **Sales** (plots 3,4)."
"# 3.7 Seasonality\nTime series exhibits **seasonality** whenever there is a regular, periodic change in the mean of the series. Seasonal changes generally follow the clock and calendar -- repetitions over a day, a week, or a year are common. Seasonality is often driven by the cycles of the natural world over days and years or by conventions of social behavior surrounding dates and times. Just like we used a moving average plot to discover the trend in a series, we can use a **seasonal plot** to discover seasonal patterns."
Here we can **plot data**:
"By lagging a time series, we can make its past values appear contemporaneous with the values we are trying to predict (in the same row, in other words). This makes lagged series useful as features for modeling serial dependence. To forecast series, we could use y_lag_1 and y_lag_2 as features to predict the target y."
"# 4.1 Lag plot\nA lag plot of a time series shows its values plotted against its lags. Serial dependence in a time series will often become apparent by looking at a lag plot. The most commonly used measure of serial dependence is known as **autocorrelation**, which is simply the correlation a time series has with one of its lags. The **partial autocorrelation** tells you the correlation of a lag accounting for all of the previous lags -- the amount of ""new"" correlation the lag contributes, so to speak. Plotting the partial autocorrelation can help you choose which lag features to use.\n"
Let's take a look at the **lag** and **autocorrelation plots** first:
And we can build plot with **predictions**:
"# 5. Hybrid Models\nLinear regression excels at extrapolating trends, but can't learn interactions. XGBoost excels at learning interactions, but can't extrapolate trends. Here we'll learn how to create **""hybrid"" forecasters** that combine complementary learning algorithms and let the strengths of one make up for the weakness of the other."
After that we train and plot
# 6. Machine learning forecasting
"Also, we need to define helpfull function, **plot_multistep**:"
"So, now, we can **plot results**:"
"So, now, we can **plot results**:"
"# 7. Conclusion\nThank you for reading my new article!\n\nHope, you liked it and it was interesting for you! There are some more my articles:\n* [SMS spam with NBC | NLP | sklearn](https://www.kaggle.com/maricinnamon/sms-spam-with-nbc-nlp-sklearn)\n* [House Prices Regression sklearn](https://www.kaggle.com/maricinnamon/house-prices-regression-sklearn)\n* [Automobile Customer Clustering (K-means & PCA)](https://www.kaggle.com/maricinnamon/automobile-customer-clustering-k-means-pca)\n* [Credit Card Fraud detection sklearn](https://www.kaggle.com/maricinnamon/credit-card-fraud-detection-sklearn)\n* [Market Basket Analysis for beginners](https://www.kaggle.com/maricinnamon/market-basket-analysis-for-beginners)\n* [Neural Network for beginners with keras](https://www.kaggle.com/maricinnamon/neural-network-for-beginners-with-keras)\n* [Fetal Health Classification for beginners sklearn](https://www.kaggle.com/maricinnamon/fetal-health-classification-for-beginners-sklearn)\n* [Retail Trade Report Department Stores (LSTM)](https://www.kaggle.com/maricinnamon/retail-trade-report-department-stores-lstm)"
\n\n### 2| IMPORT NECESSARY LIBRARIES
\n\n### 3| LOAD DATASETS
\n\n##### 5.5| Regplot
\n\n##### 5.6| Heatmap
\n\n##### 5.6| Heatmap
\n\n##### 5.7| Pandas crosstab
\n\n##### 5.8| Visualization with AutoViz
\n\n### 6| Statistical ANOVA Test for Feature Selection 
\n\n##### 8.5| ROC AUC
\n\n##### 8.6| Confusion matrix
# Import Libraries\nImport the usual libraries for pandas and plotting. You can import sklearn later on.
# Get the Data\n\n**Use pandas to read loan_data.csv as a dataframe called loans.**
"# Exploratory Data Analysis\n\n* **Let's do some data visualization!**\n\n* **We'll use seaborn and pandas built-in plotting capabilities, but feel free to use whatever library you want.** \n\n\n* **Create a histogram of two FICO distributions on top of each other, one for each credit.policy outcome.**"
"\n**Create a similar figure, except this time select by the not.fully.paid column.**"
"\n**Create a similar figure, except this time select by the not.fully.paid column.**"
"**Create a countplot using seaborn showing the counts of loans by purpose, with the color hue defined by not.fully.paid.**"
"**Create a countplot using seaborn showing the counts of loans by purpose, with the color hue defined by not.fully.paid.**"
* **Let's see the trend between FICO score and interest rate.** \n* **Recreate the following jointplot.**
* **Let's see the trend between FICO score and interest rate.** \n* **Recreate the following jointplot.**
**Create the following lmplots to see if the trend differed between not.fully.paid and credit.policy.** 
**Create the following lmplots to see if the trend differed between not.fully.paid and credit.policy.** 
# Setting up the Data\n\n**Let's get ready to set up our data for our Random Forest Classification Model!**\n\n**Check loans.info() again.**
\n# Imports:
\n# Read the data:
## PCA Visualizations:
\n### 1-D Visualization:
The plot below displays our three original clusters on the single *principal component* created for 1-D visualization:
\n### 2-D visualization:
The next plot displays the three clusters on the two *principal components* created for 2-D visualization:
\n### 3-D Visualization:
This last plot below displays our clusters on the three *principal components* created for 3-D visualization:
"## PCA Remarks:\n\nAs we can see from the plots above: if you have data that is highly *clusterable*, then PCA is a pretty good way to view the clusters formed on the original data. Also, it would seem that visualizing the clusters is more effective when the clusters are visualized using more principle components, rather than less. For example, the 2-D plot did a better job of providing a clear visual representation of the clusters than the 1-D plot; and the 3-D plot did a better job than the 2-D plot!"
The plot below displays our three original clusters on the single dimension created by T-SNE for 1-D visualization:
\n### 2-D Visualization:
The next plot displays the three clusters on the two dimensions created by T-SNE for 2-D visualization:
\n### 3-D Visualization:
This last plot below displays our clusters on the three dimensions created by T-SNE for 3-D visualization:
"## T-SNE Remarks:\n\n\nThe T-SNE algorithm did a fairly decent job in visualizing the clusters, too. But, there were a few noticable differences when comparing it's resulting plots to PCA's resulting plots. \n\nOne major difference between the plots produced by PCA and T-SNE is that T-SNE's plots seemed to have it's clusters overlapping with eachother more so than in PCA's plots. For example, if you look at the [**2-D plot**](#PCA_2D) fomed from PCA, you see three distinct sections of the data-points with strict, visible borders separating each colour into groups. Whereas, if you look at the [**2-D**](#T-SNE_2D) plot formed from T-SNE, you, again, see three sections formed within the data-points, but this time, datapoints between each cluster seem to 'intermingle' and overlap more.\n\nThe other major difference between the plots created by PCA and the plots created by T-SNE, is the shape. Because both PCA and T-SNE perform dimensionality reduction in very different ways (and with different objectives), the resulting shape or distibution of the points produced by the algorithms will almost always be very different.\n\nBear in mind that the plots resulting from the T-SNE algorithm are quite variable, in that they depend very heavily on the value chosen for `perplexity`."
## Importing libraries
## Exploratory Data Analysis
Plotting the same graph but with ratio instead.
The Sex variable seems to be a discriminative feature. Women are more likely to survive.
"Now, visualizing survival based on the fare."
"Passengers with cheaper ticket fares are more likely to die. Put differently, passengers with more expensive tickets, and therefore a more important social status, seem to be rescued first."
## The text version of scatter plot looks messy but you can zoom it for great results
Let's see what happens when we use a spaCy based bigram tokenizer for topic modelling
# Models
## KNN-XGB-SVC Ensemble
### Class Distribution
### Batch Image
### Batch Image
## P Curve
## P Curve
## PR Curve
## PR Curve
## F1 Curve
## F1 Curve
## R Curve
## R Curve
### GT Vs Pred
### GT Vs Pred
"### (Loss, Map) Vs Epoch\n"
"### (Loss, Map) Vs Epoch\n"
### Confusion Matrix
### Confusion Matrix
"### Please if this kernel is useful, please upvote !!"
### Import Libraries
### Reading Data
- Let's import the libraries
- Let's look at the data.
"- With the **default parameters, Catboost** get almost**.52 Recall and .72 Roc_Auc**"
"- By using deafult value of **`scale_pos_weight`**, **CatBoost** correctly predicted **almost half of the churned customer**\n- But model couldn't be able to correctly predict the other half."
"- With the adjusted **`scale_pos_weight=3`**, **CatBoost** got  **.84 Recall and .78 Roc_Auc**"
"- By using recommended formula for **`scale_pos_weight`**, **CatBoost** correctly predicted **84% of the churned customers**.\n- As you have seen that, we didn't change any other parameter and get almost 32% lift in our recall score."
"- With the adjusted **`scale_pos_weight=5`**, **CatBoost** got  almost **.92 Recall and .75 Roc_Auc**"
"- By using weight a little bit higher than recommended formula for **`scale_pos_weight = 5`**, **CatBoost** correctly predict almost **92% of the churned customers**.\n- On the other hand we have lost 7% for the precision and 3 points for the Roc_Auc score."
"- With the **default parameters, XGBoost** get **.47 Recall and .68 Roc_Auc**."
"- Bu using deafult value of **`scale_pos_weight`**, **XGBoost** correctly predicted **less than half of the churned customers**. "
"- With the adjusted **`scale_pos_weight`**, **XGBoost** got almost **.67 Recall and .73 Roc_Auc**"
"- Bu using recommended adjusted value of **`scale_pos_weight`**, **XGBoost** correctly predicted almost **67% of the churned customers**."
"- With the adjusted **`scale_pos_weight = 5`**, **XGBoost** got **.74 Recall and .75 Roc_Auc**"
"- When we play around a little bit with the value of  **`scale_pos_weight`**, **XGBoost** correctly predicted almost **74% of the churned customers** by using **scale_pos_weight=5** (Roc_Auc = .75)\n"
"- With the adjusted **`scale_pos_weight`**, **LightGBM** got almost **.54 Recall and .72 Roc_Auc**"
"- Bu using deafult value of **`scale_pos_weight`**, **LightGBM** correctly predicted alittle bit more than half of the churned customers. "
"- With the adjusted **`scale_pos_weight`**, **LightGBM** got **.77 Recall and .76 Roc_Auc**"
"- By using recommended formula for **`scale_pos_weight`**, **LightGBM** correctly predicted almost 77% of the churned customer.\n- As you have observed that, we didn't change any other parameter and get almost 23% lift in our recall score without overfitting.\n- We can improve this score without destroying the Roc_Auc."
"- With the adjusted **`scale_pos_weight = 5`**, **LightGBM** got **.83 Recall and almost .76 Roc_Auc**"
"- By using **`scale_pos_weight =5`**, **LightGBM** correctly predicted **83% of the churned customer**."
### Feature Engineering\n**Credit:** This amazing [notebook](https://www.kaggle.com/code/ragnar123/amex-lgbm-dart-cv-0-7977) by [Martin Kovacevic Buvinic](https://www.kaggle.com/ragnar123)
### Configurations & Setup
### Configurations & Setup
"### The Competition's Metric\n\nThe evaluation metric, *M* for this competition is the mean of two measures of rank ordering: Normalized Gini Coefficient, *G*, and default rate captured at 4%, *D*.\n\n$$M = 0.5 \cdot ( G + D )$$\n\nThe default rate captured at 4% is the percentage of the positive labels (defaults) captured within the highest-ranked 4% of the predictions, and represents a Sensitivity/Recall statistic.\n\nFor both of the sub-metrics *G* and *D*, the negative labels are given a weight of 20 to adjust for downsampling.\n\nThis metric has a maximum value of 1.0."
# 1. Import Required Libraries
# 2. Loading the dataset 
# 4. Data Visualization\n## Here we are going to plot :-\n- Count Plot :- to see if the dataset is balanced or not\n- Histograms :- to see if data is normally distributed or skewed\n- Box Plot :- to analyse the distribution and see the outliers\n- Scatter plots :- to understand relationship between any two variables\n- Pair plot :- to create scatter plot between all the variables
### **Conclusion** :- We observe that number of people who do not have diabetes is far more than people who do which indicates that our data is imbalanced.
### **Conclusion** :- We observe that number of people who do not have diabetes is far more than people who do which indicates that our data is imbalanced.
### **Conclusion** :- We observe that only glucose and Blood Pressure are normally distributed rest others are skewed and have outliers
### **Conclusion** :- We observe that only glucose and Blood Pressure are normally distributed rest others are skewed and have outliers
"Outliers are unusual values in your dataset, and they can distort statistical analyses and violate their assumptions. Hence it is of utmost importance to deal with them. In this case removing outliers can cause data loss so we have to deal with it using various scaling and transformation techniques."
**Pearson's Correlation Coefficient** : Helps you find out the relationship between two quantities. It gives you the measure of the strength of association between two variables. The value of Pearson's Correlation Coefficient can be between -1 to +1. 1 means that they are highly correlated and 0 means no correlation.\n\nA heat map is a two-dimensional representation of information with the help of colors. Heat maps can help the user visualize simple or complex information.
"### **CONCLUSION** :- Observe the last row 'Outcome' and note its correlation scores with different features. We can observe that Glucose, BMI and Age are the most correlated with Outcome. BloodPressure, Insulin, DiabetesPedigreeFunction are the least correlated, hence they don't contribute much to the model so we can drop them. Read more about this here :- https://towardsdatascience.com/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e I have used 3'rd technique method mentioned here."
"# EDA - Ion Switching\n\n> Many diseases, including cancer, are believed to have a contributing factor in common. Ion channels are pore-forming proteins present in animals and plants. They encode learning and memory, help fight infections, enable pain signals, and stimulate muscle contraction. If scientists could better study ion channels, which may be possible with the aid of machine learning, it could have a far-reaching impact.\n>\n> When ion channels open, they pass electric currents. Existing methods of detecting these state changes are slow and laborious. Humans must supervise the analysis, which imparts considerable bias, in addition to being tedious. These difficulties limit the volume of ion channel current analysis that can be used in research. Scientists hope that technology could enable rapid automatic detection of ion channel current events in raw data.\n\n\n------\n\n**I'll update this EDA notebook in the following days/weeks. Stay tuned!**"
"# Train data\n\n> In this competition, you will be predicting the number of open_channels present, based on electrophysiological signal data.\n>\n> **IMPORTANT**: While the time series appears continuous, the data is from discrete batches of 50 seconds long 10 kHz samples (500,000 rows per batch). In other words, the data from 0.0001 - 50.0000 is a different batch than 50.0001 - 100.0000, and thus discontinuous between 50.0000 and 50.0001.\n\n## `time`\nWe have 5M rows in the train dataset. According to the data description we have 50 seconds long 10kHz samples (500,000 rows per batch).\nSo we have 10 batches. The data in a batch is continuous, but discontinuous between batches.\n\n## `signal`\n\n\n## `open_channels`\nPredictions have 11 possible values of open_channels: 0-10."
# Target distribution
## Target distribution in different batches
## Target distribution in different batches
# Statistics
\nðŸ“ Note: The price range of the thrid class looks a bit suspicious compared to the 2. class. We will have a closer look at that later.  
\nðŸ“ Note: The Fare feature contains outliers in each Pclass. This looks a bit strange.
\nðŸ“ Note: The Fare feature contains outliers in each Pclass. This looks a bit strange.
\nðŸ’­ Thougts:  We can't see a direct connection between Age and Survived. Let's have a closer look...
\nðŸ’­ Thougts:  We can't see a direct connection between Age and Survived. Let's have a closer look...
\nðŸ’¡ Idea: We should consider a closer look at very young passengers.
\nðŸ“ Note: We will use one hot encoder later on. Otherwise the natural order of those numbers could irritate the ML algorithm.
"\nðŸ“ Note: We see strong correlations between the features Survived and Pclass, Sex, Fare and Embarked."
"\nðŸ“ Note: Instead of using combined_df to calculate the group_size we could use row['Parch'] + row['SibSp'] + 1 but we dont know if each group ticket is a single familie or a familie at all. So since we have the data available, we can use it."
\nðŸ’­ Thougts: This looks like a more likely price distribution.
\nðŸ’­ Thougts: This looks like a more likely price distribution.
\nðŸ“ Note: This is interesting. Being alone in the first class seems to have a negative impact on your chance of survival. Being in a large group (more than 4 people) in the third class seems to lower your survival probability as well. This could be a useful feature to predict whether someone has survived.
\n  \n 2.4 Correlations With Newly Created Features 
\nðŸ’­ Thougts: This is not looking to bad. Let's build a solid pipeline and see how we perform!
"\nðŸ’­ Thougts: This is what we want for each feature. So let's create a dataframe and plot the result for a more appealing look. Furthermore, a value_count for features like Fare_per_person and Age makes little sense so we will create a distplot here."
\nðŸ“ Note: I think we can say that a delta of the distributions of the main features between test and training data does not seem to be the main reason for our divergent performance. Only Fare_per_person looks a bit different. Let's look a little further here. 
\nðŸ“ Note: I think we can say that a delta of the distributions of the main features between test and training data does not seem to be the main reason for our divergent performance. Only Fare_per_person looks a bit different. Let's look a little further here. 
\nðŸ“ Note: That does not look alarming. So let's keep adding new features and hopefully get better performance.
"\n  \n 5.2 New Feature ""Salutation"" \nWe have completely ignored the name of the passengers, but the name usually contains a salutation and this could be relevant. Let's add it as a feature."
\nðŸ“ Note: We will integrate the 4 most common salutations as a feature. So let's customize the above test function and create a corresponding pipeline function.
"\nðŸ“ Note: So what do we have? We have the 20 most frequent Leading_tickets_numbers and their frequencies from test and training dates. It is important to note here that we have normalized the frequencies, since the size of the test data is smaller than the size of the training data. In addition, we have the associated survival probabilities based on the training data.\nI hope the steps in the code are understandable. Now let's try to represent this data in one informative plot."
"\nðŸ“ Note: The frequency of these groups is quite similar in the test and training data set. That' s good. However, the problem is that in our current pipelines we normalize the leading ticket number and thus implicitly assume that there is a natural order of magnitude. However, if we look at the survival probabilities of these groups, we see that this is not quite correct. Although the survival probability in groups starting with a 1 is generally higher, there are also relevant exceptions. We will fix this erroneously assumed order by setting up a new pipeline that processes the leading_ticket_number via one hot encoding. This will increase the number of features tremendously but we can adjust the depth of the trees accordingly."
"Machine Learning and Data Science Survey 2021 Analysis\nWhile much of the world closed down during the COVID-19 pandemic, the gates opened wide for financing early and late-stage startups in the data and AI space.[1]\n In the past year, thereâ€™s been less headline-grabbing discussion of futuristic applications of AI (self-driving vehicles, etc.), and a bit less AI hype as a result. Regardless, data and ML/AI-driven application companies have continued to thrive, particularly those focused on enterprise use trend cases. Meanwhile, a lot of the action has been happening behind the scenes on the data and ML infrastructure side, with entirely new categories (data observability, reverse ETL, metrics stores, etc.) appearing or drastically accelerating.[2]\nInsights from Kaggleâ€™s annual user survey focused on working data scientists. This is the 5th year conducting an in-depth user survey & publicly sharing the results. Over 25,000 data scientists and ML engineers submitted responses on their backgrounds and day to day experience â€“ everything from educational details to salaries to preferred technologies and techniques. This notebook is data visualization and analysis of the annual user survey result."
# Country\nIn which country do you currently reside?
# Country\nIn which country do you currently reside?
"India is obviously the number one country of Kaggle's users (28.6%). Second position is the USA (10.2%) with a big gap almost 20%, then another big gap to the 3rd place filled by users from Japan, China, Brazil, Russia and Nigeria with about the same percentage around 3%. Users from Europe are (including) Russia, UK, Germany, Spain, France and Italy, with 2.8%, 2.1%, 1.8%, 1.7%, 1.5% and 1.2% respectively. Users from the South East Asia just Indonesia (1.7%) in the top 20 countries list."
"India is obviously the number one country of Kaggle's users (28.6%). Second position is the USA (10.2%) with a big gap almost 20%, then another big gap to the 3rd place filled by users from Japan, China, Brazil, Russia and Nigeria with about the same percentage around 3%. Users from Europe are (including) Russia, UK, Germany, Spain, France and Italy, with 2.8%, 2.1%, 1.8%, 1.7%, 1.5% and 1.2% respectively. Users from the South East Asia just Indonesia (1.7%) in the top 20 countries list."
# Higher Education\nWhat is the highest level of formal education that you have attained or plan to attain within the next 2 years?
# Higher Education\nWhat is the highest level of formal education that you have attained or plan to attain within the next 2 years?
"More 77.1% of the users have master's and bachelor's degree, 12.2% have doctoral degree, 1.4% professinal doctorate, 8.3% does not have a formal degree, and 1.6% has high school education."
# Gender\nWhat is your gender?
"In 2021 Kaggle's users are dominantly by man (79.3%), and woman (18.8%). Hopefully tha gap will be smaller in the near future."
# Age\nWhat is your age (# years)?
"Kaggle's users are young, below 30 years old are more than half of population (56%). Interestingly users above 50 years old are also quite a lot, further more there are 0.5% above 70 years old. We will see on the next chart how it looks between man and woman."
"Kaggle's users are young, below 30 years old are more than half of population (56%). Interestingly users above 50 years old are also quite a lot, further more there are 0.5% above 70 years old. We will see on the next chart how it looks between man and woman."
"Overall, man and woman are spread equally over age range. However if we look more deeper, woman that are 18-21 years old age range, is much higher than other age range. \nOn the other hand, man in 25-29 years old age range are much higher than woman on the same age.\nUser age between 30 to 50 years old are declining sharply in population. Think that five years ago, they were in the five bars on the left. In fact these people are the one who were early adopters of the modern Machine Learning and Data Science knowledge."
# Programming Language\nWhat programming languages do you use on a regular basis?
"It is boldly showed that Python is the swiss army of programming languange, loved by all developers including data scientist. Python is easy to learn, can do almost anything. Second language is SQL (16.3%). SQL is the one talking to the data. In data science world SQL is a must have knowledge. And yes, this notebook is of course written in Python."
# Job Title\nSelect the title most similar to your current role (or most recent title if retired)
"Kaggle users are mostly students (28.9%). Half of it the second job role is Data Scientist (15.3%). The third is about same percentage are software engineer and data analysit, 10.4% and 9.8% respectively."
# Yearly Compensation\nWhat is your current yearly compensation (approximate $USD)?
"Most of users, 21.9% respondents answer that their yearly compensation is less than 1,000. This is an unbelievable number. If we see the data, the reason is many users (40%) didn't answer this question, and second, most users are students. The data is may be a bit bias here.\n\nWe can devide compensation ranges into three groups. First group is 0-15K, second is 15-40K, 40-125K, and 125K-1M, and >1M. We see the professionals with compensation range above 10K, which I think is still underpaid for a data scientist even in poor countries. 4.8% of users is in 30K-39K range, and about the same percentage 4.7 got 100-125K yearly. Rest of the group (7.9%) got a decent yearly compensation amount, above 125K to more than 1M. Interesting group that need to be explored more is the most right part group with more than 125K yearly compensation."
# Writting Code Experience\nFor how many years have you been writing code and/or programming?
"Kaggler's users are mostly having 1-3 years of writting code experience, dominantly by man (24.3%), while woman (6.1%). Much less users have writting code experience more than 3 years. Interesting that there are 6.5% man and 0.6% woman with experience more than 20 years. These people are the gurus of data science and machine learning."
"# Data Science Notebooks\nThe realm of machine learning is that of data science, since after all, weâ€™re trying to derive higher value insights from big data. The primary environment for data science is the â€œnotebookâ€, which is a collaborative, interactive, document-style environment that combines aspects of coding, data engineering, machine learning modeling, data visualization, and collaborative data sharing. \n[3]\nWhile data science notebooks can be used to develop models of any type, they are primarily used during the experimentation and iteration phases of model development, since data science notebooks are optimized for that sort of iterative experimentation versus being focused on organization-wide aspects of management and deployment. [4]"
"Colab and Kaggle notebooks is the top two winners with only 1% difference, 32.9% and 31.9% respectively. The third and fourth place is still Google products, Google Cloud Notebooks 6.9%, the fourth Google Cloud Datalab 6%. Binder JupyterHub on the 5th, 5.9%. Sixth place is shared by IBM Watson Studio and Azure Notebooks, 3.6%, and Amazon Sagemaker Studio Notebooks 2.5%. [5]"
"# Data Visualization\nWe live in a world where data visualisations are done through intricate code and graphic design. From Tableau to Datawrapper and Python and R, numerous possibilities exist for visualising compelling stories.\n""Often, the most important thing is to give a truthful impression of the data.""\nHowever, the goal of visualisation does not always have to be to encode information in such a way that it is easy to read off exact values. Often, the most important thing is to give a truthful impression of the data. And, the most technically correct visualisations may not always be the best way to convey that impression.[6]"
"Matplotlib is the most favorite library for data visualization by 37.8%, followed by Searborn, 27%, Plotly 12.4%, and Ggplot 10.9%. Other libraries are less popular with less than 3% users. [7] Unfortunately popular data visualization like Tableau, Google Data Studio, Looker and many others were not asked in the survey which I believe also used by many Kagglers."
# Cloud Computing Platforms\nWhich of the following cloud computing platforms do you use on a regular basis?
"Most of the large cloud providers have jumped in with both feet into the machine learning space. Amazon, Google, IBM, and Microsoft have all added core capabilities for machine learning model development, management, and iteration as well as capabilities for data preparation, engineering, and augmentation. [9]\nAmazon Web Services is the number one of user's cloud platform choice with 31.7%, followed by Google Cloud Platform, 26.8%, Microsoft Azure, 20.9%, and IBM Cloud/Red Hat in the 4th position, with 4.9% far behind the top three."
"# Big Data Products\nWhich of the following big data products (relational database, data warehouse, data lake, or similar) do you use most often?"
"The number one favorite database is still MySQL (24.2%), then half percentage of it followed by PostgresSQL (15.1%), and MSSQL (13.3%). Other databases are almost half of percentage of the top three, with MongoDB (7.4%), Oracle Database (6.9%), Google BigQuery (6.5%), and the rest with less than 5%."
# Cloud Computing and Data Storage Products\n1. Do you use any of the following cloud computing products on a regular basis?\n2. Do you use any of the following data storage products on a regular basis?
"Amazon is the top choice of cloud computing and data storage products, Amazon Elastic Compute Cloud, (49.4%), and on data storage Amaxon Simple Storage Service, with 50.3%, and almost one third followed by Google Cloud Filestore (16.8%), then Amazon Elastic File System (14.9%)."
# Share EDA\nWhere do you publicly share your data analysis or machine learning applications?
"Github is the number one for open source communities (31%), and Kaggle is not surprisingly, at number two position (20.7%). In this chart, I take out users who do not share their work in the chart (21.4%). How come a Kaggler does not share their work anyway. "
# **Exploratory Data Analysis**\n\n[Source1](https://www.kaggle.com/ambrosm/tpsjan22-01-eda-which-makes-sense) [Source2](https://www.kaggle.com/vad13irt/tps-jan-2022-exploratory-data-analysis)\n\nðŸ“Œ Histograms in the below graph are skewed with outliers . Hence choosing log(num_sold over num_sold is preferred .\n\n
ðŸ“Œ The peaks in the below graph indicates lot of sales happens during January .
ðŸ“Œ The peaks in the below graph indicates lot of sales happens during January .
ðŸ“Œ Norway has the highest sales followed by Sweden and Finland\n
ðŸ“Œ Norway has the highest sales followed by Sweden and Finland\n
ðŸ“Œ KaggleRama has higher sales compared to KaggleMart\n\n
ðŸ“Œ KaggleRama has higher sales compared to KaggleMart\n\n
ðŸ“Œ KaggleHat has the highest sales followed by KaggleMug and KaggleStickers
ðŸ“Œ KaggleHat has the highest sales followed by KaggleMug and KaggleStickers
ðŸ“Œ Monthly trend in the below graph shows the seasonal variations in sales across products
ðŸ“Œ Monthly trend in the below graph shows the seasonal variations in sales across products
"# **Feature Engineering**\n\nNew features can be created from the date column like month , year , weekend or weekday ."
"# **W & B Artifacts**\n\nAn artifact as a versioned folder of data.Entire datasets can be directly stored as artifacts .\n\nW&B Artifacts are used for dataset versioning, model versioning . They are also used for tracking dependencies and results across machine learning pipelines.Artifact references can be used to point to data in other systems like S3, GCP, or your own system.\n\nYou can learn more about W&B artifacts [here](https://docs.wandb.ai/guides/artifacts)\n\n![](https://drive.google.com/uc?id=1JYSaIMXuEVBheP15xxuaex-32yzxgglV)"
The snapshot of the artifact created is below\n\n![](https://drive.google.com/uc?id=16biHK189-q2mhyZAhE-cAvxHb3BIAfFq)
* **Residual plot shows residual error VS. true y value.**
"* **Residualplot showing a clear pattern, indicating Linear Regression no valid!**"
--------------------------------------\n# Setting Up
-------------------------------------------------------------------------------\n# Checking Target Imbalance\n\n
-----------------------------------\n# Checking the data type of features
" Observation:\n\n* All features are numerical! So, there is no need to consider categorical feature engineering!    "
"In various features, it is determined that skewness greater than 1 is skewed, and only these features are subjected to nonlinear scaling."
 Observation:\n\n* Only Amount feature is met the condition!
"--------------------------------------------------------\n# Checking Correlations\n\nFirst, draw a heatmap to find features that are related to classes."
" Observation:\n    \nV3, V7, V10, V11, V12, V14, V16, and V17 have a strong correlation with target(class) compared to other features."
" Observation:\n    \nV3, V7, V10, V11, V12, V14, V16, and V17 have a strong correlation with target(class) compared to other features."
" Observation:\n\n**Looking at the above pictures, the V14 and V17 features have a high correlation with target class. Let's dig deeper into these features.**\n\n**V14 and V17 features seem to have more correlation with Class than other features. Let's analyze the difference before and after sampling with a scatter plot of these two features.**"
### Plotting histogram Plot
 Observation:\n    \nThe imbalacne also seems to be large. Let's think about how we can solve this.
### Visualizing after Dimension Reduction
 Observation:\n    \n* Wow! It is beatiful!\n* The target is severely imbalaced. Level 1 target is too small compared to level 0 target.    
### Plotting histogram Plot
" Observation:\n\nIf you look at the distribution of the created zero-target data, you can see that it is spread over a wide range."
### Plotting Scatter Plot
" Observation:\n\nIf you look at the result of SMOTE, it looks like lines connected between points. This is because SMOTE uses an interpolation technique."
### Visualizing after Dimension Reduction
 Observation:\n\nYou can see that the blue dots are expanding their power.
### Plotting histogram Plot
 Observation:\n    \nIt can be seen that the number of data of the normal class (value = 0) is reduced. It can be seen that the distribution of the negative class (value = 0) maintains the shape of the original distribution similarly.
### Plotting Scatter Plot
 Observation:\n    \nIt can be seen that the negative values increase randomly. 
### Plotting histogram Plot
" Observation:\n    \nLooking at the historgram, it can be seen that both the V14 features increase the normal data (value = 0). For both V14 features, it can be confirmed that the overlapping section between fraud (value = 1) and normal (value = 0) is larger than other methods."
### Plotting Scatter Plot
" Observation:\n    \nSimilar to SMOTE, it can be confirmed that oversampling occurs through interpolation."
### Plotting histogram Plot
 Observation:\n    \nIt can be seen that the number of data of the normal class (value = 0) is reduced. It can be seen that the distribution of the normal class (value = 0) maintains the shape of the original distribution similarly.
### Plotting Scatter Plot
 Observation:\n\nIt can be seen that the fraud-level data are randomly sampled. The distribution of positive values was also changed.
### Plotting histogram Plot
" Observation:\n    \nIt can be seen that the number of data of the normal class (value = 0) is reduced. It can be seen that the distribution of the normal class (value=0) maintains the shape of the original distribution similarly, and it can be confirmed that the distribution of the fraud class (value=1) has a smaller variance compared to random undersampling."
### Plotting Scatter Plot
### Visualizing after Dimension Reduction
### Plotting histogram Plot
 Observation:\n    \nIt can be seen that the number of data of the normal class (value = 0) is reduced. The distribution of the normal class (value=0) became a bimodal distribution in the shape of the original distribution. The shape of the original distribution is broken and is unlikely to be conducive to learning.
### Plotting Scatter Plot
### Visualizing after Dimension Reduction
" Observation:\n    \nWhen looking at the boundary in the model using the train dataset, it was drawn very clearly."
" Observation:\n    \nIf you look at the picture above, you can check the conditions under which the decision tree is made."
"# Categorical Feature Encoding:\n\n## Introduction:\n\nIn most data science problems, our datasets will contain categorical features. Categorical features contain a finite number of discrete values. How we represent these features will have an impact on the performance of our model. Like in other aspects of machine learning, there are no silver bullets. Determining the correct approach, specific to our model and data is part of the challenge.\n\nThis tutorial aims to cover a few of these methods. We begin by covering a straight-forward technique before tackling more complicated lesser-known approaches.\n\n**List of methods covered**:\n1. One-Hot Encoding\n2. Feature Hashing\n3. Binary Encoding\n4. Target Encoding\n5. Weight of Evidence"
"For this tutorial, we will be using the '[Amazon.com Employee Access Challenge](https://www.kaggle.com/c/amazon-employee-access-challenge)' dataset. This binary classification dataset is made up of strictly categorical features, which are already converted into numerals, making it a particularly suitable choice to explore various encoding techniques. To simplify things we will only be using a subset of the features for this demonstration."
~20% of entries for passenger age are missing. Let's see what the 'Age' variable looks like in general.
"Since ""Age"" is (right) skewed, using the mean might give us biased results by filling in ages that are older than desired.  To deal with this, we'll use the median to impute the missing values. "
"There are only 2 missing values for ""Embarked"", so we can just impute with the port where most people boarded."
"By far the most passengers boarded in Southhampton, so we'll impute those 2 NaN's w/ ""S""."
"By far the most passengers boarded in Southhampton, so we'll impute those 2 NaN's w/ ""S""."
### 2.4 Final Adjustments to Data (Train & Test)
## 3.1 Exploration of Age
"The age distribution for survivors and deceased is actually very similar.  One notable difference is that, of the survivors, a larger proportion were children.  The passengers evidently made an attempt to save children by giving them a place on the life rafts. "
"The age distribution for survivors and deceased is actually very similar.  One notable difference is that, of the survivors, a larger proportion were children.  The passengers evidently made an attempt to save children by giving them a place on the life rafts. "
"Considering the survival rate of passengers under 16, I'll also include another categorical variable in my dataset: ""Minor"""
## 3.2 Exploration of Fare
"As the distributions are clearly different for the fares of survivors vs. deceased, it's likely that this would be a significant predictor in our final model.  Passengers who paid lower fare appear to have been less likely to survive.  This is probably strongly correlated with Passenger Class, which we'll look at next."
## 3.3 Exploration of Passenger Class
"Unsurprisingly, being a first class passenger was safest."
## 3.4 Exploration of Embarked Port
"Passengers who boarded in Cherbourg, France, appear to have the highest survival rate.  Passengers who boarded in Southhampton were marginally less likely to survive than those who boarded in Queenstown.  This is probably related to passenger class, or maybe even the order of room assignments (e.g. maybe earlier passengers were more likely to have rooms closer to deck).  It's also worth noting the size of the whiskers in these plots.  Because the number of passengers who boarded at Southhampton was highest, the confidence around the survival rate is the highest.  The whisker of the Queenstown plot includes the Southhampton average, as well as the lower bound of its whisker.  It's possible that Queenstown passengers were equally, or even more, ill-fated than their Southhampton counterparts."
## 3.5 Exploration of Traveling Alone vs. With Family
"Individuals traveling without family were more likely to die in the disaster than those with family aboard.  Given the era, it's likely that individuals traveling alone were likely male."
## 3.6 Exploration of Gender Variable
This is a very obvious difference.  Clearly being female greatly increased your chances of survival.
This is a very obvious difference.  Clearly being female greatly increased your chances of survival.
## 4. Logistic Regression and Results
Test data set:
"# NOTE\nI know what are you thinking, how much sense does it make to take a mean of a moving average and other derivations of it but according to the feature importance of lbgm it does. In a contrast if we were to take all 150 000 rows and calculate what we have calculated for each of them. Than (approximately) 9 minutes times 150 000 is around\n# **ONE LIFETIME**\n worth of time of calculations. Now I do understand that it is actually not that since we are taking the mean for every one ofthe variables etc.. but STILL it is a long time!"
## Import Libraries\n\n** Import the libraries you usually use for data analysis.**
## Get the Data
\nIt's time to create some data visualizations!\n\n** Create a scatterplot of Grad.Rate versus Room.Board where the points are colored by the Private column. **
**Create a scatterplot of F.Undergrad versus Outstate where the points are colored by the Private column.**
**Create a scatterplot of F.Undergrad versus Outstate where the points are colored by the Private column.**
"** Create a stacked histogram showing Out of State Tuition based on the Private column. Try doing this using [sns.FacetGrid](https://stanford.edu/~mwaskom/software/seaborn/generated/seaborn.FacetGrid.html). If that is too tricky, see if you can do it just by using two instances of pandas.plot(kind='hist'). **"
"** Create a stacked histogram showing Out of State Tuition based on the Private column. Try doing this using [sns.FacetGrid](https://stanford.edu/~mwaskom/software/seaborn/generated/seaborn.FacetGrid.html). If that is too tricky, see if you can do it just by using two instances of pandas.plot(kind='hist'). **"
**Create a similar histogram for the Grad.Rate column.**
**Create a similar histogram for the Grad.Rate column.**
** Notice how there seems to be a private school with a graduation rate of higher than 100%.What is the name of that school?**
"\n\n\nThe distribution of the `num_sold` column seems to be right skewed with some few outliers selling more than 800 books. It is extremely typical for skewed distributions to have one tail that is significantly longer or dragged out compared to the other tail. A distribution is said to be ""skewed right"" if the tail is to the right. Lower or upper boundaries on the data frequently cause skewed data. In other words, data with a lower bound are frequently skewed right, whereas data with an upper bound are typically biased left. Start-up effects can also cause skewness. For instance, some processes in reliability applications could experience a high rate of first failures, which could result in left skewness. On the other side, a dependability process can have a protracted startup phase where failures are uncommon and the data would be right-skewed. \n\nThe following recommendations should be followed if the histogram shows that the data set is right-skewed:\n        Compute and publish the sample mean, sample median, and sample mode to quantitatively summarize the data.\n        Consider a normalizing transformation such as the Box-Cox transformation\n        \n        Choose the best-fit distribution (rightly skewed) from the\n            \n                Weibull family (for the maximum)\n                Gamma Family\n                Chi-square family\n                Lognormal family\n                Power lognormal family\n                \n        \n    \n\n\nFor more in-depth information about this, You can visit this [link](https://www.itl.nist.gov/div898/handbook/eda/section3/eda33e6.htm#:~:text=For%20skewed%20distributions%2C%20it%20is,is%20on%20the%20left%20side.).\n"
"A violin plot, which depicts data peaks, is a cross between a box plot and a kernel density plot. It is used to show how numerical data is distributed. Violin plots provide summary statistics as well as the density of each variable, unlike box plots, which can only show summary statistics. In the violin plot, there are more outsude points within the Kaggle Mart feature compared to the Kaggle Rama. It can be also noted that the former has a wider Inter Quartile Range (IQR) compared to the latter. \n\n![Violin Plots](https://miro.medium.com/max/780/1*TTMOaNG1o4PgQd-e8LurMg.png)\n    \n\nYou can read about an in-depth explanation of violin plots in this link."
\n    ðŸ“Œ Hover your mouse in the graph to examine the data distributions.\n
\n    ðŸ“Œ Drag the slider or press the play button to see the different graphs per country.\n
### Feature importance\nLet us also take a very quick look at the feature importance too:
"Where here the `F score` is a measure ""*...based on the number of times a variable is selected for splitting, weighted by the squared improvement to the model as a result of each split, and averaged over all trees*."" [1] Note that these importances are susceptible to small changes in the training data, and it is much better to make use of [""GPU accelerated SHAP values""](https://www.kaggle.com/carlmcbrideellis/gpu-accelerated-shap-values-jane-street-example), incorporated with version 1.3 of XGBoost.\n### Links\n* XGBoost: [documentation](https://xgboost.readthedocs.io/en/latest/index.html), [GitHub](https://github.com/dmlc/xgboost).\n* LightGBM: [documentation](https://lightgbm.readthedocs.io/en/latest/index.html), [GitHub](https://github.com/microsoft/LightGBM).\n* CatBoost: [documentation](https://catboost.ai/docs/), [GitHub](http://https://github.com/catboost).\n\n### Videos\nFor those who enjoy learning via videos, Josh Starmer on his YouTube channel [StatQuest](https://www.youtube.com/c/joshstarmer) has created some very accessible material:\n* [Gradient Boost Part 1 (of 4): Regression Main Ideas](https://youtu.be/3CC4N4z3GJc)\n* [Gradient Boost Part 2 (of 4): Regression Details](https://youtu.be/2xudPOBz-vs)\n* [Gradient Boost Part 3 (of 4): Classification](https://youtu.be/jxuNLH5dXCs)\n* [Gradient Boost Part 4 (of 4): Classification Details](https://youtu.be/StWY5QWMXCw)\n* [XGBoost Part 1 (of 4): Regression](https://youtu.be/OtD8wVaFm6E)\n* [XGBoost Part 2 (of 4): Classification](https://youtu.be/8b1JEDvenQU)\n* [XGBoost Part 3 (of 4): Mathematical Details](https://youtu.be/ZVFeW798-2I)\n* [XGBoost Part 4 (of 4): Crazy Cool Optimizations](https://youtu.be/oRrKeUCEbq8)\n\n### Related kaggle notebooks\n* [""GPU accelerated SHAP values with XGBoost""](https://www.kaggle.com/carlmcbrideellis/gpu-accelerated-shap-values-jane-street-example)\n* [""Automatic tuning of XGBoost with XGBTune""](https://www.kaggle.com/carlmcbrideellis/automatic-tuning-of-xgboost-with-xgbtune)\n* [""20 Burning XGBoost FAQs Answered to Use Like a Pro""](https://www.kaggle.com/bextuychiev/20-burning-xgboost-faqs-answered-to-use-like-a-pro) written by [BEXGBoost](https://www.kaggle.com/bextuychiev)\n* [""A Guide on XGBoost hyperparameters tuning""](https://www.kaggle.com/prashant111/a-guide-on-xgboost-hyperparameters-tuning) by [Prashant Banerjee](https://www.kaggle.com/prashant111)\n\n### References\n\n[1] [J. Elith, J. R. Leathwick, and T. Hastie ""*A working guide to boosted regression trees*"", Journal of Animal Ecology **77** pp. 802-813 (2008)](https://doi.org/10.1111/j.1365-2656.2008.01390.x)\n\n### Appendix: The RMSLE evaluation metric\nFrom the competition [evaluation page](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview/evaluation) we see that the metric we are using is the root mean squared logarithmic error (RMSLE), which is given by\n\n$$ {\mathrm {RMSLE}}\,(y, \hat y) = \sqrt{ \frac{1}{n} \sum_{i=1}^n \left(\log (1 + \hat{y}_i) - \log (1 + y_i)\right)^2} $$\n\nwhere $\hat{y}_i$ is the predicted value of the target for instance $i$, and $y_i$\nis the actual value of the target for instance $i$.\n\nIt is important to note that, unlike the RMSE, the RMSLE is asymmetric; penalizing much more the underestimated predictions than the overestimated predictions. For example, say the correct value is $y_i = 1000$, then underestimating by 600 is almost twice as bad as overestimating by 600:"
### Tip 1.1. Import the most popular and useful main Python libraries
### Tip 1.2. Warnings - ignore all
### Tip 6.16. Feature importance diagram 
In progress...
### Competition [Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic)
"### Result: the file ""submission.csv"" gives LB = 0.80382 (Top 4%)"
"We'll take a look at the learning curves as always, and also inspect the best values for the loss and accuracy we got on the validation set. (Remember that early stopping will restore the weights to those that got these values.)"
# Your Turn #\n\nUse a neural network to [**predict cancellations in hotel reservations**](https://www.kaggle.com/kernels/fork/11887335) with the *Hotel Cancellations* dataset.
**We can also visualize the partial dependence of two features at once using 2D Partial plots.**
"# 3. SHAP Values\n\nSHAP which stands for **SH**apley **A**dditive ex**P**lanation, helps to break down a prediction to show the impact of each feature. It is based on Shapley values, a technique used in game theory to determine how much each player in a collaborative game has contributed to its success[Â¹](https://medium.com/civis-analytics/demystifying-black-box-models-with-shap-value-analysis-3e20b536fc80). Normally, getting the trade-off between accuracy and interpretability just right can be a difficult balancing act but SHAP values can deliver both.\n\nSHAP values interpret the impact of having a certain value for a given feature in comparison to the prediction weâ€™d make if that feature  took  some baseline value.\n\nShap values show how much a given feature changed our prediction (compared to if we made that prediction at some baseline value of that feature). Letâ€™s say we wanted to know what was the prediction when the insulin level was 150 instead of some fixed baseline no. If we are able to answer this, we could perform the same steps for other features as follows:\n\n`sum(SHAP values for all features) = pred_for_team - pred_for_baseline_values`"
 Titanic - Machine Learning from Disaster\n\n*Top 3% Titanic solution - For this last version best model score is 0.81100*.\n![](https://images.fineartamerica.com/images/artworkimages/mediumlarge/1/2-rms-titanic-ship-plans-jose-elias-sofia-pereira.jpg)
"Table Of Content\n* [1. EDA & Feature Engeneering](#1_bullet)\n    * [ 1.1 Passengers location analisys](#1.1_bullet) - Survival for different Deck / Cabin numbers\n    * [ 1.2 Groups and family bonds analisys](#1.2_bullet) - Ticket numbers analisys, Names / Surnames\n    * [ 1.3 Personal features analisys](#1.3_bullet) - Age / Status analisys\n* [2. Data preparation](#2_bullet)\n    * [ 2.1 Filling None values](#2.1_bullet)\n    * [ 2.2 Encoding features and droping unnecessary](#2.2_bullet)\n* [3. Model development](#3_bullet)\n    * [ 2.1 Catboost baseline](#2.1_bullet)\n    \n"
"Now we gonna plot some developed ""coordinates"" of the passangers, to find some patterns in survival."
"We can see some patterns in the data:\n  - For example, all of the passangers on the ""1"" side of the Deck D survived.\n  - For only some of decks the ""closer"" location to zero may cause the better survival (maby those passangers was closer to the ladders)"
"We can see some patterns in the data:\n  - For example, all of the passangers on the ""1"" side of the Deck D survived.\n  - For only some of decks the ""closer"" location to zero may cause the better survival (maby those passangers was closer to the ladders)"
"In the end - we have to fillna for ""Side"" feature with 0 - refering to unknown side of the ship"
Lets visualize Surviving rates for families:
"The same thing as in (*) we gonna do for those Surnames who do not exist in test set.\nAgain, i gonna do it manually, cause i dont wanna work with automatization here =)\n\nBut just before this we will put some extra feature which mean ""Has any namesakes"" to determin if the person in som relatio-like group. It may look the same as SibSp, but guess, that som connections between people is hidden."
I desided to delete values with low frequancy - for our model not to be messed up.
"###   1.3.2 Parch and Age\nThis features may have crucial effect on the target. Parants was saving there children befor them, and many other connections may be inside of the just two attributes"
"#   2. Data preparation\n##   2.1 Filling None values\n###   2.1.1 Filling ""Age"" None values"
"For the ""Age"" feature we just can replace None values with some mean value, but we gonna do it in a bit more complex way.\nWe will locate some groups, based on Sex and Pclass - lets check the difference in ages in several groups."
"We will define the method, which fits the given model and puts all the neccesery data into different structures to use them further:\n\n* deploy_acc  - model accuracy for deploy dataset\n* train_acc   - model accuracy after the validation\n* models_dict - dictionary of the models\n"
"I've tried several Gridsearches for CVs with Kfold. And after a bunch of deploys, i chose those models.\nI chose models which gave me the best deploy accuracy. This models gonna be not ""Best model for Survival prediction"" , but ""The best model to predict the test set"", which is not the exect solution for the task."
"### Transported(Dependent, Nominal)  \nThe target value is a binary label consisting of True and False. Therefore, it is necessary to make sure that it is balanced. I checked the number and percentage of each category using countplot. When I checked, I found that True was 50.36% and False was 49.64%."
"-------------------------------------------------------------------------------\n### HomePlanet (Nominal)  \nThese variables are nominal variables, so we decided to check the distribution with countplot. To see if this variable can explain the dependent variable well, we checked the distribution of the dependent variable again. I confirmed the following results.\n\n1) Among the Homeplanets, 54.19% of the earth's share is the highest.  \n2) The ratio of Europa to Mars is almost the same.  \n3) Among Europa, the percentage of Transported is certainly high.  \n4) The false percentage of Transported in Earth is certainly high.  \n5) There is not much difference in Transported among Mars.\n\n= > More than half of people belong to Earth. In addition, the difference between Earth and Europa's transported ratio is certain, so the sorting algorithm can work well. Therefore, you will be able to use the variable well to solve this problem."
-------------------------------------------------------------------------------\n### Other Continuous Variables\nThey have a one-sided distribution. Most people didn't pay for this service. These can be extracted from variables such as total consumption.
"-------------------------------------------------------------------------------\n## 4. Multinomial Explore  \nYou should also consider the relationship between two or more variable combinations and dependent variables. For example, if you look at Home Planet and CryoSleep together, you can better define the relationship between the dependent variables:"
"-------------------------------------------------------------------------------\n## 4. Multinomial Explore  \nYou should also consider the relationship between two or more variable combinations and dependent variables. For example, if you look at Home Planet and CryoSleep together, you can better define the relationship between the dependent variables:"
"\nðŸ”‘conclusion:   \nPassengers in suspended sleep are generally likely to be transmitted. Especially in Europa and Mars, most passengers during housekeeping sleep were forwarded.\n"
### 2.2 Import Libraries
### 2.3 Import Data
### 2.4 Some Visualisations
### 2.5 Feature engineering
"### 3.3 Dendrogram\n\nThis technique is specific to the agglomerative hierarchical method of clustering. The method starts by considering each point as a separate cluster and starts joining points to clusters in a hierarchical fashion based on their distances. To get the optimal number of clusters for hierarchical clustering, we make use a dendrogram which is tree-like chart that shows the sequences of merges or splits of clusters."
"# 4. K-Means\n\nK-Means Clustering may be the most widely known clustering algorithm and involves assigning examples to clusters in an effort to minimize the variance within each cluster.\nIt's a centroid-based algorithm and the simplest unsupervised learning algorithm.\nThe algorithm tries to minimize the variance of data points within a cluster. It's also how most people are introduced to unsupervised machine learning.\n\n**K-means++** (default init parameter for K-Means in sklearn) is the algorithm which is used to overcome the drawback posed by the k-means algorithm. The goal is to spread out the initial centroid by assigning the first centroid randomly then selecting the rest of the centroids based on the maximum squared distance. The idea is to push the centroids as far as possible from one another.\n\nAlthough the initialization in K-means++ is computationally more expensive than the standard K-means algorithm, the run-time for convergence to optimum is drastically reduced for K-means++. This is because the centroids that are initially chosen are likely to lie in different clusters already."
### 4.4 Comparing results
### 4.5 K-Means on online retail data
### 5.4 Comparing results
### 5.5 Hierarchical clustering on online retail data
### 6.5 Comparing results
### 6.6 DBSCAN clustering model on online retail data
### 7.4 Comparing results
### 7.5 GMM clustering model on online retail data
# 8. All algorithm comparison
Some referrals\n\nhttps://crunchingthedata.com/when-to-use-dbscan/\n\nhttps://crunchingthedata.com/when-to-use-hierarchical-clustering/\n\nhttps://crunchingthedata.com/when-to-use-gaussian-mixture-models/\n\nhttps://www.freecodecamp.org/news/8-clustering-algorithms-in-machine-learning-that-all-data-scientists-should-know/
"# Lets optimize\n\n> The objective of this NB is to showcase some **optimization (Processing & Memory optimization)** while working with Pandas \n\n> The NB includes optimization techniques involving the use of **Pandas** and **Numpy** mainly\n\n> I am also planning to add **Python optimization** techniques & maybe something on **ML & NLP models optimization** depending on the feedback this NB receives\n\n-----------------\n\n\n\n I am no expert in optimizations, the whole point of the NB is to optmize the code especially building data pipelines\n    \nPlease feel free to share any feedback or share any corrections  \n    \nFor a few usecases I have used TPS March 2022 and  Ubiquant market prediction data  & others Just to make sure these optimizations work for real data as well \n\n\n\n\n\nTricks with emojis must not be missed\n"
"#  ðŸ“ŒDatatypes\n\n\nWe generally dont give much importance to datatypes in datasets, but the fact is using correct datatypes can save us a lot of memory and time which working/loading the datasets\n\n\nFollowing tricks can be used to optimize using correct datatypes\n\n- Float & Int datatype\n- datetime vs string\n- object vs category\n--------------\n\nWill use `ubiquant-market-prediction` dataset for this section\nhttps://www.kaggle.com/c/ubiquant-market-prediction"
"# High level insight on genetic variations\nNote: As this is my first published Kernel, am open to suggestions. If this helped you, some upvotes would be very much appreciated.\n\n### Library and Settings\nImport required library and define constants"
### Files
Lets have a look at some genes that has highest number of occurrences in each class. 
Some points we can conclude from these graphs:\n 1. BRCA1 is highly dominating Class 5 \n 2. SF3B1 is highly dominating Class 9\n 3. BRCA1 and BRCA2 are dominating Class 6
As we can see there are some entries without any text data. \nNow let us get distribution of text count for each class
Distribution looks quite interesting and now I am in love with violin plots.\nAll classes have most counts in between 0 to 20000. Just as expected. \nThere should be some 
Distribution looks quite interesting and now I am in love with violin plots.\nAll classes have most counts in between 0 to 20000. Just as expected. \nThere should be some 
Frequently occurring terms for each class
We need to know more about text. Tf-idf is known as one good technique to use for text transformation and get good features out of text for training our machine learning model. [Here][1] you can find more details about tf-idf and some useful code snippets. \n\n\n  [1]: https://buhrmann.github.io/tfidf-analysis.html
Lets plot out some top features we got using Tf-Idf for each class
# 2 | MAIN DATA CLASS \n\n\n    2.1 | Class Functions\n\n\nI've combined different functions one might use for visualisation of design & response parameters in a single class TS.\n\n### GET SUBSET OF DATA\n\n- get_id : Get result data for one specific design\n- get_aoasubset : Return a list of unique data subsets \n\n### CREATE PLOTS\n\n- plot_X_clcd : Plot Design Variable against Two Response Variables \n- plot_par_coord : Create Parallel Coordinates Plot\n- plot_polar : Create Drag Polar Plot\n- plot_angle_polar : Create Drag Polar Plot with Angle Hue Variation\n- plot_scat_mat : Create Scatter Matrix Plot
"\n    2.2 | Understanding our features\n\n\n- Pathway to the CSV file; n2412_ts, which contains a summary of all CFD simulations we tested\n- Each simulation, gives us specific metrics cd & cl, as well as a performance metric performance (not used)\n- Each CFD simulation slightly alters the geometry of the airfoil maxcamber, maxcamberposition, thickness\n- For each design, we have another variable for which we create more conditions aoa\n- In total, this creates **480 different test cases** (we ran 480 simulations) to obtain our data"
"\n    3.3 | Missing Data\n \n\n- Whilst missing data in for field values can exist & we don't have any missing data for these sort of features, so we can move on to some visualisation"
"# 4 | DESIGN PARAMETER RELATION TO RESPONSE VARIABLES \n\n\n    4.1 | Airfoil Geometry Parameter\n \n\n### GEOMETRIC PARAMETERS\n\n- 3/4 design parameters are related to the geometry of the airfoil; **maxcamber**, **maxcamberposition** & **thickness**\n- Let's plot the relation of the design parameters to the response parameters; lift & drag coefficients\n- With the addition of an interpoaltion model, we can better understand the general trend of the relationship between design and response variables tested\n\n### TREND OBSERVATIONS\n\n- As **maxcamber** is increased, the lift generated tends to go up. The drag resistance also tends goes up.\n- **Maxcamberposition** relation to lift is quite nonlinear, we can observe a peak at about the half way point (50%) on the airfoil, higher and lower values tend to give smaller values of lift. Drag tends to be lower near this point as well.\n- **Thickness** has the most nonlinear relation of all the other design parameters when it comes to lift. There tends to be a few values at which lift is maximised. On the other hand variations in drag is quite minimal."
### Libraries
\n  \n
### Min Max and Counts
### Distributions
### Distributions
"**Observations:**\n\n* The distribution of the original dataset does not follow the synthetic one closely especially for the two last features above.\n* Although the train and test distributions are pefectly identical.\n\nA synthetic dataset is a type of dataset created by generating new data that mimics the original data using various techniques. However, it is possible that the synthetic dataset features may not closely follow the original dataset distribution (our case). This can occur due to a variety of factors, such as using a different sampling technique, applying different data transformations, or introducing new features that were not present in the original dataset. When the synthetic dataset features do not closely follow the original dataset distribution, it can affect the performance of machine learning models trained on the origin data, as the models may not accurately capture the underlying patterns and relationships in the original data. Therefore, it is important to carefully evaluate the quality of both datasets before using them.\n\nLet's take a look at the train dataset features against the target and target itself:"
### Correlations
**Notes:**\n\n* There are many highly correlated features (> |0.8|). We might end up dropping some of them or create simple interations.\n
### Training Summary
"**Note**:\n* By increasing the number of iterations and reducing the learning rate, the model stopped finishing the training too early.\n* It might be a good idea to increase the number of iterations or decrease `early_stopping_rounds`."
"**Few notes of why we might need to use probability calibration techniques:**\n* A well-calibrated classifier can help to prevent incorrect decisions based on the classifier's predictions, especially if those decisions are based on threshold values for the predicted probabilities.\n* Poorly calibrated classifiers can lead to incorrect decisions, which can be especially problematic in high-stakes situations.\n* Probability calibration techniques can adjust the predicted probabilities to better reflect the true probabilities of positive outcomes, leading to more accurate and reliable predictions.\n\n**The process:**\n\nTo assess the calibration of a binary classifier, we can plot a calibration curve that shows the predicted probabilities on the x-axis and the proportion of true positive outcomes for each predicted probability bin on the y-axis. The calibration curve can also show the proportion of true negative outcomes for each predicted probability bin. A perfectly calibrated classifier would have a calibration curve that is close to the diagonal line, indicating that the predicted probabilities accurately reflect the true probabilities of both positive and negative outcomes.\n\nIf the probabilities are not calibrated we can build in a sense the second level model fitting our predictions into Logistic regression or any other chosen algorithm against the target.\nAfter that we repeat the process by plotting the calibration curve and returning the new score.\n\n**Methods:**\n\nTo calibrate a binary classifier, the data set of scores and their corresponding binary outcomes is used. The aim is to find a function that can accurately estimate the relationship between the scores and the true probabilities as determined empirically in the calibration set. There are several methods of calibration that can be used, including:\n\n* Platt Scaling\n* Isotonic Regression\n* Beta Calibration\n* SplineCalib\n\nThe choice of calibration method depends on factors such as the size of the calibration set, the complexity of the classifier, and the desired level of calibration accuracy.\n\n**Let's plot the calibration plot and histogram first:**\n"
**Note**:\n\nIt might look like the classifier is not well calibrated in a range ~(0.35 < x < 0.85) and we need to focus on that. But the important thing is to look at the histogram which shows that routhly 90% of our predictions < 0.5 and we need to **zoom in**:
"This kernel is intended to be a tutorial on Keras around image files handling for Transfer Learning using pre-trained weights from ResNet50 convnet.\n\nThough loading all train & test images resized (224 x 224 x 3) in memory would have incurred ~4.9GB of memory, the plan was to batch source image data during the training, validation & testing pipeline. Keras ImageDataGenerator supports batch sourcing image data for all training, validation and testing. Actually, it is quite clean and easy to use Keras ImageDataGenerator except few limitations (listed at the end).\n\nKeras ImageDataGenerator expects labeled training images to be available in certain folder heirarchy, 'train' data was manually split into 10k for training & 2.5k for validation and re-arranged into the desired folder hierarchy. Even 'test' images had to rearranged due to a known issue in flow_from_directory."
### Global Constants
Let's look at the distribution of the target variable.
Let's eliminate the slight imbalance of classes by using the SMOTE method.
# Imports
# Reading the csv file
# Distrubution of types of articles
# Unigrams and bigrams 
# WordCloud of articles
# Articles including images vs Label
\n        \n            1. Logestic Regression\n        \n
\n        \n            2. Support Vector Machines\n        \n
\n        \n            2. Support Vector Machines\n        \n
\n        \n            3. KNeighborsClassifier\n        \n
\n        \n            3. KNeighborsClassifier\n        \n
\n        \n            4. DecisionTreeClassifier\n        \n
\n        \n            4. DecisionTreeClassifier\n        \n
\n        \n            5. RandomForestClassifier\n        \n
\n        \n            5. RandomForestClassifier\n        \n
\n        \n            6. ExtraTreesClassifier\n        \n
\n        \n            6. ExtraTreesClassifier\n        \n
\n        \n            7. AdaBoostClassifier\n        \n
\n        \n            7. AdaBoostClassifier\n        \n
\n        \n            8. XGB Classifier\n        \n
\n        \n            8. XGB Classifier\n        \n
\nComparing Multiple Models\n
  \nDistribution of Gender\n
Male: 577\nFemale: 314
  \nPclass and Age vs Survived\n
  \nCabin vs Survived\n
> Before grouping
> After grouping 
> After grouping 
* Cabin 0: ABC\n* Cabin 1: DE\n* Cabin 2: FG\n* Cabin 3: Z(missing values)
  \nSibSp vs Survived\n
  \nParch vs Survived\n\n
  \nParch vs Survived\n\n
  \nGender vs Survived\n
  \nGender vs Survived\n
  \nEmbarked and Fare vs Survived\n
  \nEmbarked and Fare vs Survived\n
  \nFare vs Survived\n
  \nFare vs Survived\n
  \nAge vs Survived\n
  \nAge vs Survived\n
  \nCorrelation\n
## Effect of parameters on Heart Disease based on Age
## Creating Dummy Variables
"The **ROC curve**. In a Receiver Operating Characteristic (ROC) curve the true positive rate (Sensitivity) is plotted in function of the false positive rate (100-Specificity) for different cut-off points. Each point on the ROC curve represents a sensitivity/specificity pair corresponding to a particular decision threshold.\n\nAs the area under an ROC curve is a measure of the usefulness of a test in general, where a greater area means a more useful test, the areas under ROC curves are used to compare the usefulness of tests. The term ROC stands for Receiver Operating Characteristic."
## I hope this kernel is helpfull for you -->> upvote will appreciate me for further work.
"# Multiome Quickstart With Sparse Matrices\n\nThis notebook is mostly for demonstrating the utility of sparse matrices in this competition. (Especially for the Multiome dataset).\n\nAs the Multiome dataset is  very sparse (about 98% of cells are zeros), it benefits greatly from being encoded as sparse matrices. \n\nThis notebook is largely based on [this notebook](https://www.kaggle.com/code/ambrosm/msci-multiome-quickstart) by AmbrosM. It is a nice first attempt at handling Multiome data, and I thought it would informative for kagglers to be able to contrast directly the performances of sparse vs dense representations. \n\nMostly, the differences with AmbrosM's notebooks are:\n- We use a representation of the data in sparse CSR format, which let us load all of the training data in memory (using less than 8GB memory instead of the >90GB it would take to represent the data in a dense format)\n- We perform PCA (actually, TruncatedSVD) on the totality of the training data (while AmbrosM's notebook had to work with a subset of 6000 rows and 4000 columns). \n- We keep 16 components (vs 4 in AmbrosM's notebook)\n- We apply Ridge regression on 50000 rows (vs 6000 in AmbrosM's notebook)\n- Despite using much more data, this notebook should run in a bit more than 10 minutes (vs >1h for AmbrosM's notebook)\n\nThe competition data is pre-encoded as sparse matrices in [this dataset](https://www.kaggle.com/datasets/fabiencrom/multimodal-single-cell-as-sparse-matrix) generated by [this notebook](https://www.kaggle.com/code/fabiencrom/multimodal-single-cell-creating-sparse-data/).\n\nSince we will only generate the multiome predictions in this notebook, I am taking the CITEseq predictions from [this notebook](https://www.kaggle.com/code/vuonglam/lgbm-baseline-optuna-drop-constant-cite-task) by VuongLam, which is the public notebook with the best score at the time I am publishing.\n"
"# The scoring function (from AmbrosM)\n\nThis competition has a special metric: For every row, it computes the Pearson correlation between y_true and y_pred, and then all these correlation coefficients are averaged."
# Preprocessing and cross-validation\n\nWe first load all of the training input data for Multiome. It should take less than a minute.
"## PCA / TruncatedSVD\nIt is not possible to directly apply PCA to a sparse matrix, because PCA has to first ""center"" the data, which destroys the sparsity. This is why we apply `TruncatedSVD` instead (which is pretty much ""PCA without centering""). It might be better to normalize the data a bit more here, but we will keep it simple."
# Predicting
# Creating submission\n\nWe load the cells that will have to appear in submission.
##  Imports : \n
â¬†ï¸Back to Table of Contents â¬†ï¸
"\n Data augmentation is a technique through which one can increase the size of the data for the training of the model without adding the new data. Techniques like padding, cropping, rotating, and flipping are the most common methods that are used over the images to increase the data size. \n\nThere are six different augumentations/transformations implemented in this notebook :\n* Random Brighness\n* Random Contrast \n* Random Saturation\n* Random Crop or Pad\n* Random Rotate\n* Sharpness\n"
#### Difference between normal tf.image functions and stateless ones:\n* There are two sets of random image operations: `tf.image.random*` and `tf.image.stateless_random*`. \n\n* Using `tf.image.random*` operations is strongly discouraged as they use the old RNGs from TF 1.x. (TensorFlow Website)
##  Utility Functions: 
##  Cosine Decay Learning Rate Scheduler: 
##  Cosine Decay Learning Rate Scheduler: 
## Training : 
"# **W & B Artifacts**\n\nAn artifact as a versioned folder of data.Entire datasets can be directly stored as artifacts .\n\nW&B Artifacts are used for dataset versioning, model versioning . They are also used for tracking dependencies and results across machine learning pipelines.Artifact references can be used to point to data in other systems like S3, GCP, or your own system.\n\nYou can learn more about W&B artifacts [here](https://docs.wandb.ai/guides/artifacts)\n\n![](https://drive.google.com/uc?id=1JYSaIMXuEVBheP15xxuaex-32yzxgglV)"
![](https://i.imgur.com/cYkCWRg.png)
# **Visualizing Random Samples**
# **Visualize Segmentation Masks using W&B**\n
# **Visualize a case day-wise using W&B Tables**\n
![](https://i.imgur.com/GVsxPGK.png)
### Changes by year\n\nAll items and stores seem to enjoy a similar growth in sales over the years.
### Changes by month\n\nAll items and stores seem to share a common pattern in sales over the months as well.\n
### Changes by month\n\nAll items and stores seem to share a common pattern in sales over the months as well.\n
### Changes by day of the week\n\nAll items and stores also seem to share a common pattern in sales over the days of the week as well.
### Changes by day of the week\n\nAll items and stores also seem to share a common pattern in sales over the days of the week as well.
"### Are these patterns degenerate?\n\nThis is an important question. Not checking for degeneracies in the data can lead to missing important trends in complex datasets. For example, when looking at the monthly patterns, we average over all days of the month, years and either items or stores. But what if sales have a multi-dimensional dependence on two of these parameters that isn't easily separable? So, always check for degeneracies in the data!"
"### Are these patterns degenerate?\n\nThis is an important question. Not checking for degeneracies in the data can lead to missing important trends in complex datasets. For example, when looking at the monthly patterns, we average over all days of the month, years and either items or stores. But what if sales have a multi-dimensional dependence on two of these parameters that isn't easily separable? So, always check for degeneracies in the data!"
"In this case, however, there don't seem to be any sneaky degeneracies. We can effectively treat the ""month"", ""year"", ""day of the week"", ""item"" and ""store"" as completely independent modifiers to sales prediction. This leads to a *very very simple* prediction model.\n\n""Relative sales"" in the plots above are the sales relative to the average. Since there are very regular patterns in the ""month"", ""day of week"", and ""year"" trends. All we have to do is simply memorize these trends and apply them to our predictions by multiplying them to the expected average sales. We get the expected average sales for an item at a store from the historical numbers in the training set."
### What about the item-store relationship?
"Same here. Just a constant pattern and no degeneracies. So, you just need a model for how items sell at different stores, which is easily captured by an average sales look-up table or yet another ""relative sales"" pattern model.\n\n> *Aside: Based on the extremely regularity of the data, how neat it is, and how few degeneracies there are - I am fairly confident this is probably simulated data.*"
"## Writing the ""slightly better predictor""\n\nWe just need an item-store average sale look-up table, and then the ""day of week"", ""monthly"", ""yearly"" models."
"We can do a simple linear regression on the yearly growth datapoints. But if you look carefully, you can tell that the growth is slowing down. The quadratic fit works better since it better captures the curvature in the growth curve. Since we only have 5 points, this is the highest degree polynomial fit you should do to avoid overfitting."
 Outline : \n1) [Brief Overview](#brief_overview) \n\n2) **Analysis by Type of Group**\na) [Gender Analysis](#gender_analysis) \nb) [Age Groups](#age_groups)\nc) [Wealth Analysis](#wealth_analysis) \n\n3) **Correlations and Purposes of Loans**\na) [Correlations](#correlations)\nb) [Loan Purpose](#purpose_loans)\n\n4) **Modeling**\na) [Predictive Modelling](#predictive_modelling)\n\n\n
"## Brief Overview: \n\nThe first phase of this project is to see what is our data made about. Which variables are numerical or categorical and which columns have ""Null"" values, which is something we will address in the feature engineering phase.\n\n## Summary:\n\n We have four numeric and four categorical features. \n The average age  of people in our dataset is 35.54\n The average credit amount borrowed is 3271 \n\n\n"
# EDA
**Observation:** \n\n* The number of people belonging to sex category 0 are 96 whereas 1 are 206.\n* The number of people in one category are more than double than the zero.
**Observation:** \n\n* The number of people belonging to sex category 0 are 96 whereas 1 are 206.\n* The number of people in one category are more than double than the zero.
"**Observation:**\n\n* cp : Chest Pain type chest pain type\n\n    * Value 0: typical angina\n    * Value 1: atypical angina\n    * Value 2: non-anginal pain\n    * Value 3: asymptomatic\n    \n* People of chest pain category '0' have the highest count, whereas of count of chest pain '3' is the lowest"
"**Observation:**\n\n* cp : Chest Pain type chest pain type\n\n    * Value 0: typical angina\n    * Value 1: atypical angina\n    * Value 2: non-anginal pain\n    * Value 3: asymptomatic\n    \n* People of chest pain category '0' have the highest count, whereas of count of chest pain '3' is the lowest"
**Observation:** People of fbs category 1 are less than 25% of people of fbs category 0.
**Observation:** People of fbs category 1 are less than 25% of people of fbs category 0.
**Observation:** Thall count is maximum for type 2 ( 165 ) and min for type 0 ( 2 ) .\n
**Observation:** Thall count is maximum for type 2 ( 165 ) and min for type 0 ( 2 ) .\n
"**Observation:** \n\n* ECG count is almost the same for type 0 and 1. \n* Also, its almost negligible for type 2 in comparision to type 0 and 1."
"**Observation:** \n\n* ECG count is almost the same for type 0 and 1. \n* Also, its almost negligible for type 2 in comparision to type 0 and 1."
"**Observation:**\n\n* This swarmplot gives us a lot of information.\n* Accoring to the figure, people belonging to caa category '0' , irrespective of their age are highly prone to getting a heart attack.\n* While there are very few people belonging to caa category '4' , but it seems that around 75% of those get heart attacks.\n* People belonging to category '1' , '2' and '3' are more or less at similar risk."
"**Observations:**\n\n* trtbps and chol looks like they are normally distributed, with some outliers highly skewed towards right.\n* In case of thalachh the data is highly skewed towards right!"
**By the pair plot we can see data destribution and identfy outlier**
Menu Reference
"You probably already saw this menu on some Kaggle notebooks, I am very happy that it was a little useful!!"
**OBSERVATIONS**: There are many things that can be inferred from the above 3 plots\n* First thing you can see that in the early stages there are no prior_explanation. \n* In the final stages you can see almost all had prior_explanation.\n\n\n* Notice that in starting time there are a lot of question that are answered incorrectly (marked by black x)\n* In the middle time session the questions that did not have prior explanation were answered wrong (look bottom of chart-2)\n* Final stages had nearly all answers correct
All user_answers are evenly distributed except user_answer=2 which is slightly lesser than others which we saw before
All user_answers are evenly distributed except user_answer=2 which is slightly lesser than others which we saw before
Now as we have seen each column of interest let us move to compare 2 different columns
Now as we have seen each column of interest let us move to compare 2 different columns
What do we understand from the above plots? \nAns: NOTHING\n\nThis is just to show that there is no correlation when you compare ID columns with other columns
What do we understand from the above plots? \nAns: NOTHING\n\nThis is just to show that there is no correlation when you compare ID columns with other columns
Interesting... \n\nWe contradict ourselves from the previous plots. \nWhen we add details to our ID columns we can see some trends. \nSimilar is the above plot... \n\nUntil `task_container_id` =3000 there are no orange points and above 3000 there are no yellow points. \nthis means `User_ID` in certain range has `task_container_id` also in certain range.
Interesting... \n\nWe contradict ourselves from the previous plots. \nWhen we add details to our ID columns we can see some trends. \nSimilar is the above plot... \n\nUntil `task_container_id` =3000 there are no orange points and above 3000 there are no yellow points. \nthis means `User_ID` in certain range has `task_container_id` also in certain range.
**OBSERVATIONS**: \n* From the above plot we can see most of the questions had an explanation. Also we can see near right bottom some points in groups. This maybe because a large number of students took their test at the same time.\n* Another thing that we can notice is a faint blue line along the y-axis where x is 0. This is where the timestamp is 0 and there were no prior explanations.
**OBSERVATIONS**: \n* From the above plot we can see most of the questions had an explanation. Also we can see near right bottom some points in groups. This maybe because a large number of students took their test at the same time.\n* Another thing that we can notice is a faint blue line along the y-axis where x is 0. This is where the timestamp is 0 and there were no prior explanations.
Above plot tells us that most of the answers are correct
**OBSERVATIONS**\n* In the bottom right we can see that `starter` value in the `type_of` column has a range of 4-7 in part.\n* There are a lot of `lecture_id` with `intention` value in the `type_of` column
As we noted before `IDs` do not explain anything much
As we noted before `IDs` do not explain anything much
There is a lot of data in part 5 and 6 and a lot of tags are of the type concept
There is a lot of data in part 5 and 6 and a lot of tags are of the type concept
**OBSERVATIONS** - \n* There are no other parts for intention field other than part-2\n* The starter type lecture has only parts above 5th part\n* Most of the 5th part comes from concept lecture type
Maybe there are some null values. Let us check with a plot
okay so we have one missing value let us drop that row
Many interesting trends which we will discuss in detail down below...
"Not much can be seen from this plot as there is a lot of data. However, we can see as part increases the question_id value also increases. Interesting...."
"Not much can be seen from this plot as there is a lot of data. However, we can see as part increases the question_id value also increases. Interesting...."
We can see the same plot as above. Let us see if `question_id` and `bundle_id` columns are same or not
"\n    From the above Analysis, Most of the kagglers are Men (77%) and Female Kagglers are (22%) \n"
\n\n    \n        WHICH COUNTRY KAGGLERS BELONGS TO?\n    \n    
\n\n    \n        WHICH COUNTRY KAGGLERS BELONGS TO?\n    \n    
"\n    From the above Analysis, Most of the kagglers are from India and USA \n"
"\n    From the above Analysis, Most of the kagglers are from India and USA \n"
\n\n    \n        WHAT IS THE AGE OF THE KAGGLERS?\n    \n    
\n\n    \n        WHAT IS THE AGE OF THE KAGGLERS?\n    \n    
"\n    From the above analysis, Most of the kagglers are in 18-21, 25-29 and 22-24 Age groups\n"
\n\n    \n        PLATFORMS USED BY KAGGLERS FOR DATA SCIENCE COURSES\n    \n    
\n\n    \n        PLATFORMS HELPED TO KAGGLERS WHEN THEY ARE GETTING STATRED INTO DATASCIENCE\n    \n    
\n\n    \n        PLATFORMS HELPED TO KAGGLERS WHEN THEY ARE GETTING STATRED INTO DATASCIENCE\n    \n    
\n\n    \n        DID KAGGLERS PUBLISHED ANY RESEARCH PAPERS?\n    \n    
\n\n    \n        DID KAGGLERS PUBLISHED ANY RESEARCH PAPERS?\n    \n    
\n\n    \n        DID THE RESEARCH MAKE USE OF ML?\n    \n\n\n    Theoretical Research - the research made advances related to some novel machine learning method\n    Applied Research - the research made use of machine learning as a tool \n
\n\n    \n        DID THE RESEARCH MAKE USE OF ML?\n    \n\n\n    Theoretical Research - the research made advances related to some novel machine learning method\n    Applied Research - the research made use of machine learning as a tool \n
\n\n    \n        HOW LONG KAGGLERS HAVE BEEN CODING?\n    \n
\n\n    \n        HOW LONG KAGGLERS HAVE BEEN CODING?\n    \n
\n\n    \n        WHAT PROGRAMMING LANGUAGES USED BY KAGGLERS?\n    \n
\n\n    \n        WHAT PROGRAMMING LANGUAGES USED BY KAGGLERS?\n    \n
\n\n    \n        HOW MUCH EXPERIENCE DOES KAGGLERS HAVE IN ML?\n    \n
![](https://media.tenor.com/qoDOnF_uBYIAAAAC/office-motion-graphics.gif)
"\n    From the above Analysis, Most of the Kagglers are working as Data Scientist (1929 members ) and  Data Analyst(1538).\n     Almost 1435 Kagglers are Currently not employed.\n"
\n\n    \n        NOTEBOOK PRODUCTS USED BY KAGGLERS ON A REGULAR BASIS\n    \n    
\n\n    \n        IDE'S USED BY KAGGLERS ON A REGULAR BASIS\n    \n    
\n\n    \n        IDE'S USED BY KAGGLERS ON A REGULAR BASIS\n    \n    
\n\n    \n        DATA VISUALIZATION LIBRARIES USED BY KAGGLERS ON A REGULAR BASIS\n    \n    
## 5.1 Project proposal is Approved or not ?
* Training data is highly imbalanced that is approx. 85 % projetcs were approved and 15 % project were not approved. Majority imbalanced class is positive.
### 5.2.a Distribution of School states
"* Out of 50 states, **California(CA)** having higher number of projects proposal submitted **approx. 14 %**  followed by **Texas(TX)(7 %)** and **Tennessee(NY)(7 %)**."
"### 5.2.b Distribution of project_grade_category (school grade levels (PreK-2, 3-5, 6-8, and 9-12))"
"* Out of 4 school grade levels, Project proposals submission in school grade levels is higher for **Grades Prek-2** which is approximately **41 %** followed by **Grades 3-5** which has approx. **34 %**."
### 5.2.c Distribution of category of the project
"* Out of 51 Project categories,  Project proposals submission for project categories is higher  for  **Literacy & Language** which is approx. **27 %** followed by **Math & Science** which has approx. **20 %**."
### 5.2.d Distribution of number of previously posted applications by the submitting teacher
### 5.2.e Distribution of subcategory of the project
### 5.2.e Distribution of subcategory of the project
"* Out of 1,82,020 Project subcategories, Project proposals submission for project sub-categoriesis is higher  for **Literacy** which is approx. **16 % ** followed by **Literacy & Mathematics** which has approx. **16 %** ."
### 5.2.f Distribution of Project titles
"* Out of 1,82,080 project titles, Project proposals submission for project titles is higher for **Flexible seating** which is approx. **27 %** followed by **Whiggle while your work** which has approx. **14 %**."
### 5.2.g Distribution of price of resource requested
### 5.2.h Distribution of quantity of resource requested
### 5.2.h Distribution of quantity of resource requested
### 5.2.i Teacher prefix Distribution
### 5.2.i Teacher prefix Distribution
"* Higher number of project proposal submitted by **married womens** which is approx. **53 %**  followed by **unmarried womens** which has approx. **37 %**.\n* Project proposal submitted by **Teacher** which is approx. **2 %** is vey low as compared to **Mrs., Ms., Mr**."
## 5.4 Word Cloud of resources requested
## 5.5 Various popularities in terms of project acceptance rate and project rejection rate
### 5.5.a Popular School states in terms of project acceptance rate and project rejection rate
### 5.5.b Popular Teacher Prefix in terms of project acceptance rate and project rejection rate
### 5.5.b Popular Teacher Prefix in terms of project acceptance rate and project rejection rate
### 5.5.c Popular school grade levels in terms of project acceptance rate and project rejection rate
### 5.5.c Popular school grade levels in terms of project acceptance rate and project rejection rate
### 5.5.d Popular category of the project in terms of project acceptance rate and project rejection rate
### 5.5.d Popular category of the project in terms of project acceptance rate and project rejection rate
### 5.5.e Popular subcategory of the project in terms of project acceptance rate and project rejection rate
### 5.5.e Popular subcategory of the project in terms of project acceptance rate and project rejection rate
### 5.5.f Popular project titles in terms of project acceptance rate and project rejection rate
### 5.5.f Popular project titles in terms of project acceptance rate and project rejection rate
## 5.6 Project Proposals by US States
### 5.8.a Teacher_prefix and project_is_approved Intervals Correlation
### 5.8.b Teacher_number_of_previously_posted_projects and project_is_approved Intervals Correlation
### 5.8.b Teacher_number_of_previously_posted_projects and project_is_approved Intervals Correlation
*  Number of previously posted applications by the submitting teacher was** Zero(0)** having more number of acceptance rate.
### 5.8.c Correlation Matrix and Heatmap of training data
## 5.9 Project Submission Time Analysis
### 5.9.a Project Submission Month Analysis
* **August month** has the second  number of proposals followed by **September month** .
### 5.9.b Project Submission Weekday Analysis
* The number of proposals decreases as we move towards the end of the week.
### 5.9.c Project Submission Date Analysis
"* Looks like we have approximately one years' worth of data (May 2016 to April 2017) given in the training set.\n* There is a sudden spike on a single day (Sep 1, 2016) with respect to the number of proposals (may be some specific reason?)"
### 5.9.d Project Submission Hour Analysis
"* From Hours 03 to 05, number of proposals decreases.\n* Hours 06 to 14, number of proposals increases.\n* At Hour 14 has more number of proposals."
## 5.10 Top Keywords in project_essay_1
## 5.11 Top keywords in project_essay_2
## 5.11 Top keywords in project_essay_2
## 5.12 Top Keywords in project_resource_summary
## 5.12 Top Keywords in project_resource_summary
##  5.13 Quantity V.S. Price
# 5.15 Month wise distribution of number of projects proposal submitted in each state
* USA state **WY** was having more price requested for resources in **March** month than others.
### 5.16.a Price requested for resources distribution by different states
* As we can see most of the price requested for resources is between **0 to 2k dollar**.
### 5.16.b Price requested for resources distribution by Teacher prefixes
"* Mostly price requested for resources is \n   * 0 to 2k Dollar by **teacher** prefix\n   * 0 to 4k Dolar by **Ms. , Mrs. and Mr.** prefixes \n   * 0 to 500 Dollar by **Dr.** prefix."
### 5.16.c Price requested for resources distribution by different Genders
* Mostly price requested for resources is \n   * 0 to 2k Dollar by **Unknowns**\n   * 0 to 4k Dolar by **Males** \n   * 0 to 5k Dollar by **Females**.
### 5.16.d Price requested for resources distribution by different project_grade_category
* Mostly price requested for resources is between approx. ** 0 to 4k** **Dollar**  for all type of project grade categories.
### 5.17.a Popularities of Teacher prefixes in California
### 5.17.b Popularities of school grade levels in California
### 5.17.b Popularities of school grade levels in California
### 5.17.c Top project titles in California
### 5.17.c Top project titles in California
### 5.17.d Trend of project submission time in California
### 5.17.d Trend of project submission time in California
## 5.18 TX(Texas)
### 5.18.a Popularities of Teacher prefixes in Texas
### 5.18.b Popularities of school grade levels in Texas
### 5.18.b Popularities of school grade levels in Texas
### 5.18.c Top project titles in Texas
### 5.18.c Top project titles in Texas
### 5.18.d Trend of project submission time in Texas
### 5.18.d Trend of project submission time in Texas
# 6. Feature Engineering
"**The ROC AUC Score is the corresponding score to the ROC AUC Curve. It is simply computed by measuring the area under the curve, which is called AUC.**\n\n**A classifiers that is 100% correct, would have a ROC AUC Score of 1 and a completely random classiffier would have a score of 0.5.**"
## 8.2 Precision Recall curve
## 8.3 Ploting Metrics during training of Light GBM
## 8.4 Feature importances by LightGBM
## 8.4 Feature importances by LightGBM
# 9. Brief Summary/Conclusion :\n--------------------------------------------------------------------------\n* I have done analysis only on training data.\n* This is only a brief summary if want more details please go through my Notebook.
## 5.6 What is highest level of formal education of Developers?
"**About highest level of formal education of Developers :**\n  * Approx. 46 % Developers having Bachelor's degree\n  * Approx. 23 % Developers having Master's degree\n  * Approx. 12 % having some college/university study without earning a degree\n  * Approx. 9 % having Secondary schools(eg. American high school etc.)\n  * Approx 3 % having Associate degree\n  * Approx. 2 % having other doctral degree (Phd. , Ed.D etc.)\n  * Approx. 2 % having Primary/elementry school\n  * Approx. 2 % having Professional degree (JD, MD etc.)\n  * Approx. 1 % never completed any formal education."
## 5.7 Main field of study of Developers
"* **Main field of study of Developers :** Most of the Developers **(63.7 %)** having **Computer Science, Computer engineering or Software engineering** field of study followed by **8.79 %** Developers having **Another engineering discipline(ex. civil, electrical or mechanical)** followed by **8.23 %** Developers having **Information Systems, Information Tecnology or system administration**. Only Approx 1 % Developers never declared their field of study."
## 5.8 How many people are employed by the company or organization they work for?
"**Number of people are employed by the company or organization they work for :**\n  * ** Approx. 10 %** people work in company or organization having **Fewer than 10 employees**.\n  * ** Approx. 11 %** people work in company or organization having **10 to 19 employees**.\n  * ** Approx. 24 %** people work in company or organization having **22 to 99 employees**.\n  * ** Approx. 20 %** people work in company or organization having **100 to 499 employees**.\n  * ** Approx. 7 %** people work in company or organization having **500 to 999 employees**.\n  * ** Approx. 11 %** people work in company or organization having **1,000 to 4,999 employees**.\n  * ** Approx. 4 %** people work in company or organization having **5,000 to 9,999 employees**.\n  * ** Approx. 14 %** people work in company or organization having **10,000 or employees**."
## 5.9 Description of people who participated in the survey
**Description of peoples who participated in the survey :**\n  * Approx. **19 %** peoples called themselves **Back-end developers**.\n  * Approx. **16 %** peoples called themselves **Full-stack developers**.\n  * Approx. **13 %** peoples called themselves **Front-end developers**.\n  * Approx. **7 %** peoples called themselves **Mobile developer**.\n  * Approx. **6 %** peoples called themselves **Desktop or enterprise application developer**.\n  * Approx. **6 %** peoples called themselves **Student**.\n  * etc.
## 5.12 How easy or difficult was this survey to complete?
**People thought about difficulty of survey :**\n  * Approx. **37 %** peoples think survey was **Somewhat easy**.\n  * Approx. **33 %** peoples think survey was **Very easy**.\n  * Approx. **23 %** peoples think survey was **Neither easy nor difficult**.\n  * Approx. **6 %** peoples think survey was **Somewhat difficult**.\n  * Approx. **1 %** peoples think survey was **Very difficult**.
## 5.13 What peoples think about the length of the survey ?
**Peoples thinking about length of the survey :**\n  * Approx **50 %** peoples thinks **The survey was an appropriate length.**\n  * Approx. **49 %** peoples thinks **The survey was too long**.\n  * Approx. **1 %** peoples thinks **The survey was too short**.
## 5.14 for how many years have peoples been coding ?
**for how many years have peoples been coding :**\n  * 3-5 years : Approx. 25 % peoples\n  * 6-8 years : Approx. 21 % peoples\n  * 9-11 years : Approx. 13 % peopels\n  * 0-2 years : Approx. 11 % peoples\n  * etc.
## 5.15 For how many years have peoples coded professionally (as a part of your work)?
**For how many years have peoples coded professionally  :**\n  * 0-2 years : Approx. 30 % peoples\n  * 3-5 years : Approx. 27 % peoples\n  * 6-8 years : Approx. 15 % peoples\n  * 9-11 years : Approx. 10 % peoples\n  * etc.
## 5.16  Peoples hope to be doing in the next five years
**Peoples hope to be doing in the next five years :**\n  * ** Approx. 34 % peoples** : Working in a different or more specialized technical role than the one I'm in now.\n  * **Approx. 26 % peoples ** : Working as a founder or co-founder of my own company.\n  * **Approx. 19 % peoples** : Doing the same work.\n  * **Approx. 10 % peoples** : Working as a product manager or project manager.\n  * **Approx. 3 % peoples** : Working in a career completely unrelated to software development.\n  * **Approx. 2 % peoples** : Retirement
## 5.17 Peoples current job-seeking status
"**Peoples current job-seeking status :**\n  * **Approx 60 % peoples** : I am not actively looking, but i am open to new opportunities.\n  * **Approx. 24 % peoples** :  I am not interested in new job opportunities.\n  * **Approx. 16 % peoples** : I am actively looking for job."
## 5.18 When was the last time that peoples took a job with a new employer?
**When was the last time that peoples took a job with a new employer :**\n  * **Approx. 35 % peoples** : Less than a year ago.\n  * **Approx. 22 % peoples** : Between 1 and 2 year ago.\n  * **Approx. 19 % peoples** : More than 4 year ago.\n  * **Approx. 19 % peoples** : Between 2 and 4 years ago.\n  * **Approx. 6 % peoples** : I've never had a job
"## 5.19  Most popular communication tools use to communicate, coordinate, or share knowledge with coworkers"
**Most popular communication tools :**\n  * **Slack** used by  approx. 19 % developers.\n  * ** Jira** used by approx. 15 % developers.\n  * **Office / productivity suite** used by approx. 14 % developers.\n  * **Stack overflow** used by only approx. 2 % developers.
## 5.20 Most popular languages 
"* **JavaScript is the most popular language** on which developer worked and want to work in over the next year. **Python** is most demandable language on which developers want to work followed by **HTML, CSS** and **SQL** etc"
## 5.21 Most popular Databases
"* **MySQL is the most popular Database** on which developer worked and want to work in over the next year. **MongoDB** is most demandable Database on which developers want to work followed by **PostgreSQL, SQLServer, redis** and **ElasticsSearch** etc."
## 5.22 Most popular platforms
"* **Linux is the most popular platform** on which developer worked and want to work in over the next year followed by  **Android** is most demandable Database on which developers want to work followed by **AWS, Raspberry pie** etc."
## 5.23 Most popular Framworks
"* **Node.js, Angular, React and .Net core are the most popular frameworks** on which developer worked and want to work in over the next year followed by  **Tansorflow** is most demandable Database on which developers want to work."
## 5.24.4 Participation on StackOverflow
**Participation on StackOverflow in Q&A :**\n  * **Approx. 39 % Respondents said** : Less than once per month or monthly\n  * **Approx. 23 % Respondents said** : A few times per month or weekly\n  * **Approx. 17 % Respondents said** : I have never participated in Q&A on StackOverflow\n  * **Approx. 12 % Respondents said** : A few times per week\n  * **Approx. 6 % Respondents said** : Daily or almost daily\n  * **Approx. 3 % Respondents said** : Multiple times per day
## 5.24.6 Up-to-date developer story on StackOverflow
"**Up-to-date developer story on StackOverflow :**\n  * **Approx. 37 % Respondents said** : No, I don't know what that is\n  * **Approx. 24 % Respondents said** : No, I know what it is but i don't have one\n  * **Approx. 23 % Respondents said** :  No, I have one but it's out of date\n  * **Approx. 17 % Respondents said** : YES"
## 5.24.11.1 StackOverflow Visit V.S. StackOverflow Recommendation
"* Majority of peoples who visit Stackoverflow **Multiple times per day**, **Daily or almost daily**, **A few times per week** or **A few times per month or weekly**	 recommending **very likely(10)**."
## 5.24.11.2 StackOverflow Visit V.S. participation in StackOverflow 
## 5.24.11.3 StackOverflow Visit V.S. peoples visiting  StackOverflow job board
## 5.24.11.3 StackOverflow Visit V.S. peoples visiting  StackOverflow job board
* Most of the peoples who are visiting stackOverflow **Daily or almost daily**	or **Multiple times per day** they know about **StackOverflow job board**.
## 5.24.11.4 StackOverflow Visit V.S. Up-to-date developer story on StackOverflow
"* Highest number of peoples who are visiting StackOverflow **Daily or almost dail**y, **Multiple times per day** or **A few times per week	**, they don't know what is **Stackoverflow developer Story**"
## 5.24.11.5 StackOverflow job board Visit V.S. StackOverflow Jobs Recommend
"* Its good to see that who are visiting stackoverflow job board, out of these most of the peoples are recommending Stackoverflow job board very likely(10)."
## 5.25 Top Reasons of upadating a CV 
**Top Reasons of upadating a CV  :**\n  * **Approx. 42 % Respondents said** : My job status and my personal status changed.\n  * **Approx. 14 % Respondents said** : A recruiter contacted me.\n  * **Approx. 11 % Respondents said** : I had a negative experience or interaction at work.\n  * **Approx. 11 % Respondents said** : A friend told me about a job opportunity.\n  * **Approx. 10 % Respondents said** : I saw an employer's advertisement.
## 5.26 Types of Non-degree education in which people participated
"**Types of Non-degree education in which people participated :**\n  * **Approx. 29 % Respondents said** : Taught yourself a new language, framework or tool without taking a formal course.\n  * **Approx. 16 % Respondents said** : Taken an online course in programming and software developement(eg. a MOOC)\n  * **Approx. 14 % Respondents said** : Contributed to open source softwares\n  * **Approx. 12 % Respondents said** : Received on-the-job training in software developement\n  * **Approx. 9 % Respondents said** : Participated in hackathon\n  * **Approx. 8 % Respondents said** : Participated in online coding compitition(eg. HackerRank, CodeChef or TopCoder)\n  * **Approx. 6 % Respondents said** : Taken a part-time in-person course in programming or software development\n  * **Approx. 5 % Respondents said** : Completed an industry certification program(eg. MCPD)\n  * **Approx. 4 % Respondents said** :  Participated in a full-time developer training program or bootcamp"
## 5.27 Top resources used by peoples who taught yourself without taking a course
"**Top resources used by peoples who taught yourself without taking a course :**\n  * **Approx. 22 % Respondents said** : The official documentation and/or standards for the technology\n  * **Approx. 22 % Respondents said** : Q&A on the StackOverflow\n  * **Approx. 13 % Respondents said** : A book or e-book from o'Reilly, Apress or a similar publisher\n  * **Approx. 13 % Respondents said** : Online developer communities other than StackOverflow(eg, forums, listservs, IRC Channels etc)\n  * **Approx. 13 % Respondents said** :The technology's online help system\n  * **Approx. 5 % Respondents said** : A college/University computer  science or software engineering books"
## 5.28 Top reasons who participated in online coding compitition or hackathon
"**Top reasons who participated in online coding compitition or hackathon :**\n  * **Approx. 26 % Respondents said** : Because I find it enjoyable\n  * **Approx. 23 % Respondents said** : To improve my general technical skills or programming ability\n  * **Approx. 18 % Respondents said** : To improve my knowledge of a specific programming language , framework or technology\n  * **Approx. 10 % Respondents said** : To improve my ability to work on a team with others programmers\n  * **Approx. 9 % Respondents said** : To build my professional network\n  * **Approx. 7 % Respondents said** : To help me find a new job opportunities\n  * **Approx. 6 % Respondents said** : to win prizes or cash awards"
## 5.30 Top most used IDE  by the developers
**Top most used IDE  by the developers :**\n  * Approx. **12 %** developers using **Visual Studio Code**\n  * Approx. **12 %** developers using Visual Studio \n  * Approx. **11 %** developers using **Notepad++**\n  * Approx. **10 %** developers using **Sublime Text**\n  * Approx. **9 %** developers using **Vim**
## 5.31 Top Used Operating system by the developers
**Top Used Operating system by the developers :**\n  * Approx. **50 %** developers are using **Windows**\n  * Approx. **27 % **developers are using **MacOS**\n  * Approx. **23 %** developers are using **Linux-based**\n  * **Only  0.2 %** developers are using **BSD/Unix**
## 5.32 Top version control system developers regularly use
**Top version control system developers regularly use :**\n  * Approx. **63 %** developers are using **Git**(**Most popular version control system**)\n  * Approx. **12 %** developers are using **Subversion**\n  * Approx. **8 %** developers are using **Team Foundation Version Control**\n  * Approx **6 %** deveopers are using **Zip file back-ups**
## 5.33 Top methodologies developers have experience working in
**Top methodologies developers have experience working in :**\n  * Approx. **33 %** developers experience in **Agile**\n  * Approx. **24 %** developers experience in **Scrum**\n  * Approx. **14 %** developers experience in **Kanban**\n  * Approx. **11 %** developers experience in **Pair programming**
## 5.34.5 Top ergonomic furniture or devices developers use on a regular basis
**Top ergonomic furniture or devices developers use on a regular basis :**\n  * **Approx. 38 %** developers use : **Ergonomic Keyboard or mouse**\n  * **Approx. 37 %** developers use : **Standing desk**\n  * **Approx. 16 %** developers use : **Wrist/hand supports or braces**\n  * **Approx. 9 %** developers use : **Fatigue-relieving floor mat**
"## 5.34.6 In a typical week, how many times do developers exercise?"
"**In a typical week, how many times do developers exercise? **\n  * **Approx. 38 %** developers said : **I don't typically excercise**\n  * **Approx. 29 %** developers said : **1 - 2 times per week**\n  * **Approx. 20 %** developers said : **3 - 4 times per week**\n  * **Approx. 14 %** developers said : **Daily or almost every day**"
## 5.35 Age of the developers of participated in the survey
**Age of the developers of participated in the survey :**\n  * **Apprx. 49 %** developers : **25 - 34 years old**\n  * **Apprx. 24 %** developers : **18- 24 years old**\n  * ** Apprx. 18 %** developers : **35 - 44 years old**\n  * **Apprx. 5 %** developers : **45 - 54 years old**\n  * **Apprx. 3 %** developers : **under 18 years old**\n  * **Apprx. 1 %** developers : **55 - 64 years old**\n  * **Apprx. 0.2 %** developers : **65 years or older**
"## 5.38.3 In a typical week, how many times do developers exercise(Male V.S. Female)?"
## 5.38.4 Age of the developers who participated in the survey(Male V.S. Female)
## 5.38.4 Age of the developers who participated in the survey(Male V.S. Female)
## 5.38.5 Fromal Education of developers(Male V.S. Female)
## 5.38.5 Fromal Education of developers(Male V.S. Female)
## 5.38.6 Top DevType with Median Salary(Male V.S. Female)
## 5.38.6 Top DevType with Median Salary(Male V.S. Female)
"**Top DevType with Median Salary(Male V.S. Female) : **\n  * Female having more median salary than Male in these DevType :\n    * **DevOps specialist**\n    * **C-suite executive (CEO, CTO etc.)**\n    * **Full Stack developer**\n    * **Educator or acadmic research**"
## 5.38.7 Top Countries where respondents are > 500 with Median Salary(Male V.S. Female)
**Top Countries where respondents are > 500 with Median Salary(Male V.S. Female) :**\n  * Female having more median salary than Male : **China**\n  * Female and Male having equal median salary : **Brazil**
"## 5.39.3 Top DevType with highest median salary (India, USA , Gobal)"
"**Top 5 DevType with highest median salary Globally in USD :**\n  * **Engineering manager :** \$ 88,573\n  * **DevOps specialist :** \$ 72,469\n  * **C-suite executive(CEO, CTO) :** \$ 69,244\n  * **Product manager : ** \$ 63,174\n  * **Data scientist or machine learning specialist :** \$ 60,000\n* Thea bove Top 5 Deytype with highest median salary  are same in **India**, **USA** and **Globally**. Only Devops specialist and C-suite executive are interchangable but in india  and USA are same."
## 5.43 Time to get a full-time job offer after doing  developer training program or bootcamp
**Time to get a full-time job offer after doing developer training program or bootcamp :**\n  * **Approx. 45 %** respondents said : **I already had a full time job as a developer when i began the program**\n  * **Approx. 16 %** respondents said : **Immediately after graduating** \n  * **Approx. 10 %** respondents said : **One to three month**\n  * **Approx. 9 %** respondents said : **I haven't gotten a developer job**\n  * **Approx. 5 %** respondents said : **Less than a month 4 to 6 month**\n  * **Approx. 4 %** respondents said : **6 month to a year**\n  * **Approx. 3 %** respondents said : **Longer than a year**
## 5.44.1 Salary  of Data Scientist / Machine Learning Specialists
"* Most of the Data Scientist / Machine Learning Specialist getting salary **\$ 0 to \$2,50,000 **"
## 5.44.5 Most popular languages (Data Scientist / Machine Learning Specialists)
"* Most popular languages in which Data Scientist / Machine Learning Specialist are working or want to work is **Python** followed by **R**, **SQL**, **Bash/Shell**.\n* **C++** and **Scala** are languages which are growing where Data Scientist / Machine Learning Specialist want to work."
## 5.44.6 Most popular Databases (Data Scientist / Machine Learning Specialists)
"* Top most  Databases in which Data Scientist / Machine Learning Specialist are working **MySQL** followed by **PostgreSQL**, **SQLServer**, **MongoDB**, **Apache Hive** etc.\n* Top most  Databases in which Data Scientist / Machine Learning Specialist want to work **PostgreSQL** followed **MongoDB**, **MySQL**, **Elasticsearch**, **Apache Hive** etc."
## 5.44.7 Most popular platforms (Data Scientist / Machine Learning Specialists)
* Most popular  Platforms on which Data Scientist / Machine Learning Specialist are working or want to wok is **Linux** followed by **AWS**. **Raspberry** Pie is the Platforn after these on which most  Databases in which Data Scientist / Machine Learning Specialist want to work.
## 5.44.8 Most popular Frameworks (Data Scientist / Machine Learning Specialists)
"* **Most popular Framworks** on which Data Scientist / Machine Learning Specialists are working or want to work is **TensorFlow** followed by **Spark**, **Torch/PyTorch**, **Hadoop**, **Django** etc."
## 5.44.9 Top most used IDE  by Data Scientist / Machine Learning Specialists)
"* **Top most used IDE**  by Data Scientist / Machine Learning Specialist is **IPython / Jupyter** followed by **RStudio**, **Vim**, **PyCharm**, **Sublime Text**."
## 5.45.1  Most dangerous aspects of increasingly advanced AI technology
"**Most dangerous aspects of increasingly advanced AI technology in decreasing order :**\n  * Approx. **29 %** peoples thinks that **Algorithms making important decisions**.\n  * Approx. **28 %** peoples thinks **Artificial intelligence surpassing human intelligence**.\n  * Aprox. **24 %** peoples thinks **Evolving definations of ""fairness"" in algorithmic V.S. human decisions**.\n  * Approx. **20 %** peoples thinks **Increasing automation of jobs**."
> ## 5.45.2  Most Exciting aspects of increasingly advanced AI technology
"**Most exciting aspects of increasingly advanced AI technology in decreasing order :**\n  * Approx. **41 %** peoples thinks **Increasing automation of jobs**.\n  * Approx. **23 %** peoples thinks that **Algorithms making important decisions**.\n  * Approx. **23 %** peoples thinks **Artificial intelligence surpassing human intelligence**.\n  * Aprox. **12 %** peoples thinks **Evolving definations of ""fairness"" in algorithmic V.S. human decisions**.\n  "
## 5.45.3 Whose responsibility is it to consider the ramifications of increasingly advanced AI technology?
**Whose responsibility is it to consider the ramifications of increasingly advanced AI technology :**\n  * Approx. **48 %** peoples said that **The developers or the people creating the AI**.\n  * Approx. **28 %** peoples said that **A governmental or other regulatory body**.\n  * Approx. **17 %** peoples said that **Prominent industry leaders**.\n  * Approx. **8 %** peoples said that **Nobody**.
## 5.45.4 What's peoples take on the future of artificial intelligence ?
"**Peoples take on the future of artificial intelligence :**\n  * Approx.** 73 %** peoples said that **I'm excited about the possibilities more than worried about the dangers**.\n  * Approx. **19 %** peoples said that **I'm worried about the dangers more than I'm excited about the possibilities**.\n  * Approx. **8 %** peoples said that **I don't care about it, or I have't thought about it**."
## Import The Necessary Libraries & Define Data Access Variables 
## Create Test Image Batches
In the dataset we have both categorical and numerical columns. Let's look at the values of categorical columns first.
### Numerical columns exploration
Now let's look at the numerical columns' values. The most convenient way to look at the numerical values is plotting histograms.
"We can see that numerical columns have outliers (especially 'pdays', 'campaign' and 'previous' columns). Possibly there are incorrect values (noisy data), so we should look closer at the data and decide how do we manage the noise.\n Let's look closer at the values of 'campaign', 'pdays' and 'previous' columns:"
"It is very important to look at the response column, which holds the information, which we are going to predict. In our case we should look at 'deposit' column and compare its values to other columns. \n First of all we should look at the number of 'yes' and 'no' values in the response column 'deposit'."
"On the diagram we see that counts for 'yes' and 'no' values for 'deposit' are close, so we can use accuracy as a metric for a model, which predicts the campaign outcome."
Get the feature importances from the trained model:
"As we can see from the diagram showing feature importances, the most important features are:\n* Customer's account balance,\n* Customer's age,\n* Number of contacts performed during this campaign and contact duration,\n* Number of contacts performed before this campaign."
> Let's create a kernel density estimation (KDE) plot colored by the value of the target. A kernel density estimation (KDE) is a non-parametric way to estimate the probability density function of a random variable. It will allow us to identify if there is a correlation between the Age of the Client and their ability to pay it back.
#### Education
> Let's plot a heatmap to visualize the correlation between Attrition and these factors.
"> As shown above, ""Monthly Rate"", ""Number of Companies Worked"" and ""Distance From Home"" are positively correlated to Attrition;  while ""Total Working Years"", ""Job Level"", and ""Years In Current Role"" are negatively correlated to Attrition."
"> **Classification Accuracy** is the number of correct predictions made as a ratio of all predictions made.  \nIt is the most common evaluation metric for classification problems. However, it is often **misused** as it is only really suitable when there are an **equal number of observations in each class** and all predictions and prediction errors are equally important. It is not the case in this project, so a different scoring metric may be more suitable."
> **Area under ROC Curve** (or AUC for short) is a performance metric for binary classification problems. \nThe AUC represents a **modelâ€™s ability to discriminate between positive and negative classes**. An area of 1.0 represents a model that made all predictions perfectly. An area of 0.5 represents a model as good as random.
> **Area under ROC Curve** (or AUC for short) is a performance metric for binary classification problems. \nThe AUC represents a **modelâ€™s ability to discriminate between positive and negative classes**. An area of 1.0 represents a model that made all predictions perfectly. An area of 0.5 represents a model as good as random.
"> Based on our ROC AUC comparison analysis, **Logistic Regression** and **Random Forest** show the highest mean AUC scores. We will shortlist these two algorithms for further analysis. See below for more details on these two algos."
"> Random Forest allows us to know which features are of the most importance in predicting the target feature (""attrition"" in this project). Below, we plot features by their importance."
> Random Forest helped us identify the Top 10 most important indicators (ranked in the table below).
#### Evaluation
> The Confusion matrix provides us with a much more detailed representation of the accuracy score and of what's going on with our labels - we know exactly which/how labels were correctly and incorrectly predicted
> AUC - ROC curve is a performance measurement for classification problem at various thresholds settings. ROC is a probability curve and AUC represents degree or measure of separability. It tells how much model is capable of distinguishing between classes. The green line represents the ROC curve of a purely random classifier; a good classifier stays as far away from that line as possible (toward the top-left corner). 
"> As shown above, the fine-tuned Logistic Regression model showed a higher AUC score compared to the Random Forest Classifier. "
"In the first article, we demonstrated how polynomial features allow linear models to build nonlinear separating surfaces. Let's now show this visually.\n\nLet's see how regularization affects the quality of classification on a dataset on microchip testing from Andrew Ng's course on machine learning. We will use logistic regression with polynomial features and vary the regularization parameter $C$. First, we will see how regularization affects the separating border of the classifier and intuitively recognize under- and overfitting. Then, we will choose the regularization parameter to be numerically close to the optimal value via (`cross-validation`) and (`GridSearch`)."
"Let's load the data using `read_csv` from the `pandas` library. In this dataset on 118 microchips (objects), there are results for two tests of quality control (two numerical variables) and information whether the microchip went into production. Variables are already centered, meaning that the column values have had their own mean values subtracted. Thus, the ""average"" microchip corresponds to a zero value in the test results.  "
"As an intermediate step, we can plot the data. Orange points correspond to defective chips, blue to normal ones."
Let's define a function to display the separating curve of the classifier.
Let's define a function to display the separating curve of the classifier.
"We define the following polynomial features of degree $d$ for two variables $x_1$ and $x_2$:\n\n$$\large \{x_1^d, x_1^{d-1}x_2, \ldots x_2^d\} =  \{x_1^ix_2^j\}_{i+j=d, i,j \in \mathbb{N}}$$\n\nFor example, for $d=3$, this will be the following features:\n\n$$\large 1, x_1, x_2,  x_1^2, x_1x_2, x_2^2, x_1^3, x_1^2x_2, x_1x_2^2, x_2^3$$\n\nDrawing a Pythagorean Triangle would show how many of these features there will be for $d=4,5...$ and so on.\nThe number of such features is exponentially large, and it can be costly to build polynomial features of large degree (e.g $d=10$) for 100 variables. More importantly, it's not needed. \n"
Let's train logistic regression with regularization parameter $C = 10^{-2}$.
"We could now try increasing $C$ to 1. In doing this, we weaken regularization, and the solution can now have greater values (in absolute value) of model weights than previously. Now the accuracy of the classifier on the training set improves to 0.831."
"We could now try increasing $C$ to 1. In doing this, we weaken regularization, and the solution can now have greater values (in absolute value) of model weights than previously. Now the accuracy of the classifier on the training set improves to 0.831."
"Then, why don't we increase $C$ even more - up to 10,000? Now, regularization is clearly not strong enough, and we see overfitting. Note that, with $C$=1 and a ""smooth"" boundary, the share of correct answers on the training set is not much lower than here. But one can easily imagine how our second model will work much better on new data."
"Then, why don't we increase $C$ even more - up to 10,000? Now, regularization is clearly not strong enough, and we see overfitting. Note that, with $C$=1 and a ""smooth"" boundary, the share of correct answers on the training set is not much lower than here. But one can easily imagine how our second model will work much better on new data."
"To discuss the results, let's rewrite the function that is optimized in logistic regression with the form:\n\n$$\large J(X,y,w) = \mathcal{L} + \frac{1}{C}||w||^2,$$\n\nwhere\n\n- $\mathcal{L}$ is the logistic loss function summed over the entire dataset\n- $C$ is the reverse regularization coefficient (the very same $C$ from `sklearn`'s implementation of `LogisticRegression`)"
"To see how the quality of the model (percentage of correct responses on the training and validation sets) varies with the hyperparameter $C$, we can plot the graph. "
"Finally, select the area with the ""best"" values of $C$."
"Finally, select the area with the ""best"" values of $C$."
"Recall that these curves are called validation curves. Previously, we built them manually, but sklearn has special methods to construct these that we will use going forward."
**1-2: Visualization Libraries**
**1-3: Machine Learning Models**
# 4-Data Visualization\n**Numerical variables correlation with survival**
# Numerical variable: SibSp
# Load Libraries 
## GPU 
## Plot 
## Seed All
"## Plot Example \nTo verify the concept \nLet's plot \n* The original TS (take the length of the sliding window + the label size - 90 days + 28 days)\n* The first array of trainX - which is the first sequence\n* The first array of trainy , which is the first label "
"## Pytorch Tensors \nPytorch use tensors as the input to the model \nVariable is a wrapper to the tensor \nThis kernel is only a preliminary starter, \n\nSo I use the Variable wrapper\nA more common way is to train with batches and use the dataset class\nBut this is for later.\n\nIf you want to learn more about Tensors \nRead this tutorial \n\nhttps://pytorch.org/tutorials/beginner/former_torchies/tensor_tutorial.html"
"Since the validation dataset was used to tune hyperparameters (the number of iterations), I predict targets for the testing dataset which the model hasn't seen yet. The metrics reported here are accuracy, precision and recall. They all have sensible values which is also confirmed by the confusion matrix shown below."
"From the confusion matrix it is clear that the model tries to predict both classes and doesn't prefer one over the other due to their imbalance. The latter is a common mistake and if you see accuracy scores without the confusion matrix, be very skeptical about those results. If over-/undersampling isn't applied for imbalanced classes, the classifier will opt for the constant prediction in favor of the majority class. In this case the accuracy for this testing dataset will be quite high 0.909 (see below), although this classifier doesn't have any predictive power! This skewness in class predictions is very visible on the confusion matrix."
"From the confusion matrix it is clear that the model tries to predict both classes and doesn't prefer one over the other due to their imbalance. The latter is a common mistake and if you see accuracy scores without the confusion matrix, be very skeptical about those results. If over-/undersampling isn't applied for imbalanced classes, the classifier will opt for the constant prediction in favor of the majority class. In this case the accuracy for this testing dataset will be quite high 0.909 (see below), although this classifier doesn't have any predictive power! This skewness in class predictions is very visible on the confusion matrix."
"# 4. Feature importances\n\nDuring the early draft of this project the analysis of feature importances helped me to realize that the [Lending Club dataset provided by Wendy Kan from Kaggle](https://www.kaggle.com/wendykan/lending-club-loan-data) was actually including features that aren't available for investors. So I decided to include this analysis here in case someone finds it useful.\n\nAmong all features I selected 10 with the largest importance values (see below). The top 3 features are `loan_amnt`, `mths_since_recent_inq` and `revol_util`. The importances of `mths_since_recent_inq` and `revol_util`, however, are quite close to each other and the rest of the features so this ranking might slightly change for a different train-test split."
"# 4. Feature importances\n\nDuring the early draft of this project the analysis of feature importances helped me to realize that the [Lending Club dataset provided by Wendy Kan from Kaggle](https://www.kaggle.com/wendykan/lending-club-loan-data) was actually including features that aren't available for investors. So I decided to include this analysis here in case someone finds it useful.\n\nAmong all features I selected 10 with the largest importance values (see below). The top 3 features are `loan_amnt`, `mths_since_recent_inq` and `revol_util`. The importances of `mths_since_recent_inq` and `revol_util`, however, are quite close to each other and the rest of the features so this ranking might slightly change for a different train-test split."
In gradient boosting the importances of highly correlated features usually split between them. From the correlation heatmap (see below) the feature `revol_util` (top 3) is quite highly correlated with `bc_util` (top 5) which leads to the decreased importance of `revol_util`.
In gradient boosting the importances of highly correlated features usually split between them. From the correlation heatmap (see below) the feature `revol_util` (top 3) is quite highly correlated with `bc_util` (top 5) which leads to the decreased importance of `revol_util`.
It is also useful to look at the distributions of the features to see how their values influence predictions.\n\nFrom the histogram for the feature `loan_amnt` (top 1) the loan is more likely to be returned (good loans) if the loan amount is lower. This makes sense because smaller loan amounts usually have smaller monthly installments that are easier to pay.
It is also useful to look at the distributions of the features to see how their values influence predictions.\n\nFrom the histogram for the feature `loan_amnt` (top 1) the loan is more likely to be returned (good loans) if the loan amount is lower. This makes sense because smaller loan amounts usually have smaller monthly installments that are easier to pay.
"From the histogram for the feature `mths_since_recent_inq` (top 2) the loan is less likely to be returned (bad loans) if the borrower had an inquiry recently. This also makes sense because inquiries are usually done when someone applies for a loan, a credit card, etc. so recent inquiries could indicate bad financial stability of the borrower."
"From the histogram for the feature `mths_since_recent_inq` (top 2) the loan is less likely to be returned (bad loans) if the borrower had an inquiry recently. This also makes sense because inquiries are usually done when someone applies for a loan, a credit card, etc. so recent inquiries could indicate bad financial stability of the borrower."
"From the histogram for the feature `revol_util` (top 3) the loan is less likely to be returned (bad loans) if the revolving utilization is lower. This actually doesn't make much sense because revolving utilization is the percentage of the used credit on your credit card so higher revolving utilization indicates worse financial stability. Nevertheless, this dataset shows otherwise and it could be an interesting topic for discussion."
"From the histogram for the feature `revol_util` (top 3) the loan is less likely to be returned (bad loans) if the revolving utilization is lower. This actually doesn't make much sense because revolving utilization is the percentage of the used credit on your credit card so higher revolving utilization indicates worse financial stability. Nevertheless, this dataset shows otherwise and it could be an interesting topic for discussion."
"# 5. Model adjustment\n\nThe previously reported model was obtained by minimizing both false positive and false negative errors that contribute to precision and recall respectively. In reality, however, one of these errors might have a larger impact so it would be better to optimize for it instead. In case of loan investing, the false positive errors are the number of bad loans that were identified as good so the investor will loose money by investing in them. This is a direct loss and should be avoided. The false negative errors are the number of good loans that were identified as bad so the investor will not earn extra money by not investing in them. This is a missed opportunity and is less critical compared to the direct loss. Therefore, the false positive errors should be decreased (higher precision) even if the false negative errors will be increased (lower recall). The connection between precision and recall can be visualized using the precision-recall curve (see below). To calculate it, one requires probabilities of belonging to class 1 rather than the predicted labels. This precision-recall curve is calculated for the validation dataset because adjusting precision or recall is similar to adjusting hyperparameters. For each precision-recall pair the function `precision_recall_curve()` also returns the corresponding probability threshold. This threshold is the actual hyperparameter that will be used to obtain the best precision."
"# 5. Model adjustment\n\nThe previously reported model was obtained by minimizing both false positive and false negative errors that contribute to precision and recall respectively. In reality, however, one of these errors might have a larger impact so it would be better to optimize for it instead. In case of loan investing, the false positive errors are the number of bad loans that were identified as good so the investor will loose money by investing in them. This is a direct loss and should be avoided. The false negative errors are the number of good loans that were identified as bad so the investor will not earn extra money by not investing in them. This is a missed opportunity and is less critical compared to the direct loss. Therefore, the false positive errors should be decreased (higher precision) even if the false negative errors will be increased (lower recall). The connection between precision and recall can be visualized using the precision-recall curve (see below). To calculate it, one requires probabilities of belonging to class 1 rather than the predicted labels. This precision-recall curve is calculated for the validation dataset because adjusting precision or recall is similar to adjusting hyperparameters. For each precision-recall pair the function `precision_recall_curve()` also returns the corresponding probability threshold. This threshold is the actual hyperparameter that will be used to obtain the best precision."
"From the precision-recall curve the best precision is 1 but then the recall would be extremely low so in the end the model might not predict good loans at all. Therefore, I exclude 1 from the precision array and find its maximum. The threshold array `t` returned by `precision_recall_curve()` is missing the threshold 0 in the beginning so I add it to match the dimension of the precision array `p`. Then I find the threshold that correspond to the maximum precision and recalculate the predicted labels. The obtained precision score for the adjusted labels is indeed the maximum (excluding 1) as can be seen from the precision-recall curve."
"Finally by using the adjusted threshold on the testing dataset, the adjusted precision 0.939 is indeed within the above 95% confidence interval. Note, however, that the recall is significantly decreased from 0.632 to 0.068 but the precision only increased from 0.931 to 0.939. Of course the gain in precision depends on the train-test split and for a different testing dataset can be closer to the right boundary of the confidence interval. Getting higher values than that, however, is unlikely."
Thanks for **UPVOTING** this kernel! Trying to become a Kernels Master. ðŸ¤˜\n\nCheck out my other cool projects:\n- [ðŸ“Š Interactive Titanic dashboard using Bokeh](https://www.kaggle.com/pavlofesenko/interactive-titanic-dashboard-using-bokeh)\n- [ðŸŒ Extending Titanic dataset using Wikipedia](https://www.kaggle.com/pavlofesenko/extending-titanic-dataset-using-wikipedia)\n- [ðŸ‘ª Titanic extended dataset (Kaggle + Wikipedia)](https://www.kaggle.com/pavlofesenko/titanic-extended)
We can visualize our models feature space to analyze class seperation. 
"We can use of learned masks to determine local and global feature importances or attributions for the model. I don't this the local attributions follow the same sensitivity and interprettations of other model agnostic or tree-based methods but are, in previous experiment, similar to those of common Boosting approaches. "
"We can use of learned masks to determine local and global feature importances or attributions for the model. I don't this the local attributions follow the same sensitivity and interprettations of other model agnostic or tree-based methods but are, in previous experiment, similar to those of common Boosting approaches. "
"## Unsupervised Pretraining\nThe TabNet authors see their approach particularly valuable in unsupervised or self-supervised learning applications where models can be pre-trained across large amounts of unlabelled data and then fine-tuned on labelled examples. To allow for such as approach, they define a decoder architures which takes in the encoders feature space of the encoder model and passes this input through a number of step of Feature Tansformer Blocks and Dense Layers. This decoder then returns the original feature input of the encoder model as output for use in training. "
We can visual the latent space described by the model for used in unsupervised applciations. 
"Unlike many unsupervised autoencoder model, we get some kind of feature importances to the model without having to rely on model agnostic explainations or gradient-based explainations. "
"Unlike many unsupervised autoencoder model, we get some kind of feature importances to the model without having to rely on model agnostic explainations or gradient-based explainations. "
We will be using this pretrained layer now for use in our next fine-tuning experiment. 
# Import Libraries
# Define Functions
# Define Functions
# Data Understanding
\n\n## 2| IMPORT NECESSARY LIBRARIES ðŸŽ¬
###  Some colors used in this notebook were selected from the following color collection: 
###  Some colors used in this notebook were selected from the following color collection: 
\n\n## 3|LOAD DATASET ðŸ“¥
\n\n#### 5.1 | Histplot
"\n\nDistribution  \n\nAnalyzing the graphs here, it turns out that the values of the variable 'fixed_acidity' are relatively normally distributed (but a bit left skewed). But there are two peaks in the distributions of other 'volatile_acidity' and 'citric_acid' variables."
"\n\nDistribution  \n\nAnalyzing the graphs here, it turns out that the values of the variable 'fixed_acidity' are relatively normally distributed (but a bit left skewed). But there are two peaks in the distributions of other 'volatile_acidity' and 'citric_acid' variables."
"\n\nDistribution  \n\nAnalyzing the graphs here, it turns out that the distributions of these variables are not normal"
\n\n#### 5.2 |Pairplot
\n\nMulticollinearity detected !  \n    \nWe see here that there is correlation between some variables. And this is what we don't want. This problem is called 'multicollinearity'
As it can be seen there is not correlation between 'residual_sugar' and 'pH' variables
As it can be seen there is not correlation between 'alcohol' and 'pH' variables
As it can be seen there is not correlation between 'alcohol' and 'pH' variables
"### What do we see when we observe the histograms above? ðŸ¤”\n\nHere we see the distributions of values of all variables. As it can be seen from the charts, the values of 'pH' and ""density"" variables are relatively normally distributed.\n\n1. Most of the values â€‹â€‹of  the ""fixed_acidity"" variable are in the range of 7 - 8;\n\n2. Most of the values â€‹â€‹of  the ""volatile_acidity"" variable are in the range of 0.4 - 0.7;\n\n3. Most values â€‹â€‹of the ""citric_acid"" variable are in the range of 0.0 - 0.1;\n\n4. Most of the values â€‹â€‹of  the ""residual_sugar"" variable are in the range of 1 - 2.5;\n\n5. Most of the values â€‹â€‹of the ""chlorides"" variable are in the range of 0.085 - 0.15;\n\n6. Most values â€‹â€‹of the ""free_sulfur_dioxide"" variable are in the range 0 - 15;\n\n7. Most values â€‹â€‹of the ""total_sulfur_dioxide"" variable are in the range 0 - 30;\n\n8. Most of the values â€‹â€‹of the ""density"" variable are in the range of 0.996 - 0.998;\n\n9. Most of the values â€‹â€‹of the ""pH"" variable are in the range of 3.2 - 3.4;\n\n10. Most of the values â€‹â€‹of the ""sulphates"" variable are in the range of 0.50 - 0.75;\n\n11. Most of the values â€‹â€‹of the ""alcohol"" variable are in the range of 9 - 10;\n\n12. Most values â€‹â€‹of the ""quality"" variable are 5 and 6."
\n\n#### 5.5 |Regplot
"**Attention** to the negative linear relationships between the ""alcohol"" - ""volatile acidity"" and ""alcohol"" - ""total_sulfur_dioxide"" variables."
"\n\n#### 5.6 |Hexagonal Binned Plot\n\n\nhexbin is a 2D histogram plot, in which the bins are hexagons and the color represents the number of data points within each bin."
\n\n#### 5.7 |Visualization with Plotly Express
"### Correlation and Causation ðŸ“\n\nReference: https://www.abs.gov.au/websitedbs/D3310114.nsf/home/statistical+language+-+correlation+and+causation\n\nTwo or more variables considered to be related, in a statistical context, if their values change so that as the value of one variable increases or decreases so does the value of the other variable (although it may be in the opposite direction). For example, for the two variables ""hours worked"" and ""income earned"" there is a relationship between the two if the increase in hours worked is associated with an increase in income earned. If we consider the two variables ""price"" and ""purchasing power"", as the price of goods increases a person's ability to buy these goods decreases (assuming a constant income).\n\nCorrelation is a statistical measure (expressed as a number) that describes the size and direction of a relationship between two or more variables. A correlation between variables, however, does not automatically mean that the change in one variable is the cause of the change in the values of the other variable.\n\nCausation indicates that one event is the result of the occurrence of the other event; i.e. there is a causal relationship between the two events. This is also referred to as cause and effect.\n\nTheoretically, the difference between the two types of relationships are easy to identify â€” an action or occurrence can cause another (e.g. smoking causes an increase in the risk of developing lung cancer), or it can correlate with another (e.g. smoking is correlated with alcoholism, but it does not cause alcoholism). In practice, however, it remains difficult to clearly establish cause and effect, compared with establishing correlation. "
"\n\n Correlation\n\n\n\n**IMPORTANT NOTE!** There is 'multicollinearity' problem\n\nHere we see that there is relatively high (0.67, positive) correlation \nbetween 'free sulfur dioxide' and 'total_sulfur_dioxide' variables.\nThere is relatively high (-0.68, negative) correlation between \n""pH"" and ""fixed_acidity"" variables. And there is about 0.5 correlation\nbetween some of other variables. That's why we must consider when build \nMachine Learning models."
\n\n#### 7.19 |ROC AUC - Light GBM Model ðŸ“‰
# Thank you very much ðŸ™‚
------------------------------------------------\n## Parch ( Number of parents/children )
----------------------------------------------------------------------\n## SibSp ( Number of siblings/spouses )
----------------------------------------------------------------------\n## SibSp ( Number of siblings/spouses )
-------------------------------------------------\n## FamilySize ( Derived variable )
 Observation:\n\nThe mortality rate is higher in the case of Mr. I think it will help with learning.
" Observation:\n* In the case of Mr, the number of survivors is small.\n* In the case of Mrs and Miss, there are many survivors.\n\nI think it will be helpful in judging survivors using this.\nHowever, it seems difficult to find the relationship between age and title from the above distributions. Therefore, it seems difficult to use this to fill in the missing values of age."
---------------------------------------\n## Embarked
" Observation:\n    \n* Many passengers on board at S port died.\n* For passengers boarding at port C, the survival rate is higher than the mortality rate."
" Observation:\n    \n* Many passengers on board at S port died.\n* For passengers boarding at port C, the survival rate is higher than the mortality rate."
" Observation:\n    \n* Among the passengers who boarded at S port, the proportion of males is higher than that of other ports."
---------------------------------------\n# Checking Correlation
" Observation and Decision:\n    \n* There is a large correlation between FamilySize and SibSp and Parch. Since the derived variable FamilySize is made of SibSp and Parch, SibSp and Parch are removed.\n* The relationship between Cabin and Has_Cabin is high. Therefore, the derived variable Has_Cabin is left and Cabin is removed.\n* The relationship between Fare and Fare_class is high. Fare is selected because skewness is removed by nonlinear transform of the Fare feature.\n* There are many features that are not related to the survived value."
" Observation and Decision:\n    \n* There is a large correlation between FamilySize and SibSp and Parch. Since the derived variable FamilySize is made of SibSp and Parch, SibSp and Parch are removed.\n* The relationship between Cabin and Has_Cabin is high. Therefore, the derived variable Has_Cabin is left and Cabin is removed.\n* The relationship between Fare and Fare_class is high. Fare is selected because skewness is removed by nonlinear transform of the Fare feature.\n* There are many features that are not related to the survived value."
"-------------------------------------------------------------\n# Selecting Features\n\nFeatures that are not helpful in judging the above heatmap and survivors, or that have other derived variables, will be removed."
**Let's check the correlation of each feature.**
Let's check the correlation between the target value (Suvived) and other features.
"Compared to the soft blending model, the boundary does not look clean."
-------------------------------------------------------------------------------------------------\n## Calibrating the final model\n\n> This function calibrates the probability of a given estimator using isotonic or logistic regression. \n
## Utility functions
"Get mean, sum and variance of image features over all 30 images."
Credits to Leonardo's Kernel : \nhttps://www.kaggle.com/kabure/extensive-eda-and-modeling-xgb-hyperopt
Ploting Transaction Amount Values Distribution
Ploting Transaction Amount Values Distribution
The Product Feature
The Product Feature
"Visualizing Card 1, Card 2 and Card 3 Distributions"
Card 4 - Categorical Feature
Card 6 - Categorical
Card 6 - Categorical
Exploring M1-M9 Features
Transactions and Total Amount per day
Fraud Transactions by Date
Fraud Transactions by Date
# Feature Engineering
## ðŸ‘¨â€âš•ï¸ Main findings of descriptive analysis 
 There are no missing values in this dataset. 
\nImporting Necessary Libraries
\nTraining Dataset
\n   \n# Q1 Age Distribution\n\nAt first Let's plot the distribution of developers age to see if there is special relation between age and Developing data science
*Data science is mostly done by the young developers belowe 35 years old*
"\n    \n    \n# Q1 , Q2 Age and Gender\n\nNow let's plot the distribution of Developers for different Age and Gender\n"
*In general Men are leading the Data science field especially those of age 20s*
\n    \n    \n# **Q3 Country of residence**\n\nLet's see the developers distribution across the countries 
*It's obvious how developers in india are the most with more than double of those in usa*
\n    \n    \n# **Q4 Education Background**
*Most of Data scientists have Master's and Bachelor's degrees*
\n\n    \n    \n# **Q5 Current Role Title**
"*Here we can notice that Data science is an ongoing educational process and that's why most of data scientists call them sleves students, so we shouldn't ever stop learning and exploring new subjects.*"
\n\n    \n    \n# **Q6 Experience Years**
"*Well about 70% of users have experience lower than 5 years, so it is never too late to start a data science career*"
"Data science can be applied using more than 10 langauges and each langauge has different corresponding tools like libraries and IDEs, so let's analyze the Programming languages used and their relation to different variables as gender, age, role title and education background"
Here we will analyze the prgramming languages used for each gender
Here we will analyze the prgramming languages used for each gender
"*Whatever your prefered programming language you will be able to code in data science with more than 10 languages to start with. But still Python, SQL,and R languages are the black horses in the field*"
\n\n    \n    \n# Q6 & Q7 Languages used for each range of age
"*It seems that Python and SQL are the most famous languages for most age ranges except for ages 18-22 where Python, C and C++ are the most common languages.*"
\n\n    \n    \n# Q8 Recommended First Programming language
*when asked about Recommended Programming language to learn first most people recommended Python as it is easy to learn and has alot of support and libraries*
\n\n    \n    \n# **Q9 Favourite IDEs**
"*While there are many Integrated Development environments, Jupyter is the most favored one.*****"
\n\n    \n    \n# **Q10 Most hosted notebooks**
"*As there are many developers prefer to use IDE's, there are others who prefer to code on Hosted Notebooks especially Google Colab and Kaggle Notebooks*"
"\n\n    \n    \n# Q24 Current yearly compensation\nAnnual compensation, in the simplest terms, is the combination of your base salary and the value of any financial benefits your employer provides ex: Annual bonuses or commissions, insurance and so on."
*It makes sense that most of beginners take lower compensations while other compensation rates varies much.*
\n\n    \n    \n# Q15 Years of using Machine learning methods
*As in the graph most Machine learning method users just started using them in the last 2 years.*
\n\n    \n    \n# Q16 Machine Learning Frameworks used
"*Scikit-learn, TensorFlow libraries and Keras are used the most while JAX and MXNet are the fewest*"
\n    \n    \n# Q17 Machine learning Algorithms used
*Ofcourse Linear and Logistic regressions are on the top followed by Decision Trees and Random Forests*
Check the class balance:
Separate the input variable names by excluding the target:
"**In statistics, linear regression is a linear approach to modeling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables). The case of one explanatory variable is called simple linear regression.**"
# To learn about more complex regression:\n\n1. Regression Neural Network: https://www.kaggle.com/milan400/regression-neural-network\n\n![image.png](attachment:image.png)
"**k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition and observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells. It is popular for cluster analysis in data mining. k-means clustering minimizes within-cluster variances (squared Euclidean distances), but not regular Euclidean distances, which would be the more difficult Weber problem: the mean optimizes squared errors, whereas only the geometric median minimizes Euclidean distances. For instance, Better Euclidean solutions can be found using k-medians and k-medoids. **"
# 2. PCA
"**PCA is defined as an orthogonal linear transformation that transforms the data to a new coordinate system such that the greatest variance by some scalar projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on.**"
# 3. Apriori Algorithm
"### Detailed and Full Solution (Step by Step)\n\nHello kagglers ..\n\nThis notebook designed to be as detailed as possible solution for the Houses pricing problem, I tried to make it typical, clear, tidy and **beginner-friendly**.\n\nIf you find this notebook useful press the **UPVOTE** button, This helps me a lot ^-^.  \nI hope you find it helpful.\n\n\n\n\n\n\n\n\n"
\n        \n            1 ) Importing the data:\n        \n
**1 ) OverallQual:**
**2 ) LotFrontage:**
**2 ) LotFrontage:**
**3 ) GrLivArea:**
**3 ) GrLivArea:**
seems that i deleted them before !!
**4 ) GarageArea:**
seems that i deleted them before !!
**5 ) LotArea:**
**6 ) Year Built:**
**6 ) Year Built:**
**7 ) TotalBsmtSF:**
**7 ) TotalBsmtSF:**
seems that i deleted them before !!
**8 ) 1stFlrSF :**
\n        \n            Fix Features Skewness:\n        \n\n
"### Regular differentiation  \n\nSometimes it may be necessary to difference the data a second time to obtain a stationary time series, which is referred to as second order differencing."
The p-value indicates that series is stationary as the computed p-value is lower than the significance level alpha = 0.05. 
### Autocorrelation  \n\nAutocorrelation is the correlation of a time series with the same time series lagged. It summarizes the strength of a relationship with an observation in a time series with observations at prior time steps.\n\nWe create autocorrelation factor (ACF) and partial autocorrelation factor (PACF) plots to identify patterns in the above data which is stationary on both mean and variance. The idea is to identify presence of AR and MA components in the residuals. 
There is a positive correlation with the first 10 lags that is perhaps significant for the first 2-3 lags.\n\nA good starting point for the AR parameter of the model may be 3.\n\nLets try out autocorrelation on the differences...
There is a positive correlation with the first 10 lags that is perhaps significant for the first 2-3 lags.\n\nA good starting point for the AR parameter of the model may be 3.\n\nLets try out autocorrelation on the differences...
There are not many spikes in the plots outside the insignificant zone (shaded) so there may not be enough information available in the residuals to be extracted by AR and MA models. \n\nThere may be a seasonal component available in the residuals at the lags of quarters (3 months) represented by spikes at these intervals. But probably not significant.
## Prediction  
"## Validation \n\nA simple indicator of how accurate out forecast is is the root mean square error (RMSE). So lets get a baseline and then calcualte the RMSE for the one-step ahead predictions starting from 2015, through to the end of 2017.\n\n### Baseline \n\nThe naive forecast, where the observation from the previous time step is used as the prediction for the observation at the next time step, can be taken as a simple baseline."
"A better representation of longer-term predictive power can be obtained using dynamic forecasts. In this case, we only use information from the time series up to a certain point, and after that, forecasts are generated using values from previous forecasted time points.\n\nIn the code chunk below, we specify to start computing the dynamic forecasts and confidence intervals from mid 2017 onwards."
This is pretty bad but not surprising given how the prices started going insane around late 2017. Lets see how well the model does in a different time period when things were a little more normal.
This is pretty bad but not surprising given how the prices started going insane around late 2017. Lets see how well the model does in a different time period when things were a little more normal.
"The results indicate that the model is still a little rough and not something we should use as trading advice, but that was not unexpected due to the extremely volatile nature of cryptocurrencies, especially in the last 6 months.\n\nIt is probably also not such a good idea to try and predict 6 months into the future as we can see how insane even the 80% confidence interval becomes out this far. Maybe sticking to 1 month advance predictitons is more sensible. Or maybe even predicting on a daily basis.\n\nThere are a number of things we could do to potentially improve the model. \n    - Use a different technique to standardise the distribution\n    - Use different differentiation techniques\n    - Discard the time period before cryptocurrencies started taking off\n    - Try modelling per day instead of month, with a smaller forecast window\n    - Use other models or machine learning instead of ARIMA\n"
\n\n\nContents\n\n1 Understanding the data at hand\n\n2 Exploratory Data Analysis\n    \n3 Feature Engineering\n    \n4 Modelling\n          \n\n\n
 \n# 1. Understanding the data at hand
From above statistics it is clear that Women were given more preference than Men while evacuation  
The above graph makes it clear that most of the people were aged between 20-50
The above graph makes it clear that most of the people were aged between 20-50
It is clear from vizualisation that most of the survivors were children and women 
It is clear from vizualisation that most of the survivors were children and women 
Most of the people who died were from Passenger Class 3 irrespective of Gender
The above stats show us survival of each class and its clear the ones in better class had a better chance of survival\n## Power of money
Most of the embarkments were from class : S\n\nLeast embarkments were from class : Q
"# Basic Fashion MNIST Classification from TensorFlow Docs\n## A look at neural network: basic classification using tf.keras\n\nThis notebook trains a neural network model to classify images of clothing, like sneakers and shirts. It's okay if you don't understand all the details, this is a fast-paced overview of a complete TensorFlow program with the details explained as we go.\n\nThis notebook uses [tf.keras](https://www.tensorflow.org/guide/keras), a high-level API to build and train models in TensorFlow."
"## Import the Fashion MNIST dataset\n\nThis guide uses the [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset which contains 70,000 grayscale images in 10 categories. The images show individual articles of clothing at low resolution (28 by 28 pixels), as seen here:\n\n\n  \n    \n  \n  \n    Figure 1. Fashion-MNIST samples (by Zalando, MIT License).Â \n  \n\n\nFashion MNIST is intended as a drop-in replacement for the classic [MNIST](http://yann.lecun.com/exdb/mnist/) datasetâ€”often used as the ""Hello, World"" of machine learning programs for computer vision. The MNIST dataset contains images of handwritten digits (0, 1, 2, etc) in an identical format to the articles of clothing we'll use here.\n\nThis guide uses Fashion MNIST for variety, and because it's a slightly more challenging problem than regular MNIST. Both datasets are relatively small and are used to verify that an algorithm works as expected. They're good starting points to test and debug code. \n\nWe will use 60,000 images to train the network and 10,000 images to evaluate how accurately the network learned to classify images. You can access the Fashion MNIST directly from TensorFlow, just import and load the data:"
"## Preprocess the data\n\nThe data must be preprocessed before training the network. If you inspect the first image in the training set, you will see that the pixel values fall in the range of 0 to 255:"
"We scale these values to a range of 0 to 1 before feeding to the neural network model. For this, we divide the values by 255. It's important that the *training set* and the *testing set* are preprocessed in the same way:"
Display the first 25 images from the *training set* and display the class name below each image. Verify that the data is in the correct format and we're ready to build and train the network.
"## Build the model\n\nBuilding the neural network requires configuring the layers of the model, then compiling the model."
We can graph this to look at the full set of 10 channels
"Let's look at the 0th image, predictions, and prediction array. "
Let's plot several images with their predictions. Correct prediction labels are blue and incorrect prediction labels are red. The number gives the percent (out of 100) for the predicted label. Note that it can be wrong even when very confident. 
"Finally, use the trained model to make a prediction about a single image. "
# Awesome Computer Vision CV Resources
"## Credits (Reference)\n\n> * [TensorFlow Docs - GitHub](https://github.com/tensorflow/docs)\n> * [GitHub Deep Learning Topic](https://github.com/topics/deep-learning)\n> * [Jiwon Kim](https://github.com/kjw0612/awesome-deep-vision)\n> * [Jia-Bin Huang](https://github.com/jbhuang0604/awesome-computer-vision)\n\n## License\n\n[![CC0](http://mirrors.creativecommons.org/presskit/buttons/88x31/svg/cc-zero.svg)](https://creativecommons.org/publicdomain/zero/1.0/)\n\n### Please ***UPVOTE*** my kernel if you like it or wanna fork it.\n\n##### Feedback: If you have any ideas or you want any other content to be added to this curated list, please feel free to make any comments to make it better.\n#### I am open to have your *feedback* for improving this ***kernel***\n###### Hope you enjoyed this kernel!\n\n### Thanks for visiting my *Kernel* and please *UPVOTE* to stay connected and follow up the *further updates!*"
"## What Kagglers think of MOOC?\nKaggle ML & DS survey asked a very interesting question (#39): \n> _How do you perceive the quality of online learning platforms [MOOCs] as compared to the quality of the education provided by traditional brick and mortar institutions?_\n\nTo answer this, a respondent did not have to have ever used a learning platform - only having an opinon was required, though an option of ""No opinion; I do not know"" was also available. In the end, a little over 60% of survey participants shared their attitude on the matter. The distribution of answers tells a different story accross education and country of residence segments.\n\n_Note_: Before diving in, I preprocess the complete dataset in order to simplify analysis and make data visualization look better: convert range values to be ordinal, shorten some long category names, rename columns, and convert strings to numeric where appropriate."
"### Kagglers generally favour online learning...\nOverall, Kagglers seem to have a favourable view of MOOCs such as those found on platforms like Courseara, Udemy, or DataCamp. From close to 16,000 responses to question 39, I've excluded 1,564 that do not have an opinion on the matter. Close to 60% of remaining respondendents believe MOOCs are better than traditional eduction, while 16% think they are worse. The sample is of course biased. After all we are asking people who actively engage with an online community - Kaggle."
"### Kagglers generally favour online learning...\nOverall, Kagglers seem to have a favourable view of MOOCs such as those found on platforms like Courseara, Udemy, or DataCamp. From close to 16,000 responses to question 39, I've excluded 1,564 that do not have an opinion on the matter. Close to 60% of remaining respondendents believe MOOCs are better than traditional eduction, while 16% think they are worse. The sample is of course biased. After all we are asking people who actively engage with an online community - Kaggle."
"### ... but higher education means higher expectations\nThe sentiment around MOOCs is not uniform accross various levels of higher eduction. Respondents who have attained Bacherlor's degrees are much more enthusiastic (66% rank better, 12% rank worse) about MOOCs than those who put in more years of study to attain a Doctoral degree (48% rank better, 23% rank worse). This makes sense if we consider the advanced classes and rigorous research supervision that is seldom, if ever, found in online learning platforms. MOOC has other shortcommings as well, such as lack of student engagement in class discussion, low completion rates, and lack of feedback [3]. As it stands, online learning can be seen as augmenting education, not replacing it. The milage varies, by country though."
"### ... but higher education means higher expectations\nThe sentiment around MOOCs is not uniform accross various levels of higher eduction. Respondents who have attained Bacherlor's degrees are much more enthusiastic (66% rank better, 12% rank worse) about MOOCs than those who put in more years of study to attain a Doctoral degree (48% rank better, 23% rank worse). This makes sense if we consider the advanced classes and rigorous research supervision that is seldom, if ever, found in online learning platforms. MOOC has other shortcommings as well, such as lack of student engagement in class discussion, low completion rates, and lack of feedback [3]. As it stands, online learning can be seen as augmenting education, not replacing it. The milage varies, by country though."
"### USA disillusioned with MOOC, while India is in love\nLooking at the top three countries, based on number of responses, I noticed something surprising. One would imagine USA, where millionaires who dropped out of college are hailed as role models and where the cost of higher education (and student debt) is evergrowing, would see online courses very favourably. However, American respondents ranked MOOCs worst vs traditional education when compared to other countries. \n\nRespondents from India and China, on the other hand, seem to embrace online learning with open arms. Almost 80% of Indian respondents, for example, see online learning as better than brick and mortal institutions. Since the MOOC content is largely the same between countries, it stands to reason that the difference in opinion is due to quality of traditional education or access to that education. There is evidence to support this, and the MIT Technology Review [4] put it best, I think:\n> In a country [India] of rigid teaching styles and scarce university slots, students and professors are exploring what online learning can be."
"### USA disillusioned with MOOC, while India is in love\nLooking at the top three countries, based on number of responses, I noticed something surprising. One would imagine USA, where millionaires who dropped out of college are hailed as role models and where the cost of higher education (and student debt) is evergrowing, would see online courses very favourably. However, American respondents ranked MOOCs worst vs traditional education when compared to other countries. \n\nRespondents from India and China, on the other hand, seem to embrace online learning with open arms. Almost 80% of Indian respondents, for example, see online learning as better than brick and mortal institutions. Since the MOOC content is largely the same between countries, it stands to reason that the difference in opinion is due to quality of traditional education or access to that education. There is evidence to support this, and the MIT Technology Review [4] put it best, I think:\n> In a country [India] of rigid teaching styles and scarce university slots, students and professors are exploring what online learning can be."
"# Which MOOC platforms are winning?\n\nHere it is useful to define two groups of respondents: one that uses a single MOOC platform (n=5,992) and one that engage with multiple platforms (n=9,679). \n\n### Single-platform learners often choose Coursera as their sole study\nAmong single-platform users, Coursera ranks number one, followed by Kaggle's very own learning resource. Koodos to Kaggle for holding their own in a fairly competitive MOOC environment. It's even more impressive given that Kaggle Learn is an ancillary to the core service offering of the data science competition platform. Of course, the sample in the survey is heavily biased toward Kaggle users. Udemy and DataCamp are not far behind Kaggle Learn when it comes to single-platform learners. The takeaway is these 4 MOOCs have enough appeal for Kagglers to stick with them exclusively and to not go looking for greener pastures... at least for some time. \n\n> _Note_: the ""Other"" category in question 36 responses will be explored separately in the appendix of this Kernel, if you are interested to know what other learning platforms your peers use."
"# Which MOOC platforms are winning?\n\nHere it is useful to define two groups of respondents: one that uses a single MOOC platform (n=5,992) and one that engage with multiple platforms (n=9,679). \n\n### Single-platform learners often choose Coursera as their sole study\nAmong single-platform users, Coursera ranks number one, followed by Kaggle's very own learning resource. Koodos to Kaggle for holding their own in a fairly competitive MOOC environment. It's even more impressive given that Kaggle Learn is an ancillary to the core service offering of the data science competition platform. Of course, the sample in the survey is heavily biased toward Kaggle users. Udemy and DataCamp are not far behind Kaggle Learn when it comes to single-platform learners. The takeaway is these 4 MOOCs have enough appeal for Kagglers to stick with them exclusively and to not go looking for greener pastures... at least for some time. \n\n> _Note_: the ""Other"" category in question 36 responses will be explored separately in the appendix of this Kernel, if you are interested to know what other learning platforms your peers use."
"### ... and so do multi-platform learners\nAmong respondents who use multiple MOOC platforms, Coursera holds the highest mindshare. Coursera is where 3,748 Kagglers spend most of their online learning time. DataCamp and Udemy share second place, with roughly equal number of respondents choosing them. Udacity in fourth place, edX in fifth, while Kaggle Learn dropped all the way to #6 spot. To me this suggests that once a learner discovers other MOOC platforms, Kaggle Learn can rarely compete. Still, an impressive performace for a completely free resource. "
"### ... and so do multi-platform learners\nAmong respondents who use multiple MOOC platforms, Coursera holds the highest mindshare. Coursera is where 3,748 Kagglers spend most of their online learning time. DataCamp and Udemy share second place, with roughly equal number of respondents choosing them. Udacity in fourth place, edX in fifth, while Kaggle Learn dropped all the way to #6 spot. To me this suggests that once a learner discovers other MOOC platforms, Kaggle Learn can rarely compete. Still, an impressive performace for a completely free resource. "
"Does this mean Coursera is three times better than its closest rivals? Not exactly, but we can quantify the comparison better by pitting the MOOCs against one another, one pair at a time. Looking at respondents who use Coursera along with another platform (e.g. DataCamp) and using question 37 (which platform do you use the most?) as a proxy for users preference among platforms, I can see what percentage of people chose Coursera over each of the other platforms in top 5. For example, 72% of respondents who have used both Coursera and Udacity (and other platforms potentially), spent more time engaging with the former. Across its 4 closest competitors, Coursera does not dicisevely dominate against either, with DataCamp doing quite well in terms of winning Data Scientists' mindshare."
"Does this mean Coursera is three times better than its closest rivals? Not exactly, but we can quantify the comparison better by pitting the MOOCs against one another, one pair at a time. Looking at respondents who use Coursera along with another platform (e.g. DataCamp) and using question 37 (which platform do you use the most?) as a proxy for users preference among platforms, I can see what percentage of people chose Coursera over each of the other platforms in top 5. For example, 72% of respondents who have used both Coursera and Udacity (and other platforms potentially), spent more time engaging with the former. Across its 4 closest competitors, Coursera does not dicisevely dominate against either, with DataCamp doing quite well in terms of winning Data Scientists' mindshare."
"Second most popular MOOC platform, DataCamp, is on much shakier ground compared to its competition. Less than 60% of respondents choose it over edX and Udacity, and slightly over 60% when compared to Udemy. Kaggle's very own learning platform, Kaggle Learn, is equally preferred to DataCamp, which is not something that's apparent from the simple count plot above."
"Second most popular MOOC platform, DataCamp, is on much shakier ground compared to its competition. Less than 60% of respondents choose it over edX and Udacity, and slightly over 60% when compared to Udemy. Kaggle's very own learning platform, Kaggle Learn, is equally preferred to DataCamp, which is not something that's apparent from the simple count plot above."
"## What's next for MOOCs?\nMOOCs have been around long enough to become established enterprises. In 2018, Forbes estimates that Coursera's revenue alone is close to $140 millionn USD[5]. With the company rumoured to go public, we will see an increased drive for growth from Coursera and its competitors [6]. This growth will be fueled by evergrowing demand in the emerging markets such as India and China, where access to higher education is more limited, as the demand for data science professionals steadily grows. While MOOCs are fighting for Kaggler's mindshare, we should ponder why we do online learning: is it to get an extra notch on our professional belt or is it for the sake of self-development? Answer to that question will help you decide if engaging with any of these learning platforms makes sense for you.\n\nFor myself, I go to MOOCs for one of two reasons: either out of interest in the subject area or to do a crash course in a technology for the purpose of passing a standardized certification exam. While I'm learning the subject I don't care so much about measurable outcome and while studying for a certification exam, I know the certificate (external to MOOC) is backed by a recognized institution. Online learning works for me in these scenarios, but I'm sure you can find many more uses for it."
"\n## Appendix: Other Learning Platforms\nAside from the learning platforms that were offered as multiple choice question in the survey, the respondents had an option to select ""Other"" and provide their own answer. Looking at these, I realize that I haven't heard about most of them. Of course, the brand names of Linkedin, YouTube, and Codecademy I'm familiar with, but stepik, pluralsight, nptel - this is the first I've heard of these. Even as I tried to Google the resources there were some curious results. For exapmle, with [edureka](www.edureka.co) the second search result on Google was ""Do you mean Udemy?"". It was a clever ad by the more famous MOOC platform. When I went to see what [cognitive class ai](cognitiveclass.ai) is all about, there is an obvious banner ad at the top of the landing page directing me to take a Coursera IBM certification. Finally, [mlcourse.ai](mlcourse.ai) is just an offshoot of [ods.ai](ods.ai), a Russia-based Data Science community.  What's clear is the ""Other"" MOOC platforms are mostly smaller, nieche players that attract only a small fraction of Kagglers to their sites. In the next year's survey I would like Kaggle to include some of the heavyweights in the multiple choice, namely Linkedin and Codecademy, while TheSchool.AI and DataQuest can be left for ""Other"" category to be filled out by those who use them.\n\n_Random fact_: more people spell Code**a**cademy (with an ""a"") rather than the actual name Codecademy. I wonder why they chose to drop the ""a"" in their brand. "
"## References\n[1] [_Udacity U-Turns on Money Back Guarantee_](https://www.insidehighered.com/news/2018/03/16/udacity-ends-pledge-students-get-hired-or-get-their-money-back). Insider Higher Ed. March 16, 2018.\n[2]  [_Will MOOCs be Flukes?_](https://www.newyorker.com/science/maria-konnikova/moocs-failure-solutions). The New Yorker. November 7, 2014.\n[3] [_The Future of Massively Open Online Courses_](https://www.forbes.com/sites/quora/2017/03/23/the-future-of-massively-open-online-courses-moocs/#4f7289046b83). Forbes. March 23, 2017.\n[4] [_India Loves MOOCs_](https://www.technologyreview.com/s/539131/india-loves-moocs/). MIT Technology Review. July 26, 2015. \n[5] [_This Company Could Be Your Next Teacher: Coursera Plots A Massive Future For Online Education_](https://www.forbes.com/sites/susanadams/2018/10/16/this-company-could-be-your-next-teacher-coursera-plots-a-massive-future-for-online-education/#2e348b5e2a39). Forbes. October 16, 2018.\n[6] [_Beware of the Great MOOC Bait-and-Switch_](https://www.forbes.com/sites/dereknewton/2018/11/19/beware-of-the-great-mooc-bait-and-switch/#7f3bce8b12f2). Forbes. November 19, 2018."
# 0. Importing libraries\n\n\n
# 1. Data exploration and pre-processing\n\n\n
 Data Visualization \n\n Scatterplot before and after StandardScaler Standardization between citric acid and total sulfur dioxide 
 Scatterplot before and after MinMaxScaler Standardization between citric acid and total sulfur dioxide 
 Scatterplot before and after MinMaxScaler Standardization between citric acid and total sulfur dioxide 
 Scatterplot before and after RobustScaler Standardization between citric acid and total sulfur dioxide 
 Scatterplot before and after RobustScaler Standardization between citric acid and total sulfur dioxide 
 Distplot before and after StandardScaler Standardization between citric acid and total sulfur dioxide 
 Distplot before and after StandardScaler Standardization between citric acid and total sulfur dioxide 
 Distplot before and after MinMaxdScaler Standardization between citric acid and total sulfur dioxide 
 Distplot before and after MinMaxdScaler Standardization between citric acid and total sulfur dioxide 
 Distplot before and after RobustScaler Standardization between citric acid and total sulfur dioxide 
 Distplot before and after RobustScaler Standardization between citric acid and total sulfur dioxide 
 Model Building 
\n## 2. Import Libraries ðŸ“š 
\n## 3. Read Dataset ðŸ“ 
"_few observation from boxplots,As we can see,SalePrice for fullbath=3 is higher than 0,1, or 2. SalePrice for OverallQal=10 which is very excellent is higher than others._"
"#### _Inference: SalePrice is not normally distributed, it is positively or right skewed_"
`Now SalePrice is normally distributed`
"_As we can see, the multicollinearity still exists in various features. However, we will keep them for now and let the models(e.g. Regularization models such as Lasso, Ridge) do the clean up later on. Let's go through some of the correlations that still exists._\n\n* There is 0.83 or 83% correlation between **GarageYrBlt** and **YearBuilt**. \n* 83% correlation between **TotRmsAbvGrd** and **GrLivArea**. \n* 89% correlation between **GarageCars** and **GarageArea**. \n* Similarly many other features such as**BsmtUnfSF**, **FullBath** have good correlation with other independent feature."
"Firstly, I will explore through 3 different columns:\n- Time\n- Amount\n- Class"
"We have a clearly imbalanced data.\nIt's very common when treating of frauds... \n\nFirst I will do some explore through the Time and Amount. \nSecond I will explore the V's Features, that are PCA's "
### Looking a scatter plot of the Time_min distribuition by Amount
### Looking a scatter plot of the Time_hour distribuition by Amount
### Looking a scatter plot of the Time_hour distribuition by Amount
I will use boxplot to search differents distribuitions: \n- We are searching for features that diverges from normal distribuition
I will use boxplot to search differents distribuitions: \n- We are searching for features that diverges from normal distribuition
"We can see a interesting different distribuition in some of our features like V4, V9, V16, V17 and a lot more.  \nNow let's take a look on time distribuition"
## Feature importance plot
"The top 4 feature are V17, V14, V12, V10 corresponds to 75% of total. \n\nAlso the f2 score that is the median of recall and precision are on a considerably value"
## Precision Recall Curve of Logistic Regression
"# CONCLUSION: \nThe highest values of Normal transactions are 25691.16 while of Fraudulent transactions are just 2125.87. \nThe average value of normal transactions are small(USD 88.29) than fraudulent transactions that is USD 122.21\n\n\nWe got the best score when we use the SMOTE (OverSampling)  + RandomForest, that performed a f2 score of 0.8669~ \n\nThis is a considerably difference by the second best model that is 0.8252 that uses just RandomForests with some Hyper Parameters.\n\nThe worst model was Logreg where I used GridSearchCV to get the Best params to fit and predict where the recall was ~0.6666 and f2 ~0.70.\n\n\n"
> # **Importing Libraries**
## **EDA**
> # **Analysis of Correlation**
> # **Label Encoding**
> # **Confusion Matrix**
> # **ROC AUC**
# Load Data
## Import the Dataset
## Distribution of Amount
"**Highlights**\n\nMost the transaction amount falls between 0 and about 3000 and we have some outliers for really big amount transactions and it may actually make sense to drop those outliers in our analysis if they are just a few points that are very extreme.\n\nMost daily transactions are not extremely expensive, but itâ€™s likely where most fraudulent transactions are occurring as well."
### Distribution of Amount for Fradulent & Genuine transactions
"**Highlights**\n\nThis graph shows that most of the fraud transaction amount is less than 500 dollars. This also shows that the fraud transaction is very high for an amount near to 0, let's find that amount."
### Distribution of Amount w.r.t Class
"**Highlights**\n\nMost the transaction amount falls between 0 and about 3000 and we have some outliers for really big amount transactions and it may actually make sense to drop those outliers in our analysis if they are just a few points that are very extreme. Also, we should be conscious about that these **outliers should not be the fraudulent transaction**. Generally, fraudulent transactions of the very big amount and removing them from the data can make the predicting model bais. \n\nSo we can essentially build a model that realistically predicts transaction as fraud without affected by outliers. It may not be really useful to actually have our model train on these extreme outliers."
## Distribution of Time
"**Highlights**\n\nBy seeing the graph, we can see there are two peaks in the graph and even there are some local peaks. We can think of these as the time of the day like the peak is the day time when most people do the transactions and the depth is the night time when most people just sleeps. We already know that data contains a credit card transaction for only two days, so there are two peaks for day time and one depth for one night time."
### Distribution of transaction type w.r.t amount
## Categorical vs Continuous Features\n\nFinding unique values for each column to understand which column is categorical and which one is Continuous
**Highlights**\n\n**Above graph shows that most of the Fraud transactions are happening at night time (0 to 7 hours) when most of the people are sleeping and Genuine transaction are happening during day time (9 to 21 hours).**
### Visualising Data for detecting any particular Pattern or Anomaly using Histogram Plots\n\nFinally visulaising all columns once and for all to observe any abnormality
### Visualising Data for detecting any particular Pattern or Anomaly using Histogram Plots\n\nFinally visulaising all columns once and for all to observe any abnormality
## Reset the index
#### Visualize frequency distribution of `target` variable
"#### Interpretation\n\n\n- The above plot confirms the findings that -\n\n   - There are 165 patients suffering from heart disease, and \n   \n   - There are 138 patients who do not have any heart disease."
We can visualize the value counts of the `sex` variable wrt `target` as follows -
"#### Interpretation\n\n- We can see that the values of `target` variable are plotted wrt `sex` : (1 = male; 0 = female).\n\n- `target` variable also contains two integer values 1 and 0 : (1 = Presence of heart disease; 0 = Absence of heart disease)\n\n- The above plot confirms our findings that -\n\n    - Out of 96 females - 72 have heart disease and 24 do not have heart disease.\n\n    - Similarly, out of 207 males - 93 have heart disease and 114 do not have heart disease.\n"
"Alternatively, we can visualize the same information as follows :"
"#### Comment\n\n\n- The above plot segregate the values of `target` variable and plot on two different columns labelled as (sex = 0, sex = 1).\n\n- I think it is more convinient way of interpret the plots."
We can plot the bars horizontally as follows :
We can use a different color palette as follows :
We can use a different color palette as follows :
We can use `plt.bar` keyword arguments for a different look :
We can use `plt.bar` keyword arguments for a different look :
#### Comment\n\n\n- I have visualize the `target` values distribution wrt `sex`. \n\n- We can follow the same principles and visualize the `target` values distribution wrt `fbs (fasting blood sugar)` and `exang (exercise induced angina)`.
#### Visualize the frequency distribution of `cp` variable
#### Frequency distribution of `target` variable wrt `cp`
We can visualize the value counts of the `cp` variable wrt `target` as follows -
"#### Interpretation\n\n- We can see that the values of `target` variable are plotted wrt `cp`.\n\n- `target` variable contains two integer values 1 and 0 : (1 = Presence of heart disease; 0 = Absence of heart disease)\n\n- The above plot confirms our above findings, "
"Alternatively, we can visualize the same information as follows :"
### Analysis of `target` and `thalach` variable \n
#### Visualize the frequency distribution of `thalach` variable
#### Comment\n\n- We can see that the `thalach` variable is slightly negatively skewed.
We can use Pandas series object to get an informative axis label as follows :
We can plot the distribution on the vertical axis as follows:-
We can plot the distribution on the vertical axis as follows:-
#### Seaborn Kernel Density Estimation (KDE) Plot\n\n\n- The kernel density estimate (KDE) plot is a useful tool for plotting the shape of a distribution.\n\n- The KDE plot plots the density of observations on one axis with height along the other axis.\n\n- We can plot a KDE plot as follows :
#### Seaborn Kernel Density Estimation (KDE) Plot\n\n\n- The kernel density estimate (KDE) plot is a useful tool for plotting the shape of a distribution.\n\n- The KDE plot plots the density of observations on one axis with height along the other axis.\n\n- We can plot a KDE plot as follows :
We can shade under the density curve and use a different color as follows:
We can shade under the density curve and use a different color as follows:
#### Histogram\n\n- A histogram represents the distribution of data by forming bins along the range of the data and then drawing bars to show the number of observations that fall in each bin.\n\n- We can plot a histogram as follows :
#### Histogram\n\n- A histogram represents the distribution of data by forming bins along the range of the data and then drawing bars to show the number of observations that fall in each bin.\n\n- We can plot a histogram as follows :
#### Visualize frequency distribution of `thalach` variable wrt `target`
#### Visualize frequency distribution of `thalach` variable wrt `target`
#### Interpretation\n\n- We can see that those people suffering from heart disease (target = 1) have relatively higher heart rate (thalach) as compared to people who are not suffering from heart disease (target = 0).
We can add jitter to bring out the distribution of values as follows :
#### Visualize distribution of `thalach` variable wrt `target` with boxplot
#### Visualize distribution of `thalach` variable wrt `target` with boxplot
#### Interpretation\n\nThe above boxplot confirms our finding that people suffering from heart disease (target = 1) have relatively higher heart rate (thalach) as compared to people who are not suffering from heart disease (target = 0).
### Heat Map 
"#### Interpretation\n\nFrom the above correlation heat map, we can conclude that :-\n\n- `target` and `cp` variable are mildly positively correlated (correlation coefficient = 0.43).\n\n- `target` and `thalach` variable are also mildly positively correlated (correlation coefficient = 0.42).\n\n- `target` and `slope` variable are weakly positively correlated (correlation coefficient = 0.35).\n\n- `target` and `exang` variable are mildly negatively correlated (correlation coefficient = -0.44).\n\n- `target` and `oldpeak` variable are also mildly negatively correlated (correlation coefficient = -0.43).\n\n- `target` and `ca` variable are weakly negatively correlated (correlation coefficient = -0.39).\n\n- `target` and `thal` variable are also waekly negatively correlated (correlation coefficient = -0.34).\n\n\n"
### Pair Plot 
"#### Comment\n\n\n- I have defined a variable `num_var`. Here `age`, `trestbps`, ``chol`, `thalach` and `oldpeak`` are numerical variables and `target` is the categorical variable.\n\n- So, I wll check relationships between these variables."
"#### Plot the distribution of `age` variable\n\nNow, I will plot the distribution of `age` variable to view the statistical properties."
#### Interpretation\n\n- The `age` variable distribution is approximately normal.
#### Visualize frequency distribution of `age` variable wrt `target`
#### Interpretation\n\n- We can see that the people suffering from heart disease (target = 1) and people who are not suffering from heart disease (target = 0) have comparable ages.
#### Visualize distribution of `age` variable wrt `target` with boxplot
#### Interpretation\n\n- The above boxplot tells two different things :\n\n  - The mean age of the people who have heart disease is less than the mean age of the people who do not have heart disease.\n  \n  - The dispersion or spread of age of the people who have heart disease is greater than the dispersion or spread of age of the people who do not have heart disease.\n
I will plot a scatterplot to visualize the relationship between `age` and `trestbps` variable.
#### Interpretation\n\n- The above scatter plot shows that there is no correlation between `age` and `trestbps` variable.
#### Interpretation\n\n- The above scatter plot shows that there is no correlation between `age` and `trestbps` variable.
#### Interpretation\n\n- The above line shows that linear regression model is not good fit to the data.
#### Box-plot of `age` variable
### `trestbps` variable
#### Box-plot of `trestbps` variable
### `chol` variable
#### Box-plot of `chol` variable
### `thalach` variable
#### Box-plot of `thalach` variable
### `oldpeak` variable
#### Box-plot of `oldpeak` variable
#### Findings\n\n- The `age` variable does not contain any outlier.\n\n- `trestbps` variable contains outliers to the right side.\n\n- `chol` variable also contains outliers to the right side.\n\n- `thalach` variable contains a single outlier to the left side.\n\n- `oldpeak` variable contains outliers to the right side.\n\n- Those variables containing outliers needs further investigation.\n
"So, friends, our EDA journey has come to an end.\n\nIn this kernel, we have explored the heart disease dataset. In this kernel, we have implemented many of the strategies presented in the book **Think Stats - Exploratory Data Analysis in Python by Allen B Downey** . The feature variable of interest is `target` variable. We have analyzed it alone and check its interaction with other variables. We have also discussed how to detect missing data and outliers.\n\nI hope you like this kernel on EDA journey.\n\nThanks\n"
[Go to Top](#0)\n
"Data Exploration\nLet's explore the data we have as this will give us a hint on the algorithm we will use if we have to choose. Exploring data is also very important because it will tell you which accuracy metric you are going to use, if the data is balanced which means all the classes have fair contribution in the dataset regarding its numbers then we can easily use accuracy, But if the data is skewed then we won't be able to use accurace as it's results will be misleading and we may use F-beta score instead."
From the previous results we can see that the dataset consists of 60000 training example each is an image of dimention 28 * 28. We can see that the number of occurances of each class is almost balanced and based on that it is safe to use accuracy as our metric later.
"A W&B Table (wandb.Table) is a two dimensional grid of data where each column has a single type of dataâ€”think of this as a more powerful DataFrame. Tables support primitive and numeric types, as well as nested lists, dictionaries, and rich media types. Log a Table to W&B, then query, compare, and analyze results in the UI.\n\nTables are great for storing, understanding, and sharing any form of data critical to your ML workflowâ€”from datasets to model predictions and everything in between.\n\n[Source ](https://docs.wandb.ai/guides/data-vis)"
Lets explore first five rows of train dataset
Distribution of Standard Deviation 
Distribution of min values 
Distribution of min values 
Distribution of max values 
Distribution of max values 
 Distribution of Skewness 
 Distribution of Skewness 
 Distribution of Kurtosis 
 Distribution of Kurtosis 
 Correlation of Features 
#  Machine Learning \n#  Multilayer Perceptron from Scratch 
"# 2. Artificial Neural Networks\n\nArtificial Neural Networks are mathematical models inspired by the human brain, specifically the ability to learn, process, and perform tasks. The Artificial Neural Networks are powerful tools that assist in solving complex problems linked mainly in the area of combinatorial optimization and machine learning. In this context, artificial neural networks have the most varied applications possible, as such models can adapt to the situations presented, ensuring a gradual increase in performance without any human interference. We can say that the Artificial Neural Networks are potent methods can give computers a new possibility, that is, a machine does not get stuck to preprogrammed rules and opens up various options to learn from its own mistakes. \n"
"## 3.1. Some Python Libraries \n\nIn the first place, Let's define some libraries to help us in the manipulation the data set, such as `numpy`, `matplotlib`, `seaborn` and `scikit-learn`. In this tutorial, I am implementing a Multilayer Perceptron without any framework like Keras or similar ones. The goal here is to be as simple as possible! So to help you with this task, we implementing the neural network without using ready-made libraries. You can use numpy to work with array operations! There is no problem it! "
## 3.2. An analysis about the Iris Flower Dataset\n
## 4.1. Plot our training Samples
## 4.2. Plot our test Samples
## 4.2. Plot our test Samples
"\n\n# 5. Multilayer Perceptron\n\nArtificial neural networks (ANNs) or connectionist systems are computing systems inspired by the biological neural networks that constitute animal brains. Such systems learn (progressively improve performance) to do tasks by considering examples, generally without task-specific programming. For example, in image recognition, ""they might learn to identify images that contain cats by analyzing example images that have been manually labeled as ""cat"" or ""no cat"" and using the analytic results to identify cats in other images"".\n\nThey have found most use in applications difficult to express in a traditional computer algorithm using rule-based programming. An ANN is based on a collection of connected units called artificial neurons, (analogous to axons in a biological brain). Each connection (synapse) between neurons can transmit a signal to another neuron. The receiving (postsynaptic) neuron can process the signal(s) and then signal downstream neurons connected to it.\n\n More information here: [Artificial Neural Network](https://en.wikipedia.org/wiki/Artificial_neural_network)\n\n\n\nThe Multilayer Perceptron Networks are characterized by the presence of many intermediate layers (hidden) in your structure, located between input layer and output layer. With this, such networks have the advantage of being able to classify more than two different classes and It also solve non-linearly separable problems.\n\n      \n      \n           \n      \n  \n"
# 6. Implementation the Multilayer Perceptron in Python\n
"## Finding the best parameters \n\nFor find the best parameters, it was necessary to realize various tests using different values to the parameters. The graphs below denote all tests made to select the best configuration for the multilayer perceptron. These tests were important in selecting the best settings and ensuring the best accuracy. The graph was drawn manually, but you can change the settings and note the results obtained. The tests involve different activation functions and the number of neurons for each layer."
## Step 3. Accuracy and precision the Multilayer Perceptron
## Step 4. Score for each one of the samples
"# Choosing the right role\n\nIf you are a student or just recently started your career and are still unsure about the technology skills to target or if you are an experienced data scientist looking for ways to improve your chances on the job market, the first step is to identify all possibilities and explore the differences between them. The roles I personally consider most relevant are:\n\n* **Data Scientist**\n* **Data Analyst**\n* **Software Engineer**\n* **Research Scientist**\n* **Data Engineer**\n* **Statistician**\n\n#### Mapping the community\n\nI cleaned up the data by imputing missing values, transforming to dummy variables and normalizing numerical values. I then used [LDA](https://sebastianraschka.com/Articles/2014_python_lda.html) to perform supervised [dimensionality reduction](https://idyll.pub/post/dimensionality-reduction-293e465c2a3443e8941b016d/) and map the resulting feature vectors to a 2 dimensional space while maximing the separation of the clusters.  The feature set I used for LDA is based on questions which I felt are the best predictors of a person's daily activity at work. \n\nThe interpretation of the resulting principal components and naming of the axes is highly subjective because it is the weighted combination of multiple features and can be too complex to describe in simple words. Based on the resulting positions of the various groups I like to think of **PC1(horizontal)** as a **Business (left) vs.  Code (right)** axis and of **PC2 (vertical)** as an **Engineering (bottom) vs. Research (top)** axis.\n\n### Where are these role situated in relation to each-other?\n\nAs you probably noticed on the first image, the clusters are clearly overlapping, the definitions of these roles are extremely fuzzy. For example the **Data Analyst** and **Business Analyst** roles share most of the same characteristics with slight differences in the application of the conclusions resulting from the analysis.  To avoid cluttering / having too many categories, in some of the charts I merged some of the less-represented groups:\n* (Student, Not employed, Consultant, Other, Developer Advocate) â†’ drop\n* (Project Manager, Chief Officer) â†’ Manager\n* (Research Assistant, Principal Investigator) â†’ Research Scientist\n* (Data Journalist, Business Analyst) â†’ Business Analyst\n* (Database Engineer) â†’ Data Engineer\n\nDespite the overlaps, the positions of the cluster centroids reveal some interesting patterns. The size of the bubbles represent the size of the group.\n"
"#### Observations:\n* The marketing & business roles form a cluster group in the center left area of the chart, not far from the closely related **Management** clusters, which use the reports and analyses provided by the **Data Analysts** to making data driven business decisions.\n* The academical roles, namely **Principal Investigator**, **Research Investigator** and **Research Scientist** form a tight cluster group in the top center part of the chart. They are the most similar to the **Data Scientist** role from a technical point of view.\n* The **Student** cluster lies close to the other academic roles, with an observable offset towards the more practical, engineering-related clusters.\n* **Database Engineer** lies further than expected from the **Data Engineer** cluster. The reason might be that these roles consist of a lot of querying, report generation and data maintenance which is more closely related to what the Business cluster group does. \n* **Student**, and **Chief Officer** and **Consultant** are the real Jack-of-All-Trades roles in the **Data Science** community. I imagine these **Chief Officers** as former **Data Scientist** now leading a **AI** or **Big Data**-driven startups."
"#### Observations\n* **Statisticians** are leaders in advanced statistics by a far margin.\n* **Research Scientists** are leaders in usage of other types of Software Packages, these are probably the domain-specific software they use for bioinformatics, physics, math."
### Black-box models?\nNah! You just haven't studied enough!
"Observations:\n* Notice that the order of the roles is exactly the same for both answers. Being able to deeply understanding **Machine Learning** models is an essential skill for **Data Scientists**.\n* The **Data Scientists** on the left side of the 0 axis are the self-doubters. They picked the **Data Scientist** title when they were asked the first time, but began to question their true identity when facing the question for the second time: ""Really? You? Look at yourself! You consider yourself to be a data scientist?""\n\nWe can estimate each role's similarity to the **Data Scientist** role using our LDA feature-space by calculating the [Euclidean distance](https://tekmarathon.com/2015/11/15/different-similaritydistance-measures-in-machine-learning/) between the cluster centroids.\n\nNow that we have a general idea of what each role entails and have seen the way they seen themselves, let's plot against each other the ""Self Assessed"", perceived similarity and the actual similarity between each role and the **Data Scientist** role.\n\nThe size of the bubble represents the size of the group, the X axis represents the distance of the group to the **Data Scientist** role measured by self-assessment (Q26 - Are you a Data Scientist?), the Y axis represents the distance of the group to the **Data Scientist** role."
#### Observations:\n \n * **Statisticians** seem to consider themselves **Data Scientists** despite the differences highlighted in the charts above.\n * **Data Engineers** on the other hand seem to be unaware of how similar their job is to one of a **Data Scientist**.
"### Learn Python. No excuses.\n\nI visualized the top 50 technologies by percent of users familiar with the technology in question (Supply) and percent of job requirements mentioning it (Demand). Technologies which appear in both lists are connected with gray ribbons.\nThe gray-colored skills are the ones missing from either the Supply or the Demand column. \n\nI decided not to include some of the terms which I found too general, but nonetheless these concepts are among the top 50 most important things required by employers. In order of importance: `Machine Learning`, `Big Data`, `Cloud`, `Sys Admin`, `Agile`, `Algorithm`, `NoSQL`, `Database`, `Data Science`, `REST`, `Deep Learning`, `Artificial Intelligence`, `Web-Services`, `Testing`, `Computer Vision`, `QA`, `Security`, `Automation`, `Design`, `Microservices`, `DevOps`, `Data Warehouse`, `NLP`, `Statistics`, `ETL`, `Data`, `Neural Network`, `Time Series`, `Data Modeling`, `UI`, `JSON`, `Apache`, `CI` and `API`.\n"
"#### Skill importance by role\nThe importance of each skill varies between the roles, so while it's useful to have an overview, breaking down the supply and demand by job title gives us a more valuable insight:"
"#### Skill importance by role\nThe importance of each skill varies between the roles, so while it's useful to have an overview, breaking down the supply and demand by job title gives us a more valuable insight:"
"#### Observations:\n    \n* Learn `Python`. No excuses. [Pandas](https://pandas.pydata.org/) and [Numpy](https://www.numpy.org/), its most important data-wrangling libraries also appear on the top 50 list.\n* Most **IDE**s, notebooks and data visualization libraries are never mentioned in job requirements. \n* On the other hand [Hadoop](https://hadoop.apache.org/), [Spark](https://spark.apache.org/) and [Elasticsearch](https://www.elastic.co/) are some of the most sought-after technologies on the job market, but were not mentioned in the Survey. Other **Big Data** technologies which appear in the top 50 are: [Hive](https://hive.apache.org/), [Cassandra](http://cassandra.apache.org/), [Kubernetes](https://kubernetes.io/) and [Kafka](https://kafka.apache.org/).\n* **Java** is important, but its importance in the global ranking may be exaggerated by the large proportion of **Software-Engineering** jobs in the dataset. The same with **Javascript**, **React** and **Angular**. These jobs were picked because they form in a way part of the data-pipeline, but they tend to over-inflate the presence of some techs in the list.\n* **OS** usage was not surveyed, but it's important to note that **Linux** is in demand, mentioned in over 5% of all data-related job requirements.\n* [Tableau](https://www.tableau.com/) was not mentioned in the survey among the data-visualization tools, even though based on **Google Trends**, [it is more than 2 times more popular](https://trends.google.com/trends/explore?geo=US&q=learn%20tableau,learn%20matplotlib) than the most popular **Python** dataviz tool, **Matplotlib**. Its language-agnostic, versatile, easy to learn and it's on the rise. \n* **Git** and [Docker](https://www.docker.com/) are in my opinion indispensable tools. Even though they appear only in the bottom half of the list, they should be among the first skills to aquire for a developer."
"# 3. Importing libraries and exploring Data\n## 3a.Importing Libraries\nPython is a fantastic language with a vibrant community that produces many amazing libraries. I am not a big fan of importing everything at once for the newcomers. So, I am going to introduce a few necessary libraries for now, and as we go on, we will keep unboxing new libraries when it seems appropriate"
## 3b.Extracting dataset
"## 4b. ....... Outliers\nOutliers are unusual values in your dataset, and they can distort statistical analyses and violate their assumptions. Unfortunately, all analysts will confront outliers and be forced to make decisions about what to do with them. Given the problems they can cause, you might think that itâ€™s best to remove them from your data. But, thatâ€™s not always the case. Removing outliers is legitimate only for specific reasons.Outliers can be very informative about the subject-area and data collection process. Itâ€™s essential to understand how outliers occur and whether they might happen again as a normal part of the process or study area. Unfortunately, resisting the temptation to remove outliers inappropriately can be difficult. Outliers increase the variability in your data, which decreases statistical power. Consequently, excluding outliers can cause your results to become statistically significant. In our case, **let's first visualize our data and decide on what to do with the outliers**"
"As you see, we have very less number of outliers in our features. Especially we have majority of the outliers in **hsc percentage** Let's clear em up!"
# 5.Data Visualizations\n## 5a. Count of categorical features- Count plot
**Inference**\n* We have **more male candidates** than female\n* We have candidates who did **commerce** as their hsc course and as well as undergrad\n* **Science background** candidates are the second highest in both the cases\n* Candidates from **Marketing and Finance** dual specialization are high \n* Most of our candidates from our dataset **don't have any work experience**\n* Most of our candidates from our dataset **got placed** in a company
## 5b. Distribution Salary- Placed Students
**Inference**\n* Many candidates who got placed received package between **2L-4L PA**\n* Only **one** candidate got around **10L PA**\n* The **average** of the salary is a little more than 2LPA\n
## 5c. Employability score vs Salary- Joint plot
**Inference**\n* Most of the candidates scored around **60 percentage** got a decent package of **around 3 lakhs PA**\n* **Not** many candidates received salary **more than 4 lakhs PA**\n* The bottom dense part shows the candidates who were **not placed**
## 5d.Distribution of all percentages
**Inference**\n* All the distributions follow **normal distribution** except salary feature\n* Most of the candidates **educational performances are between 60-80%**\n* **Salary distribution got outliers** where few have got salary of 7.5L and 10L PA
## 5e.Work experience Vs Placement Status
**Inference**\n* We have nearly **66.2%** of candidates who never had any work experience\n* Candidates who **never had work experience** have **got hired** more than the ones who had experience\n* We can conclude that **work experience doesn't influence** a candidate in the recruitment process
## 5f. MBA marks vs Placement Status- Does your academic score influence?
"**Inference** \nComparitively there's a slight difference between the percentage scores between both the groups, But still placed candidates still has an upper hand when it comes to numbers as you can see in the swarm. So as per the plot,percentage do influence the placement status"
## 5g.Does MBA percentage and Employability score correlate?
**Inference**\n* There is **no relation** between mba percentage and employability test\n* There are many candidates who **haven't got place**d when they don't have work experience\n* Most of the candidates who performed better in both tests **have got placed**
## 5h. Is there any gender bias while offering remuneration?
**Inference**\n* The **top salaries were given to male**\n* The **average salary** offered were also **higher for male**\n* **More male candidates were placed** compared to female candidates\n
## 5i. Coorelation between academic percentages
"**Inference**\n* Candidates who were good in their academics performed well throughout school,undergrad,mba and even employability test\n* These percentages **don't have any influence over their salary**"
## 5j.Distribution of our data
**Inference**\n* Candidates who has **high score in higher secondary and undergrad got placed**\n* Whomever got **high scores in their schools got placed**\n* Comparing the number of students who got placed candidates who got **good mba percentage and employability percentage**  
### ROC Curve\nLet's check out the performance of our model through ROC curve
From the ROC curve we can infer that our logistic model has classified the placed students correctly rather than predicting false positive. T**he more  the ROC curve(blue) lies towards the top left side the better our model** is. We can choose **0.8 or 0.9** for the threshold value which can reap us true positive results
### Looking at Feature Importance\nLet's see which feature influences more on making the decision and we should cut it off to make our model accurate
As we see the **school and undergrad specialisations** have less influence in classifying the model. But it is really wierd to acknowledge **ssc_p** influencing more in classifying
### Error rate vs K-value
There are a lot of ups and downs in our graph. If we consider any value between 10-15 we may get an overfitted model. So let's stick onto the first trough. Our **K value is 5**
now check outliers and noisy data. For this step we use scatter plot and box plot
"\n    ðŸ“Œ In the box plot below, you can select the columns you want, from the right side of the chart to display.ðŸ˜Š\n"
"\n    ðŸ“Œ In the box plot below, you can select the columns you want, from the right side of the chart to display.ðŸ˜Š\n"
"An outlier is an observation that is unlike the other observations and we see some of these in above boxplot for some comlumns but outliers are innocent until proven guilty. With that being said, they should not be removed unless there is a good reason for that. According to pairplot, noisy data does not appear to exist. Therefore, we do not delete any data.\n\nNow everything is ok. There is only one small point that needs to be fixed. As it was said in section 1, the Income column expresses the annual income, while the CCAvg column expresses the Avg. spending on credit cards per month, so to standardize the units of the columns, we convert the annual income to monthly."
\n# 5. Exploratory Data Analysis (EDA)\n\n[ðŸ  Tabel of Contents](#tabel)\n\nBefore univariate analysis we check distribution of each columns:
\n# 5.1. Univariate Analysis\n\n[ðŸ  Tabel of Contents](#tabel)
\n# 5.2. Bivariate Analysis\n\n[ðŸ  Tabel of Contents](#tabel)
"ðŸ‘‰ According to above plots:\n - The Income of people who have accepted a bank loan is often higher than that of people who have not accepted a bank loan. Approximately, people whose monthly Income is more than 8 thousand dollars have accepted a bank loan (Fig 1)\n - Most people who accepted a bank loan had mortgage euqal to zero (Fig 2).\n - The CCAvg of people who have accepted a bank loan is often higher than that of people who have not accepted a bank loan. Approximately, people whose CCAvg is more than 3 thousand dollars have accepted a bank loan (Fig 3).\n - It seems that age does not have much influence in determining whether or not to accept a bank loan (Fig 4)"
"ðŸ‘‰ According to above plots:\n - The Income of people who have accepted a bank loan is often higher than that of people who have not accepted a bank loan. Approximately, people whose monthly Income is more than 8 thousand dollars have accepted a bank loan (Fig 1)\n - Most people who accepted a bank loan had mortgage euqal to zero (Fig 2).\n - The CCAvg of people who have accepted a bank loan is often higher than that of people who have not accepted a bank loan. Approximately, people whose CCAvg is more than 3 thousand dollars have accepted a bank loan (Fig 3).\n - It seems that age does not have much influence in determining whether or not to accept a bank loan (Fig 4)"
"ðŸ‘‰ According to above plots:\n - Among the people who did not accept the personal loan, most of them had a family equal to 1, but among the people who accepted the personal loan, there is not much difference in terms of family (Fig 5).\n - Among the people who did not accept the personal loan, most of them had an Education of 1, but among the people who accepted the personal loan, the Education was mostly 3 or 2 (Fig 6).\n - Most of the people, both those who accepted the personal loan and those who did not, did not have a Securities Account (Fig 7).\n - Most of the people, both those who accepted the personal loan and those who did not, did not have a CD Account (Fig 8).\n - Most of the people, both those who accepted the personal loan and those who did not, used online banking facilities (Fig 9).\n - Most of the people, both those who accepted the personal loan and those who did not, did not use a Creditcard (Fig 10)."
ðŸ‘‰ According to above plots:\n - Customers whose Income is less than `$5 thousand ` per month have not accepted a personal loan.\n - Most customers whose income is less than `$10 thousand ` per month and their CCAvg is less than `$3 thousand ` per month have not accepted a personal loan.\n - 62% of customers whose CD Account was 1 and Education was 2 have accepted a personal loan.
"ðŸ‘‰ According to above plots:\n - All customers with  of more than`$10 thousand` and with Education level 2 or 3, accepted Personal Loans.\n - All customers with CCAvg of more than `$5 thousand` and with Education level 2 or 3, accepted Personal Loans.\n - All customers with Income of more than `$10 thousand` and Family 3 or 4, accepted Personal Loans.\n - All customers with CCAvg of more than `$5 thousand` and by Family 3 or 4, accepted Personal Loans.\n - Most customers with Income of more than `$10 thousand` and by CD Account 1, accepted Personal Loans.\n - Most customers with CCAvg of more than `$5 thousand` and by CD Account 1, accepted Personal Loans.\n"
"It seems that the model tends to overfit here as well, so to solve this problem, we perform parameter tuning for RF:"
"\n    ðŸ“ŒYou may see some warnings during the optimization for invalid configuration combinations. These can be safely ignored.\n\n\nThe results are summarized as follows:\n\n    Explore random forest bootstrap sample size\n    >0.1, mean:0.763, ste:0.068\n    >0.2, mean:0.824, ste:0.060\n    >0.3, mean:0.850, ste:0.049\n    >0.4, mean:0.859, ste:0.053\n    >0.5, mean:0.867, ste:0.042\n    >0.6, mean:0.867, ste:0.048\n    >0.7, mean:0.876, ste:0.048\n    >0.8, mean:0.879, ste:0.050\n    >0.9, mean:0.878, ste:0.043\n    >1.0, mean:0.877, ste:0.047\n\n\n    Explore random forest number of features effect\n    >1, mean:0.778, ste:0.061\n    >2, mean:0.866, ste:0.050\n    >3, mean:0.882, ste:0.047\n    >4, mean:0.885, ste:0.044\n    >5, mean:0.896, ste:0.035\n    >6, mean:0.899, ste:0.039\n    >7, mean:0.896, ste:0.038\n\n\n    Explore random forest tree depth effect\n    >1, mean:0.000, ste:0.000\n    >2, mean:0.119, ste:0.061\n    >3, mean:0.437, ste:0.086\n    >4, mean:0.658, ste:0.080\n    >5, mean:0.801, ste:0.059\n    >6, mean:0.840, ste:0.054\n    >7, mean:0.865, ste:0.052\n    >8, mean:0.869, ste:0.042\n    >9, mean:0.879, ste:0.044\n    >None, mean:0.877, ste:0.042\n\n"
"An Insightful Story About Successful Crowdfunding Projects \n\n\n***Crowdfunding*** is the practice of funding a project or a venture by raising monetary contributions from many people across the globe. There are a number of organisations such as DonorsChoose.org, Patreon, Kickstarter which hosts the crowdfunding projects on their platforms. Kickstarter has hosted more than 250,000 projects on their website with more than $4 Billion collective amount raised.  \n\n\n![](https://people.safecreative.org/viewimage/viewmagazine?path=kickstarter-patreon.jpg&defaultPath=)\n\n\nWhile it is true that crowdfunding is one of the most popular methods to to raise funds however the reality is that **not every project is able to completely reach the goal**. Infact, on KickStarter, only about 35 percent of the total projects have raised successful fundings in the past. This fact raises an important question - **which projects are able to successfully achieve their goal?**. In other words, can project owners somehow know what are the key project characteristics that increases the chances of success. \n\nIn many studies, Researchers and analysts have used the descriptive analysis methods on the crowdfunding data to obtain insights related to project success. While many others have also applied predictive modelling to obtain the probability of project success. However, these approaches have the fundamental problems: \n\n- Descriptive analysis - only gives surface level insights    \n- Predictive analysis - models act as the blackboxes    \n\n### About this Kernel  \nIn this kernel, I have shared a hybrid analysis approach that uses the concepts of both types of analysis enriched with the concepts of **machine learning explainability** which can be used to answer the key questions related to the success (or failure) of any crowdfunding project. The framework uses the interpretations derived from a trained machine learning model. Unlike descriptive analysis to find key insights, the focus in this approach is to make use of model behaviours and characteristics such as : Relative Feature Importances, Partial Dependencies, Permutation Importances, SHAP values. I have explained the intuition behind every approach in layman terms. Following are the contents of the kernel: \n\n## Contents \n\n1. Business Use-Case and Problem Statement    \n2. Hypothesis Generation    \n3. Dataset Preparation     \n4. Modelling the Project Success       \n5. Model Interpretation : Insights Generation    \n    5.1 Which are the most important features (relatively) of a project? ( **Relative Feature Importance** )     \n    5.2 Which features have the biggest impact on the project success? ( **Permutation Importance** )     \n    5.3 How does changes in those features affact the project success? ( **Partial Dependencies** )     \n    5.4 Digging deeper into the decisions made by the model ( **SHAP values** )    \n6. Final Conclusions       \n  \n\n1. Understanding the Business Use Case \nThe essential business use-cases in the crowdfunding scenario can be considered from two different perspectives - from the project owner's perspective and the companies perspective. \n\n1. From the **project owner's perspective**, it is highly beneficial to be aware about the key characteristics of a project that greatly influence the success of any project. For instance, it will be interesting to pre-emptively know about following questions:   \n\n     - What is an ideal and optimal range of the funding goal for my project ?  \n     - On which day of the week, I should post the project on Kickstarter ?  \n     - How many keywords should I use in my project title ?  \n     - What should be the total length of my project description ?     \n\n\n2. From the **perspective of companies** which hosts the crowdfunding projects such as DonorsChoose.org, Patreon, and Kickstarter, they receive hundreds of thousands of project proposals every year. A large amount of manual effort is required to screen the project before it is approved to be hosted on the platform. This creates the challenges related to scalability, consistency of project vetting across volunteers, and identification of projects which require special assistance. \n\nIt is due to these two perspectives, there is a need to dig deeper and find more intutive insights related to the projects success. Using these insights, more people can get their projects funded more quickly, and with less cost to the hosting companies. This also allows the hosting companies to optimize the processes and channel even more funding directly to projects.   \n\n\n2. Hypothesis Generation   \nHypothesis Generation is very powerful technique which can help an analyst to structure a very insightful and a relevant solution of a business problem. It is a process of building an intuitive approach of the business problem without even thinking about the available data. Whenever I start with any new business problem, I try to make a comprehensive list of all the factors which can be used to obtain the final output. For example, which features should affect my predictions. Or, which values of those features will give me the best possible result. In case of crowdfunding, the question can be - which features are very important to decide if a project will be successful or not.  \n\nSo, to generate the hypothesis for the use-case, we will write down a list of factors (without even looking at the available data) that can possibly be important to model the project success.   \n\n1. **Total amount to be raised** - More amount may decrease the chances that the project will be successful.  \n2. **Total duration of the project** - It is possible that projects which are active for very short or very long time periods are not successful.  \n3. **Theme of the project** - People may consider donating to a project which has a good cause or a good theme.  \n4. **Writing style of the project description** - If the message is not very clear, the project may not get complete funding.  \n5. **Length of the project description** - Very long piecies of text may not perform good as compared to shorter crisp texts.  \n6. **Project launch time** - A project launched on weekdays as compared to weekends or holidays may not get complete funding amount.  \n\nSo this is an incomplete list of possible factors we can think at this stage that may influence the project success. Now, using machine learning interpretability, not only we can try to understand which features are actually important but also what are the feature values which these features can take. \n\n\n3. Dataset Preparation  \nOur business use-case is identified, problem statement is formulated, and we have defined a hypothesis. We can now start the analysis, modelling, and interpretting in order to find out the key insights. First, we load the available dataset. \n\n### 3.1 Load Dataset "
"### 3.2 Dataset Preprocessing  \n\nIn this dataset, we can see that a number of features are about the active stage of the project. This means that a project was launched on a particular date and a partial amount is already raised. The goal of our problem statement is a little bit different, we want to focus on the stage in which the project is not launched yet and identify if it will successful or not. Additinaly, find the most important features (and the feature values) that influence this output. So we perform some pre-processing in this step which includes the following: \n\n- Get rid of unwanted columns (active stage columns)  \n- Feature Engineering (driven from our hypothesis generation)    \n- Remove Duplicates  \n- Handle Missing Values  \n- Encode the Categorical Features  "
"Now, we have a model which predicts the probability of a given project to be successful or not. In the next section we will interpret the model and its predictions. In other words, we will try to prove or disprove our hypothesis. \n\n\n5. Insights from Predictive Modelling  \n\n- 5.1 Which are the most important features (relatively) of a project? ( **Relative Feature Importance** )       \n- 5.2 Which features have the biggest impact on the project success? ( **Permutation Importance** )      \n- 5.3 How does changes in those features affact the project success? ( **Partial Dependencies** )       \n- 5.4 Digging deeper into the decisions made by the model ( **SHAP values** )     \n\n\n5.1 Which are the most important features (relatively) of a project? (Relative Feature Importance)   \n\nIn tree based models such as random forest, a number of decision tress are trained. During the tree building process, it can be computed how much each feature decreases the weighted impurity (or increases the information gain) in a tree. In random forest, the impurity decrease from each feature is averaged and the features are ranked according to this measure. This is called relative feature importance. The more an attribute is used to make key decisions with decision trees, the higher its relative importance. This indicates that the particular feature is one of the important features required to make accurate predictions. \n"
"**Inferences** \n> - From the graph, it is clear that the features which are important to predict the project success are: project goal, length of the project name, launched week, duration, and number of syllables present in the name. While the least important features are are mostly related to the project categories    \n> - **What does this mean for the project owner?** For someone who is willing to raise funds, they should consider evaluating the ideal project goal and duration. A high or a medium-high project goal may almost lead to the case of failure. Additionally, number of characters used in the project title will also affact if the project will be succeeded or failed.   \n> - **What does this mean for the company?** The company can identify the projects with high importance based on their meta - features such as length of the project.    \n\nBy applying this approach, we primariy obtained the factors to look at a high level, But still we need to answer, what are the optimal values of these features. This will be answered when we apply other techniques in the next sections. Before moving on to those techniques, I wanted to explore a little more about relative feature importance using a graph theory perspective.  \n\n A Graph Theory Perspective : Relative Feature Importances \n\nThe idea of relative feature importance is very simple (more the times a feature appear in the decision tree splits, it is important) but many a times people forget an underlying important concept that these importances are ""relative"". This means that in comparison to other features what is the importance of a particular feature. \n\nBut a question here is - Even if it is relative, what if some features from a set of features are removed, do we still obtain the same feature importances ? This problem can infact be formulated as graph problem. Consider a graph based structure, in which every feature (of the dataset) is a node, and the edges are defined between two features (nodes) if the two features appers in the *top 10 important features of the individual decision tree*. \n\n**But, What is the benefit?** Well, Some of the problems when viewed as network or graph problems can help to identify the solutions quickly and easily. For instance, using the graph properties one can identify which is the most important node of the network. A node having higher degree centrality (connections) indicates that the node is highly connected to the network. Which means that if a node is removed from the network, a large majority of the network will be disrupted. \n\nThis idea can be applied to relative feature importances, a feature which is highly important, which appears in most of the decision tree's top 10 feature can be clearly identified from the feature importance network graph. If this feature is removed from the dataset then the predictions will be affacted. Let's plot this network. "
"**Inferences** \n> - From the graph, it is clear that the features which are important to predict the project success are: project goal, length of the project name, launched week, duration, and number of syllables present in the name. While the least important features are are mostly related to the project categories    \n> - **What does this mean for the project owner?** For someone who is willing to raise funds, they should consider evaluating the ideal project goal and duration. A high or a medium-high project goal may almost lead to the case of failure. Additionally, number of characters used in the project title will also affact if the project will be succeeded or failed.   \n> - **What does this mean for the company?** The company can identify the projects with high importance based on their meta - features such as length of the project.    \n\nBy applying this approach, we primariy obtained the factors to look at a high level, But still we need to answer, what are the optimal values of these features. This will be answered when we apply other techniques in the next sections. Before moving on to those techniques, I wanted to explore a little more about relative feature importance using a graph theory perspective.  \n\n A Graph Theory Perspective : Relative Feature Importances \n\nThe idea of relative feature importance is very simple (more the times a feature appear in the decision tree splits, it is important) but many a times people forget an underlying important concept that these importances are ""relative"". This means that in comparison to other features what is the importance of a particular feature. \n\nBut a question here is - Even if it is relative, what if some features from a set of features are removed, do we still obtain the same feature importances ? This problem can infact be formulated as graph problem. Consider a graph based structure, in which every feature (of the dataset) is a node, and the edges are defined between two features (nodes) if the two features appers in the *top 10 important features of the individual decision tree*. \n\n**But, What is the benefit?** Well, Some of the problems when viewed as network or graph problems can help to identify the solutions quickly and easily. For instance, using the graph properties one can identify which is the most important node of the network. A node having higher degree centrality (connections) indicates that the node is highly connected to the network. Which means that if a node is removed from the network, a large majority of the network will be disrupted. \n\nThis idea can be applied to relative feature importances, a feature which is highly important, which appears in most of the decision tree's top 10 feature can be clearly identified from the feature importance network graph. If this feature is removed from the dataset then the predictions will be affacted. Let's plot this network. "
"> From the above plot, we can identify that the maximum degree centrality nodes are - duration, syllable count, goal etc. From the network perspective, it means that if we remove these nodes from the network (high degree centrality nodes), this will lead to network disruption. This is similar to what we obtained in the relative feature importance plot. The key takeaway from this graph is that one can not afford to ignore these features while optimizing any crowdfunding project. \n\n\n5.2 Which features have the biggest impact on the project success? (Permutation Importance)     \n\nIn the last section, we mainly identified which the features at a very high level which are relatively important to the model outcome. In this section, we will go a little deeper and understand which features has the biggest impact on the model predictions (in absolute sense). One of the ways to identify such behaviour is to use permutation importance. \n\nThe idea of permutation importance is very straightforward. After training a model, the model outcomes are obtained. The most important features for the model are the ones if the values of those feature are randomly shuffled then they lead to biggest drops in the model outcome accuracies. Let's look at the permutation importance of features of our model."
"> From the above plot, we can identify that the maximum degree centrality nodes are - duration, syllable count, goal etc. From the network perspective, it means that if we remove these nodes from the network (high degree centrality nodes), this will lead to network disruption. This is similar to what we obtained in the relative feature importance plot. The key takeaway from this graph is that one can not afford to ignore these features while optimizing any crowdfunding project. \n\n\n5.2 Which features have the biggest impact on the project success? (Permutation Importance)     \n\nIn the last section, we mainly identified which the features at a very high level which are relatively important to the model outcome. In this section, we will go a little deeper and understand which features has the biggest impact on the model predictions (in absolute sense). One of the ways to identify such behaviour is to use permutation importance. \n\nThe idea of permutation importance is very straightforward. After training a model, the model outcomes are obtained. The most important features for the model are the ones if the values of those feature are randomly shuffled then they lead to biggest drops in the model outcome accuracies. Let's look at the permutation importance of features of our model."
"**Inferences** \n> - This is an interesting plot, We can observe that the features shown in top and in green are the most important as if their values are randomized then the outcome performance suffers.   \n> - We can observe that the top features are are the features which we mostly saw in the relative importance section, but using this graph we can quantify the amount of importance associated with them. And also obtain the ones which are least important, for example - launched week, if it was weekend or not etc.  \n\nWith this method, we obtained the importance of a feature in a more absolute sense rathar than a relative sense. Let's assume that our feature space forms a majority of the universe. Now, it will be interesting to plot both permutation and relative feature importances and make some key observations. "
"**Inferences** \n> - This is an interesting plot, We can observe that the features shown in top and in green are the most important as if their values are randomized then the outcome performance suffers.   \n> - We can observe that the top features are are the features which we mostly saw in the relative importance section, but using this graph we can quantify the amount of importance associated with them. And also obtain the ones which are least important, for example - launched week, if it was weekend or not etc.  \n\nWith this method, we obtained the importance of a feature in a more absolute sense rathar than a relative sense. Let's assume that our feature space forms a majority of the universe. Now, it will be interesting to plot both permutation and relative feature importances and make some key observations. "
"**Inferences**  \n> - Very Interesting Insights can be obtained from the above plot, There are some features which showed up higher in the relative feature importance, but when we look at their permuatation importance we see that they are not important. (Though, permutation importance results cannot be reproduced exactly because of randomness, but when I first plotted this plot I observed that launched week and month had high feature importance but lower permutation importance.)  \n> - From this plot, we can again observe that our hypothesis is almost true, the project goal, duration, number of characters, number of words all are the most important features that one should look at while creating a new project page. \n\n### Pressence of which keywords makes the biggest impact in the predictions?  \n\nUsing permutation importance, we can also evaluate which keywords makes the biggest impact in the model prediction. Let's train another model which also uses keywords used in the project name and observe the permutation importance."
"**Inferences**  \n> - Very Interesting Insights can be obtained from the above plot, There are some features which showed up higher in the relative feature importance, but when we look at their permuatation importance we see that they are not important. (Though, permutation importance results cannot be reproduced exactly because of randomness, but when I first plotted this plot I observed that launched week and month had high feature importance but lower permutation importance.)  \n> - From this plot, we can again observe that our hypothesis is almost true, the project goal, duration, number of characters, number of words all are the most important features that one should look at while creating a new project page. \n\n### Pressence of which keywords makes the biggest impact in the predictions?  \n\nUsing permutation importance, we can also evaluate which keywords makes the biggest impact in the model prediction. Let's train another model which also uses keywords used in the project name and observe the permutation importance."
"**Inferences**\n> - From the first plot, we can observe that there are a certain keywords which when used in the project name are likely to increase the probability success of a project. Example -  ""project"", ""film"", and ""community"". While on the other hand, keywords like ""game"", ""love"", ""fashion"" are likely to garner less attraction. This implies that crowdfunding projects related to games or entertainment such as love or fashion may not be very successful as compared to the ones related to art, design etc. \n\n**Note -** It is possible to get different results when run again, thus it is recommended to use this approach on a much bigger dataset. But ofcourse, its not a very big problem, atleast it gives an understanding about the words to focus on. \n\n\n5.3 How does changes in features lead to changes in model outcome? (**Partial Dependencies**)     \n\nSo far we have only talked about which features are most or least important from a pool of many features. For example, we observed that Project Goal, Project Duration, Number of Characters used etc are some of the important features related to project success. In this section, we will look at what are the specific values or ranges of features which leads to project success or failure. Specifically, we will observe that how making changes such as increasing or decreasing the values affect the model outcomes. These effects can be obtained by plotting the partial dependency plots of different features. \n\n### Project Name - Features "
"**Inferences**\n> - From the first plot, we can observe that there are a certain keywords which when used in the project name are likely to increase the probability success of a project. Example -  ""project"", ""film"", and ""community"". While on the other hand, keywords like ""game"", ""love"", ""fashion"" are likely to garner less attraction. This implies that crowdfunding projects related to games or entertainment such as love or fashion may not be very successful as compared to the ones related to art, design etc. \n\n**Note -** It is possible to get different results when run again, thus it is recommended to use this approach on a much bigger dataset. But ofcourse, its not a very big problem, atleast it gives an understanding about the words to focus on. \n\n\n5.3 How does changes in features lead to changes in model outcome? (**Partial Dependencies**)     \n\nSo far we have only talked about which features are most or least important from a pool of many features. For example, we observed that Project Goal, Project Duration, Number of Characters used etc are some of the important features related to project success. In this section, we will look at what are the specific values or ranges of features which leads to project success or failure. Specifically, we will observe that how making changes such as increasing or decreasing the values affect the model outcomes. These effects can be obtained by plotting the partial dependency plots of different features. \n\n### Project Name - Features "
"> We observe that the projects having fewer number of words (<= 3) in the name does not show any improvement in model success. However, if one start increasing the number of words in the project name, the corresponding model improvement also increases linearly. For all the projects having more than 10 words in the name, the model becomes saturate and shows similar predictions. Hence, the ideal word limit is somewhere around 7 - 10.  \n"
> Change in Syllables does not show significant differences in model improvements. \n\nLet's also plot the interaction between number of words and characters used. 
"> From the above plot, it can be observed that about 40 - 65 characters and 10 - 14 words are the good numbers for the project name. \n\n### Project Launched Day and Duration "
"> From the feature definition, category count is a feature which act as the proxy of popularity of a project category. For example, if in Travel category a large number of projects are posted then its category_count will be higher so it is a popular category on Kickstarter. On the other hand, if in the Entertainment category, very rarely someone adds a project, its category_count will be lesser and so is its popularity. From the plot, we can observe that chances that a project will be successful will be higher if it belongs to a popular category. Also holds true for main category. \n\n### How about specific categories ?   \n\nBy ploting the pdp_isolate graph we can also identify the effect of specific project categories. "
"From the partial dependency plot for project category, we observe that the accuracy of model predicting the project success increases if it belongs to ""Music"", ""Comics"",  ""Theater"", or ""Dance"" categories.  It decreases if it belongs to ""Crafts"", ""Fashion Film & Video"". The same insights can be backed from the **actual predictions plot**."
"# How to understand feature importance of categorical features reported by LightGBM?\nLightGBM allows one to specify directly categorical features and handles those internally in a smart way, that might out-perform OHE. Originally, *I was puzzled about feature importance reported for such categorical features*. After  iterating in comments, and learning more about feature importance reported, it seems that:\n\n- **the default implementation is not very useful**, as there are several types of importance and importance values do not behave according to intuitive expectation. See [this blog post](https://towardsdatascience.com/interpretable-machine-learning-with-xgboost-9ec80d148d27) for clear motivation and introduction into SHAP;\n- it is beneficial to use [SHAP package in python](https://github.com/slundberg/shap) to produce stable feature-importance evaluation.\n\nIt all started with abnormally high importance reported for `ORGANIZATION_TYPE` in [an earlier version of my modified fork](https://www.kaggle.com/mlisovyi/modular-good-fun-with-ligthgbm?scriptVersionId=3888846) of  [olivier's](https://www.kaggle.com/ogrellier) very popular [Good_fun_with_LigthGBM kernel](https://www.kaggle.com/ogrellier/good-fun-with-ligthgbm). After some investigation I realized that the problem was due to missing OHE of categorical features (beacuse categorical feature were stores as `categories` instead of `objects`). I fixed that, `ORGANIZATION_TYPE` got OHE-transformed and disappeared from tops of  important features. \n\nThen I started to looking into how to use internal handling of categorical features in LightGBM. It turns out that the **sklearn API of LightGBM actually has those enabled by default**, in a sense that by default it tries to guess which features are categorical, if you provided a `pd.DataFrame` as input (because it has `feature_name='auto', categorical_feature='auto'` as the defaults in the `lgb.LGBMModel.fit()` method). And it makes that guess assuming that all features of type `category` have to be treated with the internal categorical treatment (i.e. following [this procedure from the docs](https://github.com/Microsoft/LightGBM/blob/master/docs/Advanced-Topics.rst#categorical-feature-support)). It turns out that in such case LightGBM reports unexpectedly high importance  in some cases.\n\nBelow is a minimalistic example to reproduce this behaviour and an illustrastion of SHAP usage."
## Read in the basic 'application' data
### Plot feature importance
"Upsss. `ORGANIZATION_TYPE` pops up as the most *important*. But do not celerbate- if you train the same model on the same data with OHE for categorical features you will get the same ROC AUC (and a similar importance for the `EXT_SOURCE_x` features as on this plot), i.e. most likely just importance of `ORGANIZATION_TYPE` is reported wrong, unless i misunderstand something. Any feedback will be helpful for me to make the next step in LightGBM usage."
\nloading all the required libraries.....
\nColor paleete for this notebook...
\nColor paleete for this notebook...
Lets Extract Labels from Image Names...
Lets plot vanila CNN model change in loss and accuracys with epochs ...
Lets see vanila CNN predictions with confusion matrix ...
Lets plot Resnet models change in loss and accuracys with epochs ...
Lets see ResNet predictions with confusion matrix ...
Lets see how our predictions are done on Test Data by both networks...
"\n8. Summary \n\nWhat happened so far?\n\nSo far, I have tried to build a cat and dog classifier with help of deeplearning models, lets see the steps \n\n    Data processing and visualization\n    Build a basic CNN and make prections\n    Explored Data Agumentation and Learning rate Schedule\n    Build a Resnet based Transfer leraning model and make prections\n    final comparision of both models results\n\n\nThank you so much for reading all the way here.....Hope you enjoyed my work.....!!! I am open to suggetions. Please do comment if you any advice or critical comments... Thanks again!!!"
# Step 1: Imports #\n\nWe begin by importing several Python packages.
# Step 2: Distribution Strategy #\n\nA TPU has eight different *cores* and each of these cores acts as its own accelerator. (A TPU is sort of like having eight GPUs in one machine.) We tell TensorFlow how to make use of all these cores at once through a **distribution strategy**. Run the following cell to create the distribution strategy that we'll later apply to our model.
# Step 4: Explore Data #\n\nLet's take a moment to look at some of the images in the dataset.
You can display a single batch of images from a dataset with another of our helper functions. The next cell will turn the dataset into an iterator of batches of 20 images.
"## tuning7, show a sample of data augmented"
"## tuning7, show a sample of data augmented v2"
"## tuning7, show a sample of data augmented v2"
"## tuning7, show a sample of data augmented v3"
"## tuning7, show a sample of data augmented v3"
"# Step 5: Define Model #\n\nNow we're ready to create a neural network for classifying images! We'll use what's known as **transfer learning**. With transfer learning, you reuse part of a pretrained model to get a head-start on a new dataset.\n\nFor this tutorial, we'll to use a model called **VGG16** pretrained on [ImageNet](http://image-net.org/)). Later, you might want to experiment with [other models](https://www.tensorflow.org/api_docs/python/tf/keras/applications) included with Keras. ([Xception](https://www.tensorflow.org/api_docs/python/tf/keras/applications/Xception) wouldn't be a bad choice.)\n\nThe distribution strategy we created earlier contains a [context manager](https://docs.python.org/3/reference/compound_stmts.html#with), `strategy.scope`. This context manager tells TensorFlow how to divide the work of training among the eight TPU cores. When using TensorFlow with a TPU, it's important to define your model in a `strategy.scope()` context."
"# Step 7: Evaluate Predictions #\n\nBefore making your final predictions on the test set, it's a good idea to evaluate your model's predictions on the validation set. This can help you diagnose problems in training or suggest ways your model could be improved. We'll look at two common ways of validation: plotting the **confusion matrix** and **visual validation**."
## Confusion Matrix ##\n\nA [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) shows the actual class of an image tabulated against its predicted class. It is one of the best tools you have for evaluating the performance of a classifier.\n\nThe following cell does some processing on the validation data and then creates the matrix with the `confusion_matrix` function included in [`scikit-learn`](https://scikit-learn.org/stable/index.html).
## Plot model performance report ðŸ˜€
## Plot model performance report in 3D ðŸ˜Ž
## Checking Categorical Missing Values\n\nNumerical missing values were filled in. Let's check again what the remaining missing values are.
" Observation:\n    \n* PoolQC, MiscFeature, Alley, Fence, and FireplaceQu features have too many missing values.\n* Garage-related features have 157 to 159 missing values. It is unknown whether the houses lacked garages or were intentionally omitted.\n* Basement-related features also have 79 to 82 missing values.\nIt seems that we need to focus more on the process of filling in the missing values â€‹â€‹of the corresponding Garage and Basement features."
"----------------------------------------------------------\n# Checking Target\n\nThe problem is a regression problem. Therefore, we analyze the distribution of the target and check whether there is necessary preprocessing based on this. If it is a classification problem\nWe need to check target imbalance."
" Observation:\n\nThe skewness was about 1.88. Also, since the metric is RMSLE, we will perform log scaling."
"-----------------------------------------------------------------------------\n## Continous Features\n\nContinuous variables are numeric variables that have an infinite number of values between any two values. A continuous variable can be numeric or date/time. For example, the length of a part or the date and time a payment is received."
" Observation:\n    \n* GarageCars, BsmtHalfBath, BsmtFullBath and GarageCars are discrete variables.\n* Some features have a skewed shape to one side."
" Observation:\n    \n* GarageCars, BsmtHalfBath, BsmtFullBath and GarageCars are discrete variables.\n* Some features have a skewed shape to one side."
 Observation:\n    \nIt seems that there are outliers that deviate from the regression line. Let's check some more.
"-------------------------------------------\n## Discrete Features\n\nDiscrete variables are numeric variables that have a countable number of values between any two values. A discrete variable is always numeric. For example, the number of customer complaints or the number of flaws or defects."
"In the regression problem, discrete and non-order features must be converted to one-hot encoding. "
------------------------------------------------------------------------\n## Checking Outliers
" Observation:\n\nIf you look at the picture above, you can see an outlier. Regression models are sensitive to outliers, so it is better to remove them. Outlier is like gravity. It pulls the regression line. Therefore, it is better to remove outliers.\nHowever, it is not easy to judge an outlier. Domain knowledge may also be required to remove outliers."
11 outliers were detected as the first principal component.
" Observation:\n    \nLooking at the figure above, points that are clearly judged as outliers in the figures on the left are also judged as outliers in PC1 after PCA.\nThat is, outlier judgment using PCA seems very effective."
----------------------------------------------------------------\n## Checking Outliers after removing outliers
" Observation:\n\nObserving the above figures, it can be seen that many of the points previously judged to be outliers have disappeared. Assuming that the outlier has been removed to some extent, let's try another feature engineer.\nOf course, we can delete outliers directly by looking at each graph, but in doing so, our model reads a lot of generality. The method using PCA is reasonable and can be used generally.\nIt can be used for this problem as well as other problems.\n\nAlso, as the outliers are removed, the regression line seems to be well-fitted to more general data."
## Question 1: Does the combination of underground and above-ground area have a high correlation with the Sale Price?\n\n* TotalBsmtSF: Total square feet of basement area\n* GrLivArea: Above grade (ground) living area square feet
"## Question 2: If you recently remodeled and have a large basement, will your sale price increase?\n\n* TotalBsmtSF: Total square feet of basement area\n* YearRemodAdd: Remodel date (same as construction date if no remodeling or additions)"
"## Question 2: If you recently remodeled and have a large basement, will your sale price increase?\n\n* TotalBsmtSF: Total square feet of basement area\n* YearRemodAdd: Remodel date (same as construction date if no remodeling or additions)"
## Question 3: Can the combined area of the 1st and 2nd floors affect the sale price?\n* 1stFlrSF: First Floor square feet \n* 2ndFlrSF: Second floor square feet
## Question 3: Can the combined area of the 1st and 2nd floors affect the sale price?\n* 1stFlrSF: First Floor square feet \n* 2ndFlrSF: Second floor square feet
**Good derivative features come from good questions. Good questions come from a lot of domain-knowledge.**\n
"First, let's check skewness. A skewness greater than 1 is generally judged to be skewed, so check mainly those greater than 1."
 Observation:\n    \nIt is conformed that 14 features are skewed. We will do log transformation for these features.
The number of skewed features is reduced from 14 to 9. The remaining 4 skewness was greatly reduced.
"Some features still have skewness greater than 1, but further improvement seems difficult."
"## PoolQC: Pool quality \n\n**Question: Does the lack of pool quality mean that the house does not have a pool? If so, is there a relationship between the missing value and the house price?**"
## MiscFeature: Miscellaneous feature not covered in other categories\n\n**Question: Is there a difference in house price with and without miscellaneous features?**
## MiscFeature: Miscellaneous feature not covered in other categories\n\n**Question: Is there a difference in house price with and without miscellaneous features?**
## Alley: Type of alley access to property\n\n**Question: Is there a difference in house price with and without Alley access?**
## Alley: Type of alley access to property\n\n**Question: Is there a difference in house price with and without Alley access?**
## Fence: Fence quality\n\n**Question: Is there a difference in house price with and without fence?**
## Fence: Fence quality\n\n**Question: Is there a difference in house price with and without fence?**
## FireplaceQu: Fireplace quality\n\n**Question: Is there a difference in house prices with and without Fireplace?**
## FireplaceQu: Fireplace quality\n\n**Question: Is there a difference in house prices with and without Fireplace?**
## Garage Features\n\n**Question: Is there a difference in house price with and without Garage?**
## Garage Features\n\n**Question: Is there a difference in house price with and without Garage?**
## Basement features\n\n**Question: Is there a difference in the house price with and without a Basement?**
## Basement features\n\n**Question: Is there a difference in the house price with and without a Basement?**
## MasVnrType: Masonry veneer type\n\n**Question: Is there a difference in house price with and without Masonry veneer?**
## Question 1: Is total house quality correlated with sale price?
## Question 2: Is the total number of bathrooms correlated with the sale price?
## Question 2: Is the total number of bathrooms correlated with the sale price?
## Question 3: Can Total Condition Affect Sale Price?
## Question 3: Can Total Condition Affect Sale Price?
## Question 4: Can area per room affect the sale price?
## Question 4: Can area per room affect the sale price?
"If we ask a good question and generate good derivatives from it, we will train our model further. \n\n**However, I'm not a real estate agent, and I'm not very knowledgeable about it.**"
## Selecting Features
 Observation:\n\n* The newly created derivative variable AllArea feature and house price have a high correlation!\n* Pool-related features and fireplaces with many missing values have a low correlation with house price. \n
## Plotting after dimensionality reduction to 3D
"Even looking at the 3D scaled-down picture, it is difficult to observe the special rules. If we look at it in 202 dimensions, We may be able to find some rules. However, we cannot draw a 202-dimensional picture. \n\n**Now, all we can do is create a good model and make the model learn well.**"
# 1. Articles database  \n\nThis databasecontains information about the assortiment of H&M shops. It's important not to confuse it with the number of transactions for each article what is given in a different database.
"Unique indentifier of an article:\n* ```article_id``` (int64) - an unique 9-digit identifier of the article, 105 542 unique values (as the length of the database)\n\n5 product related columns:\n* ```product_code``` (int64) - 6-digit product code (the first 6 digits of ```article_id```, 47 224 unique values\n* ```prod_name``` (object) - name of a product, 45 875 unique values\n* ```product_type_no``` (int64) - product type number, 131 unique values\n* ```product_type_name``` (object) - name of a product type, equivalent of ```product_type_no```\n* ```product_group_name``` (object) - name of a product group, in total 19 groups\n\n2 columns related to the pattern:\n* ```graphical_appearance_no``` (int64) - code of a pattern, 30 unique values\n* ```graphical_appearance_name``` (object) - name of a pattern, 30 unique values\n\n2 columns related to the color:\n* ```colour_group_code``` (int64) - code of a color, 50 unique values\n* ```colour_group_name``` (object) - name of a color, 50 unique values\n\n4 columns related to perceived colour (general tone):\n* ```perceived_colour_value_id``` - perceived color id, 8 unique values\n* ```perceived_colour_value_name``` - perceived color name, 8 unique values\n* ```perceived_colour_master_id``` - perceived master color id, 20 unique values\n* ```perceived_colour_master_name``` - perceived master color name, 20 unique values\n\n2 columns related to the department:\n* ```department_no``` - department number, 299 unique values\n* ```department_name``` - department name, 299 unique values\n\n4 columns related to the index, which is actually a top-level category:\n* ```index_code``` - index code, 10 unique values\n* ```index_name``` - index name, 10 unique values\n* ```index_group_no``` - index group code, 5 unique values\n* ```index_group_name``` - index group code, 5 unique values\n\n2 columns related to the section:\n* ```section_no``` - section number, 56 unique values\n* ```section_name``` - section name, 56 unique values\n\n2 columns related to the garment group:\n* ```garment_group_n``` - section number, 56 unique values\n* ```garment_group_name``` - section name, 56 unique values\n\n1 column with a detailed description of the article:\n* ```detail_desc``` - 43 404 unique values"
"Only one column - ```detail desc``` - has missing values but this is a very small fraction of the dataset - about 0.4%.\n\nLet's visualise some of the articles. To do so I will create a helper function below. As mentioned in the competition description not all articles have an image. Therefore, this function will show selected amount of images from a given folder. It will also write an article name. What I've found is that a leading zero in the name of the folder does not correspond to the first digit of the ```article_id``` and it has to be stripped when looking for a product in the articles database. E.g.: *0108775015.jpg* is ```article_id``` *108775015*."
In a hidden cell below there's a helper function for plotting sorted horizontal barplots.
Nice way to visualise descriptions is to create a word cloud. To do this we have to tokenize the descriptions and later join all tokens into a bag of words or like in this case into a one long text.
"# 2. Customers database\n\nThis database contains data about customers. Collecting this type of data allows company to tune their recommender system. This data contains data which can be treated as 'static' or slowly-changing, usually these are features like sex, age, address, hight, etc. Let's look what H&M gave us."
"# 2. Customers database\n\nThis database contains data about customers. Collecting this type of data allows company to tune their recommender system. This data contains data which can be treated as 'static' or slowly-changing, usually these are features like sex, age, address, hight, etc. Let's look what H&M gave us."
"Unique indentifier of a customer:\n* ```customer_id``` - an unique identifier of the customer\n\n5 product related columns:\n* ```FN``` - binary feature (1 or NaN)\n* ```Active``` - binary feature (1 or NaN)\n* ```club_member_status``` - status in a club, 3 unique values\n* ```fashion_news_frequency``` - frequency of sending communication to the customer, 4 unique values\n* ```age```  - age of the customer\n* ```postal_code``` - postal code (anonimized), 352 899 unique values"
"As we remember from the missing values analysis there is also a group of people where we do not have any data. Perhaps, these people are the ones without membership at all - to add them to the visialisation I'll fill NaN with 'N/A'."
"Most of customers have an ```active``` membership status, others are with the ```pre-create``` status. Interestingly, there is nobody with ```left club``` status."
We see here that there are two statuses that can be merged: *NONE* and *None* to improve data quality and to reduce one dimension. Most of customers do not receive any communication from H&M.\n\nLet's see now what is the age distribution.
The distribution shows that there are two main age-groups of customers: around 20-30 years old and 45-55 years old. Let's check how old is the oldest customer.
# 3. Transactions\n\nThis is the biggest database containing all transactions every day.
"Columns description:\n* ```t_dat``` - date of a transaction in format YYYY-MM-DD but provided as a string\n* ```customer_id``` - identifier of the customer which can be mapped to the ```customer_id```  column in the ```customers``` table\n* ```article_id``` - identifier of the product which can be mapped to the ```article_id```  column in the ```articles``` table\n* ```price``` - price paid\n* ```sales_channel_id``` - sales channel, 2 unique values"
No missing data!\n\nLet's investigate the price column.
It's clear from the above graph that we have a lot o outliers. Let's look at the price after cutting the values above 0.1.
It's clear from the above graph that we have a lot o outliers. Let's look at the price after cutting the values above 0.1.
Now let's see the data distribution over time. First what dates range is provided. It will be usefull to change the datatype of 
"The bar chart above show us that per day usuall number of transactions lays in range about between 25 000 and 80 000 transactions per day. We see also that sales spikes during summertime and drops during winter.\n\nNow, let's see how many transactions, on average, customers do."
Clearly there's a lot of outliers. Let's look at the distribution after cutting everything above 50 trasactions per customer.
Clearly there's a lot of outliers. Let's look at the distribution after cutting everything above 50 trasactions per customer.
"The graph above shows us that most of customers, on average, bought only few items during these 2 years.\n\nLet's see now the popularity of sale channels."
"The graph above shows us that most of customers, on average, bought only few items during these 2 years.\n\nLet's see now the popularity of sale channels."
# 4. Combined databases EDA
  \n    Importing Python Libraries ðŸ“• ðŸ“— ðŸ“˜ ðŸ“™\n    \n
  \n    Sample Images\n    \n
  \n    Sample Images\n    \n
  \n    The above sample Images after applying preprocessing Technique which is  equalize histogram\n    \n
"\n### 1.2. Scope of analysis\n\nAmong the many questions that clutter the mind, the one that occupies the top spot is this:. \nWho does this survey data respresent? In this section, we try to address this very question by looking at the sample data (2020 survey data), the auxiliary documents (viz. the `kaggle_survey_2020_methodology.pdf` and the`kaggle_survey_2020_answer_choices.pdf`), the meta-kaggle datasets. We argue that given the survey methodology ([see detailed below](#method)), most of the survey respondents are active Kaggle users and therefore represent the actively involved data-enthusiasts and data-science-experts (especially the ones who are involved enough to fill up a long multiple choice/ multiple selection survey).\n\nAbout the meta-kaggle data: Along with many other details, the meta-kaggle datasets contain the list of Kaggle 'Users' with their registration date (on Kaggle), the 'User Achievements', and the list of 'Submissions' made by the users (with data of submission). As expected the 'Users' data shows exponetial growth in Kaggle userbase over time - since inception (2010), every year, the number of new registrations on the platform has multiplied; starting with a humble 4558 users, today kaggle userbase is reaching a humongous ~6 million! Out of these ~6M users, more than a third registered in 2020 itself. "
"However compared to these huge numbers, the number of users who participated in the survey in 2020 seems too tiny at ~20k, and indicates that only a fraction of these ~6M registered users might be  actively involved in the community. To confirm the same, we chart out the number of tiered users. Turns out that as suspected, **less than 2% of all registered users, i.e. 102419 users on Kaggle are tiered** (*There are 4 performance tiers - contributor, expert, master, and grandmaster*).\n\n\n---\n\n\n\n\nThis year the survey-invitation was sent out to the community via e-mails (anyone who\nopted-in to the Kaggle Email List was invited). The survey was also promoted on the Kaggle website and on the Kaggle Twitter channel. So the target audience included both \n* professionals and non-professionals (students/ unemployed/ ones who never spent any money on cloud)\n* males/ females/ LGBTQA+\n* 18 years and older\n* people with or without any formal education\n* people residing in 171 countries\n\nThe 20,036 people who responded, can accordingly be grouped under any of these broad categories or a combintion thereof.\n\n[Source: `kaggle_survey_2020_methodology.pdf`]\n\nNOTE: Though a respondent did not need to be registered Kaggle user to fill up the Kaggle survey, because the survey was promoted on the Kaggle website and the Kaggle Twitter channel and the invitations to participate in the survey were sent to anyone who\nopted-in to the Kaggle Email List, it seems highly likely that most survey respondents are registered users because otherwise they would have been unlikely to see any of the survey promotions.\n"
"However compared to these huge numbers, the number of users who participated in the survey in 2020 seems too tiny at ~20k, and indicates that only a fraction of these ~6M registered users might be  actively involved in the community. To confirm the same, we chart out the number of tiered users. Turns out that as suspected, **less than 2% of all registered users, i.e. 102419 users on Kaggle are tiered** (*There are 4 performance tiers - contributor, expert, master, and grandmaster*).\n\n\n---\n\n\n\n\nThis year the survey-invitation was sent out to the community via e-mails (anyone who\nopted-in to the Kaggle Email List was invited). The survey was also promoted on the Kaggle website and on the Kaggle Twitter channel. So the target audience included both \n* professionals and non-professionals (students/ unemployed/ ones who never spent any money on cloud)\n* males/ females/ LGBTQA+\n* 18 years and older\n* people with or without any formal education\n* people residing in 171 countries\n\nThe 20,036 people who responded, can accordingly be grouped under any of these broad categories or a combintion thereof.\n\n[Source: `kaggle_survey_2020_methodology.pdf`]\n\nNOTE: Though a respondent did not need to be registered Kaggle user to fill up the Kaggle survey, because the survey was promoted on the Kaggle website and the Kaggle Twitter channel and the invitations to participate in the survey were sent to anyone who\nopted-in to the Kaggle Email List, it seems highly likely that most survey respondents are registered users because otherwise they would have been unlikely to see any of the survey promotions.\n"
"Similarly, the number of users who made at least one submission in that year, are consistently low. Less than 90k users have made any submission on Kaggle in 2020. So turns out that every year, the number of users on Kaggle who made at least one submission in the year is at most about 5 times the number of people who filled up the Kaggle survey in that year.\nTakeaway: Based on the number of respondents, the sample seems to be large enough  to be representative of the active Kaggle users' community (and therfore the data science community at large).\n\n\n---\n\nActive users: For the purpose of this analyis we have labeled a user as active in any particular year if s/he made at least 1 submission on Kaggle that year."
"Similarly, the number of users who made at least one submission in that year, are consistently low. Less than 90k users have made any submission on Kaggle in 2020. So turns out that every year, the number of users on Kaggle who made at least one submission in the year is at most about 5 times the number of people who filled up the Kaggle survey in that year.\nTakeaway: Based on the number of respondents, the sample seems to be large enough  to be representative of the active Kaggle users' community (and therfore the data science community at large).\n\n\n---\n\nActive users: For the purpose of this analyis we have labeled a user as active in any particular year if s/he made at least 1 submission on Kaggle that year."
"#### Response rate - question-wise and participant-wise:\n\nNow that the total number of survey participants has been put in perspective, couple of questions regarding the 2020 survey demand our attention next: \n\n1. How many of the survey-questions did each survey-respondent answer?\n2. How many of the survey-respondents did answer any particular survey-question?\n\nRegarding the first question, we find that in 2020, approximately, \n* 15k participants (75% of the respondents) answered half the questions. \n* 10k participants (half of the respondents) answered 30 or more questions.\n* a thousand of the respondents i.e. ~5% of all respondents responded to 5 questions or less.\n\n---\n\nNote: Thoughout this analysis, 'NA' responses are considered as missing responses.\n"
"#### Response rate - question-wise and participant-wise:\n\nNow that the total number of survey participants has been put in perspective, couple of questions regarding the 2020 survey demand our attention next: \n\n1. How many of the survey-questions did each survey-respondent answer?\n2. How many of the survey-respondents did answer any particular survey-question?\n\nRegarding the first question, we find that in 2020, approximately, \n* 15k participants (75% of the respondents) answered half the questions. \n* 10k participants (half of the respondents) answered 30 or more questions.\n* a thousand of the respondents i.e. ~5% of all respondents responded to 5 questions or less.\n\n---\n\nNote: Thoughout this analysis, 'NA' responses are considered as missing responses.\n"
"Before addressing the second question (i.e. what percentage of respondents answered a particular question), it is important to note that -\n* 7 of the questions were follow-up questions and therefore shown to a few respondents only (selected based on their response to the related mandatory questions). These follow up are: Q18, Q19, Q27A, Q28A, Q30, Q32, and Q34.\n* 8 questions (Q26-Q29, Q31, and Q33-35) all had two versions (A, and B) - version A was for the professionals and version B was for the non-professionals (as discussed earlier in [[1](#s1)])\nSo throughout this analysis, we combine the responses to the two versions of each of these questions wherever needed.\n\nSo given all these details, it is but natural that the questions with worst response rates are the follow-up questions (which were asked to a very few respondents to begin with and therefore only a tiny fraction of the 2020 survey respondents answered these questions).\n\nIt is also worth noting that:\n* The demographic-questions, like age, gender, country of residence, etc. have the highest response rate i.e. most respondents answered these questions properly (with non-NA replies).\n* Most respondents (more than 4-out-of-5) also answered all the questions relating to their current designation and programming experience (languages, notebooks, IDEs, etc.)\n* Apart from the follow-up questions (which naturally have low response rate), the questions relating to professional details and practices, and paid services (e.g. team size, company size, compensation, Auto ML/ cloud products and services,etc.) have relatively poor response rate. \n\nNote: The question-wise response rates clearly hint at a high percentage of non-professionals (students, unemployed, and people who have never spent any money on cloud/ML products/services) among the respondents. ðŸ’­\n"
"Before addressing the second question (i.e. what percentage of respondents answered a particular question), it is important to note that -\n* 7 of the questions were follow-up questions and therefore shown to a few respondents only (selected based on their response to the related mandatory questions). These follow up are: Q18, Q19, Q27A, Q28A, Q30, Q32, and Q34.\n* 8 questions (Q26-Q29, Q31, and Q33-35) all had two versions (A, and B) - version A was for the professionals and version B was for the non-professionals (as discussed earlier in [[1](#s1)])\nSo throughout this analysis, we combine the responses to the two versions of each of these questions wherever needed.\n\nSo given all these details, it is but natural that the questions with worst response rates are the follow-up questions (which were asked to a very few respondents to begin with and therefore only a tiny fraction of the 2020 survey respondents answered these questions).\n\nIt is also worth noting that:\n* The demographic-questions, like age, gender, country of residence, etc. have the highest response rate i.e. most respondents answered these questions properly (with non-NA replies).\n* Most respondents (more than 4-out-of-5) also answered all the questions relating to their current designation and programming experience (languages, notebooks, IDEs, etc.)\n* Apart from the follow-up questions (which naturally have low response rate), the questions relating to professional details and practices, and paid services (e.g. team size, company size, compensation, Auto ML/ cloud products and services,etc.) have relatively poor response rate. \n\nNote: The question-wise response rates clearly hint at a high percentage of non-professionals (students, unemployed, and people who have never spent any money on cloud/ML products/services) among the respondents. ðŸ’­\n"
"Time taken to complete the survey:\n\nBefore moving on further, it might be worth noting that while the survey seems to be longish, the median user took about 10 mins only to complete the survey, and even the median professional user, who by design was faced with more questions (relating to their profession and data-science related spending behavior and experiences), completed the survey in only 12.5 minutes. ~85% repondents completed the 2020 survey in less than half an hour.\n\n\n---\n\nNote that, 95% of all respondents completed the 2020 survey in less than 4 hours. So we have removed the respondents who took more than 4 hrs to complete the survey while creating the following chart. "
"\n### 2.1. Gender distribution\nAccoring to [United Nations](https://population.un.org/wpp/Download/Standard/Population) estimates, the world housed 49.58% females, as of 1 July 2020. Compared to that, only 21% of the survey participants in 2020 identified themselved as female/ LGBTQA+! So as far as gender-equality is concerned, we as a community, have a lot to do.  On the brighter side however, this **21% participation rate in 2020** implies a significant **improvement (by 3%)** over the previous year (2019), when the female/ LGBTQA+ survey participation rate was 18%!"
"On dissecting the data further, we find that the improvement in the gender-distribution data comes purely from the female segment (and not LGBTQA+) of participants."
"On dissecting the data further, we find that the improvement in the gender-distribution data comes purely from the female segment (and not LGBTQA+) of participants."
The improvement in gender-distribution in the community in 2020 becomes even more pronounced when we look at all 4 years of survey data (adjusted for the total number of participants in each of those years). We find that the **female participation rate in 2020 has seen a sharp 3% improvement this year** and after remaining stable for past three years (2017-2019). \n\nTakeaway: Female Kagglers are on the rise. ðŸ“ˆ
The improvement in gender-distribution in the community in 2020 becomes even more pronounced when we look at all 4 years of survey data (adjusted for the total number of participants in each of those years). We find that the **female participation rate in 2020 has seen a sharp 3% improvement this year** and after remaining stable for past three years (2017-2019). \n\nTakeaway: Female Kagglers are on the rise. ðŸ“ˆ
[Go back to the top](#qa)
"\n### 2.2. Country-wise expanse\nGiven the enormous amount of data being generated every day (source: [visualcapitalist](https://www.visualcapitalist.com/wp-content/uploads/2019/04/data-generated-each-day-full.html)), it is but obvious that the world is in need for a large number of sufficiently trained people capable of handling and utilizing it. The natural question then is how well-distributed is our community? Do we have enough people distributed around the world addressing the world's data needs or are there specific data-expert-hubs, that could cater to the data science needs of the rest of the world.  This year (2020), the survey was sent out to Kagglers in 171 countries, and to protect the identities of the survey-respondents, countries with less than 50 respondents were grouped together under the common country bucket - 'Other'. As a result, in 2020, we had *54 countries*, each with 50 or more survey-participants and another group of countries, under the name 'Other'.\nCharting world population estimates , as of 1 July 2020, from [United Nations](https://population.un.org/wpp/Download/Standard/Population) and the country-wise survey partication on world maps placed side-by-side, it becomes clear that -\n* At ~30%, India has the highest head-count contribution to the 2020 survey. This is much greater than India's contribution to the world population, which currently stands at ~17.5%\n* While less than 5% of the world's population resides in the U.S.A, and yet more than 11% of the 2020 survey participants reside in the U.S.A.\n* The world's largest country by population, China (>20%), has a suspiciously low participation rate in the 2020 survey (<2.5%). Given that China is one of the leading nations when it comes to data science according to different media sources including [Analytics Insight](https://www.analyticsinsight.net/countries-which-hold-the-greatest-opportunities-for-data-scientists), this low rate of participation from China seems to point at a cultural difference between different countries regarding survey participation. \n\n*Note that for the world population chart, we have used only the data pertaining to people in the age bracket of 15 years or older, since our survey data includes people of age (i.e. 18 years or older only). We could not use 18 years as the minimum age for the UN world-popuation data, because the in raw data from UN, the people in the age bucket 15-19 years were grouped together.*"
"Looking at the historical survey data (2017-20), we find that consistently :\n* India has replaced U.S.A. to become the home to the largest number of Kaggle survey participants. (Almost a third of the 2020 survey participants live in India).\n* With more than 1-in-10 participants residing in the U.S.A., U.S.A. is currently home to the second largest number of participants.\n* ~40% of the survey participants live in just two countries, India and U.S.A.\n* ~Half of the survey participants come from just 5 countries every year.\n* Russia with ~3% of the survey participants has also consistently featured in the list of top 5 countries where the participants lived in 2017-20. "
"Looking at the historical survey data (2017-20), we find that consistently :\n* India has replaced U.S.A. to become the home to the largest number of Kaggle survey participants. (Almost a third of the 2020 survey participants live in India).\n* With more than 1-in-10 participants residing in the U.S.A., U.S.A. is currently home to the second largest number of participants.\n* ~40% of the survey participants live in just two countries, India and U.S.A.\n* ~Half of the survey participants come from just 5 countries every year.\n* Russia with ~3% of the survey participants has also consistently featured in the list of top 5 countries where the participants lived in 2017-20. "
"Looking at the female/LGBTQA+ participants only, we find that \n* India and U.S.A. again feature at the top and in that order.\n* U.K. consistently comes third with ~3% of the female/LGBTQA+ participants.\n* Russia did not feature among the top 5 countries after 2018.\n* ~Half of the female/LGBTQA+ reside in just the top 2 countries, viz. India and U.S.A.\n* Interestingly, Turkey with 95 female/LGBTQA+ respondants has stormed its way into the list of top 5 countries for female/LGBTQA+ users in the 2020."
"Looking at the female/LGBTQA+ participants only, we find that \n* India and U.S.A. again feature at the top and in that order.\n* U.K. consistently comes third with ~3% of the female/LGBTQA+ participants.\n* Russia did not feature among the top 5 countries after 2018.\n* ~Half of the female/LGBTQA+ reside in just the top 2 countries, viz. India and U.S.A.\n* Interestingly, Turkey with 95 female/LGBTQA+ respondants has stormed its way into the list of top 5 countries for female/LGBTQA+ users in the 2020."
A closer look reveals that among the 10 countries with largest number of female/LGBTQA+ participants in 2020:\n* Indonesia(30%) and Turkey (27%) have the highest share of female/LGBTQA+ participants.\n* Brazil(13%) and Russia(15%) have the lowest share of female/LGBTQA+ participants.\n* Only 5 out of these top 10 countries have average or above-average share of female/LGBTQA+ respondents ([21% of all respondents are female/LGBTQA+](#q3)).\n\nTakeaway: The jump in the proportion of female respondents is largely driven by the rise in female participation from India.
A closer look reveals that among the 10 countries with largest number of female/LGBTQA+ participants in 2020:\n* Indonesia(30%) and Turkey (27%) have the highest share of female/LGBTQA+ participants.\n* Brazil(13%) and Russia(15%) have the lowest share of female/LGBTQA+ participants.\n* Only 5 out of these top 10 countries have average or above-average share of female/LGBTQA+ respondents ([21% of all respondents are female/LGBTQA+](#q3)).\n\nTakeaway: The jump in the proportion of female respondents is largely driven by the rise in female participation from India.
"**India vs. U.S.A: A comparative analysis of the paticipation trends**\n\n* As already noted previously, there is a conistently growing number of Indians among the survey-participants (29% in 2020). \n* The same holds true for the cohort of female/LGBTQA+ survey-participants; Indians account for 32% of the female/LGBTQA+ survey participants.\n* Meanwhile, it is a bit surprising to see that even based on the absolute numbers of survey-participants, Americans are a shrinking community as far as the Kaggle surveys are concerned. \n* Another positive takeaway from India: the share of female/LGBTQA+ respondents in India saw a sharp jump this year (thus taking the number of female/LGBTQA+ among every 100 Indian survey-participants to 23, straight up from 17 just a year ago!)\n* The share of female/LGBTQA+ respondents in the U.S.A. meanwhile remained stable at ~25%\n\n\n---\n\nThe receding number of respondents from U.S.A. adds to our inhibition that there might be a fewer number of repeat survey-respondents i.e. people who submit the survey once, might be less enthusiastic about filling it up again. This is just a hunch and at the moment unverifiable using the available data. \nRelevant suggestions:ðŸ’¡\n- Going forward, it might be worthwhile to ask the respondents to select whether or not they filled the Kaggle survey in any of the previous years.\n- A prefilled survey (with data from the user's public Kaggle profile) could also be provided to the users to encourage more users to respond to the annual survey."
"**India vs. U.S.A: A comparative analysis of the paticipation trends**\n\n* As already noted previously, there is a conistently growing number of Indians among the survey-participants (29% in 2020). \n* The same holds true for the cohort of female/LGBTQA+ survey-participants; Indians account for 32% of the female/LGBTQA+ survey participants.\n* Meanwhile, it is a bit surprising to see that even based on the absolute numbers of survey-participants, Americans are a shrinking community as far as the Kaggle surveys are concerned. \n* Another positive takeaway from India: the share of female/LGBTQA+ respondents in India saw a sharp jump this year (thus taking the number of female/LGBTQA+ among every 100 Indian survey-participants to 23, straight up from 17 just a year ago!)\n* The share of female/LGBTQA+ respondents in the U.S.A. meanwhile remained stable at ~25%\n\n\n---\n\nThe receding number of respondents from U.S.A. adds to our inhibition that there might be a fewer number of repeat survey-respondents i.e. people who submit the survey once, might be less enthusiastic about filling it up again. This is just a hunch and at the moment unverifiable using the available data. \nRelevant suggestions:ðŸ’¡\n- Going forward, it might be worthwhile to ask the respondents to select whether or not they filled the Kaggle survey in any of the previous years.\n- A prefilled survey (with data from the user's public Kaggle profile) could also be provided to the users to encourage more users to respond to the annual survey."
[Go back to the top](#qa)
"\n### 2.3. Age distribution\nComparing all four years of data, it is clear that on an overall basis, age-group-wise:\n* Historically, (25-29) year old paricipants formed the single largest user-group.\n* The <=21 and (22-24) year old groups are catching up fast. \n* So combining these three age buckets, we find that more than half of the participants are less than 30 years old.\n* Less than 10% respondents are above 50.\n* The <=21 year old cohort is the only consistently expanding age-group. \n\n---\n\n2018 onward, the survey-participants were asked to select their agegroup (out of **10 options**). Only in 2017, the respondents were given a free-choice to specify their age. As a result, in 2017, the respondents had an option of not specifying their age (which ~20% of the people exercised). For the purpose of this analysis therefore, we disregarded the participants who did not specify their age (so the data for 2017 consists of only ~13.5k (instead of 16.7k) users.  \n"
"Limiting our analysis to the female/LGBTQA+ respondents, we find that:\n* 63% of all female/LGBTQA+ respondents are below 40 (vs 56% below 40 out of all survey-respondents).\n* The trend however remains largely same among female/LGBTQA+ participants as well."
"Limiting our analysis to the female/LGBTQA+ respondents, we find that:\n* 63% of all female/LGBTQA+ respondents are below 40 (vs 56% below 40 out of all survey-respondents).\n* The trend however remains largely same among female/LGBTQA+ participants as well."
"Dissecting the data further, we find that among the top 10 countries with largest number of 2020 survey participants: \n* India has an overwhelmingly high proportion of very young (<=21 year old) survey respondents. \n* Apart from India, China, Nigeria, and Turkey also have a lot of young respondents (younger than 30).\n* On the other end of the spectrum, most of the respondents from countries like U.S.A,, Japan, U.K., and Germany are above 30.  \n\n---\n\n\n*Note that the following table shows the percentage of respondents in each age-group in each of the top 10 countries (by total number of respondents in 2020). Thus, for instance, it shows that 79% of all Indian survey-participants in 2020 are aged 21 years or less, and only 2% of all survey-participants in 2020 were aged 60 years or above.*"
"Looking back at the historical data (2017-20) for India and U.S.A., we find that:\n* The young Indians (<=21 year old) are the most consistently growing bunch.\n* In comparison, the U.S.A. cohort seems to be much more stagnant with minor growth in the older age-groups.\n* Currently, more than a third of the Indian respondents are <=21 year old (vs only 5% users in U.S.A aged <=21 years). \n* The users in the U.S.A. are much more uniformly distributed across different age groups.\n* And a significant 8% of the users in the U.S.A. are aged 60 years or above.\n\nThe age-distribution is largely similar among female/LGBTQA+ Kagglers as well, though -\n* In India: \n    - female/LGBTQA+ are still younger (2-out-of-3 below 25 year old).\n    - In India, only 62 female/LGBTQA+ Kagglers are 40 years or above.\n    - there are no female/LGBTQA+ Kagglers older than 60 years.\n* In U.S.A:\n    - as well, very few women (only 40 female/LGBTQA+ respondents) are in 55 years old or above."
[Go back to the top](#qa)
"\n### 2.4. Formal education\nWhile people with masters degree continue to dominate the Kaggling community, the number of users with bachelors degree has started to catch up. There is a sharp drop in the percentage of docatoral candidates this year. Very few users (< 7%) possess no formal degree at all and roughly about 3% users have a professional degree."
"**Takeaways: **\n1. Users with higher degrees dominate Kaggle for the time being (more than half of the users have doctoral, master's, or professional degrees).\n2. Users with only a bachelor's degree or no degree at all are fast catching up (reaching 50%).\n3. The trend is similar across genders.\n\nThese developments seem to directly connect with the growing number of young respondents from India."
"**Takeaways: **\n1. Users with higher degrees dominate Kaggle for the time being (more than half of the users have doctoral, master's, or professional degrees).\n2. Users with only a bachelor's degree or no degree at all are fast catching up (reaching 50%).\n3. The trend is similar across genders.\n\nThese developments seem to directly connect with the growing number of young respondents from India."
"* As expected, more than half of the Indian responents in 2020 hold a Bachelors degree.\n* 1-in-4 respondents from U.K. and Germany have a doctoral degree.\n* About 1-in-10 of the respondents from Japan, Russia, and China are college drop-outs (i.e. studied at some college/ university without any degree). "
"* Majority of the Kagglers in India only have a Bachelor's degree or no degree at all.\n* On the contrary, majority of the Kagglers in U.S.A have a higher degree (master's, doctoral or professional).\n* It is interesting to note that, in India, on a percentage basis, female/LGBTQA+ respondents are consitently more qualified (have a masters degree or higher) compared to their male counterparts. 49% females/ LGBTQA+ respondents have a higher degree (masters, doctoral, or professional) vs. 40% male respondents with higher degrees in India. \n\nTakeaway\n\n* A lot of very young Indians are getting involved in Kaggling. Given their age, they are mostly students/ youngsters with Bachelor's degree only. Given that these young Indians constitute a significantly large portion of the overall user base, their demographics dominate the survey data (and trends) on the whole. \n\n* Though the overall trend is similar across genders, it seems that in India in particular, fewer women/ LGBTQA+ with only a Bachelor's degree (or no degree at all) have access to Kaggle.\n"
[Go back to the top](#qa)
"\n### 2.5. Profession (including compensation, company/ team size, and job profile)\n\n* 1-in-4 male respondents is a student.\n* A slightly higher proportion (1-in-3) female/ LGBTQA+ Kagglers is student.\n* Among the professionals, as expected, majority are data scientists (~1-in-10).\n* Software engineers form the next most dominant group (overall).\n* A sizeable portion of the respondents is also currently not employed (about 1-in-10). \n\nNote: \n* Compared to the group of male users, the group of female/ LGBTQA+ users have a higher proportion of students and unemployed people (i.e. non-professionals).  \n* Overall 27+9=36% are either students or unemployed. \n* Another ~4% (759 respondents) left their profession as 'NA' and are therefore left out of any analyses involving the profession of the respondents. "
"Looking at the country-wise data, we find that:\n* Both India (40%) and China (50%) have high percantage of student respondents.\n* In each of Nigeria (15%), Russia (10%) and India (10%), at least 1-in-10 respondents is currently not employed."
"* As expected, data analysis is the most common data responsibility and is undertaken by more than half of all professionals who have at least one data responsibility\n* Building prototypes to explore applying machine learning to new areas is the next most common data-task at daily jobs for the professional respondents.\n\nIt is especially promising to see high proportion of professional involved in building prototypes to find new application areas for ML. With this finding in place, one can be doubly sure of the future growth prospects in the industrial use of data-science and especially machine learning."
"Size of employer companies:\n\n* More than a third of the professionals in the data community are employed with small startups with less than 50 employees.\n* Of these professionals, ML engineers stand out as more than half of these ML enineers are employed with small startups with less than 50 employees.\n* Large and very large companies are much more likely to have professionals in traditional data roles like Data Engineer, Data Scintist, Software Engineer, and Business Analyst.\n\n\n---\n\n\nðŸ’­ These observations seem to suggest that much of the cutting edge data science work is being undertaken at small startups. The largest firms (with more than 10k employees), meanwhile, are taking a more traditional approach with their data."
"By looking at each particular programming language indiviadually, we find that:\n* With 78% of the respondents using Python on a regular basis, it is the undisputed favorite programming language for the data-science community\n* SQL and R are also used regularly by a sizable portion of the population.\n* More than 25% those who use these three languages, (viz. Python, SQl, and R), do so exclusively. This goes to show how powerful these languages are, even on a standalone basis.\n* All the rest of the languages specified in teh survey are predominantly used alongside with one or more of the other programming languages.\n* Julia, and Swift, for instance, are only used by a tiny minority, and that too in unison with a host of other programming languages. "
"Over the years, Python is gaining ground as the community favorite (even as the recommended first programming language for aspiring data scientists); meanwhile, R is phasing out."
"Over the years, Python is gaining ground as the community favorite (even as the recommended first programming language for aspiring data scientists); meanwhile, R is phasing out."
[Go back to the top](#qa)
"Among the list of IDEs provided in the 2020 survey -\n* Jupyter is the community favorite.\n* Visual studio and Pycharm are next in line.\n* ~1-in-5 users, use a single IDE exclusivel, and very few use more than 3 IDE. This clearly shows how powerful and self-sufficient each of these IDEs are.  "
[Go back to the top](#qa)
"\n### 4.1. Machine Learning experience, in years\n\n* In India, 88% female/ LGBTQA+ users and 84% male users have ML experience of only 2 years or less.\n* Half of all users in India, male or female/ LGBTQA+, have less than 1 year of ML experience.\n* Compared to users in India, users in U.S.A. are more experienced in ML, especially the U.S.A. males. (Male users from U.S.A. are the most experienced in ML, among all users in India and U.S.A.)\n\n**Takeaway**\nConsistent with the previous analysis, we find that between India ans U.S.A., the male users from U.S.A. are most experienced, followed by the female/ LGBTQA+ users. Indian users are mostly younger with fewer years of coding experience and ML experience as well.\n"
[Go back to the top](#qa)
Let's start by loading all necessary libraries:
"### Part 1. Toy dataset ""Will They? Won't They?"""
"Data Scientists & Analysts: What's the difference?\n\nIntroduction \n\nThe WEF 2020 Future of Jobs Report lists Data Analysts and Data Scientists as the highest emerging job roles of the decade. The report, like many others, groups the professions of data scientists and analysts together, regarding them to be one and the same. This is part of the common trend that people fail to understand how the two professions differ, often them lumping both these data professions together for all intents and purposes.\n\nA simliar situation is how the terms Artificial Intelligence(AI), deep learning, and machine learning are used interchangeably in the news and spurious marketing campaigns. A similar situation is encountered when trying to understand the difference between data scientists and data analysts. "
"Even a simple search for â€œdata scientist vs data analystâ€ shows no shortage of articles which delve into this common misunderstanding. Clearly this is a problem that a lot of people seem to have...\n\n    \n\n    \n    \nScouring through these 50 odd search results youâ€™ll find multiple cases where the definitions of the two terms overlap, offering little to no clarity at times. In worse cases the two are even used as synonyms for one another. Clearly even turning to Google to give us a cut and dry answer is problematic!\n\nOne of the major reasons for this is that over the past decade terms like data analysis, have **attained the status of buzzwords**. With more businesses and online content creators getting involved in the hype around data science and analytics, these buzzwords are thrown around with little care for correct terminology.\n    \n Source - Google Trends data for data science, machine learning and data analysis. "
"Even a simple search for â€œdata scientist vs data analystâ€ shows no shortage of articles which delve into this common misunderstanding. Clearly this is a problem that a lot of people seem to have...\n\n    \n\n    \n    \nScouring through these 50 odd search results youâ€™ll find multiple cases where the definitions of the two terms overlap, offering little to no clarity at times. In worse cases the two are even used as synonyms for one another. Clearly even turning to Google to give us a cut and dry answer is problematic!\n\nOne of the major reasons for this is that over the past decade terms like data analysis, have **attained the status of buzzwords**. With more businesses and online content creators getting involved in the hype around data science and analytics, these buzzwords are thrown around with little care for correct terminology.\n    \n Source - Google Trends data for data science, machine learning and data analysis. "
"And indeed, searches for data science related terms have taken off over the past decade. Some of the popular related search terms include.\n\n Source - Google Trends data for data science and data analysis - ""Related queries"" section \n\n\n \n    Data Scientistrelated queries     Data Analystrelated queries  \n  1  data scientist salary   1  big data analytics  \n  2  data science jobs   2  data analytics certificate  \n  3  machine learning   3  coursera  \n  4  data science online   4  data analytics meaning  \n  5  master data science   5  ms data analytics  \n  6  data science courses   6  how to become data analyst  \n  7  data analyst   7  data analytics career  \n  8  big data   8  data analysis excel 2013  \n  9  data science salary   9  mba in data analytics  \n  10  r data science   10  udacity  \n\n"
"With degrees and online courses on both topics being incredibly popular at the moment, not having a clear understanding of what each profession entails causes further issues down the line. Students, especially those planning on pursuing them for graduate studies, should understand **what skills each field will teach them** and the **job opportunities** that will be opened up to them as a result of their chosen path.\n\nWhen looking at Google trend data, we see how interest in these fields of learning has increased dramatically over the years.\n\n Source - Google Trends data for data science, machine learning and data analysis courses. "
"The Basics\n\nData Analysts\n\nSo what do data analysts do? Well, in simple terms they examine large datasets, generate insights and present their findings to help organizations make better decisions. It is their job to discover, interpret and communicate meaningful patterns and trends in the data to a non-technical audience. \n\nHowever the job isnâ€™t as simple as playing around with perfect data to answer business questions. To get to this stage analysts work closely with teams to manage collection and storage of data, cleaning the obtained data, at times defining processes to automate these tasks.\n\n Source: Kaggle ML & DS Survey 2021 - Q24. Select any activities that make up an important part of your role at work. "
"The Basics\n\nData Analysts\n\nSo what do data analysts do? Well, in simple terms they examine large datasets, generate insights and present their findings to help organizations make better decisions. It is their job to discover, interpret and communicate meaningful patterns and trends in the data to a non-technical audience. \n\nHowever the job isnâ€™t as simple as playing around with perfect data to answer business questions. To get to this stage analysts work closely with teams to manage collection and storage of data, cleaning the obtained data, at times defining processes to automate these tasks.\n\n Source: Kaggle ML & DS Survey 2021 - Q24. Select any activities that make up an important part of your role at work. "
"Data Scientists\nData science has had various definitions thrown around over the years, but in essence it describes a field which uses a combination of mathematics, statistics and machine learning to clean, process and interpret data to extract insights from it. They design and construct new processes for data modelling and production using prototypes, algorithms, predictive models and custom analysis.\n\n Source: Kaggle ML & DS Survey 2021 - Q24. Select any activities that make up an important part of your role at work. "
"Data Scientists\nData science has had various definitions thrown around over the years, but in essence it describes a field which uses a combination of mathematics, statistics and machine learning to clean, process and interpret data to extract insights from it. They design and construct new processes for data modelling and production using prototypes, algorithms, predictive models and custom analysis.\n\n Source: Kaggle ML & DS Survey 2021 - Q24. Select any activities that make up an important part of your role at work. "
"I understand that reading the above might leave many feeling like they still canâ€™t see a clear distinction between the two professions - and the truth is that the lines are a bit blurred when it comes to data science and data analysis. Both the roles perform varying degrees of data collection, cleaning, and analysis to gain actionable insights for data-driven decision making - **leaving room for a lot of overlap**.\n\n Source: Kaggle ML & DS Survey 2021 - Q24. Select any activities that make up an important part of your role at work. "
"I understand that reading the above might leave many feeling like they still canâ€™t see a clear distinction between the two professions - and the truth is that the lines are a bit blurred when it comes to data science and data analysis. Both the roles perform varying degrees of data collection, cleaning, and analysis to gain actionable insights for data-driven decision making - **leaving room for a lot of overlap**.\n\n Source: Kaggle ML & DS Survey 2021 - Q24. Select any activities that make up an important part of your role at work. "
"The main takeaway we can gain from the survey respondents is that data scientists have a greater focus on machine learning related tasks, whereas analysts have their main role as deriving insights from data and dabble in the machine learning aspects of the job."
"It is quite clear that across almost all nations, analysts are paid less than data scientists. It might be possible that a reason for this is that data scientists possess skills in machine learning - a field that is greatly sought after in the job market. We have also seen earlier how lower percentages of data analysts reported that they performed any machine learning related roles. \n\nWe look at what data analysts are paid depending on whether or not they possess each of the previous roles.\n\n Source: Kaggle ML & DS Survey 2021 - Q25. What is your current yearly compensation?  note: Only data analysts in United States of America considered. \n\n Salary differences in Analysts based on roles \n \n  Role   with role   without   difference      \n  Analyse data for business decisions  21844  4610  17234  \n  Build and manage data infrastructure  20114  16433  3681  \n  Build ML prototypes  29996  14396  15600  \n  Build ML services for workflows  24589  16922  7667  \n  Improve existing ML models  33490  15774  17716  \n  Research in ML  16406  18056  -1650  \n  None of these roles  7277  18559  -11282  \n  Other  12665  18114  -5449  \n\n\n\nWith data scientists earn on average 50,005 USD in similar conditions, we see how none of these roles by themselves help data analysts reach this level of compensation.   \nData analysts that perform machine learning related tasks, namely building machine learning prototypes and improving ML models seem to be paid the highest amounts on average. In both cases, simply having these skills almost doubled their average earnings.\n\n\n Source: Kaggle ML & DS Survey 2021 - Q25. What is your current yearly compensation?  note: Only data analysts in United States of America considered. \n\n Salary differences in Data Scientists based on roles \n \n      Role   with role   without   difference  \n      Build ML prototypes   170374   109055   61319  \n      Improve existing ML models   167579   132222  35356  \n\n\n\nEven for data scientists these two roles seem to make a world of difference in terms of their earnings.\n\n\nDo data analysts need to know how to code?\n\nThis is a question that a lot of individuals entering the field ask, especially those switching careers from non-technical backgrounds. Fortunately for them, a simple google search will tell us that when it comes to data analysis, advanced coding skills aren't always necessary. Basic coding ability to wrangle data and having an understanding of analytics tools like Tableau, Power BI, etc. is often more than sufficient for most data analysis roles.\n\nHowever when it comes to data science, the public opinion seems to agree that having a good command of coding is essential to make it in this field Whether its basic data preparation, analysis, modeling or writing production code, data scientists are bound to have to write code for most of their daily tasks. I will admit that it is rather odd generalisation that *one field doesn't require much coding while in the other its essential*.\n\n Source: Kaggle ML & DS Survey 2021 - Q6. For how many years have you been writing code and/or programming? "
"From the survey data we see that data scientists in general have more experience writing code. That said, the difference between the two isn't as drastic as the many articles would lead you to believe. For example, more data analysts have experience with SQL than data scientists.\n\nWith the majority of data professionals having less than three years of coding experience, we also see how they prefer Python and SQL when starting out on their learning journey.\n\nThe tasks that a data professional performs on a day to day basis may not always require that they have to write code. Looking at coding experience based on the role performed reveals the following insights.\n\n Source: Kaggle ML & DS Survey 2021\n    - Q6. For how many years have you been writing code and/or programming? \n    - Q24. Select any activities that make up an important part of your role at work.\n"
"From the survey data we see that data scientists in general have more experience writing code. That said, the difference between the two isn't as drastic as the many articles would lead you to believe. For example, more data analysts have experience with SQL than data scientists.\n\nWith the majority of data professionals having less than three years of coding experience, we also see how they prefer Python and SQL when starting out on their learning journey.\n\nThe tasks that a data professional performs on a day to day basis may not always require that they have to write code. Looking at coding experience based on the role performed reveals the following insights.\n\n Source: Kaggle ML & DS Survey 2021\n    - Q6. For how many years have you been writing code and/or programming? \n    - Q24. Select any activities that make up an important part of your role at work.\n"
"A few key takeways from the above are:\n* Roles prominent in data analysis ('analysing data for business decisions' and 'build data infrastructure') show higher percentages of individuals with coding experience of 3 years or less.\n* Machine learning related roles have greater percentages at the higher experience ranges.\n* Data Analysts/Scientists that perform none of the mentioned roles also have lower coding experience in comparison.\n\nThis all falls in line with how the data analysis fields deal with coding, to many it is simply a means to an end. They need to be able to be able to surf through vast datasets at blazing speeds, analyse and recognise patterns and trends. This quick generation of insights may often result in sloppy or ad-hoc coding practices, which is rarely expected to be pushed to the production code.\n\nDo data analysts require Machine Learning?\nIf we were to google this question, we see a similar situation as we did in the previous section - numerous articles tell us that data analysts are not expected to have hands-on machine learning experience or build statistical models, which fall under the responsibilities of data scientists.\n\nSource: Kaggle ML & DS Survey 2021 - Q15. For how many years have you used machine learning methods? "
"A few key takeways from the above are:\n* Roles prominent in data analysis ('analysing data for business decisions' and 'build data infrastructure') show higher percentages of individuals with coding experience of 3 years or less.\n* Machine learning related roles have greater percentages at the higher experience ranges.\n* Data Analysts/Scientists that perform none of the mentioned roles also have lower coding experience in comparison.\n\nThis all falls in line with how the data analysis fields deal with coding, to many it is simply a means to an end. They need to be able to be able to surf through vast datasets at blazing speeds, analyse and recognise patterns and trends. This quick generation of insights may often result in sloppy or ad-hoc coding practices, which is rarely expected to be pushed to the production code.\n\nDo data analysts require Machine Learning?\nIf we were to google this question, we see a similar situation as we did in the previous section - numerous articles tell us that data analysts are not expected to have hands-on machine learning experience or build statistical models, which fall under the responsibilities of data scientists.\n\nSource: Kaggle ML & DS Survey 2021 - Q15. For how many years have you used machine learning methods? "
"In general, machine learning tasks are often out of the scope of a data analystâ€™s work, resulting in them having far less hands-on experience with machine learning models as compared to their data science counterparts. However when we zoom out and look at machine learning experience in all data fields, it is surprising how low analysts rank overall.\n\nSource: Kaggle ML & DS Survey 2021 - Q15. For how many years have you used machine learning methods? "
"In general, machine learning tasks are often out of the scope of a data analystâ€™s work, resulting in them having far less hands-on experience with machine learning models as compared to their data science counterparts. However when we zoom out and look at machine learning experience in all data fields, it is surprising how low analysts rank overall.\n\nSource: Kaggle ML & DS Survey 2021 - Q15. For how many years have you used machine learning methods? "
"How early in their coding journey did they start learning ML?\nFollowing the same line of thought, it might be interesting to look at how machine learning fits into the coding journey of professionals in different fields. This gives us an idea of those who might have had prior experience with coding before delving into machine learning and those whose start in ML was hand-in-hand with their learning to code.\n\nIn the following chart we see how:\n* A lot of data scientists started learning machine learning roughly around the same time that they started to learn to code(represented by the cells on the diagonal of the heatmap, i.e. their ML experience lines up with their coding experience).\n* Data analysts fall in the category of fields whose coding skills far exceed the machine learning abilities, especially in the later stages of their career.\n\nSource: Kaggle ML & DS Survey 2021\n    - Q6. For how many years have you been writing code and/or programming?  \n    - Q15. For how many years have you used machine learning methods?  \n"
"How early in their coding journey did they start learning ML?\nFollowing the same line of thought, it might be interesting to look at how machine learning fits into the coding journey of professionals in different fields. This gives us an idea of those who might have had prior experience with coding before delving into machine learning and those whose start in ML was hand-in-hand with their learning to code.\n\nIn the following chart we see how:\n* A lot of data scientists started learning machine learning roughly around the same time that they started to learn to code(represented by the cells on the diagonal of the heatmap, i.e. their ML experience lines up with their coding experience).\n* Data analysts fall in the category of fields whose coding skills far exceed the machine learning abilities, especially in the later stages of their career.\n\nSource: Kaggle ML & DS Survey 2021\n    - Q6. For how many years have you been writing code and/or programming?  \n    - Q15. For how many years have you used machine learning methods?  \n"
"Additionally, you can use the following piece to see how this plays out for the other data-related professions"
Education in Data roles\nFrom previous sections we see how data scientists require a solid understanding of both coding practises as well as a solid grasp on machine learning concepts. It might also make sense that they pursue higher education in order to get a better understanding of the theoretical aspects of the field.\n\n\nSource: Kaggle ML & DS Survey 2021 - Q4. What is the highest level of formal education that you have attained or plan to attain within the next 2 years? 
"With more than 60% of candidates having a masters degree or higher level of education, it becomes apparent that having at least a master's degree is a common route as a data scientist. Data analysts show a higher percentage of individuals that entered the field with a bachelorâ€™s degree at the expense of far fewer pursuing doctorate-level programs.\n\nA [deep-dive into requirements on job postings for Facebook/Meta](https://www.reproducible-hq.com/notebook/land-that-data-job-at-meta.html) highlights another point of difference between data analysts and scientists - for data scientists having a PHD gives is not only uncommon, but is often preferred for data scientist positions.\n\nSource: Job posting data collected from Facebook Careers "
"In order to make some analysis, we need to set our environment up. To do this, I firstly imported some modules and read the data. The below output is the head of the data but if you want to see more details, you might try removing ***#*** signs in front of the ***df.describe()*** and ***df.info()***. "
#  k-Nearest Neighbors (k-NN)\n#### [Return Contents](#0)\n
"Before starting to predict test points' labels, I wanted to see the places of these points according to some features. Thus, I drew the below charts which will help us to guess new points' labels."
#  k-NN from Scratch
"In this section, I compare my kNN function with the scikit learn's k-NN function and determine the confusion matrixes for the both models. The results look same but by removing the *random_state* in the *train_test_split* function and chaging the *train_size* different results can be found."
#  Logistic Regression from Scratch
Below confusion matrices show that each model gives the same results when the parameters are selected the same.
"Moreover, below line charts determine the costs for different learning rates. These plots generally used for the sanity check. They look like same for each model. They look pretty good and the effect of learning rate can be observed clearly. Since I used one vs rest, for each learning rate I drew 3 cost lines. "
"Moreover, below line charts determine the costs for different learning rates. These plots generally used for the sanity check. They look like same for each model. They look pretty good and the effect of learning rate can be observed clearly. Since I used one vs rest, for each learning rate I drew 3 cost lines. "
#  Logistic Regression from Scratch vs scikit-learn Logistic Regression
"This time, I compare my logistic function with the scikit learn's logistic function and determine the confusion matrixes for the both models. Also, I changed the regularization parameter ($\lambda$) for both models and determined the confusion matrixes for them too. The results look similar but they are not the same as expected. By removing the *random_state* in the *train_test_split* function and chaging the *train_size* different results can be obtained. "
#  k-Fold Cross Validation from Scratch\n#### [Return Contents](#0)\n
"In the **bias-variance behavior**, higher training set means higher variance. At this point, depending to our choice of number of folds, accuracies might change. This change can be observed from the below figures for my models.\n"
#  k-Fold Cross Validation from Scratch vs scikit-learn k-Fold Cross Validation
## Let's start a mini-analysis of the data by constructing a general distribution of features by their number.
"Conclusions from the presented graphs:\n* Variable study Gender is balanced, applied to the same number of men and women to obtain a relevant result.\n* most of the subjects correspond to the age of 11â€“25 years, this is the age when we can confidently talk about sustainable adaptation to learning.\n* values â€‹â€‹are contributed by the level of education indicates that most of the subjects have only a school education. These data correlate with a certain schedule, where most of the subjects are in the age range from 7 to 20 years.\n*existing education services are provided by non-private institutions. This is due to the development of the education market.\n* it should also be noted that most of the commercials are sold on the phone through the 4G network, which shows that people get education in any place convenient for them.\n* It should be noted that some of the online courses are not assimilated by students."
## Let's study the distribution of the number of students depending on the level of their adaptation.
"Conclusions from the obtained distributions:\n* Men are easier to adapt to new knowledge, the level of poor adaptation between men and women is approximately the same.\n* the best adaptation is shown at the age of 21-25, and 11-15, the worst - after 26 years and in the interval from 16 to 20 years. Worse adaptation to new knowledge can be explained by social and physiological factors.\n* It should also be noted that the best digestibility of the material is observed in the middle class. We can talk about this phenomenon for a very long time :)\n* high adaptation to the material is also noted among urban residents, this is due to social and economic factors.\n* The level of adaptation to new knowledge also depends on the quality of the Internet.\n"
# Let's check target distrubution\n\n# Check for Class Imbalance
# Check Text Content
# What about test data?
## I think there is not a big difference in location between train data and test data
# Part-of-Speech Tagging for questions Corpus
"# Topic Modelling\n\n#### Now we will apply a clustering algorithm to the headlines corpus in order to study the topic, as well as how it has evolved through time. To do so, we first experiment with a small subsample of the dataset in order to determine which of the two potential clustering algorithms is most appropriate â€“ once this has been ascertained, we then scale up to a larger portion of the available data."
"### Thus we have converted our initial small sample of headlines into a list of predicted topic categories, where each category is characterised by its most frequent words. The relative magnitudes of each of these categories can then be easily visualised though use of a bar chart.\n\n"
"#### However, this does not provide a great point of comparison with other clustering algorithms. In order to properly contrast LSA with LDA we instead use a dimensionality-reduction technique called  *t*-SNE, which will also serve to better illuminate the success of the clustering process."
"### All that remains is to plot the clustered questions. Also included are the top three words in each cluster, which are placed"
"### Evidently, this is a bit a of a failed result. We have failed to reach any great degree of separation across the topic categories, and it is difficult to tell whether this can be attributed to the LSA decomposition or instead the  t -SNE dimensionality reduction process. Let's move forward and try another clustering method"
"Welcome!\n\nThis will be an extensive visualization, analysis, and prediction notebook. \n\nHere are some of my other notebooks:\n\n**Gold price prediction using Prophet**\n\nhttps://www.kaggle.com/joshuaswords/eda-gold-price-prediction-prophet\n\n\n**Computer Vision with FastAI - Pneumonia prediction**\n\nhttps://www.kaggle.com/joshuaswords/computer-vision-pneumonia-prediction-fastai\n\n\n**Stroke Prediction with SMOTE and LIME explainer**\n\nhttps://www.kaggle.com/joshuaswords/predicting-a-stroke-95-acc-with-lime-explainer\n\n\n**2021 World Happiness Index EDA**\n\nhttps://www.kaggle.com/joshuaswords/awesome-eda-2021-happiness-population\n\n\nLet's get to it...\n\n\n\n# Context\n\n**Problem Statement**: Predict the probability of a candidate looking for a new job.\n\nEssentially, I'll be attempting to predict **who is job-seeking and who is not.** \n\n**Note** - The main focus of this notebook is on **data visualization** so the predictive models can almost certainly be improved upon with some basic tweaks. \n\nRemember though, in industry it is difficult to deploy models that take hours to retrain daily. A good model is often better - and more practical - than a perfect one. Although this should be assessed on a case-by-case basis of course.\n\n"
# Loading the data
"# Data Visualization \n\n**This will be an EXPLORATORY visualization, as opposed to EXPLANATORY.**\n\n\nI will also make use of **GridSpec** as I want to practice this technique. \n\n\nLet's see if we can understand why people might look for a new job...\n\n\nThe colour palette I will use is below:"
"# How many job-seekers are there?\n\nFirst of all, I want to see how many job-seekers there are in our training set. \n\nDo we have a balanced dateset? Or Imbalanced? The answer to these question may influence our models later on."
"# How many job-seekers are there?\n\nFirst of all, I want to see how many job-seekers there are in our training set. \n\nDo we have a balanced dateset? Or Imbalanced? The answer to these question may influence our models later on."
"We have an imbalanced dataset - that is,cmany **more non job-seekers than job-seekers**.\n\nThis is a problem that we can address later. For now, let's continue **exploring the data**"
Here I'll show an example of how you can use GridSpec.\n\nGridSpec enables you to plot multiple plots and is highly customisable - a skill worth picking up!\n\nThis is really valuable because you can convey a lot of information in a small amount of space. \n\nI'll compare the Train & Test sets here...
"# The Train & Tests sets are similar - that's good news\n\nIf the training set has wildy different characteristics to our test set then we really are in for a difficult time.\n\nWe'd need to ask if the training population can really help us predict the target. \n\nIn this case though, we're fine. \n\n# Now let's focus on the Training set and explore the data..."
"You'll note that I often incoroprate text in to my visuals. I'll often an annotations to the plots themselves, for example at the 'mean', or at peaks in the data etc. \n\nIn this case, I've included an explanation of what we're seeing and what it might mean. This helps your audience to understand your data, but it also helps them to get thinking in a way that is in line with the story you are trying to craft."
# Let's now explore other factors like company size & employee experience...\n\nDo more experienced employees seek new challenges?\n\nDo employees at larger companies feel less valued?\n\nDo employees at smaller companies crave new opportunities?\n\nBoth seem plausible. These are the questions that a good EDA can answer!\n
# Let's now explore other factors like company size & employee experience...\n\nDo more experienced employees seek new challenges?\n\nDo employees at larger companies feel less valued?\n\nDo employees at smaller companies crave new opportunities?\n\nBoth seem plausible. These are the questions that a good EDA can answer!\n
"It appears the as employee experience increases, they tend to work for larger companies.\n\nWhy might this be? Perhaps larger companies pay better, or perhaps more job security.\n\n# Employee Experience & Company Size\n\nIs there a noticeable difference between job-seekers and non-job-seekers?"
"It appears the as employee experience increases, they tend to work for larger companies.\n\nWhy might this be? Perhaps larger companies pay better, or perhaps more job security.\n\n# Employee Experience & Company Size\n\nIs there a noticeable difference between job-seekers and non-job-seekers?"
"# Education\n\nWe've explored some interesting feautures of our data, including employee experience and company size. \n\nLet's throw education in to the mix. \n\nDo job-seekers have a higher education level? Are they less educated? Or is there no difference at all?"
"# Education\n\nWe've explored some interesting feautures of our data, including employee experience and company size. \n\nLet's throw education in to the mix. \n\nDo job-seekers have a higher education level? Are they less educated? Or is there no difference at all?"
"These results are interesting. It does appear that job-seekers are less educated - I would suggest that this is becuase they are younger and still on their education journey, and are also seeking new challenges.\n\nWhat do you think?"
# So far...
"# Implementing SMOTE\n\nSMOTE is a technique that helps deal with imbalanced data sets. \n\nA great introductory article can be found here:\n\nhttps://www.geeksforgeeks.org/ml-handling-imbalanced-data-with-smote-and-near-miss-algorithm-in-python/\n\nThe common error I see people making is to use SMOTE and THEN split their data in to train & test sets. This is a big mistake as you will get some serious data leakage and end up predicting synthetic results that you have just created - it does not make sense. \n\nInstead, split your data first, and THEN use SMOTE on the training data only.\n\nLet's see if it helps here..."
# Viewing our results in an accesible way\n\nWe now now need to find a way to view our results which can be easily explained to business stakeholders. \n\nA simple annotated heatmap works well for this!
"# Something else... Borderline SMOTE\n\nThere are many oversampling techniques that one could employ. \n\nA variation of the technique used above is **Borderline SMOTE**.\n\nBorderline SMOTE involves selecting those instances of the minority class that are misclassified.\n\nWe can then oversample just those difficult instances, providing more resolution only where it may be required\n\nA great article cab be found here:\n\nhttps://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/\n\n"
  \nImport Libraries\n
  \nImport Dataset ðŸ“ƒ\n
* In Train data we have *42000* instances and *785* features.\n* In Test data we have *28000* instances and *784* features.\n\nNow we see that we have 785 features in train data which means that there are 784 pixel valuess in the image and one column is about label that which digit it is.\n
### **Checking the Values in Image Form.**
### **Checking the Values in Image Form.**
### **Checking the Images One by One:**
# **Categorical Features Exploration**
`Year_Factor:` is the year in which the weather and energy usage factors were observed . \n\nðŸ“Œ There are more number of observations for Year6 and Year5 in comparison with other years
`facility_type:` is the building usage type .\n\nðŸ“Œ There are more number of observations for 'Multifamily Uncategorized'
# ** Numerical Features VS Target **
# ** Numerical Features VS Target **
### ðŸŽ¯floor_area vs site_eui
### ðŸŽ¯Year_Factor vs site_eui
# **Feature Correlation**
# **Feature Correlation**
# **Log Plots to W&B environment**
# **Log Plots to W&B environment**
# **Preprocessing**
"# **W & B Artifacts**\n\nAn artifact as a versioned folder of data.Entire datasets can be directly stored as artifacts .\n\nW&B Artifacts are used for dataset versioning, model versioning . They are also used for tracking dependencies and results across machine learning pipelines.Artifact references can be used to point to data in other systems like S3, GCP, or your own system.\n\nYou can learn more about W&B artifacts [here](https://docs.wandb.ai/guides/artifacts)\n\n![](https://drive.google.com/uc?id=1JYSaIMXuEVBheP15xxuaex-32yzxgglV)"
# **Categorical Feature Transformation**
"## Plotting 4D probabilistic atlas maps...\n\nProbabilistic atlasing is a research strategy whose goal is to generate anatomical templates that retain quantitative information on inter-subject variations in brain architecture (Mazziotta et al., 1995). A digital probabilistic atlas of the human brain, incorporating precise statistical information on positional variability of important functional and anatomic interfaces, may rectify many current atlasing problems, since it specifically stores information on the population variability.\n\nFor further reading you may visit.[click here](http://users.loni.usc.edu/~thompson/prob_atlas.html)"
"## Plotting a statistical map...\n\nStatistical parametric mapping or SPM is a statistical technique for examining differences in brain activity recorded during functional neuroimaging experiments.The measurement technique depends on the imaging technology (e.g., fMRI and PET). The scanner produces a 'map' of the area that is represented as voxels. Each voxel represents the activity of a specific volume in three-dimensional space. The exact size of a voxel varies depending on the technology. fMRI voxels typically represent a volume of 27 mm3 (a cube with 3mm length sides).\n\nParametric statistical models are assumed at each voxel, using the general linear model to describe the data variability in terms of experimental and confounding effects, with residual variability. Hypotheses expressed in terms of the model parameters are assessed at each voxel with univariate statistics.\n\nAnalyses may examine differences over time (i.e. correlations between a task variable and brain activity in a certain area) using linear convolution models of how the measured signal is caused by underlying changes in neural activity.\n\nBecause many statistical tests are conducted, adjustments have to be made to control for type I errors (false positives) potentially caused by the comparison of levels of activity over many voxels. A type I error would result in falsely assessing background brain activity as related to the task. Adjustments are made based on the number of resels in the image and the theory of continuous random fields in order to set a new criterion for statistical significance that adjusts for the problem of multiple comparisons."
# Get our environment set up\n\nThe first thing we'll need to do is load in the libraries we'll be using. 
"# Scaling vs. Normalization: What's the difference?\n\nOne of the reasons that it's easy to get confused between scaling and normalization is because the terms are sometimes used interchangeably and, to make it even more confusing, they are very similar! In both cases, you're transforming the values of numeric variables so that the transformed data points have specific helpful properties. The difference is that:\n- in **scaling**, you're changing the *range* of your data, while \n- in **normalization**, you're changing the *shape of the distribution* of your data. \n\nLet's talk a little more in-depth about each of these options. \n\n# Scaling\n\nThis means that you're transforming your data so that it fits within a specific scale, like 0-100 or 0-1.  You want to scale data when you're using methods based on measures of how far apart data points are, like [support vector machines (SVM)](https://en.wikipedia.org/wiki/Support_vector_machine) or [k-nearest neighbors (KNN)](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm). With these algorithms, a change of ""1"" in any numeric feature is given the same importance. \n\nFor example, you might be looking at the prices of some products in both Yen and US Dollars. One US Dollar is worth about 100 Yen, but if you don't scale your prices, methods like SVM or KNN will consider a difference in price of 1 Yen as important as a difference of 1 US Dollar! This clearly doesn't fit with our intuitions of the world. With currency, you can convert between currencies. But what about if you're looking at something like height and weight? It's not entirely clear how many pounds should equal one inch (or how many kilograms should equal one meter).\n\nBy scaling your variables, you can help compare different variables on equal footing. To help solidify what scaling looks like, let's look at a made-up example. (Don't worry, we'll work with real data in [**the following exercise**](https://www.kaggle.com/kernels/fork/10824404)!)"
"# Scaling vs. Normalization: What's the difference?\n\nOne of the reasons that it's easy to get confused between scaling and normalization is because the terms are sometimes used interchangeably and, to make it even more confusing, they are very similar! In both cases, you're transforming the values of numeric variables so that the transformed data points have specific helpful properties. The difference is that:\n- in **scaling**, you're changing the *range* of your data, while \n- in **normalization**, you're changing the *shape of the distribution* of your data. \n\nLet's talk a little more in-depth about each of these options. \n\n# Scaling\n\nThis means that you're transforming your data so that it fits within a specific scale, like 0-100 or 0-1.  You want to scale data when you're using methods based on measures of how far apart data points are, like [support vector machines (SVM)](https://en.wikipedia.org/wiki/Support_vector_machine) or [k-nearest neighbors (KNN)](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm). With these algorithms, a change of ""1"" in any numeric feature is given the same importance. \n\nFor example, you might be looking at the prices of some products in both Yen and US Dollars. One US Dollar is worth about 100 Yen, but if you don't scale your prices, methods like SVM or KNN will consider a difference in price of 1 Yen as important as a difference of 1 US Dollar! This clearly doesn't fit with our intuitions of the world. With currency, you can convert between currencies. But what about if you're looking at something like height and weight? It's not entirely clear how many pounds should equal one inch (or how many kilograms should equal one meter).\n\nBy scaling your variables, you can help compare different variables on equal footing. To help solidify what scaling looks like, let's look at a made-up example. (Don't worry, we'll work with real data in [**the following exercise**](https://www.kaggle.com/kernels/fork/10824404)!)"
"Notice that the *shape* of the data doesn't change, but that instead of ranging from 0 to 8ish, it now ranges from 0 to 1.\n\n# Normalization\n\nScaling just changes the range of your data. Normalization is a more radical transformation. The point of normalization is to change your observations so that they can be described as a normal distribution.\n\n> **[Normal distribution:](https://en.wikipedia.org/wiki/Normal_distribution)** Also known as the ""bell curve"", this is a specific statistical distribution where a roughly equal observations fall above and below the mean, the mean and the median are the same, and there are more observations closer to the mean. The normal distribution is also known as the Gaussian distribution.\n\nIn general, you'll normalize your data if you're going to be using a machine learning or statistics technique that assumes your data is normally distributed. Some examples of these include linear discriminant analysis (LDA) and Gaussian naive Bayes. (Pro tip: any method with ""Gaussian"" in the name probably assumes normality.)\n\nThe method we're using to normalize here is called the [Box-Cox Transformation](https://en.wikipedia.org/wiki/Power_transform#Box%E2%80%93Cox_transformation). Let's take a quick peek at what normalizing some data looks like:"
"Notice that the *shape* of the data doesn't change, but that instead of ranging from 0 to 8ish, it now ranges from 0 to 1.\n\n# Normalization\n\nScaling just changes the range of your data. Normalization is a more radical transformation. The point of normalization is to change your observations so that they can be described as a normal distribution.\n\n> **[Normal distribution:](https://en.wikipedia.org/wiki/Normal_distribution)** Also known as the ""bell curve"", this is a specific statistical distribution where a roughly equal observations fall above and below the mean, the mean and the median are the same, and there are more observations closer to the mean. The normal distribution is also known as the Gaussian distribution.\n\nIn general, you'll normalize your data if you're going to be using a machine learning or statistics technique that assumes your data is normally distributed. Some examples of these include linear discriminant analysis (LDA) and Gaussian naive Bayes. (Pro tip: any method with ""Gaussian"" in the name probably assumes normality.)\n\nThe method we're using to normalize here is called the [Box-Cox Transformation](https://en.wikipedia.org/wiki/Power_transform#Box%E2%80%93Cox_transformation). Let's take a quick peek at what normalizing some data looks like:"
"Notice that the *shape* of our data has changed. Before normalizing it was almost L-shaped. But after normalizing it looks more like the outline of a bell (hence ""bell curve""). \n\n# Your turn\n\nIt's time to [**apply what you just learned**](https://www.kaggle.com/kernels/fork/10824404) a dataset of Kickstarter projects."
"![](https://www.wsp.com/-/media/Hubs/Global/Congestion-Management/bnr-congestion.jpg)\n\n# Goal\n\nForecast twelve-hours of traffic flow in a U.S. metropolis. The time series in this dataset are labelled with both location coordinates and a direction of travel -- a combination of features that will test your skill at spatio-temporal forecasting within a highly dynamic traffic network.\n\n# Metric\n\nSubmissions are evaluated on the **mean absolute error** between predicted and actual congestion values for each time period in the test set. The congestion target has integer values from 0 to 100.\n\n# Data\n\n**train.csv** - the training set, comprising measurements of traffic congestion across 65 roadways from April through September of 1991.\n* row_id - a unique identifier for this instance\n* time - the 20-minute period in which each measurement was taken\n* x - the east-west midpoint coordinate of the roadway\n* y - the north-south midpoint coordinate of the roadway\n* direction - the direction of travel of the roadway. EB indicates ""eastbound"" travel, for example, while SW indicates a ""southwest"" direction of travel.\n* congestion - congestion levels for the roadway during each hour; the target. The congestion measurements have been normalized to the range 0 to 100.\n\n**test.csv** - the test set; you will make hourly predictions for roadways identified by a coordinate location and a direction of travel on the day of 1991-06-30."
# Load data
# Target: congestion level
The abnormal columns are originated by the constant values filling the missing observations during the morning of each day. See also the daily animations in the [notebook](https://www.kaggle.com/sytuannguyen/tps-mar-2022-time-space-animation) for more details.
# Morning vs Afternoon
"If we add 5 congestion units to the moning data, its distribution fits quite well with that of the afternoon"
"If we add 5 congestion units to the moning data, its distribution fits quite well with that of the afternoon"
# Histograms for each roadway
# Histograms for each roadway
"# Correlation between 12 locations\n\nSome strong correlations of congestion can be observed between the locations. For example, the location x=1, y=2 and the one with x=1 and y=3 have a high correlation coefficient of 0.79."
"# Correlation between 12 locations\n\nSome strong correlations of congestion can be observed between the locations. For example, the location x=1, y=2 and the one with x=1 and y=3 have a high correlation coefficient of 0.79."
# Correlation between 12 locations at two consecutive instants
# Correlation between 12 locations at two consecutive instants
"# Daily correlation between the locations (x,y)=(1,2) and (1,3)\n\nThese two locations seem to be highly connected during the rush hours (8am and 5pm)."
# Correlation between different directions at each location
# Average congestion per month
# Average congestion per month
# Average congestion per month for each roadway
# Average congestion per month for each roadway
# Average montly congestion
# Average montly congestion
# Average montly congestion for each roadway
# Average montly congestion for each roadway
# Average congestion per week
# Average congestion per week
# Average congestion per week for each roadway
# Average congestion per week for each roadway
# Average weekly congestion
# Average weekly congestion
# Average weekly congestion for each roadway
# Average weekly congestion for each roadway
# Average congestion per day
# Average congestion per day
# Average congestion per day for each roadway
# Average congestion per day for each roadway
# Average daily congestion
# Average daily congestion
# Average daily congestion for each roadway
# Average daily congestion for each roadway
# Average Monday congestion\n\nAs 30 Sept 1991 where locating the test set is a Monday. It is of interest to examine data of this day of week.
# Average Monday congestion for each roadway
# The nearest Monday 23 Sept
# The nearest Monday 23 Sept
# Correlation between the days
# Score between the congestion of the Monday afternoons\n\nBelow I compute the mean absolute error between all the Monday afternoons and the nearest Monday afternoon of the test (the 23 Sept)
# Score between the congestion of the Monday afternoons w.r.t. the median congestion\n\nThe mean absolute error between all the Monday afternoons and the median congestion over all the afternoons is shown below.
# Score between the congestion of the Monday afternoons w.r.t. the median congestion\n\nThe mean absolute error between all the Monday afternoons and the median congestion over all the afternoons is shown below.
# Score between the congestion of the afternoons\n\nThe mean absolute error between all the afternoons and the nearest afternoon of the test (the 29 Sept) is shown below.
# Score between the congestion of the afternoons\n\nThe mean absolute error between all the afternoons and the nearest afternoon of the test (the 29 Sept) is shown below.
# Score between the congestion of the afternoons w.r.t. the median congestion\n\nThe mean absolute error between all the afternoons and the median congestion over all the afternoons is shown below.
# Score between the congestion of the afternoons w.r.t. the median congestion\n\nThe mean absolute error between all the afternoons and the median congestion over all the afternoons is shown below.
# The ouliers\n\nWe may consider the afternoons with high MAE w.r.t. the median congestion as outliers.
Notebooks with SVC:\n* [Fast SVC using scikit-learn-intelex for Digit Recognizer](https://www.kaggle.com/kppetrov/fast-svc-using-scikit-learn-intelex-for-mnist)\n* [Fast SVC using scikit-learn-intelex for NLP with Disaster Tweets](https://www.kaggle.com/kppetrov/fast-svc-using-scikit-learn-intelex-for-nlp)\n* [Fast SVC using scikit-learn-intelex for What's cooking](https://www.kaggle.com/kppetrov/using-scikit-learn-intelex-for-what-s-cooking)\n* [Fast SVC using scikit-learn-intelex for TPS - December 2021](https://www.kaggle.com/alexeykolobyanin/tps-dec-svc-with-sklearnex-20x-speedup)
\n## NuSVR\nGo to Algorithms
Notebooks with NuSVR:\n* [Fast NuSVR with scikit-learn-intelex for House prices](https://www.kaggle.com/alexeykolobyanin/house-prices-nusvr-sklearn-intelex-4x-speedup)\n* [Fast NuSVR with scikit-learn-intelex for TPS - August 2021](https://www.kaggle.com/alexeykolobyanin/tps-aug-nusvr-with-intel-extension-for-sklearn )
\n## RandomForest\nGo to Algorithms
Notebooks with RandomForest:\n* [Fast RF with scikit-learn-intelex for TPS - August 2021](https://www.kaggle.com/andreyrus/tps-apr-rf-with-intel-extension-for-scikit-learn)\n* [Fast RF with scikit-learn-intelex for TPS - July 2021](https://www.kaggle.com/alexeykolobyanin/tps-jul-rf-with-intel-extension-for-scikit-learn)
\n## KNN\nGo to Algorithms
Notebooks with KNN:\n* [Fast KNN using scikit-learn-intelex for Digit Recognizer](https://www.kaggle.com/kppetrov/fast-knn-using-scikit-learn-intelex-for-mnist)\n* [Fast KNN using scikit-learn-intelex on synthetic classification data](https://www.kaggle.com/kppetrov/accelerate-sklearn-algorithms-using-sklearnex)
\n## SVM\nGo to Algorithms
Notebooks with SVM:\n* [Fast SVM with scikit-learn-intelex for TPS - May 2021](https://www.kaggle.com/napetrov/svm-tps-may-2021-with-scikit-learn-intelex)\n* [Fast SVM with scikit-learn-intelex for TPS - April 2021](https://www.kaggle.com/napetrov/tps04-svm-with-intel-extension-for-scikit-learn)
\n## Stacking\nGo to Algorithms
Notebooks with Stacking:\n* [Fast Stacking with scikit-learn-intelex for TPS - June 2021](https://www.kaggle.com/masdevas/fast-ml-stack-with-scikit-learn-intelex)\n* [Fast Stacking with scikit-learn-intelex for TPS - July 2021](https://www.kaggle.com/alexeykolobyanin/tps-jul-stacking-with-scikit-learn-intelex)
\n## Logistic Regression\nGo to Algorithms
Notebooks with Logistic Regression:\n* [Fast Logistic Regression with scikit-learn-intelex for TPS - June 2021](https://www.kaggle.com/kppetrov/tps-jun-fast-logreg-with-scikit-learn-intelex#%F0%9F%93%9C-Conclusions)\n* [Fast Logistic Regression with scikit-learn-intelex for TPS - November 2021](https://www.kaggle.com/alexeykolobyanin/tps-nov-log-regression-with-sklearnex-17x-speedup)
\n## AutoML\nGo to Algorithms
Notebooks with AutoML:\n* [Fast AutoGluon using scikit-learn-intelex for TPS - June 2021](https://www.kaggle.com/alex97andreev/tps-jun-autogluon-with-sklearnex)\n* [Fast PyCaret using scikit-learn-intelex for TPS - January 2022](https://www.kaggle.com/lordozvlad/tps-jan-fast-pycaret-with-scikit-learn-intelex)\n* [Fast AutoGluon using scikit-learn-intelex for Titanic](https://www.kaggle.com/lordozvlad/titanic-automl-with-intel-extension-for-sklearn)
\n## Feature Importance\nGo to Algorithms
Notebooks with Feature Importance:\n* [Fast Feature Importance using scikit-learn-intelex for TPS - November 2021](https://www.kaggle.com/lordozvlad/fast-feature-importance-using-scikit-learn-intelex)\n* [Fast Feature Importance using scikit-learn-intelex for TPS - December 2021](https://www.kaggle.com/lordozvlad/fast-feature-importance-using-scikit-learn-intelex)
"# Installation\n\nIntel(R) Extension for Scikit-learn is available at the [Python Package Index](https://pypi.org/project/scikit-learn-intelex/), on Anaconda Cloud in [Conda-Forge channel](https://anaconda.org/conda-forge/scikit-learn-intelex) and in [Intel channel](https://anaconda.org/intel/scikit-learn-intelex). Intel(R) Extension for Scikit-learn is also available as a part of [IntelÂ® oneAPI AI Analytics Toolkitâ€¯(AI Kit)](https://www.intel.com/content/www/us/en/developer/tools/oneapi/ai-analytics-toolkit.html)."
" Observation:\n* latitue and longitude are important features.\n* rating is of relatively low importance.\n    \nI still don't know if this information is helpful for learning. If it is helpful, I still don't know how to use it for learning."
------------------------------------\n# Understanding Train Dataset\n\nTrain data are audio files. Let's hear it for ourselves and see the waveform to see what strategy can solve this problem.
------------------------------------\n# Understanding Train Dataset\n\nTrain data are audio files. Let's hear it for ourselves and see the waveform to see what strategy can solve this problem.
-------------------------------\n## Normoc\n\n![](https://live.staticflickr.com/7070/6873951614_2dd80c1d7c_b.jpg)\n\nRef: https://cdn.download.ams.birds.cornell.edu
--------------------------------------------------------------------\n## Checking batch
" Observation:\n* We converted audio to image. In other words, the above figures can be regarded as fingerprints of each audio file.\n   \nThe format of the question has been changed. Looking at the picture above, it can be seen that the problem of audio classification has been changed to the problem of image classification.\n    \nQuantum mechanics cannot explain the behavior of quantum clearly, but as it can be solved mathematically, rather than understanding the above figures accurately, we will only think about how the model can understand and learn the above picture."
 Most of the Airlines has Economic Class as common
" Does price vary with Airlines?\n\n\n    \n\n    As we can see Vistara has Maximum Price range\n    Vistara and Air_India Airlines Have Maximum Price when compared to Others\n    SpiceJet , AirAsia , GO_First and Indigo has some what equal prices  \n    \n\n"
" Does price vary with Airlines?\n\n\n    \n\n    As we can see Vistara has Maximum Price range\n    Vistara and Air_India Airlines Have Maximum Price when compared to Others\n    SpiceJet , AirAsia , GO_First and Indigo has some what equal prices  \n    \n\n"
\n     How Does the Ticket Price vary between Economy and Business Class?\n   \n \n    Ticket Price is Maximum for Bussiness Class When compared to Economy Class\n\n    
\n     How Does the Ticket Price vary between Economy and Business Class?\n   \n \n    Ticket Price is Maximum for Bussiness Class When compared to Economy Class\n\n    
 How Does the Ticket Price vary with the number of stops of a Flight?\n\n \nFlights having one stop has maximum ticket price\n
 How Does the Ticket Price vary with the number of stops of a Flight?\n\n \nFlights having one stop has maximum ticket price\n
"\n    \n How the Ticket Price change based on the Departure Time and Arrival Time?\n\n    \n1. Departure Time Vs Ticket Price\n\n   \n    Ticket Price is More for the Flights when the Departure Time is at Night\n   Ticket Price is almost equal for flights Having Departure time at Early_morning , Morning and Evening\n   Ticket Price is Low for the Flights Having Departure Time at Late_night\n   \n    \n \n \n2. Arrival Time Vs Ticket Price\n\n   \n    Ticket Price is More for the Flights when the Arrival Time is at Evening\n    Ticket Price is almost equal for flights Having Arrival time is at Morning and Night\n   Ticket Price is Low for the Flights Having Arrival Time at Late_night as same as Departure Time\n    \n "
"\n    \n How the Ticket Price change based on the Departure Time and Arrival Time?\n\n    \n1. Departure Time Vs Ticket Price\n\n   \n    Ticket Price is More for the Flights when the Departure Time is at Night\n   Ticket Price is almost equal for flights Having Departure time at Early_morning , Morning and Evening\n   Ticket Price is Low for the Flights Having Departure Time at Late_night\n   \n    \n \n \n2. Arrival Time Vs Ticket Price\n\n   \n    Ticket Price is More for the Flights when the Arrival Time is at Evening\n    Ticket Price is almost equal for flights Having Arrival time is at Morning and Night\n   Ticket Price is Low for the Flights Having Arrival Time at Late_night as same as Departure Time\n    \n "
"\n How the price changes with change in Source city and Destination city?\n    \n1. Source City Vs Ticket Price\n\n    Ticket Price is More for the Flights whose Source City is Kolkata\n   Ticket Price is almost equal for flights Having Source Cities as Mumbai and chennai , Hyderabad and Bangalore\n     Ticket Price is Low for the Flights Having Source City as Delhi\n    \n \n2. Destination City Vs Ticket Price\n\n    Ticket Price is More for the Flights whose Destination City is kolkata and Chennai\n    Ticket Price is almost equal for flights Having Destination Cities as Mumbai and Bangalore\n   Ticket Price is Low for the Flights Having Destination City as Delhi\n"
"\n How the price changes with change in Source city and Destination city?\n    \n1. Source City Vs Ticket Price\n\n    Ticket Price is More for the Flights whose Source City is Kolkata\n   Ticket Price is almost equal for flights Having Source Cities as Mumbai and chennai , Hyderabad and Bangalore\n     Ticket Price is Low for the Flights Having Source City as Delhi\n    \n \n2. Destination City Vs Ticket Price\n\n    Ticket Price is More for the Flights whose Destination City is kolkata and Chennai\n    Ticket Price is almost equal for flights Having Destination Cities as Mumbai and Bangalore\n   Ticket Price is Low for the Flights Having Destination City as Delhi\n"
"\n     How Price Varies with the Flight Duration Based on Class?\n\n With increase in Duration, the Ticket Price is also Increases In both the Economy and Business classes\n\n"
"\n     How Price Varies with the Flight Duration Based on Class?\n\n With increase in Duration, the Ticket Price is also Increases In both the Economy and Business classes\n\n"
\n     How does the price affected on the days left for Departure?\n    \n As we can see when compared to others when there are two days remaining for departure then the Ticket Price is very High for all airlines\n    \n
# Model setup
We are going to use the dataset I have cleaned up of the original data. More info in a kaggle [thread](https://www.kaggle.com/competitions/amex-default-prediction/discussion/328514).
We can see that three `D` (as for **D**elinquency) features stand out. We will investigate this further. But first let's see how accurate our very simple model is in predicting the time rank.
It seems the model is able to predict the rank correctly 40-50% of all instances. Quite good. But this rejects the hypothesis that we have a `time_since_customer` feature in the dataset - the accuracy should be 100% if the hypothesis was true.\n\nLet's dig in a bit deeper!
### segment with temporal `D_59`:
### segment without temporal `D_59`:
### segment without temporal `D_59`:
"As expected, our model accuracy is 2-3x better for the segment showing temporal `D_59` behavior."
"# Finding Similar Books\n\nWe've trained the model and extracted the embeddings - great - but where is the book recommendation system? Now that we have the embeddings, we can use them to recommend books that our model has learned are most similar to a given book.\n\n\n### Function to Find Most Similar Entities\n\nThe function below takes in either a book or a link, a set of embeddings, and returns the `n` most similar items to the query. It does this by computing the dot product between the query and embeddings. Because we normalized the embeddings, the dot product represents the [cosine similarity](http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/) between two vectors. This is a measure of similarity that does not depend on the magnitude of the vector in contrast to the Euclidean distance. (The Euclidean distance would be another valid metric of similary to use to compare the embeddings.)\n\nOnce we have the dot products, we can sort the results to find the closest entities in the embedding space. With cosine similarity, higher numbers indicate entities that are closer together, with -1 the furthest apart and +1 closest together."
"(We know that this function works if the most similar book is the book itself. Because we multiply the item vector times all the other embeddings, the most similar should be the item itself with a similarity of 1.0.)"
"We've now taken the initial 37,000 dimension book vector and reduced it to just 2 dimensions."
"There do appear to be a few noticeable clumps. However, it's difficult to derive any meaning from this plot since we aren't distinguishing books in any way."
"There do appear to be a few noticeable clumps. However, it's difficult to derive any meaning from this plot since we aren't distinguishing books in any way."
### Book Embeddings by Genre\n\nThe above graphs are difficult to interpret. Let's plot the embeddings by the `genre` which is contained in the `Infobox` template data for each book. We'll limit it to the 10 most popular genres.
"Finally, we can plot the embedding colored by the genre"
The books do seem to be slightly separated based on the genre. The categories aren't exactly that helpful but we did what we could! We can also try UMAP. 
The books do seem to be slightly separated based on the genre. The categories aren't exactly that helpful but we did what we could! We can also try UMAP. 
"There doesn't appear to be much separation between the categories in the UMAP clustering. There are a lot of parameters to play around with in UMAP, and changing some of them might result in better clusters."
## Most Popular Books in Embedding\n\nLet's see the embedding labeled with the 10 books most often mentioned by other books. 
The `Encyclopedia`s of Science Fiction and Fantasy have nearly perfect overlap. 
The next image shows all the link embedded with the 10 most popular categories labeled.
"We do see some clumpings, but it's difficult to label them. If this was interactive, then we could get a lot more use from it. (This will be an upcoming topic for an article)."
"\n# Appendix\n\n\n## Dice example\n\nlet's have a look at an example to calculate the probability. I deliberately chose a basic and simple dice example with discrete values so it is easier to understand connecting to our available/intuitive knowledge about chances when throwing dice. Bayesian logic is not always very intuitive, so I do want to recommend playing around with some examples to familiarize yourself with Bayesian logic.  \n\nIn this dice example, a person will throw secretly either 1 or 2 six-sided dice, mostly 2 dice (60%) but also 1 dice (40%). You will just get the resulting number and you have to guess how many dice were thrown.  \n(a) If the person tells you the result is 1 you probably figured out it had to be with 1 six-sided dice, kind of hard to throw 1 with 2 dice.  \n(b) If the person tells you the result is 4 what would be more likely 1 or 2 six-sided dice? A naive approach would be to guess 2 dice because mostly 2 dice are thrown, or isn't it? Bayes' Theorem can help you to figure it out.  \n\nSo we are asking:\n\n(a) $$p(Dice=2 \mid  Observation=1)=\frac{p(Observation=1 \mid Dice=2) * p(Dice=2)}{p(Observation=1)}$$ \nwhich is 0 because $p(Observation=1 \mid Dice=2)$ is 0. So also Bayes's theorem confirms it is kind of hard to throw a 1 with 2 dice. Good start.\n\n(b) $$p(Dice=2 \mid  Observation=4)=\frac{p(Observation=4 \mid Dice=2) * p(Dice=2)}{p(Observation=4)}$$ \n  \nwhich is equal to (apply sum rule, see equation [2a] next paragraph):\n$$=\frac{p(Observation=4 \mid Dice=2) * p(Dice=2)}{p(Observation=4 \mid Dice=1) * p(Dice=1)+p(Observation=4 \mid Dice=2) * p(Dice=2)}$$\n  \nSince this is a dice example and we remember the dice probabilities from high school the probability can be evaluated directly:\n$$=\frac{\frac{3}{36} * \frac{6}{10}}{\frac{1}{6} * \frac{4}{10}+\frac{3}{36} * \frac{6}{10}}=\frac{0,05}{0,1166}\approx 43\% $$\n  \nBoth (a) and (b) are point estimates for easy understanding. Bayesian logic also works with complete probability distributions (discrete and continuous). Did generate the discrete probability distributions below for reference. Please play around with the example below to better familiarize yourself with Bayes' Theorem."
"\n# Conventions and Symbols\n\nUnfortunately have observed various types of symbols over various documents, not making it easier to read or understand. Below the symbols used in this notebook  \n\nSymbol | Description | Alternative |\n--- | --- | --- |\nx  | external hidden environment states the brain tries to infer. Shorthand notation for the vector $\vec{x}_{hypotheses}\in\mathbb{R}^n$ with n number of states. | |\ny | sensory observations, the data from the available sensory set (effect of the environment on the Â¨systemÂ¨). Shorthand notation for the vector $\vec{y}_{observation} \in  \mathbb{R}^{q}$ with q number of sensors.  | also sometimes noted Â¨sÂ¨ as of sensory states |\nu | actions, control signal, The action that can be performed on the environment. (effect of the Â¨systemÂ¨ on the environment) Shorthand notation for the vector $\vec{u} \in  \mathbb{R}^{l}$ with l number of controls.  | also noted as Â¨aÂ¨ or Â¨$\alpha $Â¨ |\n$\mathcal{F}(y,\zeta)$ or $\mathcal{F}(y_{observation},\zeta)$ | The Free Energy | simplifies under lapace transformation to $\mathcal{F}(y,\mu)$\n$q(x; \zeta )$ or $q(x_{hypotheses}; \zeta )$    | recognition probability density with sufficient statistics $\zeta$, the brain to approximate posterior  $p(x_{hypotheses}\mid  y_{observation})$ probability density. Expectation of the states x given observations y  | Ensemble density; simplifies under lapace transformation to $q(x; \mu )$ |\n$p(x,y)$ or $p(x_{hypotheses},y_{observation})$ | Generative probability density: the brain encoding a probabilistic model of the environment/world in which it is immersed. |  |\n$\zeta$ | sufficient statistics (e.g. for a Gaussian distribution: mean ðœ‡, variance $ðœŽ^2$) for the recognition probability density | $\mu$ (because under laplace approximation sufficient statistics simplifies to mean $\mu$, see next notebook), in few papers noted as $\lambda$ (but in other papers $\lambda$ sometimes also used for precision of random noise) |\n\n\n"
"As we can see from our graphs and the MSE values above, a random forest of 10 trees achieves a better result than a single decision tree and is comparable to bagging with 10 trees. The main difference between random forests and bagging is that, in a random forest, the best feature for a split is selected from a random subset of the available features while, in bagging, all features are considered for the next best split.\n\nWe can also look at the advantages of random forests and bagging in classification problems:"
"The figures above show that the decision boundary of the decision tree is quite jagged and has a lot of acute angles that suggest overfitting and a weak ability to generalize. We would have trouble making reliable predictions on new test data. In contrast, the bagging algorithm has a rather smooth boundary and has no obvious signs of overfitting.\n\nNow, let's investigate some parameters which can help us increase the model accuracy.\n\n## 3. Parameters\n\nThe [scikit-learn library](http://scikit-learn.org/stable/) implements random forests by providing two estimators: `RandomForestClassifier` and `RandomForestRegressor`.\n\nThe full list of random forest parameters for regression is shown below:"
"As you can see, when a certain number of trees is reached, our accuracy on the test set is very close to the asymptote. You can decide by yourself which value would be the optimal number of trees for your problem.\n\nThe figures also show that we achieved 100% accuracy on the training set, which tells us that we overfit. In order to avoid overfitting, we need to add regularization parameters to our model. \n\nWe will start with the maximum depth of trees `max_depth` and fix the number of trees at 100:"
Parameter `max_depth` copes well with the regularization of our model and it does not overfit as badly as before. The model accuracy has increased slightly.\n\nAnother important parameter worth tuning is `min_samples_leaf`. It also contributes to regularization.
"In this case, we do not see an improvement in accuracy on the validation set, but we significantly reduce the overfitting down to 2% while keeping the accuracy at about 92%.\n\nLet's consider the parameter `max_features`. For classification, the value $\large \sqrt{d}$ (the total number of features) is typically used as the default choice. Let's check whether it would be optimal to use 4 features in our case:"
"In our case, the optimal number of features is equal to 10. This is the value at which the best result is achieved.\n\nWe have seen how the learning curves change with different values of the basic parameters. Now, let's use `GridSearch` to find the optimal parameters for our example:"
"# PLOTLY ULTIMATE GUIDE FOR BEGINNERSðŸ“ˆðŸ“Š\n## If you find this notebook useful, support with an upvoteðŸ‘"
\n## Imports
\n## Imports
â¬†ï¸Back to Table of Contents â¬†ï¸
\n### Basic Scatter PLot
â¬†ï¸Back to Table of Contents â¬†ï¸
\n### Scatter PLots with Variable-sized circular markers
â¬†ï¸Back to Table of Contents â¬†ï¸
\n### Continous Color Scatter Plots
â¬†ï¸Back to Table of Contents â¬†ï¸
Different marker styles available here - https://plotly.com/python/marker-style/
â¬†ï¸Back to Table of Contents â¬†ï¸
\n#### Single Linear Refression fit
â¬†ï¸Back to Table of Contents â¬†ï¸
\n#### Multiple Linear Refression fit
â¬†ï¸Back to Table of Contents â¬†ï¸
\n#### Single Trendline with multiple traces
â¬†ï¸Back to Table of Contents â¬†ï¸
\n### Scatter Plot on large dataset
â¬†ï¸Back to Table of Contents â¬†ï¸
\n### Animated Scatter Plot
####  Press the play â–¶ï¸ button to see the animation
\n### Basic Line Plot
â¬†ï¸Back to Table of Contents â¬†ï¸
\n### Line Plot with Column Encoding Color
â¬†ï¸Back to Table of Contents â¬†ï¸
\n### Line Plot with markers(symbols)
â¬†ï¸Back to Table of Contents â¬†ï¸
\n### Line Plot with Date Axes 
â¬†ï¸Back to Table of Contents â¬†ï¸
\n### Style Line Plot
â¬†ï¸Back to Table of Contents â¬†ï¸
\n### Basic Pie Plot
â¬†ï¸Back to Table of Contents â¬†ï¸
\n### Pie Plot with Hover Text
â¬†ï¸Back to Table of Contents â¬†ï¸
\n### Donut Plot
â¬†ï¸Back to Table of Contents â¬†ï¸
\n### Pulled- out Pie Plot
â¬†ï¸Back to Table of Contents â¬†ï¸
\n### Pie Chart in Subplots
â¬†ï¸Back to Table of Contents â¬†ï¸
\n### Basic Bar Plot
â¬†ï¸Back to Table of Contents â¬†ï¸
\n### Bar Plot with Hover Text
â¬†ï¸Back to Table of Contents â¬†ï¸
\n### Bar Plot with continous Color scale
â¬†ï¸Back to Table of Contents â¬†ï¸
\n### Horizontal Bar Plot
â¬†ï¸Back to Table of Contents â¬†ï¸
\n### Stacked Bar Plot
â¬†ï¸Back to Table of Contents â¬†ï¸
\n###  Grouped Bar Plot
â¬†ï¸Back to Table of Contents â¬†ï¸
\n### Facetted Bar Subplots
â¬†ï¸Back to Table of Contents â¬†ï¸
\n### Animated Bar Plot
####  Press the play â–¶ï¸ button to see the animation
#### Basic Box Plot -1 
â¬†ï¸Back to Table of Contents â¬†ï¸
#### Basic Box Plot - 2 
â¬†ï¸Back to Table of Contents â¬†ï¸
\n###  Horizontal Bar Plot
â¬†ï¸Back to Table of Contents â¬†ï¸
\n#### Box Plots with all points
â¬†ï¸Back to Table of Contents â¬†ï¸
\n#### Box Plots with outlier points
â¬†ï¸Back to Table of Contents â¬†ï¸
\n#### Box Plots with only points
â¬†ï¸Back to Table of Contents â¬†ï¸
\n###  Grouped Bar Plot
â¬†ï¸Back to Table of Contents â¬†ï¸
\n###  Styled Bar Plot
â¬†ï¸Back to Table of Contents â¬†ï¸
\n###  Rainbow Bar Plot
â¬†ï¸Back to Table of Contents â¬†ï¸
"Kaggle Survey 2021\n\nIn the fifth year running this survey, Kaggle staffs are once again awed by the global, diverse, and dynamic nature of the data science and machine learning industry. This survey data EDA provides an overview of the industry on an aggregate scale, but it also leaves them wanting to know more about the many specific communities comprised within the survey. For that reason, theyâ€™re inviting the Kaggle community to dive deep into the survey datasets and help them tell the diverse stories of data scientists from around the world.\n\nThe challenge objective: tell a data story about a subset of the data science community represented in this survey, through a combination of both narrative text and data exploration. A â€œstoryâ€ could be defined any number of ways, and thatâ€™s deliberate. The challenge is to deeply explore (through data) the impact, priorities, or concerns of a specific group of data science and machine learning practitioners. That group can be defined in the macro (for example: anyone who does most of their coding in Python) or the micro (for example: female data science students studying machine learning in masters programs). This is an opportunity to be creative and tell the story of a community you identify with or are passionate about!"
Our Data:
The First Question! What is your age?
"What did we find?\n\nHere, we can clearly see that majority of Kagglers are between 21 to 29 years of age.\nVery few Kagglers are over the age of 50.\nThis is not very unexpected, as many students who like to explore Machine Learning(ML) or Data Science(DS) usually start off with Kaggle. But! we are not sure about this claim as of now. This is just a speculation. Let's explore further to find out, if this is really the case."
"Next Question, What is your gender?"
"What do we see here?\n\nKaggle community mostly comprises of Male ML Enthusiasts. They consists of almost 80% of the people here on Kaggle.\nThere are also 18% Female Kagglers. The ratio is not that bad, but it can be improved upon.\nFinally, about 2% are either Non-Bianary, Do not wish to disclose or Prefer to self explain."
Where is everyone from?
"A lot of Kagglers are from India!\n\nThe higher the saturation, the more the number of people from that country. Therefore, it is clearly visible that most of the Kagglers live in India, USA, Japan and China.\nVery less number of people can be found from- Kazakhstan, Norway, Algeria, Ehiopia and Iraq.\nThis maybe due to the fact that people from that Demography, probably prefer some other profession rather than ML, AI, DataScience or Machine Learning.\nEfforts can be made to introduce students and working professionals of those countries to the world of Data! This will encourage them to explore more datasets and ML problem statements. Who knows, maybe one day, these essential skills might help solve a serious problem in the country or even the whole wide world. Think big!"
Let's take a look at Kaggler's Highest Level of Formal Education...
"Very Qualified Professionals here\n\n![image.png](attachment:289e12ba-674d-40c6-bf5a-9c6d1a830d71.png)\n\nMost of the Kagglers either hold a Master's degree or a Bachelor's degree\nFew people have a Professional Doctorate or No Formal education past High School.\nA decent amount of people have Doctoral Degree.\nWe have diversity in terms of qualifications, however, it is dominated by people holding a Bachelor's, Master's or Doctoral degrees."
"A Quick Peek, at Professional Role/Job Titles of Kagglers"
"I am Studying here...\n\nWell, I'll be honest with you guys. I thought most of the Kagglers were gonna be Data Scientists or ML Enginneers, however, after this analysis, we can see that majority of the Kagglers are Students!. That is DOUBLE! the amount of Data Scientists we have here.\nTherefore, Students, followed by Data Scientists, Software Engineers and Data Analysts are pre-dominantly the main professions here on this platform.\nThere is also an interesting CATCH here! Notice how 'Others' bar is pretty high? This goes to show that, ML, AI and working with Data is clearly attracting people from various different professions onto Kaggle! Now this is good news!\nThis is expected as we are moving into a whole new generation where we have loads of devices, sensors etc that generate BIG! data every second. With more and more availablility of data, it opens the ground to train various machine learning models and opens up a lot of opportunities to seek and explore. This has proved to be  very essential for people working in any domain!"
"Okay, but for how long have they been programming?"
"Let me just paste this one from StackOverflow...\n\nNow this is what I was expecting atleast. If you see the graphs above, you will realise that ""Our Kaggle Community is very young!"" like MarÃ­lia Prata put it in her words.\nTherefore, there are many Kagglers who's Experience in Programming is just 1-3 years.\nWe see that, as the years of experience increases, the number of Kagglers in our community decreases.\nKaggle is a great place for anyone willing to step into the World of Data. There is, or at-least I would say, that there are good proportion of skilled and novice Kagglers on the platform!"
User Resource Preferences:
"print(""Python is better!"")\n\nThis is not a surprise at all! From our previous analysis, we had concluded that most of the Kagglers are Students. They have barely started to code. Therefore, for beginners, Python is a very friendly coding language to pick up and learn.\nBecause of this, most of the newbies love to use Python. Not only that, Python has a large number of modules that make life easier for any developer, analyst etc!\n\n![meme](https://i.pinimg.com/originals/a4/31/4a/a4314a37a2a0a1ce775d55b4c8b1383d.png)\n\nPython therefore, wins this battle against R on Kaggle by a large margin! Python:80% and R:20%"
"print(""Python is better!"")\n\nThis is not a surprise at all! From our previous analysis, we had concluded that most of the Kagglers are Students. They have barely started to code. Therefore, for beginners, Python is a very friendly coding language to pick up and learn.\nBecause of this, most of the newbies love to use Python. Not only that, Python has a large number of modules that make life easier for any developer, analyst etc!\n\n![meme](https://i.pinimg.com/originals/a4/31/4a/a4314a37a2a0a1ce775d55b4c8b1383d.png)\n\nPython therefore, wins this battle against R on Kaggle by a large margin! Python:80% and R:20%"
"It is widely used everywhere...\n\nThough in a life of a Data Scientist, several programming languages like Java, C++, C etc are used, Python still holds a distinct place in most organizations across the globe\nAlso, as per our previous analysis, we can clearly see that many Kagglers prefer Python over several other programming languages. Hence, we can safely conclude that 7/10 people will always recommend Python over any other programming languages."
"It is widely used everywhere...\n\nThough in a life of a Data Scientist, several programming languages like Java, C++, C etc are used, Python still holds a distinct place in most organizations across the globe\nAlso, as per our previous analysis, we can clearly see that many Kagglers prefer Python over several other programming languages. Hence, we can safely conclude that 7/10 people will always recommend Python over any other programming languages."
"We like it SIMPLE!\n\nJupyter Notebook is very user friendly! We simply type the codes in the cell and press Shift+Enter to execute. Simple! And the result is also displayed just below.\nSimilar case for VS Code and PyCharm. They are very user friendly! Also, there are several plugins available to make the user experience even better! Therefore, Jupyter Notebook, PyCharm and VS Code are favourite among many developers."
"We like it SIMPLE!\n\nJupyter Notebook is very user friendly! We simply type the codes in the cell and press Shift+Enter to execute. Simple! And the result is also displayed just below.\nSimilar case for VS Code and PyCharm. They are very user friendly! Also, there are several plugins available to make the user experience even better! Therefore, Jupyter Notebook, PyCharm and VS Code are favourite among many developers."
"https://colab.research.google.com/\n\nGoogle colab just barely edges out Kaggle Hosted Notebook services. Just like the Kaggle hosted interface, it is simple to use and has no limits on the GPU usage. Maybe that is one of the reason why people prefer this? Or is it because, users can directly get their datasets from the google drive? The fact that we can connect to our local runtime? Who knows.\nWhatever might be the case, it is worth to notice that a decent number of people still prefer to run their codes on their local machine. Probably, in my opinion if Kaggle Kernels can be made more user friendly and provide more features similar to that of Google colab, it might turn out to be a popular choice by next year!"
"https://colab.research.google.com/\n\nGoogle colab just barely edges out Kaggle Hosted Notebook services. Just like the Kaggle hosted interface, it is simple to use and has no limits on the GPU usage. Maybe that is one of the reason why people prefer this? Or is it because, users can directly get their datasets from the google drive? The fact that we can connect to our local runtime? Who knows.\nWhatever might be the case, it is worth to notice that a decent number of people still prefer to run their codes on their local machine. Probably, in my opinion if Kaggle Kernels can be made more user friendly and provide more features similar to that of Google colab, it might turn out to be a popular choice by next year!"
"I spy... A nimbus in the sky!\n\nFrom the word cloud, it is evident that most Kagglers like to use their own Laptop, followed by Cloud Platform and Personal Computer.\nRemember, that from our previous analysis, we saw how most of the individuals on Kaggle are Students? They might not have the money for fancy Deep Learning Platforms and cool gadgets like that. Hence, a Laptop is a very popular choice.\nAnother thing noticing here is that, the next best choice are the cloud computing platforms. Usually this is the case for people who's Laptops or Personal Computers are not compatible enough to run Machine Learning libraries."
"A Kaggler's Secret Ingredient:\n\nKaggle provides free access to NVIDIA TESLA P100 GPUs. These GPUs are useful for training deep learning models, though they do not accelerate most other workflows (i.e. libraries like pandas and scikit-learn do not benefit from access to GPUs).\n\nYou can use up to a quota limit per week of GPU. The quota resets weekly and is 30 hours or sometimes higher depending on demand and resources\n\nRead more about GPUs here."
"Dad... I need an Nvdia RTX 3090Ti for SchoolðŸ‘‰ðŸ‘ˆ\n\nOfcourse I need a 3090Ti for studies. Kidding. Nvdia GPUs are preferred by many for it's ability to process ML models and especially DL models faster!\nThat is followed by Google Cloud TPUs due to it's easy availability.\nAlso, if you notice, there are many Kagglers that do not use any specialized Hardware. Now there is a reason for that. Not everyone knows Deep Learning and Scikit Learn does not support GPUs. Therefore, for most Kagglers, they do not require any hardware as such to increase their computing power!\nNow this is not the case for people using TPUs and GPUs. As of now, I am speculating these Kagglers participate in Competitions a lot! And performing large number of cross-validations/training on several models takes a lot of time and effort if there is no GPUs or TPUs."
"Let's look at Kaggler's TPU usage next: \n\nTPUs are now available on Kaggle, for free. TPUs are hardware accelerators specialized in deep learning tasks. They are supported in Tensorflow 2.1 both through the Keras high-level API and, at a lower level, in models using a custom training loop.\n\nYou can use up to 30 hours per week of TPUs and up to 9h at a time in a single session.\n\nRead more about TPUs here."
âˆž Power \n\nTPUs are now available on Kaggle and are mainly used during Competitions or for other Deep Learning and Computer Vision purposes.\nThe reason why most people do not use TPUs remains almost the same as why people do not use GPUs\nMostly Kagglers tend to use it 2-5 times a week or maybe once in a while.
"Tools of Trade:\n\n â€œThrough all of your snares, Blade Fury cuts like silk.â€ â€“ Juggernaut (DOTA 2) \n\nThe above quote is one of my favourite quotes from the online MOBA game Defense of The Ancients a.k.a DOTA 2. The reason why I put this quote here is because, the data that we get most of the time are unexplored with many small traps! If we understand a data incorrectly or interpret it differently, it may lead us astray. Hence, these are similar to snares in this case. Blade Fury are the tools that we have at our disposal to cut through these snares!\n\nCome, let's find out, the popular tools used by every Kaggler."
"Everyone's Favourite \n\nMatplotlib, Seaborn and Plotly continues to be the popular choice among many Kagglers followed by GGPlot for R users. Especially because they are so easy to implement. We can also see that there are other libraries like Geoplotlib, Bokeh and Folium trending these days on kaggle.\nMoving onto the Machine Learning usage of Kagglers, we notice the correlation again that since there are more number of Students on this platform, we have more number of kagglers who's ML experiece ranges from less than 1 year to 3 years. Another interesting thing to notice here is that a lot of people here on Kaggle do not use Machine Learning. They work on Data Analysis or Descriptive Statistical Analysis, but are not actively involved with Predictive Analystics.\nSkLearn continues to be the top ML framework used by many Kagglers followed by Tensor-flow and Keras. It is also worth noticing that Pytorch, XGBoost, LightGBM and CatBoost also have decent popularity among Kagglers. On the other hand, JAX, MXNET and H2O 3 continue to be at the bottom of the list.\nMoving onto the popular ML Algorithms, we can clearly see that Linear Models, Tree Models and CNN are the most widely preferred Algorithms here on Kaggle. This is followed by Dense Neural Net, RNN and Bayesian Approaches. Now this is expected since most of the population of Kagglers are students and they start usually by learning about Linear and Tree models.\nImage classification and other general purpose networks (VGG, Inception, ResNet, ResNeXt, NASNet, EfficientNet, etc), Image segmentation methods (U-Net, Mask R-CNN, etc) and Object detection methods (YOLOv3, RetinaNet, etc) are the most popular Computer Vision Modules used by Kagglers here. Now this is not used by many Kagglers. These are only used by a little proportion of people.\nNLP methods are also used by a certain part of the population. Not everyone uses this. But for those that do - Word embeddings/vectors (GLoVe, fastText, word2vec), Transformer language models (GPT-3, BERT, XLnet, etc) and Encoder-decoder models (seq2seq, vanilla transformers) are the most popular among them."
# 2. Import Packages\n[Table of contents](#0.1)
"Before going further, let's have a quick look on what is **bfloat16 floating-point format**.\n\n> This format is a truncated (16-bit) version of the 32-bit IEEE 754 single-precision floating-point format (binary32) with the intent of accelerating machine learning and near-sensor computing.\n\n> Bfloat16 is used to reduce the storage requirements and increase the calculation speed of machine learning algorithms. \n\n[wiki](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format)"
Example Plot:
"First, we have to calculate P_trans from the true states:"
Here you can see each probability distribution:
"They are all Gaussian with a common standard deviation, however, there are not enough data points for low states. We could calculate smooth Gaussians for each state (I did this via calculating weighted mean / std) and linear interpolation, but it really doesn't change much. It's simply not a problem when p_signal is bad in places where there are anyway not many points.\n\nLet's visualize P_signal again in matrix form:"
"They are all Gaussian with a common standard deviation, however, there are not enough data points for low states. We could calculate smooth Gaussians for each state (I did this via calculating weighted mean / std) and linear interpolation, but it really doesn't change much. It's simply not a problem when p_signal is bad in places where there are anyway not many points.\n\nLet's visualize P_signal again in matrix form:"
We finally need to put the actual signal into these bins as well:
Designed and run in a Python 3 Anaconda environment on a Windows 10 computer.  
### Code Setup
"These are the needed library imports for problem setup.  Many of these libraries request a citation when used in an academic paper.  Numpy is utilized to provide many numerical functions needed in the EDA (van der Walt, Colbert & Varoquaux, 2011). Pandas is very helpful for its ability to support data manipulation (McKinney, 2010).  SciPy is utilized to provide signal processing functions (Jones E., et al, 2001).  Matplotlib is used for plotting (Hunter, 2007).  The Jupyter environment in which this code is presented and was run is a descendant of the IPython environment originated by PÃ©rez & Granger (2007)."
Define some constants for data location.
"A basic time series plot of the raw data.  Because of the length of the data, the plot samples every 100th data point. These plots are very common on the Kaggle site's kernels section, this one is taken from Preda (2019).  Earthquakes occur when the time-to-failure signal (blue) jumps up from very near zero to a much higher value where that new higher value is the time to what is then the next quake.  There appears to be a short term high amplitude oscillation very shortly before each quake.  But, there also several similar such peaks that occur nearer the region centered in time between quakes.  Signal noise seems to increase as time gets closer to failure, though there is also a drop after the big peak.  A signal's standard deviation may prove to be a helpful predictor.  The region just after the big peak may be especially hard to predict."
"From the above plot we can see that 16 earthquakes occur in the data.  The earthquakes happen when the time-to-failure reaches very nearly zero and then jumps up.  There are only 15 complete time ramps that result in an earthquake and 2 incomplete time ramps.  One challenge of this competition is that there are only these very few earthquakes to work with.  The is a signal spike (high amplitude) just before an earthquake, but there are also signal spikes in other places that may complicate matters.  While the acoustic signal is very large at over 600m rows, the very small number of actual earthquakes available will make machine learning a challenge."
"Having validated that the test samples are 150,000 samples long, it could help to examine the training data in 150,000 sample chunks.  Here are some random samples of the training data and plots of the acoustic signal along with the time-to-failure.  Note that the y-axis scales vary for all plots.  It is very interesting that the high level spike in the signal occurs in row 3, column 1 in a signal only 0.32 seconds before failure.  This spike reaches vales above 2000 and below 4000.  A similar situation occurs in row 4, column 2.  Contrast this with other plots where the time-to-failure is many seconds away and the signal level peaks around 100.  Time-to-failure plots as a ramp because we are slowly approaching the next quake as the signal progresses in time.  Apparently the time-to-failure has limited resolution and so is represented by a stair step rather than a smooth line.  It appears that if signal spikes could be captured in some way it might help the effort to model time-to-failure.  Somewhat shockingly, we are trying to predict an earthquake from only around 0.04 seconds or so worth of data, as can be noted from examining the right hand axes of the plots. "
"Find the indices for where the earthquakes occur, then plotting may be performed in the region around failure."
"Below is a look at the signal just before failure (an earthquake).  It is very interesting that the signal becomes quiet in the 150k sample slice before an earthquake.  Thus, the signal spike observed in the big picture plot above must occur more than one slice (more than 150k samples) before the earthquake.  These do not appear much different than plots 5 or even 8 seconds before the quake that are presented above where the time to failure ramps are shown.  This looks like it will create major problems for accurate prediction.  Earthquakes are deemed to have occurred where the blue line jumps up in value, representing the time to the next quake."
"By expanding the time to failure plots, it appears that the big spike in the signal before failure is remarkably consistent.  The problem is that the 150k sample slices are actually very short compared to the overall time between quakes and thus these big apparently meaningful signal spikes are unlikely to be present in many training or test samples.  If slicing the training data directly into 150k chunks, then only 16 of 4194 training samples (0.38%) would contain a meaningful high-valued spike.  It is possible that the analysis of these spikes in some manner could add predictive capability to any small number of test samples that might contain these features, this possibility has not been explored by this author yet."
"By expanding the time to failure plots, it appears that the big spike in the signal before failure is remarkably consistent.  The problem is that the 150k sample slices are actually very short compared to the overall time between quakes and thus these big apparently meaningful signal spikes are unlikely to be present in many training or test samples.  If slicing the training data directly into 150k chunks, then only 16 of 4194 training samples (0.38%) would contain a meaningful high-valued spike.  It is possible that the analysis of these spikes in some manner could add predictive capability to any small number of test samples that might contain these features, this possibility has not been explored by this author yet."
"A check of the test data in the time domain is presented below, it is difficult to tell from such short signal bursts if they match the character of the training data.  None of the very high valued signal spikes were caught in a partial look at the test data.  However, these are rare and the existence of the spikes will be taken up again later. "
"A check of the test data in the time domain is presented below, it is difficult to tell from such short signal bursts if they match the character of the training data.  None of the very high valued signal spikes were caught in a partial look at the test data.  However, these are rare and the existence of the spikes will be taken up again later. "
"Frequency components of the signal could be very interesting to look at.  This is a plot of the Fourier transform magnitude for some of the test signals.  Note that there appears to be little information in the signal above the 20,000th frequency line.  Noise appears to mostly disappear above the 25,000th frequency line.  It is difficult to translate this to a frequency because of the signal gaps noted earlier.  Still, it may be best to concentrate signal analysis on frequencies below those represented by the 20,000th frequency line.  Also, there are peaks in the frequency analysis that may be valuable to collect in some manner.  The DC component was eliminated for plotting purposes because it would otherwise dominate the plot and make the other frequencies hard to see.  Also note that while referred to as an ""FFT"" in the code below, this is actually a Discreet Fourier Transform (DFT) because the signal length of 150k samples is not a number that is a power of two."
"Frequency components of the signal could be very interesting to look at.  This is a plot of the Fourier transform magnitude for some of the test signals.  Note that there appears to be little information in the signal above the 20,000th frequency line.  Noise appears to mostly disappear above the 25,000th frequency line.  It is difficult to translate this to a frequency because of the signal gaps noted earlier.  Still, it may be best to concentrate signal analysis on frequencies below those represented by the 20,000th frequency line.  Also, there are peaks in the frequency analysis that may be valuable to collect in some manner.  The DC component was eliminated for plotting purposes because it would otherwise dominate the plot and make the other frequencies hard to see.  Also note that while referred to as an ""FFT"" in the code below, this is actually a Discreet Fourier Transform (DFT) because the signal length of 150k samples is not a number that is a power of two."
"This is a set of plots of the Fourier transform, windowed by short cosine tapers near the ends.  The idea of using the window is to avoid any start up transients that might cause ringing in filters applied to the signal.  It is very interesting that windowing seems to emphasize noise.  This noise, however, would almost entirely be removed by a low pass or band pass filter.  Windows are used to force the signal to be periodic in the time domain which reduces leakage effects at the signal endpoints in the Fourier transform."
"This is a set of plots of the Fourier transform, windowed by short cosine tapers near the ends.  The idea of using the window is to avoid any start up transients that might cause ringing in filters applied to the signal.  It is very interesting that windowing seems to emphasize noise.  This noise, however, would almost entirely be removed by a low pass or band pass filter.  Windows are used to force the signal to be periodic in the time domain which reduces leakage effects at the signal endpoints in the Fourier transform."
"Phase plots for the Fourier transform.  The signal was windowed by small sections taken from a Hanning window along the edges in order to not cause an impulse-like transient in a potential filter at start-up.  Phase plots show what appears to be just noise. Probably limited phase features will be all that is desired for the model, perhaps just the standard deviation."
"Phase plots for the Fourier transform.  The signal was windowed by small sections taken from a Hanning window along the edges in order to not cause an impulse-like transient in a potential filter at start-up.  Phase plots show what appears to be just noise. Probably limited phase features will be all that is desired for the model, perhaps just the standard deviation."
"From the code below, the test set ""big peaks"" seem to match the proportion with which they are found in the training data at 0.38% of signals (test set signals are all 150k samples long).  The number of earthquakes in the training set seems to be approximately proportional to the overall size relationship between the two sets. With 16 quakes in the training set we might expect 10 quakes in the test set based on their relative sizes.  As seen by the code output, there are 10 such files within the test set that exhibit the peak behavior.   However, there is also concern about the possibility of bogus peaks in the test data that do not correlate well with an actual quake.  The expectation of 10 possible quakes in the test set should be considered cautiously and as only an approximation."
 RandomForestClassifier  
"> **Accuracy is determining out of all the classifications, how many did we classify correctly? This can be represented mathematically as:**\n\n"
> True Positives (TP) â€“ True Positives occur when we predict an observation belongs to a certain class and the observation actually belongs to that class.\n\n> True Negatives (TN) â€“ True Negatives occur when we predict an observation does not belong to a certain class and the observation actually does not belong to that class.\n\n> False Positives (FP) â€“ False Positives occur when we predict an observation belongs to a certain class but the observation actually does not belong to that class. This type of error is called Type I error.\n\n> False Negatives (FN) â€“ False Negatives occur when we predict an observation does not belong to a certain class but the observation actually belongs to that class. This is a very serious error and it is called Type II error.\n\nThese four outcomes are summarized in a confusion matrix given below.
\n    \n
\n    \n
Precision Recall Curve 
\n    \n
**References:**\n\nhttps://towardsdatascience.com/evaluation-metrics-for-classification-problems-in-machine-learning-d9f9c7313190
## **Import Modules** 
* With this code below you can check if the kernel use GPU or not.
* It is important to know the distribution of data according to the labels they have.\n* This data set is __homogeneously__ distributed as you see below.
"* __If the data wasn't homogeneously__ distributed what would we do?\n    1. Then we could use data augmentation techniques to generate new data for low quantity labels,\n    2. Or if we have enough data we can discard some high quantity labels"
#### **Image of Handwritten Character** \n[Return Contents](#0)\n\n    \n * An overview of a picture\n * You can change the $num$ variable to see other numbers.
"## **Data Preprocessing** \n### **Normalizing Data** \n[Return Contents](#0)\n\n* What is normalizing? Normalization means that adjusting values measured on different scales to a notionally common scale.\n* Why should you normalize the data?  With a normalized data weight values reach optimum value faster.\n* On image processing applications generally we normalize data to 0-1 scale with dividing data to 255.\n* Because each pixel in every sample of training set has integer values from 0 to 255.\n* In order to normalize training set data, we need to convert x to float type."
"* If 90% of the data set is cat image and 10% is dog image, your accuracy will be 90% even if you estimate the entire test set as a cat.\n* But in another aspect, the model's success in predicting dogs is 0%.\n* In this context, accuracy may not always give us realistic information about the actual performance of the model.\n* The confusion matrix shows how confused your classification model is for which classes by detailing the relationship between actual class and predicted class.\n* If there is an anomaly something like above mentioned, you can specify the problem with confusion matrix and improve accuracy by various methods like adding more data for a specific class, etc."
### **F1 Score Calculation** \n[Return Contents](#0)
![rsz_1form%C3%BCl.jpg](attachment:rsz_1form%C3%BCl.jpg)
### **Evaluate with Another Dataset** \n[Return Contents](#0)
\n# Importing Libraries
\n# Basic Exploration
> Gender and Response
It is noticable that Gender does not play much role in Response of the Customer and the ratio for both positive and negative response is almost equal.\n
> Age and Response
Young people (under 35) have little interest in insurance.\nPeople over the age of 35 tend to be more interested in insurance.
> Driving License and Response
Having a driver's license doesn't play much of a role in Customer Feedback.\n
> Region code and Response
Less correlation is there between region and response.
> Vintage and Response
It seems that Vintage has no effect on the response. The rate of positive and negative feedback seems to be about the same
> Vehicle_Damage and Response
Damaged vehicles tend to respond positively
> Correlation between attributes
> Highly correlated columns wrt to target columns which can give us `better accuracy`.
\n# Importing Libraries
\n# About Dataset
\n# Custom Palette For Visualization
\n# Top Anime Community\n
\n# Top Anime Community\n
**Insights:**\n\n* **Death Note** wears the crown for highest community members followed by **Shingeki no Kyojin** and **Sword Art Online**
\n# Overall Anime Ratings\n
**Insights:**\n\n* Most of the Anime ratings are spread between 5.5 - 8.0\n* Most of the users ratings are spread between 6.0 - 10.0\n* The mode of the users ratings distribution is around 7.0 - 8.0\n* Both the distribution are left skewed\n* Users rating(-1) is an outlier in ratings of users which can be discarded
\n# Top Animes Based On Ratings\n
**Insights:**\n\n* **Mogura no Motoro** wears the crown for highest rating followed by **Kimi no Na wa.** and **Fullmetal Alchemist: Brotherhood**
\n# Category-wise Anime Ratings Distribution\n
**Insights:**\n\n* Most of the Anime ratings are spread between 6.0 - 8.0\n* Most of the users ratings are spread between 6.0 - 10.0\n* The mode of the users ratings distribution is around 7.0 - 9.0\n* Both the distribution are left skewed\n* Users rating(-1) is an outlier in ratings of users which can be discarded
**Insights:**\n\n* Most of the Anime ratings are spread between 6.0 - 8.0\n* Most of the users ratings are spread between 6.0 - 10.0\n* The mode of the users ratings distribution is around 7.0 - 9.0\n* Both the distribution are left skewed\n* Users rating(-1) is an outlier in ratings of users which can be discarded
**Insights:**\n\n* Most of the Anime ratings are spread between 5.5 - 7.5\n* Most of the users ratings are spread between 5.5 - 10.0\n* The mode of the users ratings distribution is around 7.0 - 8.0\n* Both the distribution are left skewed\n* Users rating(-1) is an outlier in ratings of users which can be discarded
**Insights:**\n\n* Most of the Anime ratings are spread between 5.5 - 7.5\n* Most of the users ratings are spread between 5.5 - 10.0\n* The mode of the users ratings distribution is around 7.0 - 8.0\n* Both the distribution are left skewed\n* Users rating(-1) is an outlier in ratings of users which can be discarded
**Insights:**\n\n* Most of the Anime ratings are spread between 4.5 - 8.5\n* Most of the users ratings are spread between 5.0 - 10.0\n* The mode of the users ratings distribution is around 7.0 - 9.0\n* Both the distribution are left skewed\n* Users rating(-1) is an outlier in ratings of users which can be discarded
**Insights:**\n\n* Most of the Anime ratings are spread between 4.5 - 8.5\n* Most of the users ratings are spread between 5.0 - 10.0\n* The mode of the users ratings distribution is around 7.0 - 9.0\n* Both the distribution are left skewed\n* Users rating(-1) is an outlier in ratings of users which can be discarded
**Insights:**\n\n* Most of the Anime ratings are spread between 5.5 - 8.0\n* Most of the users ratings are spread between 5.0 - 10.0\n* The mode of the users ratings distribution is around 7.0 - 8.0\n* Both the distribution are left skewed\n* Users rating(-1) is an outlier in ratings of users which can be discarded
**Insights:**\n\n* Most of the Anime ratings are spread between 5.5 - 8.0\n* Most of the users ratings are spread between 5.0 - 10.0\n* The mode of the users ratings distribution is around 7.0 - 8.0\n* Both the distribution are left skewed\n* Users rating(-1) is an outlier in ratings of users which can be discarded
**Insights:**\n\n* Most of the Anime ratings are spread between 4.0 - 7.0\n* Most of the users ratings are spread between 5.0 - 10.0\n* The mode of the users ratings distribution is around 7.0 - 8.0\n* Both the distribution are left skewed\n* Users rating(-1) is an outlier in ratings of users which can be discarded
**Insights:**\n\n* Most of the Anime ratings are spread between 4.0 - 7.0\n* Most of the users ratings are spread between 5.0 - 10.0\n* The mode of the users ratings distribution is around 7.0 - 8.0\n* Both the distribution are left skewed\n* Users rating(-1) is an outlier in ratings of users which can be discarded
**Insights:**\n\n* Most of the Anime ratings are spread between 4.0 - 7.5\n* Most of the users ratings are spread between 5.0 - 10.0\n* The mode of the users ratings distribution is around 6.5 - 8.0\n* Both the distribution are left skewed\n* Users rating(-1) is an outlier in ratings of users which can be discarded
"## General information\n\nRussian community on Kaggle is quite strong. We started taking part in Kaggle competitions long ago and most competitions have at least several teams in medal zone, currently there are a lot of russian Grandmasters. It is worth noticing, that most of them are a part ods.ai - an open DS community, which has more than 38k users.\n\nIf we look at other countries, we can see that the number of respondents in Russia is in the 5th place in the overall ranking (excluding ""other"").\n\nMore than 600 Russians took part in the survey this year. Why this number is lower than the last year? I think this year the information of the survey was less spread, not sure why."
"Not surprisingly, the average age is increasing over time. One of the reasons is that people grow older over time (obviously :)). Another reason - more elder experts are switching career from other spheres to DS.\n\nIt is interesting to notice that most of women are younger than men. I suppose it means that more women go into DS after university which is great!"
"---\n\n## **3.Doc2Vec**\n\n---\n\n- Python implementation and application of doc2vec with Gensim\n- Original paper: Le, Q., & Mikolov, T. (2014). Distributed representations of sentences and documents. In Proceedings of the 31st International Conference on Machine Learning (ICML-14) (pp. 1188-1196)."
## **Import training dataset**\n* Import Shakespeare's Hamlet corpus from nltk library
"---\n\n## **4.Word2Vec**\n\n---\n\n- Python implementation and application of word2vec with Gensim\n- Original paper: [Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.](https://arxiv.org/pdf/1301.3781)"
## **Import training dataset**\n- Import Shakespeare's Hamlet corpus from nltk library
## Setup
## Introduction to this project on Time Series Forecasting
We will also create a function that plots the original distribution before and after an imputation(s) is performed:
We will start trying out techniques with `SimpleImputer` from Sklearn:
"Since this operation is so common, Pandas has the `diff` function that computes the differences based on the period:"
### 3.2 Percentage changes 
"The above output shows that for the first 3 dates, Apple stocks didn't change. Then, it increased by 1% of what it was on the first date ('2010â€“12â€“16'). Google's prices are more volatile, fluctuating between 1 and 2% increases during the first 10 dates.\n\nNow, let's plot them to compare growth:"
Both Apple's and Google's achieved over 300% growth from 2011 to 2017. This plot may be even more interesting if we compare their growth to other 500 Fortune Companies:
Both Apple's and Google's achieved over 300% growth from 2011 to 2017. This plot may be even more interesting if we compare their growth to other 500 Fortune Companies:
"As you can see, Apple and Google have much higher growth than other top 500 companies in the US."
"Now, let's plot the running min and max of S&P500 stocks:"
## Summary 
# Show the Scores\nHere we see the scores and we have to decide about a cut-off for counting an image as ship or not. We can be lazy and pick 0.5 but some more rigorous cross-validation would definitely improve this process.
# Make the RLE data if there is a ship\nHere we make the RLE data for a positive image (assume every pixel is ship)
Let's look at the missing values in the dataset.
"Great, there is no missing information in the data."
"First, we visualize our entire dataset completely, and then we proceed to the analysis of the dependencies we are interested in."
"As we can see, most features have a positive correlation with the target variable.\n\nHypothesis: There is multicollinearity in the dataset."
Let's visualize the top 5 countries according to our numerical features.
Conclusions: mainly developed countries from the European Union are in the lead.
We visualize the relationship of the target feature with the rest of the numerical features.
Hypothesis: Happiness score and Whisker signs are multicollinear.
"because Since our target feature is numeric and continuous, and the dependent features are also continuous, then it will be enough to build a correlation matrix."
"The hypothesis about the multicollinearity of the features was confirmed, let's remove the Whisker features."
Let's Detect The Outliers:-
Categorical Variables:-
Find out the relationship between categorical variable and dependent feature SalesPrice:-
Let's Find The Missing values:-
Correlation of training dataset:-
-We can see that there isnâ€™t much correlation among the input features . Thus there is no Multicollinearity among the features. This is good.\n-Few Features have greater than 0.5 Pearson Correlation with output feature.\n-A value closer to 0 implies weaker correlation (exact 0 implying no correlation).\n-A value closer to 1 implies stronger positive correlation.\n-A value closer to -1 implies stronger negative correlation.\n
Let's Find The Outliers:-
Categorical Variables:-
# Prepare the data analysis  \n\n## Load packages
## Load data
\n\n\n\n0Â Â IMPORTSÂ Â Â Â â¤’
\n\n\n\n1Â Â BACKGROUND INFORMATIONÂ Â Â Â â¤’\n\n---\n
4.1 TRAIN METADATA\n\n---\n
4.2 VISUALIZE THE UNIQUE SPECIES PRESENT IN THE DATASET\n\n---\n\n
4.2 VISUALIZE THE UNIQUE SPECIES PRESENT IN THE DATASET\n\n---\n\n
4.2 VISUALIZE EXAMPLES OF UNIQUE INDIVIDUALS IN THE DATASET\n\n---\n\n
5.0 HELPFUL FUNCTIONS\n\n---\n
5.1 LOAD EMBEDDINGS AND OTHER DATA\n\n---
### 5.3.3 Do 1 backpropagation: Compute the LOSS and OPTIMIZE for our images_example:
"Until now we:\n1. Created a Vanilla FNN\n2. Took 1 image through the network and create prediction\n3. Look at the prediction vs actual and computed the loss\n4. Using the loss we updated the weights and biases\n\nThis is called training. The next chapters will be dedicated to training the network and improving it.\n\n# 6. Training the Neural Network\nOur purpose now that we have the structure in place and the data is to make the Vanilla FNN perform well.\n\n## 6.1 Batches\nWith an artificial neural network, we may want to use more than one image at one time. That way, we can compute the *average* loss across a **mini-batch** of **multiple** images, and take a step to optimize the **average** loss. The average loss across multiple training inputs is going to be less ""noisy"" than the loss for a single input, and is less likely to provide ""bad information"" because of a ""bad"" input.\n\nBatches can have different sizes:\n* one extreme is `batch_size` = 1: meaning that we compute the loss and update after EACH image (so we have 60,000 batches of size 1)\n* a `batch_size` = 60: means that, for 60,000 training images, we'll have 1000 batches of size 60\n* the other extreme is `batch_size` = 60,000: when we input ALL images and do 1 backpropagation (we have 1 batch of size 60,000 images)\n\nThe actual batch size that we choose depends on many things. We want our batch size to be large enough to not be too ""noisy"", but not so large as to make each iteration too expensive to run.\n\n\n\nIn the above example, instead of having 70 noisy losses we'll have just 7 averaged losses."
### 6.1.1 Training the Example Network on a batch instead of image by image:
"## 6.2 Accuracy of the Classifier\nDuring Training, we would usually want to check for the accuracy of the model, to see how good or how bad is performing.\n\n \nNote: During training, it is highly important to set the model into training mode by calling your_model.train(). This enables gradients training, the Dropout() function etc. When you evaluate the model call your_model.eval(). This disables the gradients, Dropout() function etc and sets the model in evaluation mode.\n"
### 6.4.2 Predefined Training Function\n\n
"# 7. Model Evaluation\n\nNow that we have our functions ready, we can start training on the ENTIRE dataset.\n\nBut first, to make the training faster, we will:\n* select 500 training images and 500 testing images\n* `batch_size` will be by default set to 20 images/batch\n* we'll iterate through the data 200 times (`num_epochs`=200)"
"# 8. Overfitting\n\nAs any other Machine Learning Model, Neural Nets can suffer from overfitting. Overfitting is when a neural network model learns about the quirks of the training data, rather than information that is generalizable to the task at hand.\n\n## 8.1 Data Augmentation\nWhy try to collect more data when you can create some on your own? *Data Augmentation* generates more data points from our existing data set by:\n* Flipping each image horizontally or vertically (won't work for digit recognition, but might for other tasks)\n* Shifting each pixel a little to the left or right\n* Rotating the images a little\n* Adding noise to the image\n\n\n\nFor our example we'll rotate the images randomly up to 35 degrees."
"### 8.1.1 Training on Augmented Data:\n\n* `transforms.Normalize()`: means to scale the input features of a neural network, so that all features are scaled similarly. For images is not really necessary, as they all have the same structure, but I threw it here just for reference."
The above graph indicates that men are more likely with dementia than women.
The chart shows Nondemented group got much more higher MMSE scores than Demented group.
The chart shows Nondemented group got much more higher MMSE scores than Demented group.
The chart indicates that Nondemented group has higher brain volume ratio than Demented group. This is assumed to be because the diseases affect the brain to be shrinking its tissue. 
The chart indicates that Nondemented group has higher brain volume ratio than Demented group. This is assumed to be because the diseases affect the brain to be shrinking its tissue. 
There is a higher concentration of 70-80 years old in the Demented patient group than those in the nondemented patients.\nWe guess patients who suffered from that kind of disease has lower survival rate so that there are a few of 90 years old.
There is a higher concentration of 70-80 years old in the Demented patient group than those in the nondemented patients.\nWe guess patients who suffered from that kind of disease has lower survival rate so that there are a few of 90 years old.
"## Intermediate Result Summary\n1. Men are more likely with demented, an Alzheimer's Disease, than Women.\n2. Demented patients were less educated in terms of years of education.\n3. Nondemented group has higher brain volume than Demented group.\n4. Higher concentration of 70-80 years old in Demented group than those in the nondemented patients."
"The seasonality patterns can be explored in detail by using boxplots. Seasonality is clearly confirmed for the categories of R03, R06 and N02BE. Some additional conclusions: R03 and N05C has more outliers that the others, indicating that their sales is more difficult to predict."
"Below, boxplots on a weekly scale are shown, for the purpose of exploring the weakly seasonality. Some weekly seasonality is visible."
"Below, boxplots on a weekly scale are shown, for the purpose of exploring the weakly seasonality. Some weekly seasonality is visible."
"Another visualization that can be useful for discovering seasonality patterns is related to rolling window means. Rolling window operations are another important transformation for time series data. Similar to downsampling, rolling windows split the data into time windows and the data in each window is aggregated with a function such as mean(), median(), sum(), etc. However, unlike downsampling, where the time bins do not overlap and the output is at a lower frequency than the input, rolling windows overlap and ""roll"" along at the same frequency as the data, so the transformed time series is at the same frequency as the original time series. That means also that the curve is smoother. Time series data often exhibit some slow, gradual variability in addition to higher frequency variability such as seasonality and noise. An easy way to visualize these trends is with rolling means at larger time scales. Analysis below shows 30-day and 365-day rolling mean and 30-day rolling standard deviation of sales data."
"Another visualization that can be useful for discovering seasonality patterns is related to rolling window means. Rolling window operations are another important transformation for time series data. Similar to downsampling, rolling windows split the data into time windows and the data in each window is aggregated with a function such as mean(), median(), sum(), etc. However, unlike downsampling, where the time bins do not overlap and the output is at a lower frequency than the input, rolling windows overlap and ""roll"" along at the same frequency as the data, so the transformed time series is at the same frequency as the original time series. That means also that the curve is smoother. Time series data often exhibit some slow, gradual variability in addition to higher frequency variability such as seasonality and noise. An easy way to visualize these trends is with rolling means at larger time scales. Analysis below shows 30-day and 365-day rolling mean and 30-day rolling standard deviation of sales data."
"Image below shows trends for each of the drug categories, represented by the 365-d rolling means for each of those categories."
"Image below shows trends for each of the drug categories, represented by the 365-d rolling means for each of those categories."
"Trends and seasonality can be explored in time series decomposition view, based on 30d rolling means."
"Trends and seasonality can be explored in time series decomposition view, based on 30d rolling means."
"Function seasonal_decompose can be used for analysis of the portions of each component of time series. This is especially useful when determining uptake of residuals in data, based on the decomposed data. The volume of this uptake implies the predictability of the time series - higher the residuals, lower the predictability. To some extent, the proportion of the residuals when comparing with trend and seasonality can be also illustrated by the rolling means and standard deviation plots above."
"Autocorrelation analysis illustrates the potential for time series data prediction. Autocorrelation plots graphically summarize the strength of a relationship with an observation in a time series with observations at prior time steps. Pearson coefficient is used to measure autocorrelation. Thus, the following analysis is relevant only for data with normal Gaussian distribution.\n\nA plot of the autocorrelation of a time series by lag is called the AutoCorrelation Function (ACF). This plot is sometimes called a correlogram or an autocorrelation plot. Plot shows the lag value along the x-axis and the correlation on the y-axis between -1 and 1. Confidence intervals are drawn as a cone. By default, this is set to a 95% confidence interval, suggesting that correlation values outside of this code are very likely a correlation."
"In general, the ""partial"" correlation between two variables is the amount of correlation between them which is not explained by their mutual correlations with a specified set of other variables. For example, if we are regressing a variable Y on other variables X1, X2, and X3, the partial correlation between Y and X3 is the amount of correlation between Y and X3 that is not explained by their common correlations with X1 and X2."
"In general, the ""partial"" correlation between two variables is the amount of correlation between them which is not explained by their mutual correlations with a specified set of other variables. For example, if we are regressing a variable Y on other variables X1, X2, and X3, the partial correlation between Y and X3 is the amount of correlation between Y and X3 that is not explained by their common correlations with X1 and X2."
"Minor autocorrelation is observed at ACF (Auto-Correlation Function) and PACF (Partial Auto-Correlation Function) plots for all series, with exception of N05C sales. N02BE, R03 and R06 series were found to exhibit annual seasonality. "
"Chart with daily sales for different categories of interest is shown below. N02BE and N05B charts, though showing the similar trends, are suppresed because of the larger scale which makes the other illustrations less readable."
"Chart with weekly sales for different categories of interest was shown below. N02BE and N05B charts, though showing the similar trends, are suppresed because of the larger scale which makes the other illustrations less readable."
"Chart with weekly sales for different categories of interest was shown below. N02BE and N05B charts, though showing the similar trends, are suppresed because of the larger scale which makes the other illustrations less readable."
## 4.3. Time series forecasting 
#### 4.3.1.1. NaÃ¯ve forecasting
#### 4.3.1.2. Average method forecasting
#### 4.3.1.2. Average method forecasting
#### 4.3.1.3. Seasonal NaÃ¯ve forecasting
#### 4.3.1.3. Seasonal NaÃ¯ve forecasting
### 4.3.2. ARIMA Forecasting 
#### 4.3.2.2. Rolling forecasting with ARIMA model
#### 4.3.2.3. Long-term forecasting with ARIMA model
#### 4.3.2.3. Long-term forecasting with ARIMA model
#### 4.3.2.4. Rolling forecasting with Auto-ARIMA model
#### 4.3.2.4. Rolling forecasting with Auto-ARIMA model
#### 4.3.2.5. Long-term forecasting with Auto-ARIMA model
#### 4.3.2.5. Long-term forecasting with Auto-ARIMA model
### 4.3.3. Prophet forecasting 
#### 4.3.3.2. Rolling forecasts with Prophet
#### 4.3.3.3. Long-term forecasting with Prophet
#### 4.3.3.3. Long-term forecasting with Prophet
### 4.3.4. Forecasting with LSTM
#### 4.3.4.1. Long-term forecasting with Vanilla LSTM configuration
#### 4.3.4.2. Long-term forecasting with Stacked LSTM model
#### 4.3.4.2. Long-term forecasting with Stacked LSTM model
#### 4.3.4.3. Long-term forecasting with Bidirectional LSTM
#### 4.3.4.3. Long-term forecasting with Bidirectional LSTM
# 5. Conclusion
#### Plot the time series data to detect patterns
"If we want to predict the  temperature for the next few months, we will try to look at the past values and try to gauge and extract the pattern. \nHere we observe a pattern within each year indicating a seasonal effect. Such observations will help us in predicting future values.\n\n**Note: We have used only one variable here , Temp (the temperature of the past 19 years).**\n\nHence this is called as the Univariate Time Series Analysis/Forecasting. "
"### Trend & Seasonality\n\nConsider the example of **shampoo sales dataset**. \n\nThis Dataset describes the monthly number of sales of shampoo over a 3 year period. The units are a sales count and there are 36 observations. The original dataset is credited to Makridakis, Wheelwright, and Hyndman (1998).\n\n**Data source**:https://github.com/jbrownlee/Datasets\n\nBelow is a sample of the first 5 rows of data, including the header row.\n\n| Month | Sales |\n| ---- | -------- |\n| 1-01 | 266.0 | \n| 1-02 | 145.9 | \n| 1-03 | 183.1 | \n| 1-04 | 119.3 | \n| 1-05 | 180.3 | \n\n\n"
### The above plot shows an increasing trend.\n
"**Minimum Daily Temperatures Dataset**\n\nhttps://machinelearningmastery.com/time-series-seasonality-with-python/\n\nThis dataset describes the minimum daily temperatures over 10 years (1981-1990) in the city Melbourne, Australia.\n\nData source: Data Market https://datamarket.com/data/set/22r0/sales-of-shampoo-over-a-three-year-period\n\nThe units are in degrees Celsius and there are 3,650 observations. The source of the data is credited as the Australian Bureau of Meteorology.\n\nA sample of the first 5 rows of data, including the header row is shown below:\n\n| Date | Temperature |\n| ------- | -------- | \n| 1981-01-01 | 20.7 | \n| 1981-01-02 | 17.9 | \n| 1981-01-03 | 18.8 | \n| 1981-01-04 | 14.6 | \n| 1981-01-05 | 15.8 | \n"
### The above plot shows a strong seasonality component.
We can draw a boxplot to check the variation across months in a year (1990).\nIt appears that we have a seasonal component each year showing swing from summer to winter.
"This plot shows the signiï¬cant change in distribution of minimum temperatures across the months of the year from the Southern Hemisphere summer in January to the Southern Hemisphere winter in the middle of the year, and back to summer again."
We group the Minimum Daily Temperatures dataset by years. A box and whisker plot is then created for each year and lined up side-by-side for direct comparison.
We don't observe much year-by-year variation 
The above plot shows a strong seasonality and trend component.\n\nWe can draw a boxplot to check the variation across months in a year (2011). 
It appears that we have a seasonal component each year showing swing from May to Aug.
**Let us use Petrol data and observe seasonality using visualization techniques**
"### Seasonal Indices\n\n* Seasonality is the presence of variations that occur at specific regular intervals less than a year, such as weekly, monthly, or quarterly. \n* Seasonality may be caused by various factors, such as weather, vacation, and holidays and consists of periodic, repetitive, and generally regular and predictable patterns in the levels of a time series."
#### Plot the average temp
#### Plot the average forecast
#### Plot the average forecast
#### Plot the moving average forecast and average temperature
#### Plot the moving average forecast and average temperature
### Moving average of window size 5 for US GDP
"### Inference\n\nWe observe that the resample() function has created the rows by putting NaN values as new values for dates other than day 01. \n\nNext we can interpolate the missing  values at this new frequency. The function, interpolate() of pandas library is used to interpolate the missing values. \nWe use a linear interpolation which draws a straight line between available data, on the first day of the month and fills in values at the chosen frequency from this line. "
**Another common interpolation**\n\n* Another common interpolation method is to use a polynomial or a spline to connect the values.\nThis creates more curves and look more natural on many datasets.\n* Using a spline interpolation requires you specify the order (count of terms in the polynomial); we use 2.
"**Down-sampling Frequency**\n\n* The sales data is monthly, but we prefer the data to be quarterly. The year can be divided into 4 business quarters, 3 months a piece. \n* The resample() function will group all observations by the new frequency.\n* We need to decide how to create a new quarterly value from each group of 3 records. We shall use the mean() function to calculate the average monthly sales numbers for the quarter"
"### Example \nWe can turn monthly data into yearly data. Down-sample the data using the alias, A for year-end frequency and this time use sum to calculate the total sales each year."
"### Example \nWe can turn monthly data into yearly data. Down-sample the data using the alias, A for year-end frequency and this time use sum to calculate the total sales each year."
"**Outliers**\nData may contain corrupt or extreme outlier values that need to be identified and handled.\n\n####  Detection of outliers in time series is difficult.\n* If a trend is present in the data, then usual method of detecting outliers by boxplot may not work.\n* If seasonality is present in the data, one particular season's data may be too small or too large compared to others.\n\n#### Decomposition helps in identifying unsual observations\n\n* If trend and seasonality are not adequate to explain the observation\n\n#### Outliers cannot be eliminated - they need to be imputed as closely as possible by using the knowledge gained from decomposition."
### Types of Trends\n\n* Deterministic Trends: They consistently increase or decrease and are easier to identify.\n* Stochastic Trends: They increase and decrease inconsistently \n\n#### Detrend a time series is by differencing
#### Inference\n\nWe don't see any particular trend in the data.
We will use Shampoo dataset.\n\n* A linear model can be fit on the time index to predict the observation. \n* Get a trend line from the predictions from this model.\n* Subtract these predictions from the original time series to provide a detrended version of the dataset.\n\nWe will use a scikit-learn LinearRegression model to train the data.
#### Inference\n\nWe have plotted the trend line in orange colour over the original dataset in blue colour.
"## Seasonal variation may be present in Time series data.\n\n* Seasonal variation, or seasonality, are cycles that repeat regularly over time.\n\n* By plotting and reviewing the data, you can determine if there is any seasonality in the data.\n* We can try with different scales and by adding a trend line.\n* Once the seasonality is identified, it can be modeled. When you remove the model of seasonality from the time series, it is called deseasonalizing or seasonal adjustment.\n\n** Seasonal adjustment with differencing**\n\nWe can test the seasonality differencing method on the daily minimum temperature data."
### Accuracy measures
Let us get the optimum value for $\alpha$ by omitting the value and leave it for the model to decide.
"### Inference\n\nWe observe that for the optimum $\alpha$ value, both RMSE and MAPE are smallest when compared to other $\alpha$ values of 0.1,0.5 and 0.99."
### Check for stationarity using dickey fuller test
"Since the test statistics is more than 5 % critical value and the p-value is larger than 0.05 , the moving average is not constant over time and the null hypothesis of the Dickey-Fuller test cannot be rejected. This shows the weekly time series is not stationary. \n\nAs such , we need to transform this series into a stationary time series. "
### Some of our key observations from this analysis:\n\n1) Trend: 12-months moving average looks quite similar to a straight line hence we could have easily used linear regression to estimate the trend in this data.\n\n2) Seasonality: Seasonal plot displays a fairly consistent month-on-month pattern. The monthly seasonal components are average values for a month after removal of trend. Trend is removed from the time series using the following formula:\n\nSeasonality_t Ã— Remainder_t = Y_t/Trend_t\n \n3) Irregular Remainder (random): is the residual left in the series after removal of trend and seasonal components. Remainder is calculated using the following formula:\n\nRemainder_t = Y_t / (Trend_t Ã— Seasonality_t)
We observe seasonality even after differencing.
We observe seasonality even after differencing.
We observe trend and seasonality even after taking log of the observations.
"Nonstationary series have an ACF that remains significant for half a dozen or more lags, rather than quickly declining to zero. You must difference such a series until it is stationary before you can identify the process\n\nThe above ACF is â€œdecayingâ€, or decreasing, very slowly, and remains well above the significance range (blue band) for at least a dozen lags. This is indicative of a non-stationary series."
### Inference\n\nThe above ACF has â€œdecayedâ€ fast and remains within the significance range (blue band) except for a few (5) lags. This is indicative of a stationary series.
### Plot ACF and PACF for residuals of ARIMA model to ensure no more information is left for extraction
"### Inference\n\nWe need to ensure that the residuals of our model are uncorrelated and normally distributed with zero-mean. If it is not that it signifies that the model can be further improved and we repeat the process with the residuals.\n\nIn this case, our model diagnostics suggests that the model residuals are normally distributed based on the following:\n\n1. The KDE plot of the residuals on the top right is almost similar with the normal distribution.\n2. The qq-plot on the bottom left shows that the ordered distribution of residuals (blue dots) follows the linear trend of the samples taken from a standard normal distribution with N(0, 1). Again, this is a strong indication that the residuals are normally distributed.\n3. The residuals over time (top left plot) don't display any obvious seasonality and appear to be white noise. This is confirmed by the autocorrelation (i.e. correlogram) plot on the bottom right, which shows that the time series residuals have low correlation with lagged versions of itself.\n\nThose observations coupled with the fact that there are no spikes outside the insignificant zone for both ACF and PACF plots lead us to conclude that that residuals are random with no information or juice in them and our model produces a satisfactory fit that could help us understand our time series data and forecast future values. It sems that our ARIMA model is working fine."
## DivisiÃ³n de gÃ©nero
"La grÃ¡fica muestra claramente que hay muchos mÃ¡s encuestados masculinos en comparaciÃ³n con las mujeres. Parece que las damas estaban ocupadas con su trabajo, **o las damas no trabajan** ...: p. Es una broma."
## Los encuestados por paÃ­s
"**USA e India**, constituyen mÃ¡ximos encuestados, alrededor de 1/3 del total. Del mismo modo, Chile tiene el nÃºmero mÃ¡s bajo de encuestados. Â¿Este grÃ¡fico es suficiente para decir que la mayorÃ­a de los usuarios de Kaggle son de India y EE. UU. No lo creo, ya que los usuarios totales en Kaggle son mÃ¡s de 1 millÃ³n, mientras que el nÃºmero de encuestados es de solo 16k."
Â¡Mira ese enorme salario! Eso es **incluso mÃ¡s grande que el PIB de muchos paÃ­ses**. Otro ejemplo de respuesta falsa. El salario mÃ­nimo tal vez un caso de un estudiante. El salario medio muestra que Data Scientist disfruta de buenos beneficios salariales.
### CompensaciÃ³n por paÃ­s
### CompensaciÃ³n por paÃ­s
"El grÃ¡fico de la izquierda muestra los 15 principales paÃ­ses con salarios medios altos. Es bueno ver que estos paÃ­ses proporcionan un salario mejor que el salario promedio del conjunto de datos completo. De manera similar, el grÃ¡fico de la derecha muestra el salario promedio de los 15 paÃ­ses principales por encuestados. El grÃ¡fico mÃ¡s impactante es para **India**. India tiene los 2dos mÃ¡s altos encuestados, pero todavÃ­a tiene el salario medio mÃ¡s bajo en el grÃ¡fico. Las personas en los EE. UU. Tienen un salario casi 10 veces mÃ¡s que sus contrapartes en la India. Â¿CuÃ¡l puede ser la razÃ³n? Â¿Son los profesionales de TI en la India realmente mal pagados? Lo comprobaremos mÃ¡s tarde."
### Salario por gÃ©nero
El salario para los hombres parece ser alto en comparaciÃ³n con otros.
## Edad
"Los encuestados son jÃ³venes, la mayorÃ­a de ellos en el rango de edad entre 25 y 35 aÃ±os."
## Profesion & Especialidad
"La ciencia de datos y el aprendizaje automÃ¡tico se utilizan en casi todas las industrias. Esto es evidente en el grÃ¡fico de la izquierda, ya que personas de diferentes Ã¡reas de interÃ©s como FÃ­sica, BiologÃ­a, etc. lo estÃ¡n tomando para una mejor comprensiÃ³n de los datos. El grÃ¡fico del lado derecho muestra el trabajo actual de los encuestados. Una gran parte de los encuestados son Dats Scientists. Pero como se trata de datos de encuestas, sabemos que puede haber muchas respuestas ambiguas. MÃ¡s adelante veremos si estos encuestados son cientÃ­ficos de datos reales o cientÃ­ficos de datos autoproclamados."
## CompensaciÃ³n por tÃ­tulo de trabajo
El Practicante de InvestigaciÃ³n de Operaciones tiene el salario medio mÃ¡s alto seguido por el Predictive Modeler y el Data Scientist. El informÃ¡tico y los programadores tienen la compensaciÃ³n mÃ¡s baja.
## Machine Learning
"Es evidente que la mayorÃ­a de los encuestados estÃ¡n trabajando con Aprendizaje supervisado, y la RegresiÃ³n logÃ­stica es la favorita entre ellos. No existe un algoritmo que sea el mejor para todos los dominios de clasificaciÃ³n. Una forma de seleccionar un algoritmo para un dominio en particular es mediante la validaciÃ³n cruzada en los datos de entrenamiento."
"Es evidente que la mayorÃ­a de los encuestados estÃ¡n trabajando con Aprendizaje supervisado, y la RegresiÃ³n logÃ­stica es la favorita entre ellos. No existe un algoritmo que sea el mejor para todos los dominios de clasificaciÃ³n. Una forma de seleccionar un algoritmo para un dominio en particular es mediante la validaciÃ³n cruzada en los datos de entrenamiento."
"Es evidente que el prÃ³ximo aÃ±o va a ver un aumento en el nÃºmero de practicantes de **Deep Learning**. El aprendizaje profundo y las redes neuronales o, en definitiva, la IA es un tema candente para el prÃ³ximo aÃ±o. TambiÃ©n en tÃ©rminos de herramientas, Python se prefiere mÃ¡s a R. Las herramientas de Big Data como Spark y Hadoop tambiÃ©n tienen una buena participaciÃ³n en los prÃ³ximos aÃ±os."
## Las mejores plataformas para aprender
"Mi favorito es Kaggle, es la fuente mÃ¡s buscada para aprender Data Science, ya que tiene notebooks de Data Scientists realmente experimentados. La siguiente opciÃ³n son los cursos en lÃ­nea, es decir, los MOOC. Plataformas como coursera, udacity proporcionan videos interactivos y ejercicios para el aprendizaje. De manera similar, los canales de Youtube como Siraj Raval y otros ofrecen un medio gratuito para estudiar. Todos estos medios estÃ¡n por encima de los libros de texto. La razÃ³n quizÃ¡s sea que los libros de texto a menudo tienen un contenido limitado, o que las personas les gusta mÃ¡s mirar videos y aprender."
## Hardware utilizado
"Dado que la mayorÃ­a de los encuestados se encuentran en la categorÃ­a de edad inferior a 25, que es donde la mayorÃ­a de los estudiantes se clasifican, una computadora portÃ¡til bÃ¡sica es la mÃ¡quina mÃ¡s utilizada para el trabajo."
## De dÃ³nde obtengo conjuntos de datos??\n
"Con cientos de conjuntos de datos disponibles, Kaggle es la fuente mÃ¡s buscada para datasets."
## CÃ³digo compartido
"Github es la plataforma mÃ¡s utilizada para compartir cÃ³digo y proyectos. Las ventajas de usar github son:\n\n1) Control de versiones de tus proyectos.\n\n2) Explore los proyectos de otros en GitHub, obtenga un cÃ³digo inspirado mÃ¡s o contribuya a su proyecto.\n\n3) Colabore con otros, permitiendo que otras personas contribuyan a sus proyectos o usted contribuya a otros proyectos, y muchos mÃ¡s.\n\n## DesafÃ­os en la ciencia de datos"
"Github es la plataforma mÃ¡s utilizada para compartir cÃ³digo y proyectos. Las ventajas de usar github son:\n\n1) Control de versiones de tus proyectos.\n\n2) Explore los proyectos de otros en GitHub, obtenga un cÃ³digo inspirado mÃ¡s o contribuya a su proyecto.\n\n3) Colabore con otros, permitiendo que otras personas contribuyan a sus proyectos o usted contribuya a otros proyectos, y muchos mÃ¡s.\n\n## DesafÃ­os en la ciencia de datos"
"El principal desafÃ­o en Data Science es **obtener la informaciÃ³n adecuada**. El grÃ¡fico muestra claramente que los datos sucios son el desafÃ­o mayor. Ahora, Â¿quÃ© son los datos sucios? Los datos sucios son un registro de base de datos que contiene errores. Los datos sucios pueden ser causados por una serie de factores que incluyen registros duplicados, datos incompletos o desactualizados y el anÃ¡lisis incorrecto de campos de registros de sistemas dispares. Afortunadamente, los conjuntos de datos de Kaggle estÃ¡n bastante limpios y estandarizados.\n\nAlgunos otros desafÃ­os importantes son la **falta de ciencia de datos y talento para el aprendizaje automÃ¡tico, dificultad para obtener datos y falta de herramientas**. Es por eso que la ciencia de datos es el trabajo mÃ¡s sexy en el siglo XXI. Con la creciente cantidad de datos, esta demanda crecerÃ¡ sustancialmente."
## SatisfacciÃ³n laboral
"Los cientÃ­ficos de datos y los ingenieros de Aprendizaje automÃ¡tico son las personas mÃ¡s satisfechas (quienes no estarÃ¡n contentos con tanto dinero), mientras que los programadores tienen la satisfacciÃ³n laboral mÃ¡s baja. Pero lo que hay que notar aquÃ­ es que incluso si los Computer Scientist  tienen un salario mÃ¡s bajo que los programadores, tambiÃ©n tienen un buen nivel de satisfacciÃ³n laboral que los programadores. AsÃ­, el salario no es el Ãºnico criterio o la satisfacciÃ³n laboral.\n\n## SatisfacciÃ³n laboral por paÃ­s"
### Lenguaje recomendado para principiantes
Claramente Python es el lenguaje recomendado para principiantes. La razÃ³n de esto tal vez se deba a su sintaxis similar a la del inglÃ©s y su funcionalidad de propÃ³sito general.
### Necesario o No??
"Claramente, Python es una habilidad mucho mÃ¡s necesaria en comparaciÃ³n con R.\n\nGracias especiales a [Steve Broll](https://www.kaggle.com/stevebroll) por ayudar en la combinaciÃ³n de colores."
### NÃºmero de usuarios por lenguajes
"El nÃºmero de usuarios de Python es definitivamente mÃ¡s que los usuarios de R. Esto puede deberse a la curva de aprendizaje fÃ¡cil de Python. Sin embargo hay mÃ¡s usuarios que conocen ambos lenguajes. Estas respuestas pueden ser de cientÃ­ficos de datos establecidos, ya que tienden a tener conocimientos en varios lenguajes y herramientas."
"Los codificadores de Python tienen un salario medio ligeramente mÃ¡s alto en comparaciÃ³n con sus homÃ³logos de R. Sin embargo, las personas que conocen ambos idiomas tienen un salario medio bastante alto en comparaciÃ³n con los dos.\n\n## Languaje utilizado por profesionales"
"Como mencionÃ© anteriormente, R vence a Python en visualizacion. Por lo tanto, las personas con tÃ­tulos de trabajo como analista de datos, analista de negocios donde los grÃ¡ficos y visuales desempeÃ±an un papel muy importante, prefieren R sobre Python. De manera similar, casi el 90% de los estadÃ­sticos usan R. TambiÃ©n como se indicÃ³ anteriormente, Python es mejor en materia de Aprendizaje automÃ¡tico, por lo que los ingenieros de Aprendizaje automÃ¡tico, cientÃ­ficos de datos y otros como DBA o programadores prefieren Python sobre R.\n\nPor lo tanto, para datos visuales ---> R de lo contrario ----> Python.\n\n**Nota: esta grÃ¡fica no es para el lenguaje recomendado por los profesionales, sino para las herramientas utilizadas por los profesionales.**"
## FunciÃ³n en el trabajo vs lenguaje
"Como ya mencionÃ©, **R sobresale en analÃ­tica, pero Python vence en Aprendizaje automÃ¡tico.** La grÃ¡fica muestra que R tiene influencia cuando se trata de analÃ­tica pura, pero otras formas en que Python gana."
## Permanencia Vs lenguaje usado
"Como habÃ­amos visto anteriormente, Python es muy recomendable para principiantes. Por lo tanto, la proporciÃ³n de usuarios de Python es mayor en los aÃ±os iniciales de codificaciÃ³n. Sin embargo, la brecha entre los lenguajes se reduce con los aÃ±os, a medida que aumenta la experiencia de codificaciÃ³n.\n\n## Industria vs lenguaje utilizado"
"Como habÃ­amos visto anteriormente, Python es muy recomendable para principiantes. Por lo tanto, la proporciÃ³n de usuarios de Python es mayor en los aÃ±os iniciales de codificaciÃ³n. Sin embargo, la brecha entre los lenguajes se reduce con los aÃ±os, a medida que aumenta la experiencia de codificaciÃ³n.\n\n## Industria vs lenguaje utilizado"
"R vence a Python en las industrias gubernamentales, de seguros y sin fines de lucro. Del mismo modo, Python vence a R con un margen muy grande en la industria de tecnologÃ­a y militar. En el resto de otras industrias, la proporciÃ³n de Python parece ser aproximadamente un 15-20% mÃ¡s que la de R.\n\n## Herramientas comunes con Python y R"
"R vence a Python en las industrias gubernamentales, de seguros y sin fines de lucro. Del mismo modo, Python vence a R con un margen muy grande en la industria de tecnologÃ­a y militar. En el resto de otras industrias, la proporciÃ³n de Python parece ser aproximadamente un 15-20% mÃ¡s que la de R.\n\n## Herramientas comunes con Python y R"
"**SQL** parece ser la herramienta complementaria mÃ¡s comÃºn utilizada con ambos idiomas. SQL es el lenguaje principal para consultar grandes bases de datos, por lo que saberlo bien es una gran ventaja."
"Entonces, aproximadamente el 26% del total de los encuestados se consideran a sÃ­ mismos como cientÃ­ficos de datos. Â¿QuÃ© significa algo de esto? Â¿Siguen aprendiendo o estÃ¡n desempleados? Por ahora vamos a considerarlos como un No.\n\n## TÃ­tulos de trabajo actuales"
"Sorprendentemente no hay **ninguna entrada para el cientÃ­fico de datos del tÃ­tulo del trabajo**. Esto podrÃ­a deberse a que las personas con CurrentJobTitleSelect como Data Scientist (que podrÃ­a estar trabajando como Data Scientist) podrÃ­an no haber respondido la pregunta: **""Â¿Actualmente te consideras un Data Scientist?""**\n\nHay muchas habilidades comunes y superpuestas entre los trabajos como analista de datos, cientÃ­fico de datos y expertos en aprendizaje automÃ¡tico, estadÃ­sticos, etc. Por lo tanto, ellos tambiÃ©n tienen habilidades similares y se consideran a sÃ­ mismos como cientÃ­ficos de datos, aunque no estÃ©n etiquetados de la misma manera. Ahora vamos a comprobar si la suposiciÃ³n anterior era verdadera."
"Entonces, del total de encuestados, alrededor del **40%** de ellos son cientÃ­ficos de datos o tienen habilidades para el mismo.\n\n## Pasion y Division de DS"
El grÃ¡fico es similar al grÃ¡fico demogrÃ¡fico en el que mostramos el nÃºmero de usuarios por paÃ­s. La diferencia es que los nÃºmeros se han reducido ya que solo hemos considerado cientÃ­ficos de datos.\n\n## SituaciÃ³n laboral y educaciÃ³n
El grÃ¡fico es similar al grÃ¡fico demogrÃ¡fico en el que mostramos el nÃºmero de usuarios por paÃ­s. La diferencia es que los nÃºmeros se han reducido ya que solo hemos considerado cientÃ­ficos de datos.\n\n## SituaciÃ³n laboral y educaciÃ³n
"About **67%** of the data scientists are employed full-time, while about **11-12%** of them are unemployed but looking for job. On the education side it is evident that about **45-46%** of the data scientists hold a **master's degree**, while about **23-24%** of them have a **bachelor's degree or a doctoral degree**. Thus education seems to be an important factor for becoming a data scientist. Let's see how does the salary vary according to the education.\n\nAlrededor de **67%** de los datos se emplean a tiempo completo, mientras que alrededor de **11-12%** de ellos estÃ¡n desempleados pero buscando trabajo. Por el lado de la educaciÃ³n, es evidente que alrededor del **45-46%** de los cientÃ­ficos de datos tienen un **grado de maestrÃ­a**, mientras que alrededor del **23-24%** de ellos tienen un **tÃ­tulo de licenciatura o un doctorado**. Por lo tanto, la educaciÃ³n parece ser un factor importante para convertirse en un cientÃ­fico de datos. Veamos cÃ³mo varÃ­a el salario segÃºn la educaciÃ³n.\n\n## CompensaciÃ³n por la educaciÃ³n formal\n"
"About **67%** of the data scientists are employed full-time, while about **11-12%** of them are unemployed but looking for job. On the education side it is evident that about **45-46%** of the data scientists hold a **master's degree**, while about **23-24%** of them have a **bachelor's degree or a doctoral degree**. Thus education seems to be an important factor for becoming a data scientist. Let's see how does the salary vary according to the education.\n\nAlrededor de **67%** de los datos se emplean a tiempo completo, mientras que alrededor de **11-12%** de ellos estÃ¡n desempleados pero buscando trabajo. Por el lado de la educaciÃ³n, es evidente que alrededor del **45-46%** de los cientÃ­ficos de datos tienen un **grado de maestrÃ­a**, mientras que alrededor del **23-24%** de ellos tienen un **tÃ­tulo de licenciatura o un doctorado**. Por lo tanto, la educaciÃ³n parece ser un factor importante para convertirse en un cientÃ­fico de datos. Veamos cÃ³mo varÃ­a el salario segÃºn la educaciÃ³n.\n\n## CompensaciÃ³n por la educaciÃ³n formal\n"
"Esto es sorprendente, ya que los rangos salariales para licenciatura, maestrÃ­a y doctorado parecen muy similares. La mediana de la licenciatura parece ser un poco alta en comparaciÃ³n con la maestrÃ­a y el doctorado. No esperaba esto ya que muchos de los cientÃ­ficos de datos tienen una maestrÃ­a. Pero creo que **Experiencia laboral** es mÃ¡s importante que cualquier tÃ­tulo. Tal vez los titulares de la licenciatura tienen mÃ¡s experiencia en comparaciÃ³n con los otros dos.\n\n## Trabajo previo y cambio salarial"
"Esto es sorprendente, ya que los rangos salariales para licenciatura, maestrÃ­a y doctorado parecen muy similares. La mediana de la licenciatura parece ser un poco alta en comparaciÃ³n con la maestrÃ­a y el doctorado. No esperaba esto ya que muchos de los cientÃ­ficos de datos tienen una maestrÃ­a. Pero creo que **Experiencia laboral** es mÃ¡s importante que cualquier tÃ­tulo. Tal vez los titulares de la licenciatura tienen mÃ¡s experiencia en comparaciÃ³n con los otros dos.\n\n## Trabajo previo y cambio salarial"
"Es evidente que la mayorÃ­a de las personas que se cambian a Data Science obtienen un aumento salarial de **6-20% o mÃ¡s**. Con esta creciente demanda de Data Scientists, el salario tambiÃ©n puede aumentar con el tiempo.\n\n## Herramientas utilizadas en el trabajo"
"Es evidente que la mayorÃ­a de las personas que se cambian a Data Science obtienen un aumento salarial de **6-20% o mÃ¡s**. Con esta creciente demanda de Data Scientists, el salario tambiÃ©n puede aumentar con el tiempo.\n\n## Herramientas utilizadas en el trabajo"
"Observaciones similares, Python, R y SQL son las herramientas o lenguajes mÃ¡s utilizados en Data Science"
"Coursera es la plataforma mÃ¡s favorecida por los cientÃ­ficos de datos para el aprendizaje de la ciencia de datos. Mi voto personal tambiÃ©n va para Coursera, donde puedes aprender cosas desde cero hasta avanzadas en la misma plataforma. No se limita a un solo lenguaje como Python o R, sino que tambiÃ©n tiene cursos que cubren otros lenguaje como Scala, etc. Del mismo modo KDNuggets es el blog mÃ¡s preferido.\n\n## Tiempo empleado en tareas\n\nUn cientÃ­fico de datos no siempre estÃ¡ construyendo modelos predictivos, tambiÃ©n es responsable de la calidad de los datos, la recopilaciÃ³n de los datos correctos, anÃ¡lisis, etc. Veamos cuÃ¡nto tiempo pasa un cientÃ­fico de datos en estas diferentes tareas."
"La lÃ­nea punteada es la lÃ­nea media.\nVamos a hacerlo paso a paso:\n\nÂ Â - **TimeGatheringData:** Es, sin duda, la parte mÃ¡s lenta. Obtener los datos es la tarea mÃ¡s minuciosa de todo el proceso, a la que sigue la Limpieza de datos (que no se muestra como datos no disponibles), que es otro proceso que consume mucho tiempo. Por lo tanto, la recopilaciÃ³n de datos correctos y la limpieza de los datos son el proceso mÃ¡s lento.\nÂ Â \nÂ Â - **TimeVisualizing:** Es probablemente el proceso que consume menos tiempo (y probablemente el mÃ¡s agradable ...: p), y se reduce aÃºn mÃ¡s si usamos Enterprise Tools como Tableau, Qlik, Tibco, etc., lo que ayuda a Construyendo grÃ¡ficos y tableros con caracterÃ­sticas simples de arrastrar y soltar.\nÂ Â \nÂ Â - **TimeFindingInsights:** Se sigue despuÃ©s de visualizar los datos, lo que implica encontrar datos y patrones en los datos, dividirlos y cortarlos en trozos para encontrar informaciÃ³n sobre los procesos de negocios. Parece un poco mÃ¡s lento que el de TimeVisualizing.\nÂ Â \nÂ Â - **TimeModelBuilding:** Es donde los cientÃ­ficos de datos construyen modelos predictivos, sintonizan estos modelos, etc. Es el segundo proceso que consume mÃ¡s tiempo despuÃ©s de TimeDataGathering.\n"
"## Servicios en la nube\n\nCon el aumento del tamaÃ±o de los datos, no es posible procesar los datos y realizar anÃ¡lisis predictivos en las infraestructuras del servidor fÃ­sico. Por lo tanto, la nube lleva el anÃ¡lisis predictivo a un nivel superior, con escalabilidad su principal ventaja. Gestionan un servicio que le permite construir fÃ¡cilmente modelos de aprendizaje automÃ¡tico que funcionan con cualquier tipo de datos, de cualquier tamaÃ±o. Permite comprobar cuÃ¡les son las plataformas en la nube mÃ¡s utilizadas por los cientÃ­ficos de datos."
"It is evident that **AmazonAWS**, which is a public cloud service provider is the most used cloud platform, followed by Hadoop. Hadoop is an open-source software framework used for distributed storage and processing of dataset of big data. For reading more about Hadoop, **Check this**\n\nEs evidente que **AmazonAWS**, que es un proveedor de servicios de nube pÃºblica, es la plataforma de nube mÃ¡s utilizada, seguida por Hadoop. Hadoop es un marco de software de cÃ³digo abierto utilizado para el almacenamiento distribuido y el procesamiento de conjuntos de datos de big data. Para leer mÃ¡s sobre Hadoop, **Revise esto**"
## Importancia de las visualizaciones
"Las visualizaciones son una parte muy integral de los proyectos de Data Science, y el grÃ¡fico anterior tambiÃ©n muestra lo mismo. Casi todos los proyectos de ciencia de datos, es decir, el **99%** de los proyectos tienen visualizaciones, no importa lo grande o pequeÃ±o que sea. Aproximadamente **95%** de los cientÃ­ficos de datos dicen que las habilidades de visualizaciÃ³n son agradables de tener o necesarias. Las visuales ayudan a comprender y comprender los datos con mayor rapidez, no solo para los profesionales, sino tambiÃ©n para los clientes objetivo, que pueden no ser tÃ©cnicamente expertos.\n\n## Herramientas de BI\n\nEl software de inteligencia empresarial es un tipo de software de aplicaciÃ³n diseÃ±ado para recuperar, analizar, transformar e informar datos para inteligencia empresarial. Hacen que la visualizaciÃ³n y el anÃ¡lisis de datos sean muy simples en comparaciÃ³n con la forma de codificaciÃ³n normal en Python o R. El Ãºnico inconveniente es que son **exclusivos y costosos**. Permite comprobar cuÃ¡les son las herramientas de BI empresariales mÃ¡s utilizadas por los cientÃ­ficos de datos."
"Los cientÃ­ficos de datos tienen un buen conocimiento de conceptos matemÃ¡ticos como EstadÃ­stica y Ãlgebra Lineal, que son la parte mÃ¡s importante de los algoritmos de aprendizaje automÃ¡tico. Pero, Â¿es esta matemÃ¡tica realmente necesaria, ya que muchas bibliotecas estÃ¡ndar como scikit, tensorflow, keras, etc. ya tienen todas estas cosas implementadas? Pero los cientÃ­ficos de datos experimentados dicen que deberÃ­amos tener una buena comprensiÃ³n de las matemÃ¡ticas detrÃ¡s de los algoritmos. Alrededor de **95%** de los cientÃ­ficos de datos dicen que las estadÃ­sticas son un activo importante en Data Science\n\n## Utilidad de la plataforma de aprendizaje"
"Los grÃ¡ficos de anillos anteriores muestran la opiniÃ³n de los cientÃ­ficos de datos sobre las distintas plataformas para aprender la ciencia de datos. La trama se ve mejor para **Proyectos**, donde el porcentaje no es Ãºtil casi **0%**. SegÃºn mi opiniÃ³n personal, los proyectos son la mejor plataforma o forma de aprender algo en la industria de TI. Las otras plataformas excelentes son **Cursos en lÃ­nea y Kaggle**. Los grÃ¡ficos para otras plataformas son bastante similares entre sÃ­.\n\n## Â¿QuÃ© debe tener el curriculum vitae?"
"Los grÃ¡ficos de anillos anteriores muestran la opiniÃ³n de los cientÃ­ficos de datos sobre las distintas plataformas para aprender la ciencia de datos. La trama se ve mejor para **Proyectos**, donde el porcentaje no es Ãºtil casi **0%**. SegÃºn mi opiniÃ³n personal, los proyectos son la mejor plataforma o forma de aprender algo en la industria de TI. Las otras plataformas excelentes son **Cursos en lÃ­nea y Kaggle**. Los grÃ¡ficos para otras plataformas son bastante similares entre sÃ­.\n\n## Â¿QuÃ© debe tener el curriculum vitae?"
"Es evidente que la experiencia laboral en proyectos ML y en las competiciones Kaggle refleja el conocimiento de Data Science. TambiÃ©n un rango de Kaggle puede ser algo bueno en el currÃ­culum. Como mencionÃ© anteriormente, la experiencia laboral relevante podrÃ­a tener un valor mayor en comparaciÃ³n con cualquier tÃ­tulo de maestrÃ­a o doctorado. Por lo tanto, esta afirmaciÃ³n es vÃ¡lida, ya que los cientÃ­ficos de datos prefieren la experiencia laboral sobre el grado, como se ve en el grÃ¡fico anterior.\n\n\n## Â¿CÃ³mo buscaron empleo?"
"Es evidente que la experiencia laboral en proyectos ML y en las competiciones Kaggle refleja el conocimiento de Data Science. TambiÃ©n un rango de Kaggle puede ser algo bueno en el currÃ­culum. Como mencionÃ© anteriormente, la experiencia laboral relevante podrÃ­a tener un valor mayor en comparaciÃ³n con cualquier tÃ­tulo de maestrÃ­a o doctorado. Por lo tanto, esta afirmaciÃ³n es vÃ¡lida, ya que los cientÃ­ficos de datos prefieren la experiencia laboral sobre el grado, como se ve en el grÃ¡fico anterior.\n\n\n## Â¿CÃ³mo buscaron empleo?"
"Muchos cientÃ­ficos de datos conocen los trabajos a travÃ©s de sus amigos o familiares o fueron contactados directamente por la compaÃ±Ã­a. Por lo tanto, debemos mantener adecuadamente nuestros perfiles profesionales como Linkedin y seguir actualizÃ¡ndolos, ya que dichos sitios de redes podrÃ­an ayudarlo a conseguir el trabajo de sus sueÃ±os."
"## Comprobando las respuestas libres\n\nEste archivo contiene las respuestas de forma libre respondidas por los encuestados. El problema con este es que al ser una respuesta de forma libre, cada usuario responderÃ¡ a su manera. Lo que quiero decir es que tendremos diferentes respuestas para la misma cosa. Un ejemplo de esto que observÃ© es la biblioteca **Pandas estÃ¡ escrito como pandas, Pandas, panda y en muchas formas diferentes.** Por lo tanto, tratarÃ© de analizar este archivo utilizando **nltk (Natural Language Toolkit).**"
## MotivaciÃ³n detrÃ¡s de trabajar en Kaggle
"### Vamos a flexibilizar nuestra fuerza en la ciencia de datos ...\n\nEl wordcloud muestra la motivaciÃ³n de los usuarios para trabajar en kaggle. Claramente, el aprendizaje de la ciencia de datos, el aprendizaje automÃ¡tico, el interÃ©s en el mismo, la curiosidad, la diversiÃ³n y la recuperaciÃ³n de conjuntos de datos son algunos de los mÃ¡s relevantes.\n\n## Bibliotecas mÃ¡s utilizadas"
"Una breve informaciÃ³n sobre las bibliotecas:\n\n### Python:\n\n1) **Sklearn** - Para algoritmos de aprendizaje automÃ¡tico. Esta biblioteca tiene casi todos los algoritmos importantes de aprendizaje automÃ¡tico utilizados para las industrias.\n\n2) **Pandas, Matlotlib y Seaborn** Generalmente se usan juntos para el trabajo de anÃ¡lisis y visualizaciÃ³n.\n\n3) **TensorFlow y Keras** Usado para Deep Learning\n\n4) **Numpy and Scipy** Usado para cÃ¡lculos cientÃ­ficos.\n\n5) **nltk** Se utiliza para el procesamiento de lenguaje natural.\n\n### R:\n\n1) **dply** dplyr es el paquete para la manipulaciÃ³n rÃ¡pida de datos.\n\n2) **ggplot2 and shiny** El famoso paquete de R para hacer hermosos grÃ¡ficos. Los efectos visuales de Python no se parecen en nada a la calidad de los elementos visuales creados con esta biblioteca.\n\n3) **Caret and randomforest** Para fines de aprendizaje automÃ¡tico.\n\n4) **tidyr** Herramientas para cambiar el diseÃ±o de sus conjuntos de datos.\n\n5) **stringr** Herramientas fÃ¡ciles de aprender para expresiones regulares y cadenas de caracteres.\n\n**Los prospectos (folium en Python) y Plotly son bibliotecas comunes en ambos idiomas, y se utilizan para crear grÃ¡ficos interactivos como mapas geogrÃ¡ficos, etc.**"
#### Distribution of Categorical Features :
- **Outcome** displays a **Normally Distributed** data.
### Numerical Features :\n\n#### Distribution of Numerical Features :
"- **Pregnancies**, **Insulin**, **DiabetesPedigreeFunction** and **Age** have **positively or rightly** skewed data distribution.\n- **BloodPressure** and **Skin Thickness** display a **bidmodal data distribution**. \n- Data distributions of **Glucose** & **BMI** are a bit tricky. This is because they near about highlight a **normal distribution** or **bimodal distribution**. This is because of the small peak present at the value **0**.\n- For this notebook, we will consider these distributions as **bimodal**."
### Target Variable Visualization (Outcome) : 
"- The dataset is **unbalanced** in a **2 : 1** ratio for **Non-Diabetes : Diabetes** cases!\n- Due to this, predictions will be biased towards **Non-Diabetes** cases.\n- Visualizations will also display this bias, thus making it difficult to gain insights."
#### Visualizing Part 1 Features w.r.t Outcome :
"- For **Pregnancies**, cases of diabetes is present throughout the data. There is no specific range of values for which higher diabetes cases are found. **Pregnancies** range of values from **7 - 9** does highlight more cases of diabetes than non-diabetes for the 1st indicating a pattern but this claim gets rejected if we observe the values ahead.\n- **BloodPressure** range of values **60 - 90 mm/hg** highlights a high number of diabetes patients.\n- **SkinThickness** displays very low number of diabetes cases for all the values. Out of those values, **24 - 42** has some prominent peaks of diabetes cases.\n- When it comes to **Age**, young females are more prone to diabetes than older women. As the age increases, the number of diabetes cases have decreased. **Age** group **21 - 50** displays a higher probability of being diagnosed with diabetes."
### Numerical features vs Numerical features w.r.t Target variable (Outcome) :
"- **Pregnancies** with values between **7 - 10** have high chances of diabetes. This range does not display a complete dominance but it has some presence.\n- **Glucose** values higher than **125** indicate very high chances of diabetes.\n- **BloodPressure** values between **60 - 100** highlight many cases of diabetes coupled with any feature.\n- When both, **BMI** and **SkinThickness**, feature values are between **20 - 50**, probability of diabetes is very high.\n- **Insulin** values between **0 - 300** increases the risk of diabetes. When above **400**, the female has a sure shot chance of being diabetic. \n- For **Age** group of **20 - 50** as well as **DiabetesPedigreeFunction** values ranging from **0 - 1.5** results in a diabetic condition."
### Correlation Matrix :
- It is a huge matrix with too many features. We will check the correlation only with respect to **Outcome**. 
- It is a huge matrix with too many features. We will check the correlation only with respect to **Outcome**. 
"- **SkinThickness** and **BloodPressure** does not display any kind of correlation.\n- **Glucose** displays the highest positive correlation with **Outcome**. It is followed by **BMI**, **Age**, **Pregnancies**, **DiabetesPedigreeFunction** & **Insulin**!"
#### ANOVA Test :
"- According to the **ANOVA test**, **higher the value of the ANOVA score, more is the importance of the feature**.\n- From the above results, we need to drop **SkinThickness** & **BloodPressure**. We will take the remaining features into consideration for modeling."
- Selecting the features from the above conducted tests and splitting the data into **75 - 25 train - test** groups.
#### 1] Xgboost Classifier :
###  Closing stock price visualizations & maximum price during 5 years   \n
"**Key findings:**\n    \n- **We could find TOP 10 most traded stocks duing period of 2013-2018.**\n- **Out of 10 companies one is bank, 7 of them are tech companies, another two are non-tech lagacy companies namely General electric and Ford motors.**\n- **From closing stock price visualization, we can learn that stocks 'GE' and 'F' are declining and other tech stocks are rising over a five year period time.**\n- **As we can see visualizations are self-explanatory and we can learn all time high stock prices of all the tickers.**"
###  Trade volume of stocks over a period of 2013-2018 \n
"**Observations:**\n\n- **Above visualizations depicts what was the maximum, minimum and average trade volume overa period of 2013-2018.**\n- **As visualizations are self-explanatory in nature we can also learn variance of trade volume for example stock ticker 'INTC' has a higest variance in trade volume compared other tickers.**"
"### What is comparative analysis of stocks in finance and investment industry?\n\n- **An important aspect of the fundamental analysis of stocks is comparing stocks of the same sector. The most basic way to analyse and compare stocks from the same sector is to conduct an analysis of different ratios like Earnings per share (EPS), Price-to-Earnings (P/E Ratio), Return on Equity (ROE), Return on Capital Employed (ROCE), and Debt-to-Equity ratios, and stock-prices of various companies, trade volume of stocks.**\n- **In this project, due to limited data of companies, we can only compare daily mean stock price of companies and can make inferences like stock price comparison and relative stock price fluctuations that I have mentioned below chart.**\n\n![image.png](attachment:d58c76f2-bc7b-4465-9ef2-df7dba41c157.png)"
"**Observations**\n\n- **It is very clear that 'FB' stock was one of the most expensive among all 7 tech stocks**\n- **stock ticker 'AMD' was among the cheapest to buy compared to other stocks**\n- **From above chart we can also conclude that stocks like 'FB' and 'AAPL' were also among the most volatile in nature than other stocks**\n\n### What is volatility?ðŸ‘€ðŸ“ˆ\n\n> **In finance, volatility (usually denoted by Ïƒ) is the degree of variation of a trading price series over time, usually measured by the standard deviation of logarithmic returns.**\n> \n\n*source: wikipedia*\n\n**In layman terms, volatility is nothing but the fluctuation of stock price during given period of time**\n\n![image.png](attachment:3710fd6a-fef5-4c22-b2a3-335d68119b63.png)"
"### Growth of stock prices\n\n- **In finance and investment industry, stock price growth is really important metric one needs to measure to find out the how is stock or investment of an individual is growing**\n- **Below is the formula to find out growth of stock prices**\n\n*source: educba.com*\n\n![image.png](attachment:0712cf5f-a2fd-4aaf-9636-8ee4d9ae0c7b.png)"
**Observations**\n\n- **We can observe that growth of stock 'Facebook' is the highest among all other 10 stocks over a period of 5 years**\n- **It is very much self-explanotary that stocks of 'Ford Motors' and 'General Electric' has given negative return over a years of period.**
"###  Daily return of stock prices analysis \n\n\n\n### Daily return hypothesis test\n\n- **In stock market, you will often hear that daily return of any stock price is 0% which means you will get zero return on your investment in one day.**\n- **So let's prove the hypothesis by analysing top 10 most traded stocks and assesing their daily return distribution in this section**\n\n- **H0: Daily return is zero**\n- **Ha: Daily return is not zero**\n\n- **We will prove this hypothesis as a one sample t-test as we know population mean but are not aware of std deviation. if p-value is greater than 0.05 than we can not reject the null hypothesis and if it is less than 0.05 than we have to reject the null hypothesis**"
"- **From above results, we can learn that pvalues of stocks 'MSFT'.'INTC' and 'CSCO' are less than 0.05 so we can reject the null hypothesis and accept alternative hypothesis that is 'Daily return is not zero' while for other stocks we cannot reject null hypothesis.**\n\n- **Stistically it proves that 7 out of 10 have daily return of zero percentage which is the most general case.**"
"- **From above results, we can learn that pvalues of stocks 'MSFT'.'INTC' and 'CSCO' are less than 0.05 so we can reject the null hypothesis and accept alternative hypothesis that is 'Daily return is not zero' while for other stocks we cannot reject null hypothesis.**\n\n- **Stistically it proves that 7 out of 10 have daily return of zero percentage which is the most general case.**"
#  Technical analysis of stocks using candle stick charts and moving average \n\n
"### Technical analysis of stocks\n\n- **An open-high-low-close chart (also OHLC) is a type of chart typically used to illustrate movements in the price of a financial instrument over time. Each vertical line on the chart shows the price range (the highest and lowest prices) over one unit of time, e.g., one day or one hour. Tick marks project from each side of the line indicating the opening price (e.g., for a daily bar chart this would be the starting price for that day) on the left, and the closing price for that time period on the right. The bars may be shown in different hues depending on whether prices rose or fell in that period.**\n\n*source: Wikipedia*\n\n![image.png](attachment:422da312-c37d-4824-ab3c-4ef82ff1c966.png)"
###  Candlestick charts of stocks to visualize OHLC prices \n
###  Candlestick charts of stocks to visualize OHLC prices \n
###  Moving Averages charts of Facebook and Apple \n
# **3. Import Libraries**\n\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks.
# **4. Reading the Data**
### How many Survived??
"Above we can see that 38% out of the training-set survived the Titanic. \n\nWe can also see that the passenger ages range from 0.4 to 80. \n\nOn top of that we can already detect some features, that contain missing values, like the â€˜Ageâ€™ and 'Cabin' feature."
### **Age and Sex:**
**Observations:**\n\n1)The number of children increases with Pclass and the survival rate for passenegers below Age 10(i.e children) looks to be good irrespective of the Pclass.\n\n2)Survival chances for Passenegers aged 20-50 from Pclass1 is high.\n
"In order to be more accurate, Sex feature is used as the second level of groupby while filling the missing Age values.\n\n**Let's see why**"
"You can see that men have a high probability of survival when they are between 18 and 30 years old, which is also a little bit true for women but not fully.\n\nFor women the survival chances are higher between 14 and 40.\n\nFor men the probability of survival is very low between the age of 5 and 18, but that isnâ€™t true for women. \n\nAnother thing to note is that infants also have a little bit higher probability of survival.\n\nWhen passenger class increases, the median age for both males and females also increases. However, females tend to have slightly lower median Age than males. The median ages below are used for filling the missing values in Age feature."
**Chances for Survival by Port Of Embarkation --**
"`Embarked` is a categorical feature and there are only **2** missing values in whole data set. Both of those passengers are female, upper class and they have the same ticket number. This means that they know each other and embarked from the same port together. The mode `Embarked` value for an upper class female passenger is **C (Cherbourg)**, but this doesn't necessarily mean that they embarked from that port."
### **Pclass**
"Here we see clearly, that `Pclass` is contributing to a persons chance of survival, especially if this person is in class 1. \n\nLooking at the BarPlot , we can easily infer that survival for Women from Pclass1 is about 95-96%, as only 3 out of 94 Women from Pclass1 died.\n\nIt is evident that irrespective of Pclass, Women were given first priority while rescue. Even Men from Pclass1 have a very low survival rate.\n\nLooks like Pclass is also an important feature. "
"Here we see clearly, that `Pclass` is contributing to a persons chance of survival, especially if this person is in class 1. \n\nLooking at the BarPlot , we can easily infer that survival for Women from Pclass1 is about 95-96%, as only 3 out of 94 Women from Pclass1 died.\n\nIt is evident that irrespective of Pclass, Women were given first priority while rescue. Even Men from Pclass1 have a very low survival rate.\n\nLooks like Pclass is also an important feature. "
"The plot above confirms our assumption about pclass 1, but we can also spot a high probability that a person in pclass 3 will not survive."
"### **Continuous Features**\nBoth of the continuous features (`Age` and `Fare`) have good split points and spikes for a decision tree to learn. One potential problem for both features is, the distribution has more spikes and bumps in training set, but it is smoother in test set. Model may not be able to generalize to test set because of this reason.\n\n* Distribution of `Age` feature clearly shows that children younger than 15 has a higher survival rate than any of the other age groups\n* In distribution of `Fare` feature, the survival rate is higher on distribution tails. The distribution also has positive skew because of the extremely large outliers"
#### **Categorical Features**\nEvery categorical feature has at least one class with high mortality rate. Those classes are very helpful to predict whether the passenger is a survivor or victim. Best categorical features are `Pclass` and `Sex` because they have the most homogenous distributions.\n\n* Passengers boarded from **Southampton** has a lower survival rate unlike other ports. More than half of the passengers boarded from **Cherbourg** had survived. This observation could be related to `Pclass` feature\n* `Parch` and `SibSp` features show that passengers with only one family member has a higher survival rate
#### **Categorical Features**\nEvery categorical feature has at least one class with high mortality rate. Those classes are very helpful to predict whether the passenger is a survivor or victim. Best categorical features are `Pclass` and `Sex` because they have the most homogenous distributions.\n\n* Passengers boarded from **Southampton** has a lower survival rate unlike other ports. More than half of the passengers boarded from **Cherbourg** had survived. This observation could be related to `Pclass` feature\n* `Parch` and `SibSp` features show that passengers with only one family member has a higher survival rate
"### **Conclusion(EDA)**\nMost of the features are correlated with each other. This relationship can be used to create new features with feature transformation and feature interaction. Target encoding could be very useful as well because of the high correlations with `Survived` feature.\n\nSplit points and spikes are visible in continuous features. They can be captured easily with a decision tree model, but linear models may not be able to spot them.\n\nCategorical features have very distinct distributions with different survival rates. Those features can be one-hot encoded. Some of those features may be combined with each other to make new features.\n\nCreated a new feature called `Deck` and dropped `Cabin` feature at the **Exploratory Data Analysis** part."
# Correlation Between The Features
"### Interpreting The Heatmap\n\nThe first thing to note is that only the numeric features are compared as it is obvious that we cannot correlate between alphabets or strings. Before understanding the plot, let us see what exactly correlation is.\n\n**POSITIVE CORRELATION:** If an **increase in feature A leads to increase in feature B, then they are positively correlated**. A value **1 means perfect positive correlation**.\n\n**NEGATIVE CORRELATION:** If an **increase in feature A leads to decrease in feature B, then they are negatively correlated**. A value **-1 means perfect negative correlation**.\n\nNow lets say that two features are highly or perfectly correlated, so the increase in one leads to increase in the other. This means that both the features are containing highly similar information and there is very little or no variance in information. This is known as **MultiColinearity** as both of them contains almost the same information.\n\nSo do you think we should use both of them as **one of them is redundant**. While making or training models, we should try to eliminate redundant features as it reduces training time and many such advantages.\n\nNow from the above heatmap,we can see that the features are not much correlated. The highest correlation is between **SibSp and Parch i.e 0.37**. So we can carry on with all features."
"### **Frequency Encoding**\n`Family_Size` is created by adding `SibSp`, `Parch` and **1**. `SibSp` is the count of siblings and spouse, and `Parch` is the count of parents and children. Those columns are added in order to find the total size of families. Adding **1** at the end, is the current passenger. Graphs have clearly shown that family size is a predictor of survival because different values have different survival rates.\n* Family Size with **1** are labeled as **Alone**\n* Family Size with **2**, **3** and **4** are labeled as **Small**\n* Family Size with **5** and **6** are labeled as **Medium**\n* Family Size with **7**, **8** and **11** are labeled as **Large**"
### **Ticket**
#### ROC AUC Curve\n\n**[What is an ROC AUC Curve ?](https://www.analyticsvidhya.com/blog/2020/06/auc-roc-curve-machine-learning/)**
"The red line in the middel represents a purely random classifier (e.g a coin flip) and therefore your classifier should be as far away from it as possible. Our Random Forest model seems to do a good job.\nOf course we also have a tradeoff here, because the classifier produces more false positives, the higher the true positive rate is."
\n\nImport necessary libraries
\n\nLoad dataset and get initial information
#### Histogram
"\n\n Distributions\n    \n    \n    \n    \nAs it can be seen from the graphs above, the distributions of the variables (except 'mileage' variable) are not normal."
"\n\n Positive skewness and high kurtosis\n    \n    \n    \n    \n* Positive skewness, more weight is on the left side of the distribution.\n    \n* Kurtosis is greater than 3. It is leptokurtic."
"\n\n What the graph shows?\n    \n    \n    \n    \nWe see positive skewness from the graph above. As the graphs shows, more weight is on the left side of the distribution. We will try to fix it using ""log1p"" function of numpy."
