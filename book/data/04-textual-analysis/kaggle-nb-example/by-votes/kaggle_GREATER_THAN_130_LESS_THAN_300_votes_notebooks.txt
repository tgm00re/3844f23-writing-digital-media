## Loading Libraries
## Reproducibility
## Distributions After Rank Gauss and PCA
"### Distributions of ""data_all"""
"### There is 77% of missing data in the cabin column, it's usually way too much for this column to be exploitable, but as we have a small amount of data, we will still try to use it in feature engineering. \n### For the age, we will either interpolate missing values or we will fill it with the mean for the corresponding category (in term of class, age, sex) of passenger. \n### There is only two missing values for the embarked column, let's try to replace it. Below is the distribution of Embarked according to Fare and sex, and the two observations with missing ""Embarked"" value. Let's look at there two observations and choose the best matching embarked value according to their fare value and sex:"
"### Both passengers are female who paid 80 dollars as fare for their tickets. Moreover, they have the same ticket and cabin, so they probably had to board at the same place! According to the distribution above, the more probable embarked value for them is Cherbourg (C). We'll replace these two missing values later during features engineering part. \n\n### Finally, let's plot some histograms to visualise the distributions of our variables:"
"### Both passengers are female who paid 80 dollars as fare for their tickets. Moreover, they have the same ticket and cabin, so they probably had to board at the same place! According to the distribution above, the more probable embarked value for them is Cherbourg (C). We'll replace these two missing values later during features engineering part. \n\n### Finally, let's plot some histograms to visualise the distributions of our variables:"
"### With this first exploration, we can see that :\n\n* Only aproximately 35% of passengers survived ...\n* More than the half of passengers are in the lowest class (pclass = 3)\n* Most of the fare tickets are below 50\n* Majority of passengers are alone (sibsp and parch)\n\n*Note : this EDA is not complete at all since it is not the purpose of this kernel to make a deep exploration of data. However, you can look at my [**EDA kernel**](https://www.kaggle.com/nhlr21/titanic-colorful-eda) for this competition if this interests you.*"
"### We are not done yet ! What about the results ? I've tried to make 30 submissions with this classifier, here are the results :\n    \n| Score   	 |   Nb of occurrences 	|\n|---------	    |  -------------------------- |\n| 0.77511 	|                2                |\n| 0.77990 	|                3             	  |\n| 0.78468 	|               10               |\n| 0.78947 	|               11    	         |\n| 0.79425 	|                2                |\n| 0.79904 	|                2     	          |\n\n\n### Although I know that we can do much better, the 0.79904 still places us in the top 16%. On the other hand, we see that our homemade classifier has reduced the variance so it is reliable and constant in its performance (something that is not tested by the leaderboard but still important for a data sceintist). Moreover, our solution remains simple and accessible even for beginners. \n\n### Thank you for your reading, feel free to fork this kernel and improve it, and enjoy datascience :D"
*Check out this beautiful distribution of kaggle scoring for our homemade classifier !*
"# Characteristics/ Demographics\nLet's look at the demographics or characteristics first, like Country, Gender , age , highest education etc.\nAlthough you can't work your way through most of these, but it is crucial in understanding our data."
">*  Most of the Data Scientists in the Survey come from USA and India, like the last two years.They are \nfollowed by Brazil and Russia. \n* We have just 5 countries from the African Sub-continent, with very low participation. The other countries because of very low responses must be grouped in Others.\n* The same goes for South America,except for Brazil, there is very low participation in the survey.\n"
">*  Most of the Data Scientists in the Survey come from USA and India, like the last two years.They are \nfollowed by Brazil and Russia. \n* We have just 5 countries from the African Sub-continent, with very low participation. The other countries because of very low responses must be grouped in Others.\n* The same goes for South America,except for Brazil, there is very low participation in the survey.\n"
"> * **Male Data Scientits are more than 5 times their female counterparts**. This leads to *bias* in machine learning. ML algorithms are representative of the data they eat and the people who build them. An interesting incident can be cited here, back in 2014 Amazon trained a ML system to hire people and it was unreasonably throwing away women's resumes. A further study in the problem found out that the Consequently, the AI concluded that men were preferable. It downgraded resumes containing the words ""women's"",and filtered out candidates who'd attended two women's only colleges.[[1]](https://www.businessinsider.in/tech/amazon-built-an-ai-to-hire-people-but-reportedly-had-to-shut-it-down-because-it-was-discriminating-against-women/articleshow/66148420.cms)\n* **Most of the population is in their late twenties**. We find fewer and fewer people in Data Science as we move away from this age group. This is probably because Data Science is a relatively new field, the older generation does not know about it , let alone be skilled at it. However, I believe this trend will shift with time.\n* Having work environments more inclusive of women and the elderly will help us to create better and more diverse models.\n* It seems like having a Master's is quite common for a Data Scientist. So ,getting a Master's seems like a safe bet. But is it really necessary to invest that amount of time and money? I think this question is better answered in [Shivan Bansal's notebook](https://www.kaggle.com/shivamb/spending-for-ms-in-data-science-worth-it)."
# Common Practices and Preferences of Data Scientists\nLet's start with preferences first. Getting to know which MOOC platforms or Blogs most of the Data Scientists prefer can help you steer your learning process in the right direction.\n
">* Coursera seems like the obvious choice of Data Scientists,maybe becuase of the quality of content and assignments over there. I personally like it too. There is also an increasing trend in University Courses and it's the second most  prefered thing.\n* When it comes to  favorite Media sources for DS related topics , Medium has a clear edge followed by Kaggle.Probably because a lot of Data Scientists themselves publish there articles on Medium, it seems like a credible source. Adding to this there is a huge variety of Data Science stuff that can be found on Medium from collecting data to deploying models.\n* Kaggle is the second most popular information resource, because of it's content and probably because it's fully free. They won't ask you to make a subscription after you have reached your monthly limit.\n\nI'm not plotting the most common programming language among Data Scientits because python always comes as the clear winner. So, just go with it. There are a whole lot of Machine Learning and Deep Learning libraries in python, plus it is also easy to use.\n\nLet's move on to the next question now.\n\n### What activity makes up most of their time at work?\n\nAs aspiring Data Scientists people spend most of their time building models but this is evidently not the case in actual Data Science scenarios. Data Scientists spend most of their times analyzing and cleaning data which is confirmed by this survey."
">* Coursera seems like the obvious choice of Data Scientists,maybe becuase of the quality of content and assignments over there. I personally like it too. There is also an increasing trend in University Courses and it's the second most  prefered thing.\n* When it comes to  favorite Media sources for DS related topics , Medium has a clear edge followed by Kaggle.Probably because a lot of Data Scientists themselves publish there articles on Medium, it seems like a credible source. Adding to this there is a huge variety of Data Science stuff that can be found on Medium from collecting data to deploying models.\n* Kaggle is the second most popular information resource, because of it's content and probably because it's fully free. They won't ask you to make a subscription after you have reached your monthly limit.\n\nI'm not plotting the most common programming language among Data Scientits because python always comes as the clear winner. So, just go with it. There are a whole lot of Machine Learning and Deep Learning libraries in python, plus it is also easy to use.\n\nLet's move on to the next question now.\n\n### What activity makes up most of their time at work?\n\nAs aspiring Data Scientists people spend most of their time building models but this is evidently not the case in actual Data Science scenarios. Data Scientists spend most of their times analyzing and cleaning data which is confirmed by this survey."
">* Analyzing data to find insights that drive decisions and finding places to apply ML models seems to be the most time consuming process. \n* These days every other company wants to use Data Science to generate more revenue and help their business grow. But deciding where actually Data Science can do wonders, is the job of a Data Scientist. To quote someone,pardon me because I don't remember the source *""A good Data Scientist knows where to apply Machine Learning and a great Data Scientist knows where not to apply Machine Learning.""*\n* It's a huge responsibility to put a Machine Learning model into production and believe me the real game actually starts once you have trained and deployed the model. When it goes live unanticipated scenarios are encountered and changes have to be made. Therefore, Data Scientists spend most of their time in improving existing models.\n* Data Scientists are least interested in improving the state of the art, that's the job of a Research Scietist.\n* **Note**:- Spending more time on finding insights and looking out for areas where Machine Learning can be applied can prepare you better for the job.\n\n**Since analayzing data seems like the most widely performed activity, let's find out how Data Scientists do it.**"
">* Analyzing data to find insights that drive decisions and finding places to apply ML models seems to be the most time consuming process. \n* These days every other company wants to use Data Science to generate more revenue and help their business grow. But deciding where actually Data Science can do wonders, is the job of a Data Scientist. To quote someone,pardon me because I don't remember the source *""A good Data Scientist knows where to apply Machine Learning and a great Data Scientist knows where not to apply Machine Learning.""*\n* It's a huge responsibility to put a Machine Learning model into production and believe me the real game actually starts once you have trained and deployed the model. When it goes live unanticipated scenarios are encountered and changes have to be made. Therefore, Data Scientists spend most of their time in improving existing models.\n* Data Scientists are least interested in improving the state of the art, that's the job of a Research Scietist.\n* **Note**:- Spending more time on finding insights and looking out for areas where Machine Learning can be applied can prepare you better for the job.\n\n**Since analayzing data seems like the most widely performed activity, let's find out how Data Scientists do it.**"
"> * Okay, so most Data Scientists have 3-5 years of experience writing code to ananlyze data.\n* Local development environments are widely used (Jupyter and RStudio),almost  more than 4 times of the other tools.\n* No Surprises that Tableau and other BI softwares are not popular among Data Scientists, because they are primarily used by Business Analysts .\n* Regarding the Visualization Libraries, Matplolib comes as a winner and most of the other libraries(Seaborn and Plotly) are wrappers built on it's powerful internals.\n\n#### Mostly used hosted notebook products against years of experience."
"> * Okay, so most Data Scientists have 3-5 years of experience writing code to ananlyze data.\n* Local development environments are widely used (Jupyter and RStudio),almost  more than 4 times of the other tools.\n* No Surprises that Tableau and other BI softwares are not popular among Data Scientists, because they are primarily used by Business Analysts .\n* Regarding the Visualization Libraries, Matplolib comes as a winner and most of the other libraries(Seaborn and Plotly) are wrappers built on it's powerful internals.\n\n#### Mostly used hosted notebook products against years of experience."
"> * Kaggle Notebooks and Google Colab are the most widely used notebook products because of the good computation power provided by them with GPUs(all for free with a limit). Another biggest advantage over traditional environments is there is no need to download and setup libraries and I think most people would agree it's time consuming and awfully boring stuff.\n* Still a lot of people prefer to work in the traditional way on their local systems and there are more experienced people in this category.\n* Kaggle notebooks and Google Colab are mostly used by Data Scientists for competitions and independent projects and hence we have more of the younger generation using these.\n* People do not use these for company work and rather go for local systems and if the ML infrastructure of the company is huge they go for paid products like AWS and GCP, and hence we can see a lot of the older generation using these.\n\n#### ML algo used on a regular basis vs years of experience"
"> * Kaggle Notebooks and Google Colab are the most widely used notebook products because of the good computation power provided by them with GPUs(all for free with a limit). Another biggest advantage over traditional environments is there is no need to download and setup libraries and I think most people would agree it's time consuming and awfully boring stuff.\n* Still a lot of people prefer to work in the traditional way on their local systems and there are more experienced people in this category.\n* Kaggle notebooks and Google Colab are mostly used by Data Scientists for competitions and independent projects and hence we have more of the younger generation using these.\n* People do not use these for company work and rather go for local systems and if the ML infrastructure of the company is huge they go for paid products like AWS and GCP, and hence we can see a lot of the older generation using these.\n\n#### ML algo used on a regular basis vs years of experience"
"Deductions:-\n> * Looking at the ML algorithms used on a Daily basis, I can say most of the Data Scientists work on machine learning as is evident by the number of people using Linear/Logistic Regression, Decision Trees  and Boosting methods.\n* I think most Data Scientits would agree that a lot of problems can be solved by Linear algorithms, if not by them then by using trees. Jumping on neural networks without exhausting ML algorithms for classification/regression problems is always avoided by Data scientits. More you move towards a complex model, the more assumptions it make. It might give you better performance but debugging becomes trickier. One should never underestimate the power of simple ML algorithms.\n* A smaller subset of Data Scientists deal with Deep Learning problems and most of them work on Computer Vision(image data) problems, seen through the Convolutional Neural Network bars.\n* Only a few work on NLP problems(text data) and Recurrent Neural Networks are more popular amongst the younger generation while Transformers could be equally or more popular with the older population( I can't be sure because we have unequal number of samples for each age group).\n* I know there could be an overlap between Data Scientists using ML algorithms of all the three types(ML,NLP,CV), but that would rarely happen given the question asks on a *""regular basis""*.\n\nLet's also look at other ML algorithms not provided as options in the survey."
"Deductions:-\n> * Looking at the ML algorithms used on a Daily basis, I can say most of the Data Scientists work on machine learning as is evident by the number of people using Linear/Logistic Regression, Decision Trees  and Boosting methods.\n* I think most Data Scientits would agree that a lot of problems can be solved by Linear algorithms, if not by them then by using trees. Jumping on neural networks without exhausting ML algorithms for classification/regression problems is always avoided by Data scientits. More you move towards a complex model, the more assumptions it make. It might give you better performance but debugging becomes trickier. One should never underestimate the power of simple ML algorithms.\n* A smaller subset of Data Scientists deal with Deep Learning problems and most of them work on Computer Vision(image data) problems, seen through the Convolutional Neural Network bars.\n* Only a few work on NLP problems(text data) and Recurrent Neural Networks are more popular amongst the younger generation while Transformers could be equally or more popular with the older population( I can't be sure because we have unequal number of samples for each age group).\n* I know there could be an overlap between Data Scientists using ML algorithms of all the three types(ML,NLP,CV), but that would rarely happen given the question asks on a *""regular basis""*.\n\nLet's also look at other ML algorithms not provided as options in the survey."
"SVM and clustering algorithms seem like popular ones. In deep learning I can see neural networks and reinforcement learning but ML algorithms dominate here too.\n\nIf you are interested in Deep Learning and would want to work in the field, let's look at some common practices in those fields exclusively.I'll start with Computer Vision first.\n\nWhile filtering people for CV or NLP, I won't consider Dense Networks for algorithms as it is rather ambigous, whether a person using them works on NLP or CV or ML(tabular data). For CV, I consider CNN and GANs and for NLP, I take RNNs and Transformers."
"SVM and clustering algorithms seem like popular ones. In deep learning I can see neural networks and reinforcement learning but ML algorithms dominate here too.\n\nIf you are interested in Deep Learning and would want to work in the field, let's look at some common practices in those fields exclusively.I'll start with Computer Vision first.\n\nWhile filtering people for CV or NLP, I won't consider Dense Networks for algorithms as it is rather ambigous, whether a person using them works on NLP or CV or ML(tabular data). For CV, I consider CNN and GANs and for NLP, I take RNNs and Transformers."
"Deductions:-\n> * Image Classification methods seem like the most common group of methods used by CV people along with general purpose pretrained models like VGG16, Inception, Resnets etc that could be used in a variety of tasks.\n* General purpose Image/ video tools are used in preprocessing basically therefore are the second most widely used methods.\n* Data Scientists in the Autonomous Vehicle industry extensively use Image segmentation and Object detection.\n* Traditional word embedding techniques like GLoVe and Word2Vec remain the most popular one in the Data Science Industry(Even I work with them).\n* Encoder-decoder models are used in making chatbots and machine translation.\n* Transformer models are slowly catching wind, but I think they are more complex to use.\n* Note:- If you want to enter the CV/NLP field, having worked on an Image classification or text classification model with good understanding of it would give you an edge. It worked for me, it could work for you too.\n\n\n\n# Common practices followed by Companies\nI believe there are certain trends that companies follow in hiring Data Scientists, or deciding which cloud based platform to use or whether to automate some processes or not according to thier need and resource availability.\n\nLet's look at some of these.\n\n#### Role of years of experience in determining the size of the company (and hence the worth) that will hire you.\n\n"
"Deductions:-\n> * Image Classification methods seem like the most common group of methods used by CV people along with general purpose pretrained models like VGG16, Inception, Resnets etc that could be used in a variety of tasks.\n* General purpose Image/ video tools are used in preprocessing basically therefore are the second most widely used methods.\n* Data Scientists in the Autonomous Vehicle industry extensively use Image segmentation and Object detection.\n* Traditional word embedding techniques like GLoVe and Word2Vec remain the most popular one in the Data Science Industry(Even I work with them).\n* Encoder-decoder models are used in making chatbots and machine translation.\n* Transformer models are slowly catching wind, but I think they are more complex to use.\n* Note:- If you want to enter the CV/NLP field, having worked on an Image classification or text classification model with good understanding of it would give you an edge. It worked for me, it could work for you too.\n\n\n\n# Common practices followed by Companies\nI believe there are certain trends that companies follow in hiring Data Scientists, or deciding which cloud based platform to use or whether to automate some processes or not according to thier need and resource availability.\n\nLet's look at some of these.\n\n#### Role of years of experience in determining the size of the company (and hence the worth) that will hire you.\n\n"
">* There is an obvious trend here, which seems quite practical too. \n* People with less experience in ML work in a small sized company or startup, as the work experience increases more people shift towards big MNCs.\n* So , as a fresher if you land a job in a big company, consider yourself lucky.\n\nNow,let's take a look at the **Company size vs. the number of people responsible for DS workloads** to analyze what kind of Companies do Data Scientists work in."
">* There is an obvious trend here, which seems quite practical too. \n* People with less experience in ML work in a small sized company or startup, as the work experience increases more people shift towards big MNCs.\n* So , as a fresher if you land a job in a big company, consider yourself lucky.\n\nNow,let's take a look at the **Company size vs. the number of people responsible for DS workloads** to analyze what kind of Companies do Data Scientists work in."
">* The first bar where there are 0 people responsible for DS workload in a company seems a bit strange to me , as a person calling himself a Data Scientist should atleast count himself.\n* In the last bar, the small companies(0-49,50-249 range) employing 20+ Data Scientists must be DS Consulting firms or companies fully based on AI like H20.ai or OpenAI.\n\n> There are two kinds of majorities here. \n* Small companies(0-49 employees) having 1-2 DS people to improve decision making or work on specific tasks like recommender systems, Sentiment Analysis, Sales Prediction etc.\n* MNCs with more than 10k employees having 20+ DS leveraging AI in full capacity like Facebook, Google etc.\n\nIt would also be useful to know which ML product companies use and you can add some experience in them to get you some edge over other contenders."
">* The first bar where there are 0 people responsible for DS workload in a company seems a bit strange to me , as a person calling himself a Data Scientist should atleast count himself.\n* In the last bar, the small companies(0-49,50-249 range) employing 20+ Data Scientists must be DS Consulting firms or companies fully based on AI like H20.ai or OpenAI.\n\n> There are two kinds of majorities here. \n* Small companies(0-49 employees) having 1-2 DS people to improve decision making or work on specific tasks like recommender systems, Sentiment Analysis, Sales Prediction etc.\n* MNCs with more than 10k employees having 20+ DS leveraging AI in full capacity like Facebook, Google etc.\n\nIt would also be useful to know which ML product companies use and you can add some experience in them to get you some edge over other contenders."
"> * Okay, so most of the companies do not use a machine learning product.\n* I have learned this from my own experience that most companies using Data Science do not want to use third party  APIs like Google Cloud Natural language or Google Cloud Translation or Google Cloud Speech-to-text or Google Cloud Vision. They prefer to have a propreitary in-house ML apllication(that's what they have hired the Data Scientists for, right?) and even when they do ,they use it as an intermediate process while building their ML applications.\n* Amazon Sagemaker is different from the above, it lets you build your own ML application on top of it's computing power and ML framework and hence it is mostly used by companies along with Google Cloud Machine Learning Engine.\n* I think, it takes a little time to get used to the quirks of Sagemaker when compared to the traditional methods. You might as well start with a free account."
"# What determines your Salary as a Data Scientist?\n\nThere are a lot of factors on which the salary of a Data Scientist depends, the common ones being the skill set and the country of work. Take a look.\n\n### What salary can a Data Scientist expect acording to his/her highest education?"
"In the mid to higher income range(let's say above 30,000 USD) we have much more people with a Master's degree.\n### How does your salary increases based on your years of experience?"
"In the mid to higher income range(let's say above 30,000 USD) we have much more people with a Master's degree.\n### How does your salary increases based on your years of experience?"
>* I think the most important factor in deciding your salary as a Data Scientist is the years of experience you have in the field.\n* The lowest salary range is dominated by people with very less experience(less than 2 years). Most of the Data Scientists having high income fall in the 5-10 years of experience range.
## Does the state of ML in the company decide the salary?\nI'm also interested to know whether the state of ML in the company has any effect on the salary (I'm pretty sure it does).
"> Some major deductions from the plot:\n* Most of the Highly paid Data Scientists come from companies which have well established ML methods. There models are in production and they generate revenue from it , therefore they invest more in their Data Science Team.\n* Companies that do not use ML methods( I don't know why would they hire a Data Scientist then), or are exploring ML methods or use ML to just generate insights pay less to their Data Scientists.\n* Most of the mid income DS are from companies that have just started using ML methods.\n\n## Does the DS Field play a role in deciding salary?\nI want to explore what is the trend in salary for different fields, let's dive into it."
"> Some major deductions from the plot:\n* Most of the Highly paid Data Scientists come from companies which have well established ML methods. There models are in production and they generate revenue from it , therefore they invest more in their Data Science Team.\n* Companies that do not use ML methods( I don't know why would they hire a Data Scientist then), or are exploring ML methods or use ML to just generate insights pay less to their Data Scientists.\n* Most of the mid income DS are from companies that have just started using ML methods.\n\n## Does the DS Field play a role in deciding salary?\nI want to explore what is the trend in salary for different fields, let's dive into it."
"> Deductions:\n* Most of the people in ML belong to the high income Salary range and on the contrary most people in Computer Vision and NLP belong to the low and mid income range.\n* I had a hypothesis that has more to do with years of experience, since Machine learning is an older field compared to Deep Learning, so I made a plot to confirm this.\n* Seems like the hypothesis is true, most of the higher salary income people have a lot of experience.\n\n## Conclusions\n> Landing a Data Science job ain't that hard. You should know what it takes and where to look.\n* Having a university degree in Data Science is not that important, a lot of Data Scientists learn from MOOCs. However, it is very important to stay up-to-date with trends in the field.Medium Blogs and Kaggle Forums are a popular choice among Data Scientits.\n* Spend more time in cleaning and analyzing data than making models because that's what people do while solving real-world problems. Local environment with matplotlib is best for analyzing data.\n* Whenever solving a problem try to stick with easily interpretable algorithms before jumping to neural networks. If you want to enter the field of NLP or CV making an image or text classifier with good background on the working and basics of it would also work. You can also go for some not so common projects to give an edge to your resume.\n* Set realistic goals, after making a good profile( with some nice independent projects or open-source contributions) and a good understanding of what you do,apply at a start-up or a place that has more chances of hiring you.\n* The picture won't be that pretty and you will have a lot of responsibilities, but you will learn a lot here.\n* You can try your hand at learning to use a ML product.\n* Years of experience will gradually but surely add worth to you.\n"
"## How to detrend a time series?\n\nDetrending a time series is to remove the trend component from a time series. But how to extract the trend? There are multiple approaches.\n\n- Subtract the line of best fit from the time series. The line of best fit may be obtained from a linear regression model with the time steps as the predictor. For more complex trends, you may want to use quadratic terms (x^2) in the model.\n- Subtract the trend component obtained from time series decomposition we saw earlier.\n- Subtract the mean\n- Apply a filter like Baxter-King filter(statsmodels.tsa.filters.bkfilter) or the Hodrick-Prescott Filter (statsmodels.tsa.filters.hpfilter) to remove the moving average trend lines or the cyclical components."
 üóÉ Patterns Recognition - Seasonality
"## How to test for seasonality of a time series?\n\nThe common way is to plot the series and check for repeatable patterns in fixed time intervals. So, the types of seasonality is determined by the clock or the calendar:\n- Hour of day\n- Day of month\n- Weekly\n- Monthly\n- Yearly"
 üö≤ Patterns Recognition - Cyclic behaviour
"Autocorrelation is the correlation between two observations at different points in a time series. For example, values that are separated by an interval might have a strong positive or negative correlation. When these correlations are present, they indicate that past values influence the current value. Analysts use the autocorrelation and partial autocorrelation functions to understand the properties of time series data, fit the appropriate models, and make forecasts.\n\nIn this post, I cover both the autocorrelation function and partial autocorrelation function. You‚Äôll learn about the differences between these functions and what they can tell you about your data. In later posts, I‚Äôll show you how to incorporate this information in regression models of time series data and other time-series analyses.\n\n#### Autocorrelation and Partial Autocorrelation Basics\nAutocorrelation is the correlation between two values in a time series. In other words, the time series data correlate with themselves‚Äîhence, the name. We talk about these correlations using the term ‚Äúlags.‚Äù Analysts record time-series data by measuring a characteristic at evenly spaced intervals‚Äîsuch as daily, monthly, or yearly. The number of intervals between the two observations is the lag. For example, the lag between the current and previous observation is one. If you go back one more interval, the lag is two, and so on.\n\nIn mathematical terms, the observations at yt and yt‚Äìk are separated by k time units. K is the lag. This lag can be days, quarters, or years depending on the nature of the data. When k=1, you‚Äôre assessing adjacent observations. For each lag, there is a correlation.\n\n#### Autocorrelation Function (ACF)\nUse the autocorrelation function (ACF) to identify which lags have significant correlations, understand the patterns and properties of the time series, and then use that information to model the time series data. From the ACF, you can assess the randomness and stationarity of a time series. You can also determine whether trends and seasonal patterns are present.\n\nIn an ACF plot, each bar represents the size and direction of the correlation. Bars that extend across the red line are statistically significant.\n\n#### Partial Autocorrelation Function (PACF)\nThe partial autocorrelation function is similar to the ACF except that it displays only the correlation between two observations that the shorter lags between those observations do not explain. For example, the partial autocorrelation for lag 3 is only the correlation that lags 1 and 2 do not explain. In other words, the partial correlation for each lag is the unique correlation between those two observations after partialling out the intervening correlations.\n\nAs you saw, the autocorrelation function helps assess the properties of a time series. In contrast, the partial autocorrelation function (PACF) is more useful during the specification process for an autoregressive model. Analysts use partial autocorrelation plots to specify regression models with time series data and Auto Regressive Integrated Moving Average (ARIMA) models. I‚Äôll focus on that aspect in posts about those methods.\n\n*https://statisticsbyjim.com/time-series/autocorrelation-partial-autocorrelation/*"
 üìñ Lag plots
"### Smoothening of a time series may be useful in:\n\n- Reducing the effect of noise in a signal get a fair approximation of the noise-filtered series.\n- The smoothed version of series can be used as a feature to explain the original series itself.\n- Visualize the underlying trend better\n\n### So how to smoothen a series? Let‚Äôs discuss the following methods:\n\n- Take a moving average\n- Do a LOESS smoothing (Localized Regression)\n- Do a LOWESS smoothing (Locally Weighted Regression)\n- Moving average is nothing but the average of a rolling window of defined width. But you must choose the window-width wisely, because, large window-size will over-smooth the series. For example, a window-size equal to the seasonal duration (ex: 12 for a month-wise series), will effectively nullify the seasonal effect."
 üìÑ ARIMA Model
# 2. Importing Libraries üìö\nüëâ **Importing libraries** that will be used in this notebook.
"# 3. Reading Data Set üëì\nüëâ After importing libraries, we will also **import the dataset** that will be used."
## 5.1 Drug Type Distribution üíä
## 5.2 Gender Distribution üë´
## 5.2 Gender Distribution üë´
## 5.3 Blood Pressure Distribution ü©∏
## 5.3 Blood Pressure Distribution ü©∏
## 5.4 Cholesterol Distribution ü•õ
## 5.4 Cholesterol Distribution ü•õ
## 5.5 Gender Distribution based on Drug Type üë´üíä
## 5.5 Gender Distribution based on Drug Type üë´üíä
## 5.6 Blood Pressure Distribution based on Cholesetrol ü©∏ü•õ
## 5.6 Blood Pressure Distribution based on Cholesetrol ü©∏ü•õ
## 5.7 Sodium to Potassium Distribution based on Gender and Age üß™üë´üë¥
## 5.7 Sodium to Potassium Distribution based on Gender and Age üß™üë´üë¥
# 6. Dataset Preparation ‚öô\nüëâ This section will prepare the dataset before building the machine learning models.
Let's take look at how number of atoms is connected with target variable:
No clear dependence is observed so we need some more features to be exctracted.\nThe next obvious step is to count numbers of the most common atoms.\nRDkit supports subpattern search represented by ***GetSubstructMatches()*** method. It takes a MOL of a substructure pattern as an argument. So you can futher extract occurance of each pattern you'd like.
\n\n1.1 Libraries and Utilities
\n\n1.2 Data Preprocessing 
Data storytelling is a popular method to convey most of the information in terms of simple plots rather with complex analysis. This techique's main objective to perform explinatory data analysis rather than exploratory data analysis. All the plots made are as simple as possible with no interactivity and complecations. For whole visualization a simple color palette shown in palplot was used. Few key points are highlighted in the plot it self.
\n  2.0 Distribution of Targets \n
\n  2.0 Distribution of Targets \n
"From distribution it is clear that every 5 people out of 100 people are having strokes from our sampling data. Moreover,this is a highly unbalanced data distribution, and null accuracy score of this distribution it self is 95%, whcih imploys any dump model should randomly predictions of stroke could reach accuracy of 95%. So, while modeling and training data, either over sampling or under sampling has to be done to obtain best results."
\n 2.1 Univariate analysis of continuous variables\n
"Age is an important feature. Age feature distribution is not a normal distriubtion, which needs to be tranformed later. From catergorical features it can be seen that old age people are mostly having strokes, compared to younger ones."
"Age is an important feature. Age feature distribution is not a normal distriubtion, which needs to be tranformed later. From catergorical features it can be seen that old age people are mostly having strokes, compared to younger ones."
Glucose level distribution is skewed towards left and most ofhte strkes can be seen people with regular glucose levels.
Glucose level distribution is skewed towards left and most ofhte strkes can be seen people with regular glucose levels.
"BMI is highly skewed and high bmi , high possibility of having strokes."
\n\n 2.2 Overview of univariate categorical variables\n
Overview of the categorical features shows the value counts of the strokes and no strokes for each categorical feature. This overview could give some insight where strokes are high in number.
 Gender Distribution \n
 HyperTension Distribution \n
 HyperTension Distribution \n
 HeartDisease Distribution \n
 HeartDisease Distribution \n
 Marriage Distribution \n
 Marriage Distribution \n
 Residence Distribution \n
 Residence Distribution \n
 Smoking Distribution \n
 Smoking Distribution \n
 Work Distribution \n
 Work Distribution \n
\n\n 2.4 Relationship between two continuous variables\n
 Visualization of UnderSampling Technique\n
Visualization of OverSampling Techinque\n
Visualization of OverSampling Techinque\n
Choosing SMOTE oversampling data for modeling as the number of datapoints generated are in equal proportion from this technique.
"# Shopee Price Match Guarantee: Before we start\n![](https://storage.googleapis.com/kaggle-competitions/kaggle/24286/logos/header.png?t=2021-01-07-16-57-37)\n\nDo you scan online retailers in search of the best deals? You're joined by the many savvy shoppers who don't like paying extra for the same product depending on where they shop. Retail companies use a variety of methods to assure customers that their products are the cheapest. Among them is product matching, which allows a company to offer products at rates that are competitive to the same product sold by another retailer. To perform these matches automatically requires a thorough machine learning approach, which is where your data science skills could help.\n\nTwo different images of similar wares may represent the same product or two completely different items. Retailers want to avoid misrepresentations and other issues that could come from conflating two dissimilar products. Currently, a combination of deep learning and traditional machine learning analyzes image and text information to compare similarity. But major differences in images, titles, and product descriptions prevent these methods from being entirely effective.\n\nShopee is the leading e-commerce platform in Southeast Asia and Taiwan. Customers appreciate its easy, secure, and fast online shopping experience tailored to their region. The company also provides strong payment and logistical support along with a 'Lowest Price Guaranteed' feature on thousands of Shopee's listed products.\n\nIn this competition, you‚Äôll apply your machine learning skills to build a model that predicts which items are the same products.\n\nThe applications go far beyond Shopee or other retailers. Your contributions to product matching could support more accurate product categorization and uncover marketplace spam. Customers will benefit from more accurate listings of the same or similar products as they shop. Perhaps most importantly, this will aid you and your fellow shoppers in your hunt for the very best deals."
Work directory
"An MVP Aproach\n=========\n\n 1. Introduction\n---------------------------------\nAs a (admittedly lazy) Data Scientist I love accessing information in the easiest way possible. Google is my best friend, for example. With the exception of a few things, there is almost nothing you can't find with a simple search.\n\nThis kernel is written and developed using IPython Notebook and XGBoost, with the aim of being ready-to-submit. By that I mean any user can run each code block, submit the output, and achieve a comfortable top-half score. Breaking into the top 10% of the submissions would require quite a bit more effort and a more sophisticated approach. In the final section I include some comments and suggestions of some alterations/variations one might try to improve the score. Those suggestions, if carried out correctly, do result in a strong score on the leaderboard. \n\nThis then serves as an introductory tutorial to approaching this particular problem (rather than giving the best solution outright) leaving enough room for creativity and innovation in the solution space to allow any user to better the end result.\nIn short, the whole project is about LEARNING, sharing some ideas, and ultimately improving the degree of Data Scientists out there. \nEnjoy! \n\n1.1 The Problem \nNew York is riddled with one-ways, small side streets, and an almost incalculable amount of pedestrians at any given point in time. Not to mention the amount of cars/motorcycles/bicycles clogging up the roads. Combine this with a mad rush to get from point A to point B, and you'll find yourself late for whatever you need to be on time for.\n\nThe solution to getting from A to B when living in a city like New York (without losing your mind) is easy: take a taxi/Uber/Lyft/etc. You don't need to stress about the traffic or pedestrians and you have a moment to do something else, like catch up on emails. Although this sounds simple enough, it doesn't mean you'll get to your destination in time. So you need to have your driver take the shortest trip possible. By shortest, we're talking time. If a route A is X kilometers *longer*, but gets you there Y minutes *faster* than route B would, rather take that one.\n\nTo know which route is the best one to take, we need to be able to predict how long the trip will last when taking a specific route. Therefore, *the goal of this playground competition is to predict the the duration of each trip in the test data set, given start and end coordinates.*\n\n1.2 The Libraries & Functions\n\nUsing Python 3.6.1, import the following libraries. Note the use of `%matplotlib inline`, allowing the display of graphs inline in iPython Notebook.\n\nDocumentation\n[Scikit-Learn](http://scikit-learn.org/stable/documentation.html ""Scikit-Learn"")\n[Pandas](http://pandas.pydata.org/pandas-docs/stable/ ""Pandas"")\n[Numpy](https://docs.scipy.org/doc/ ""Numpy"")\n[XGBoost](http://xgboost.readthedocs.io/en/latest/python/python_intro.html ""XGBoost"")\n[Seaborn](https://seaborn.pydata.org/index.html ""Seaborn"")\n\nI used Scikit-Learn (or sklearn) for a few of the machine learning operations that was carried out. Pandas is used for data manipulation. Numpy is the fundamental package for scientific computation in Python. XGBoost is the classification algorithm used to make the final predictions. Seaborn is a nice tool for data visualisation built on top of matplotlib. The import code is as follows:"
1.3 Loading the Data\nLoad the data using the Pandas `read_csv` function:
"1. 3. Data Visualisation and Analysis\n----------------------------------\n These next steps involve looking at the data visually. Often you'll discover looking at something significant as a graph rather than a table (for example) will give you far greater insight into its nature and what you might need to do to work with it. Of course, the opposite could also be considered true, so don't neglect the first section we went through.\n\n3.1 Initial Analysis\n\n Let's plot a simple histogram of the trip duration, throwing the data into 100 bins. Change this around to get a feel for what binning does to your data. Simply put, binning involves taking your data's max and min points, subtracting it to get the length, dividing that length by the number of bins to get the interval length, and grouping the data points into those intervals. Here's what that looks like: "
"This is a good opportunity to play with some data transformations to see if notable patterns emerge in the data when applying certain transforms, for example a log transform. In this case, applying a log transformation to the trip duration makes sense, since we are doing this to accommodate the leaderboard's scoring metric. That would look like this:"
"This is a good opportunity to play with some data transformations to see if notable patterns emerge in the data when applying certain transforms, for example a log transform. In this case, applying a log transformation to the trip duration makes sense, since we are doing this to accommodate the leaderboard's scoring metric. That would look like this:"
"One might also be interested to view the number of trips over time, since this could reveal not only apparent seasonality in the data and certain trends, but could point out any siginficant outliers (if not already cleaned out of the data set of course) and indicate missing values (again, only if it wasn't already checked and cleaned in the data set).\n\nFor this we'll simply plot a timeseries line graph of both the test and training data to not only look into identifying possible trends/seasonality but to see if both data sets follow the same pattern shape. Reasonably we'd expect the two datasets to follow a very similar shape since the test data would be/is a randomly selected sample from the original dataset containing all data points. By picking the test points randomly, each data point has the same likelihood of being picked as a test point, thus ensuring uniformity in the test data sample."
"One might also be interested to view the number of trips over time, since this could reveal not only apparent seasonality in the data and certain trends, but could point out any siginficant outliers (if not already cleaned out of the data set of course) and indicate missing values (again, only if it wasn't already checked and cleaned in the data set).\n\nFor this we'll simply plot a timeseries line graph of both the test and training data to not only look into identifying possible trends/seasonality but to see if both data sets follow the same pattern shape. Reasonably we'd expect the two datasets to follow a very similar shape since the test data would be/is a randomly selected sample from the original dataset containing all data points. By picking the test points randomly, each data point has the same likelihood of being picked as a test point, thus ensuring uniformity in the test data sample."
"Clearly the test and train datasets follow a very simila shape, as expected. A couple of points stand out at first glance. Around late-Jan/early-Feb there is a massive drop in the number of trips taken. A slightly less drastic drop is apparent about four months later. The first drop could be related to the season: it's winter in New York so you'd expect less trips being taken (who wants to ride around when it's near freezing outside?). However, this seems unlikely because the dip looks to be isolated around a single day or couple of days. In my opinion there's a greater chance that there were strikes (if you've got the South African mindset I do, but this is New York, so probably not that) or that there was an issue with the data system recording the trips. Whatever the reason, it's worth noting these 'outliers'.\n\n Let's see how significantly (or not significantly at all) the two vendors differ in their respective mean trip durations:"
"Clearly the test and train datasets follow a very simila shape, as expected. A couple of points stand out at first glance. Around late-Jan/early-Feb there is a massive drop in the number of trips taken. A slightly less drastic drop is apparent about four months later. The first drop could be related to the season: it's winter in New York so you'd expect less trips being taken (who wants to ride around when it's near freezing outside?). However, this seems unlikely because the dip looks to be isolated around a single day or couple of days. In my opinion there's a greater chance that there were strikes (if you've got the South African mindset I do, but this is New York, so probably not that) or that there was an issue with the data system recording the trips. Whatever the reason, it's worth noting these 'outliers'.\n\n Let's see how significantly (or not significantly at all) the two vendors differ in their respective mean trip durations:"
"So it doesn't look like there's much of a difference between travel times for the two vendors. One would assume that knowing which routes are the fastest to take from A to B is no secret, and that it is more a trick of the trade, rather than IP. But something doesn't look right, so there's another feature we can use to see if after all there is a significant difference in mean travel time: the `store_and_fwd_flag`."
"So it doesn't look like there's much of a difference between travel times for the two vendors. One would assume that knowing which routes are the fastest to take from A to B is no secret, and that it is more a trick of the trade, rather than IP. But something doesn't look right, so there's another feature we can use to see if after all there is a significant difference in mean travel time: the `store_and_fwd_flag`."
"So it would seem that the `store_and_fwd_flag` discriminates well between travel times. Clearly there is a slight skew in the data where some of the vendor employees didn't record their travel times accurately.\n\nAs mentioned earlier when digging into the variables, I thought about the impact that the number of passengers per trip might have on travel time. The thought process was that more passengers might equal more stops, hence prolonging the travel time from start to finish (unless the different drop-offs were recorded as separate trips). So in order to find out if there is infact a significant influence in travel time, let's group the mean travel times by the `passenger_count`:"
"So it would seem that the `store_and_fwd_flag` discriminates well between travel times. Clearly there is a slight skew in the data where some of the vendor employees didn't record their travel times accurately.\n\nAs mentioned earlier when digging into the variables, I thought about the impact that the number of passengers per trip might have on travel time. The thought process was that more passengers might equal more stops, hence prolonging the travel time from start to finish (unless the different drop-offs were recorded as separate trips). So in order to find out if there is infact a significant influence in travel time, let's group the mean travel times by the `passenger_count`:"
"So no significant difference evident that could be explained by the number of passengers in the vehicle for any given trip. It is interesting to note that there is are on average +-4min trips associated with no passengers. This is probably a mistake made in recording the data unless the vendor employee is into charging him/herself for trips whilst on the job.\n\nAgain, we need to check whether the test and train data matches with respect to the number of trips containing X-number of passengers:"
"![](http://)3.2 Coordinate Mapping\n\nJust like we compared the travel time data and the number of passengers between the test and train sets, we can try and verify that the pickup location data in both sets are fairly similar and representative of one another. \n\n3.2.1 Pickup Locations\nTo do this, we utilise the city map border coordinates for New York, mentioned earlier in the kernel to create the canvas wherein the coordinate points will be graphed. To display the actual coordinates a simple scatter plot is used:"
"> We can tell from the two graphs that the pickup locations are quite similar, with the notable difference being that the train data set simply has more data points (which makes sense).\n\n3.2.2 Distance and Directionality\nThis next part is quite interesting. Thanks to Beluga's post, we can determine the distance and direction of a specific trip based on the pickup and dropoff coordinates. For this I've made three functions, as:"
"So, excellent! We can safely use the different date parts in their extracted forms as part of the modelling process. Let's take a look at the average speed and how it changes over time, specifically focusing on how the hour of the day, the day of the week, and the moth of the year affects average speed. It's important to note though that average speed is a function of distance and time so it wouldn't add anything to the modelling output. We'll therefore need to remove it eventually before we train our model."
"So the interesting thing to notice here is the average speed by hour of day. We're I'm from traffic usually peaks between 5am and 9am, and then again from about 4pm to around 6 or 7pm. But it would seem in manhattan that average speed diminishes as the day goes by from around 6am and picks up again around 7 or 8pm. So most of the travelling in the Big Apple happens during work hours. The average speed by weekday follows an expected trend. Over the weekend (Friday, Saturday, Sunday) the average speed picks up quite nicely, indicating less traffic. Finally, the average trip speed by month follows an expected trend. In the winter months there are less trips (see the previous timeseries plot we made) indicating less traffic in general in the city which means you can average a higher speed on the roads.\n\n\nThis next part uses the pick-up locations and the average speed data we've got and plots the average speed by location. "
"So the interesting thing to notice here is the average speed by hour of day. We're I'm from traffic usually peaks between 5am and 9am, and then again from about 4pm to around 6 or 7pm. But it would seem in manhattan that average speed diminishes as the day goes by from around 6am and picks up again around 7 or 8pm. So most of the travelling in the Big Apple happens during work hours. The average speed by weekday follows an expected trend. Over the weekend (Friday, Saturday, Sunday) the average speed picks up quite nicely, indicating less traffic. Finally, the average trip speed by month follows an expected trend. In the winter months there are less trips (see the previous timeseries plot we made) indicating less traffic in general in the city which means you can average a higher speed on the roads.\n\n\nThis next part uses the pick-up locations and the average speed data we've got and plots the average speed by location. "
"So clearly, by neighbourhood, the average speed definitely changes. To a greater extent the center of the city is the busiest (we'd expect this since the majority of activity in large cities is focused around the center) and the average speed picks up nicely around the outskirts.\n\nWe can expect good performance from our clustering data during modelling just from looking at how well we can distinguish average speeds by neighbourhood (i.e. cluster). Something definitely worth exploring, which could boost the performance of the XGBoost model significantly, is to create a data set that can be used with [Xiaolin Wu's line algorithm](https://en.m.wikipedia.org/wiki/Xiaolin_Wu%27s_line_algorithm ""Xiaolin Wu's line algorithm""). This would involve pixelating the graph area and recording every pixel crossed by the line from the pick-up location to the drop-off location. If you can make the resolution as high as possible some of the pixels shoudl encapsulate traffic junctions, traffic lights, bridges, etc. Using the ""has crossed coordinate X"" features one could potentially create an extra +-10 000 features to train the alogrithm with.\n\n**Note:** XGBoost will be able to handle up to about 10 000 columns (features) with about 1 million rows of data on a Mac Book Pro."
# Load Libraries
# Load Data
### Import the Libraries
### Load the Data
### Exploratory Data analysis\n
### EDA using Pandas Profiling\n\npandas_profiling extends the pandas DataFrame with df.profile_report() for quick data analysis.\n\nGithub Repo : https://github.com/pandas-profiling/pandas-profiling\n\n\n![](https://drive.google.com/uc?id=1QEEcCjfj5cnA_9vRfj2nZLRK0QlQGuBV)\n
üìå From the graph we can infer that most of the area is filled with air as air has HU around -1000 and the next highest is water: HU around 0\n\n## Sorting our slices and GIF creation\n\nGo To Table of Contents\n
We are arranging the slices in order and skipping 5 slices at a time to look at wider variety of slices
We are arranging the slices in order and skipping 5 slices at a time to look at wider variety of slices
## 3D Reconstruction using slices\n\nGo To Table of Contents\n
### Pixelspacing\n* The pixelspacing attribute you can find in the dicom files is an important one. It tells us how much physical distance is covered by one pixel. You can see that there are only 2 values that describe the x- and y-direction in the plane of a transversal slice.\n* For one patient this pixelspacing is usually the same for all slices.\n* But between patients the pixelspacing can differ due to personal or institutional preferences of doctors and the clinic and it also depends on the scanner type. Consequently if you compare two images in the size of the lungs it does not automatically mean that the bigger one is really larger in the physical size of the organ!
"We can see that the values really vary a lot from patient to patient! As they are given in mm and ct-scans usually cover 512 row and column values\n\n### Physical area & slice volume covered by a single ct-scan\n\nNow, we know some important quantities to compute the physical distance covered by a ct-scan!"
# Light EDA\n\nGo To Table of Contents\n
We can see an unusual heatmap this is because:\n\nüìå Most of our values are dervied which makes some features highly correlated
We can see an unusual heatmap this is because:\n\nüìå Most of our values are dervied which makes some features highly correlated
üìå Slice_thickness is directly proportional to the volume
"## Step 1: Load in your packages\nTo help with the readability of your notebooks, it is helpful to have all of your packages loaded in at the top of your notebook. As you go through and find that you need more packages to accomplish what you want, go back to this first cell block, write in the code to import each new package, and re-run the cell. Then get back to coding."
"## Step 2: Where is the data?\nThis code block is an example of a for loop that walks through the directory for this competition and prints out all the file names. This is a lot of files though, so it's a good opportunity to show of %%capture. This hides the output of a cell block if you put it at the beginning of a cell!\n\nHere's what the first 5 lines would look like without %%capture:\n1. /kaggle/input/petfinder-pawpularity-score/sample_submission.csv\n2. /kaggle/input/petfinder-pawpularity-score/train.csv\n3. /kaggle/input/petfinder-pawpularity-score/test.csv\n4. /kaggle/input/petfinder-pawpularity-score/test/c978013571258ed6d4637f6e8cc9d6a3.jpg\n5. /kaggle/input/petfinder-pawpularity-score/test/4e429cead1848a298432a0acad014c9d.jpg"
"Note that the dataframe still contains the Id's of the photos. You won't be using this when you build your models.\n\nIt's also useful to first see the distribution of your target variable when you first start analyzing a dataset. In our case, that is the Pawpularity score, which is in the range of 1-100. One way to do this is with a simple histogram. "
"Looking at this histogram, note the skew in the distribution of the pawpularity scores. Interesting there is a small curve close to zero Pawpularity as well. Also, note the close to 300 pawpularity scores at 100! Start thinking about why this might be. \n* Is there something unique about these animals? \n* Is there a particular breed, color, age of animal that is most desirable by the people visiting the site?\n* Is there something about the way the photos were taken that is leading to more clicks and thus a higher Pawpularity score?\n* Does this have do to with the Pawpularity score itself? \n* Are these outliers that need to be removed from the training data to improve the models you will build?\n\nKeep these types of questions in mind as you go about solving this problem."
"### Now lets visualize the distribution of Pawpularity scores across each feature variable\n\nTo do this, we can use some simple box plots and histograms. Basically, we'll plot the Pawpularity scores on the y axes and the 0s and 1s of each feature variable on the x axes for the boxplots. For the histograms, we plot the pawpularity on the x axes and count the 0s and 1s at each Pawpularity score. This could help us visualize if 0s or 1s in each feature variable have an impact on the Pawpularity scores."
"Notice how the boxplots are almost identical? Going to be difficult for any algorithm to use this data to predict Pawpularity using this feature. With respect to the histogram, notice that there are many more 1 values than 0 values for this feature, but again distribution is very similar."
"Notice how the boxplots are almost identical? Going to be difficult for any algorithm to use this data to predict Pawpularity using this feature. With respect to the histogram, notice that there are many more 1 values than 0 values for this feature, but again distribution is very similar."
"### Quick Analysis\nYou can't always tell just from looking at chart if you're going to be able to build a highly predictive model, but intuition is telling me that models are not going to be great based on these charts. The reason is that the distribution of pawpularity scores is very similar for each variable and class - in other words changing the features doesn't seem to influence the pawpularity scores that much. This might mean that a competition winning solution to this problem will require using the images and not the .csv metadata provided."
Nice! We have read in and displayed our first image! Looks like a cute dog to me. It would be better if we could see the Pawpularity score along with each image though. Let's try that next. We'll put the Pawpularity score for each image as the title. We can get this because the name of the image files is also stored in the Id column in the .csv metadata. We have this in our train_df DataFrame from earlier.
"Cool we've used a for loop to visualize the first 3 images and their dimensions. Notice that image 0's dimensions are: (960, 720, 3), while image 2's dimensions are: (514, 315, 3). This tells us that we need to reshape or resize the images when we end up building our models. The 3 at the end tells that this is an RGB image where each color channel has pixel values between 0-255. Try out type(image_array) to show that it is a numpy array.\n\nTime to build on our loop to display the Pawpularity score of each associated image.\n\nTo do this, we need to get the image filenames without the directory and .jpg at the end. This will let us search the Id column in the train_df dataframe for corresponding Pawpularity scores. When you have a file path like: '../input/petfinder-pawpularity-score/train/7954ebb5c90d9618e34959df0ad5f062.jpg'and you want to get just that stem, you can use from pathlib import Path. "
**Now let's put it all together again**
"Sweet, now we can see the Pawpularity score for a given image file. We pulled this information from two different places and put it together in a loop. \n\n**Let's do something a little different now and build a function that returns pet pictures based on given pawpularity scores.**"
"Sweet, now we can see the Pawpularity score for a given image file. We pulled this information from two different places and put it together in a loop. \n\n**Let's do something a little different now and build a function that returns pet pictures based on given pawpularity scores.**"
### Pawpularity 10
"# Preparation \n\nOk, we need to load the packages..."
... and I would like to select an example to try out the preprocessing methods that are usually used. Let's load the data together with the path of the npy files:
# Understanding the data \n\n## How does the data look like?
"### Insights\n\nOk, the three signals originating from different detectors all look a bit different. How was this data generated? We are given...\n\n* detector noise from three real detectors (LIGO Hanford, LIGO Livingston, and Virgo). As far as I understand this noise is not simulated. \n* A simulated gravitational wave signal hidden in this noisy data in the case of hot targets. You can't see it with your eyes! "
"Don't wonder, the stuff inside the sine wave is a bit more complicated than what we used above. Instead of $y=sin(t)$ it holds $y=r \cdot sin(\omega  t)$ with $\omega=2 \pi f$ being [the angular frequency](https://en.wikipedia.org/wiki/Angular_frequency) and $r$ being the radius that we can set to adjust the amplitude $\hat{y}$ (max elongation). You can see that it's just a constant times frequency. So there is nothing new to learn. A higher frequency means more ups and downs of our wave within a fixed time period compared to a low frequency. "
"Ok, with these values we obtain one wave with high frequency and low radius and another wave with low frequency and higher radius. Adding up both we can see the same kind of ""problem"" like above - a ""big wave"" with a small one ""hidden"". "
"Ok, let's try it:\n\n* Perform the Fast Fourier Transform with $y_{1}$, $y_{2}$ and $y_{1}+y_{2}$\n* Compute the Power Spectral Density to get the components\n* Show that $y_{1}+y_{2}$ is like PSDs of $y_{1}$ and $y_{2}$ added up."
"### Insights\n\nCool, isn't it? :-) We can clearly see that the wave that was built by adding up the two sine waves has both components of them in its PSD. This is awesome as we can now use such kind of PSD components to filter out signals of our desire! To get closer to the competition I have added a noisy version of $y1+y2$ that shows a lot of fluctuations and high frequency components in the PSD.\n\nLet's try it out using the competition data!"
Now we can use the same idea as above to compute the Power Spectral Density:
"### Insights\n\n* We can see that we have more fluctuations in the PSD for high frequencies. :-( That's bad as there should be our hidden signal as well.\n* The low frequencies belong to our ""big waves"" that are caused by instrumental vibrations or terrestrical forces etc. Here we won't find our signal.\n\nLet's pick a few component examples (or regions in the PSD) to show which kind of wave belongs to it. This way we can compare with the original data and get some feeling about its structure:"
Let's take a look how the window functions change our data:
Hmm... what if our signal can be found in the beginning our end of the data? Then windowing would be very bad... wouldn't it?!
We can plot the bars horizontally as follows :-
#### Visualize `income` wrt `sex` variable
#### Visualize `income` wrt `sex` variable
#### Interpretation\n\n\n- We can see that males make more money than females in both the income categories.
#### Visualize `income` wrt `race`
#### Interpretation\n\n\n- We can see that whites make more money than non-whites in both the income categories.
#### Visualize `workclass` variable
#### Interpretation\n\n\n- We can see that there are lot more private workers than other category of workers.
#### Visualize `workclass` variable wrt `income` variable
#### Interpretation\n\n\n- We can see that workers make less than equal to 50k in most of the working categories.\n\n- But this trend is more appealing in Private `workclass` category.
#### Visualize `workclass` variable wrt `sex` variable
#### Interpretation\n\n\n- We can see that there are more male workers than female workers in all the working category.\n\n- The trend is more appealing in Private sector.
#### View the distribution of `age` variable
We can see that `age` is slightly positively skewed.
We can use Pandas series object to get an informative axis label as follows :-
We can shade under the density curve and use a different color as follows:-
We can shade under the density curve and use a different color as follows:-
#### Detect outliers in `age` variable with boxplot
#### Detect outliers in `age` variable with boxplot
We can see that there are lots of outliers in `age` variable.
#### Explore relationship between `age` and `income` variables
"#### Interpretation\n\n- As expected, younger people make less money as compared to senior people."
#### Visualize relationship between `race` and `age`
#### Interpretation\n\n- Whites are more older than other groups of people.
#### Find out the correlations
#### Interpretation\n\n- We can see that there is no strong correlation between variables.
#### Plot pairwise relationships in dataset
#### Interpretation\n\n- We can see that `age` and `fnlwgt` are positively skewed.\n\n- The variable `education_num` is negatively skewed while `hours_per_week` is normally distributed.\n\n- There exists weak positive correlation between `capital_gain` and `education_num` (correlation coefficient=0.1226). 
"## 14. Visualize feature scores of the features \n\n\n[Back to Table of Contents](#0.1)\n\n\nNow, I will visualize the feature scores with matplotlib and seaborn."
#### Interpretation\n\n\n- The above plot confirms that the most important feature is `fnlwgt` and least important feature is `native_country_41`.
"### 2.1) Importing Libraries \n\nThis is our very first step, we shall import the necessary libraries that we want into python. We can add libraries along the way. Always import the basic libraries first."
"### 2.2) Import Datasets \n\nOur second step is to import our datasets, in this case, we import train and test datasets. There are various ways to import your datasets into python. Here I give an overview of what train and test datasets mean.\n\n\n**Training and Testing Sets**\n\n   a) **Training data:** Training data is the information used to train an algorithm. The training data includes both input data and the corresponding expected output. We can train different machine learning algorithms on the training data, in the hopes of finding one best algorithm for a particular problem that predicts well. Bear in mind that even if our algorithm performs extremely well on the training data, it may be a result of overfitting, and may not replicate the same accuracy when the algorithm is applied to some unseen (test) data. \n\n   b) **Testing data:** On the other hand, includes only input data, not the corresponding expected output. The testing data is used to assess how well your algorithm was trained, and to estimate model properties.\n   \n\n     \nSo in our training data, we see that our second column says whether the passenger survived or not. The survived column is our output.  And in our test data, the output is not there, it is for us to TEST if our model developed based on the training set is accurate. However the preceding sentence seems to have a problem, IF, in the real world, we do not have the 'answers' for the test data, how then, would we be able to tell how well the training algorithm is performing on the test set. To resolve that issue, we usually perform train test split/cross validation method on the given training set first. See section 5.03 for more details."
"The code below is useful to understand if you want to plot a barplot using Seaborn package. You can mimic my code for future scenarios.\n\n\n1. We specify the plot size in the first line and name our axis survive_bar.\n\n2. I wish to use the package seaborn to plot my bar plot. So it is a good habit to look up the seaborn package online to see what parameters I should input.\n\n3. we want to input x = train[""Survived""].value_counts().index, meaning that in the x axis, we have the index of the survived - which is 0 and 1. Next, y = train[""Survived""].value_counts() means that in the y axis input the frequency counts for 0 and 1 respectively. Lastly, ax = survive_bar just means we want to input all our graph on this axis called survive_bar where we defined a size for survive_bar earlier. Note that you can define your own axis to your likings.\n\n4. survive_bar.set_xticklabels(['Not Survived', 'Survived']) means we manually change the x axis's variable name from 0 and 1 to Not survived and Survived. survive_bar.set_ylabel('Frequency Count') just means we label the y axis as Frequency Count.\n\n5. survive_bar.set_title('Count of Survival', fontsize = 16) simply means setting the title for our graph.\n\n6. The last chunk of long codes just gives the count labels inside the graph. Sometimes it is visually pleasing to label the counts on the graph."
The survive_bar plot gives you the information that 62% of the passengers died.
#### **Embarked Count**
Some interesting observations is that most of the passengers boarded from Southampton. 
"#### CODING classes gather here! \nI always thought that knowing how to make your graphs neater is a good skill to have. Let us try,\n\n#### Category variables count\n\nThe below plot is very useful:\n\n\n1. We first create a plot grid, with subplots on it, we define the figure size as usual, and now we pass some more conditions in it, we want to make plot multiple graphs on a grid of say 2 by 3 grid. In this way, if your code consists of repetitive graphing of a bar chart in the same fashion, we can just run a loop on this grid! \n\n2. For example, we want to plot a very simple frequency count for titanic's categorical variables, namely, survived, pclass, sex, sibsp, parch and embarked. So we can run the below code.\n\n3. Note for (u,v,z) in [[""Survived"",0,0],[""Pclass"",0,1],[""Sex"",0,2], [""SibSp"",1,0], [""Parch"",1,1],[""Embarked"",1,2]] just means for each element (u,v,z) in the list, we have u,v,z corresponding to each cross product in the list. Note, to say it explicitly, u is ""survived"", ""pclass"", etc.. v =0,1,2,..., z = 0,1,2,...\n\n    Now I am telling you that you need to pass 3 elements (u,v,z) in the for loop, but when you are creating the code the first time, you may not realise at first, so the computational thinking is you write out the codes like how you would manually:\n    \n    -sns.barplot(x = train[""Survived""].value_counts().index,y = train[""Survived""].value_counts(), ax  = myplot[0,0])\n    -sns.barplot(x = train[""Pclass""].value_counts().index,y = train[""Pclass""].value_counts(), ax  = myplot[0,1])\n    -sns.barplot(x = train[""Sex""].value_counts().index,y = train[""Sex""].value_counts(), ax  = myplot[0,2])\n    etc...\n    \n    So I suspect you need to loop through the categorical variables, survived, pclass, sex etc. And of course since you are plotting your graph on a 2x3 grid, we let our first graph appears in the place [0,0], second graph appears in the place [0,1], etc... So we want to loop through 3 things! So I should tell myself I need 3 elements to loop through in the for loop. And to do that it is now easy to see why I said: for (u,v,z) in [[""Survived"",0,0],[""Pclass"",0,1],...], However the [[..],[..],...] is hardcoded, but hey it is a good start to slowly cultivate your programming logic. \n    \n   I added on a bit of matplotlib codes, which should be relatively googlable, just want to add the same set of features onto each of my graph on myplot[v,z]. Lastly, adjust is to make some spacing between each graphs, I am very particular about visuals, although I am not very good at it yet."
"**Age Distribution**\n\nAge is a variable that we can create bins for, but that will be another topic for another time. For now, let us look at the distribution and see what the distribution is. But be reminded, the distribution we are showing here is the training data set, whereby there are 177 missing ages supposedly. We **have not account for the missing values**. We call this plot **preimputation**. We will definitely come back to this plot later to compare when we have imputed the missing values of age."
"#### **Correlation Heatmap**\n\nWe shall do a correlation Heatmap to visualize which variables are more correlated with each other, in most context, correlation can help in predicting one attribute from another, which is a great way to impute missing values (we do have a lot of missing values and we will see some examples later). Of course, in regression wise, correlation matrix can be used to detect multi collinearity or say outliers.\n\n\nBy default, sns.heatmap will not include categorical features which are not factorized. As a result, the heatmap below did not include those categorical variables, only the columns with numerical are plotted against each other.\n\nSo since some categorical variables are not inside this map, we need to view their correlation with the other variables seperately. For example, our earlier plot suggests that ""sex/gender"" being female has a higher survival rate than males. Note that it is difficult to compare correlation of categorical and continuous variables and hence it brings up to the next section - where we plot more graphs to attempt to obtain an intuition between all the variables."
"### 3.2) Plot variables against Survived  \n\nNote that one should see that Name, Ticket, PassengerID will not play any crucial role in predicting the survival rate.\nHence I do not want to analyse them, later I will drop them. However, after reviewing many kernels by other experienced data analysts, we can do some feature engineering with Name - We will mention it later. We will do some basic overview of the survival rate in different categories."
"Bear in mind, we can use plot 1 when our categorical pclass does not have many categories/levels. What if pclass has 30 different levels. Then our graph will look messy. This is why I also used a line plot in plot 2 to illustrate that line plot can be a useful visualization tool as well.\n\n\n\nFrom the above 2 plots, we have the same conclusion, in Pclass 1, 63 percent of the passengers in Pclass 1 survived. In Pclass 2, 47.3 percent of passengers in Pclass 2 survived. In Pclass 3, 24.2 percent of Pclass 3 passengers survived. **So we have a feel that the the better your class, the higher your chance of survival.**\n\n\n\nBut do recall in our first part 3.3.1, we did see that gender DO affect one's chance of survival. We can further confirm our hypothesis by plotting Gender + Pclass vs Survival Rate.\n\n\n\n**How many men and women survived grouped by their Passenger Class?**\n\nHere is a neat code from https://www.kaggle.com/poonaml/titanic-survival-prediction-end-to-end-ml-pipeline to plot this.\n\nOne may ask what can we get from this plot? Well for a start, we do know that women has a much higher survival rate than men, so is this still true if we categorize them by different Pclass? Apparently, you will see later that for Pclass 1 and 2, women still maintain a very high survival rate, but in Pclass 3, we can see the women's survival rate dip down by quite a lot. This may suggest Pclass 3 is the ""poorer"" people's class - and hence they are valued less when evacuating (unfortunately life is unfair).\n\nAlso note that we can see in a correlation heatmap, fare and pclass are quite strongly correlated with a value of 0.55. So we reckon higher fare corresponds to Pclass 1.\n\n\n\n\nOne side note on the coding part below: catplot returns you a function called g which is a FacetGrid object with the plot on it for further tweaking. So the below code is how we usually add on different plottings on the facetgrid itself. Note there is a difference between the way we did on barplot, on barplot it returns you a function called ax which is a single grid.\n"
"Factor/cat Plot is quite useful for comparing multiple levels. Please explore it. The above plots can be interpreted as follows: In Pclass 1, about 38 percent of the **male** survived, for all the female passengers in Pclass 1, more than 90 percent of the **female** survived! \n\n\n\nSimilarly, we can find the trend for Pclass 2 and Pclass 3. \n\n\n\nA good conclusion we can make is: Female in Pclass 1 and 2 have a very high survival rate of 90 percent while their male counterparts are significantly lesser. All passengers in Pclass 3 have a low survival rate. My hypothesis is that male in general has a low survival rate irregardless of their Pclass."
**Pclass vs Embarked**\n
"Indeed, Passengers embarked from Chersboug has a high **proportion** of Pclass 1. This may account for one of the reasons that passengers who boarded from Chersboug has a significantly higher suvival rate than the rest of the cities."
**Gender vs Embarked**
"Although this plot did not tell us anything informative on whether females play a role in why Chersboug has a higher survival rate, it did hint to us that why Southampton has the lowest survival rate. A good reason is that there is way more males embarked from Southampton than females. "
"### 3.2.4) Plot Age against Survived  \n\nNow we do an EDA on Age vs Survival Rate. Age is a continuous variable, and we should not use bar chart here. I present one of the way to plot a KDE distribution below, where you superimpose the age plot for survived and the age plot for not survived. The idea of the code is you first plot the age plot for not survived, and then you plot the age plot for survived on the same axis: ax = myaxis, then the graph will present both graphs on the same grid.  "
"The above plot shows the age vs survival rate, represented by a density plot; for survivors and non survivors, the distribution seems to be similar. However, for survivors we see an obvious local maximum at around 0-5 years old, indicating small children has a higher survival rate. **Reminder: This distribution is before filling in the missing values of age.**\n\nBelow are some more visualizations graphs."
"Since fare is all filled up, we compare the fare distribution as below. Also when you look carefully at the plot below, I compared the test and training set's distribution of Fare vs Survival rate side by side and realized that the training set is not so ""smooth"" as compared to the test set. This has an underlying problem, and the idea is that when our algorithm is searching for patterns in the training set, our algorithm might adapt to certain underlying patters in the training set that DO NOT exist in the test set  - as a result, the machine learning algorithm may not be able to generalize well to the test set - which is why when we use a certain algorithm on our training set, it seems to give us a high accuracy like 80% but when we do it on the test set, it becomes much lower at maybe 75%."
### 4.1.2) Missing Values: Embarked 
1. Comparing the KDE plot for the age of those who perished before imputation against the KDE plot for the age of those who perished after imputation.
2. Comparing the KDE plot for the age of those who survived before imputation against the KDE plot for the age of those who survived after imputation.
2. Comparing the KDE plot for the age of those who survived before imputation against the KDE plot for the age of those who survived after imputation.
#### **Contrasting the first imputation method with the imputation by median method**
Median imputation: Comparing the KDE plot for the age of those who perished before imputation against the KDE plot for the age of those who perished after imputation. Here we can see that the distribution between pre-imputation and after-imputation using this method is quite large - much more difference than our first method.
Median imputation: Comparing the KDE plot for the age of those who survived before imputation against the KDE plot for the age of those who perished after imputation. 
Median imputation: Comparing the KDE plot for the age of those who survived before imputation against the KDE plot for the age of those who perished after imputation. 
#### **Important final note for imputation**
Let's import some standard libraries
We will demonstrate and compare different algorithms on diabetes dataset from sklearn.datasets. Let's load it.
We have managed to improve the results. But spent a lot of time on it. Let's look how our parameters have been changing from iteration to iteration:
"We can see that for example max_depth is the least important parameter it does not influence score significantly. But we are searching over 8 different values of max_depth, and with any fixed value search over other parameters. It is obvious waste of time and resources.\n\nLet's try a RandomizedSearch approach now."
Let's try to use RandomizedSearchCV from sklearn.model_selection.\n\nWe will start with very broad parameters space and make only 50 random steps:
"As we can see, the results are already better than GridSearchCV. We have spent less time and made more complete search. Let's look at our visualization:"
"As we can see, the results are already better than GridSearchCV. We have spent less time and made more complete search. Let's look at our visualization:"
"As we can see every step is completely random. It helps not to spent time on useless parameters, but it still does not use the information gathered on the first steps to improve outcomes of the latter ones."
We have managed to find even better solution comparing to the RandomizedSearch.\nLet's look at the visualization of the process
"We can see that the movement of the parameters are quite random but the results become better with time: there are no extremely bad scores after 25 iterations but the number of good solutions increases. Algorithm started to predict quite good solutions, using information from the previous steps."
Let's plot best_cumulative_score vs. number_of_iterations for all approaches:
"We can see than TPE and Annealing algorithms actually keeps improving search results over time even on later steps while Random search randomly found quite a good solution in the beginning and then only slightly improved the results. The current difference between TPE and RandomizedSearch results is quite small but in some real life applications with more diversified range of hyperparameters hyperopt can give you significant time/score improvement.\n\nNote: in real life it is more correct to use time and not a number of iterations for comparison, but in our toy example the proportion of time spent on the additional calculations in tpe and annealing is to high comparing to cross_val_score calculation time so I have decided not to mislead you about computational speed of the hyperopt and plot scores in relation to the iteration number."
"So, without doing anything we have a **CV score** of **0.553**."
"# 4. PyTorch Dataset\n\nWe'll create a Dataset class called `ShopeeDataset` that will:\n1. Receive the metadata\n2. Read in the `image` and `title`\n3. Perform image augmentation and tokenization\n4. Return the necessary information to feed into the model afterwards\n\n\n### The Bert Tokenizer ([data from Abhishek Thakur](https://www.kaggle.com/abhishek/bert-base-uncased/code?datasetId=431504&sortBy=voteCount)):\n* Pretrained tokenizer that splits sentences into tokens (source from `transformers` library - [click here for more info](https://huggingface.co/transformers/preprocessing.html))\n* The output is as follows:\n    * `input_ids`: indices corresponding to each token in the sentence\n    * `attention_mask`: indicates to the model which tokens should be attended to, and which should not ([documentation on attention_mask here](https://huggingface.co/transformers/glossary.html#attention-mask))\n"
"Let's plot the data. Informally, the classification problem in this case is to build some ""good"" boundary separating the two classes (the red dots from the yellow). Machine learning for this case boils down to choosing a good separating border. A straight line will be too simple while some complex curve snaking by each red dot will be too complex and will lead us to making mistakes on new samples. Intuitively, some smooth boundary, or at least a straight line or a hyperplane, would work well on new data."
Let's try to separate these two classes by training an `Sklearn` decision tree. We will use `max_depth` parameter that limits the depth of the tree. Let's visualize the resulting separating boundary.
Let's try to separate these two classes by training an `Sklearn` decision tree. We will use `max_depth` parameter that limits the depth of the tree. Let's visualize the resulting separating boundary.
"And how does the tree itself look? We see that the tree ""cuts"" the space into 8 rectangles, i.e. the tree has 8 leaves. Within each rectangle, the tree will make the prediction according to the majority label of the objects inside it."
#### Example\nLet's generate some data distributed by the function $f(x) = e^{-x ^ 2} + 1.5 * e^{-(x - 2) ^ 2}$ with some noise. Then we will train a tree with this data and predictions that the tree makes.
We see that the decision tree approximates the data with a piecewise constant function.
"### Complex Case for Decision Trees\n\nTo continue the discussion of the pros and cons of the methods in question, let's consider a simple classification task, where a tree would perform well but does it in an ""overly complicated"" manner. Let's create a set of points on a plane (2 features), each point will be one of two classes (+1 for red, or -1 for yellow). If you look at it as a classification problem, it seems very simple: the classes are separated by a line. "
"However, the border that the decision tree builds is too complicated; plus the tree itself is very deep. Also, imagine how badly the tree will generalize to the space beyond the $30 \times 30$ squares that frame the training set."
"However, the border that the decision tree builds is too complicated; plus the tree itself is very deep. Also, imagine how badly the tree will generalize to the space beyond the $30 \times 30$ squares that frame the training set."
"We got this overly complex construction, although the solution is just a straight line $x_1 = x_2$."
The method of one nearest neighbor does better than the tree but is still not as good as a linear classifier (our next topic).
"###  Decision Trees and k-NN in a Task of MNIST Handwritten Digits Recognition\n\nNow let's have a look at how these 2 algorithms perform on a real-world task. We will use the `sklearn` built-in dataset on handwritten digits. This task is an example where k-NN works surprisingly well.\n \nPictures here are 8x8 matrices (intensity of white color for each pixel). Then each such matrix is ""unfolded"" into a vector of length 64, and we obtain a feature description of an object.\n \nLet's draw some handwritten digits. We see that they are distinguishable."
## Look at a few train/test input/output pairs \n\nThese are some of the pairs present in the training data. I use functions from Walter's excellent starter kernel to plot these pairs.
## Number frequency 
## Number frequency 
"From the above graph, we can clearly see that the number distribution has a string positive skew. Most numbers in the matrices are clearly 0. This is reflected by the dominance of black color in most matrices."
### Train the CNN model on loop
# Submission 
"## Loading packages \n\nFirst of all, let's load some packages..."
## Loading data \n\nNow we will use the digits of the digit-recognizer competition. Let's check: 
"Before we start with building targeted and non-targeted attacks, let's have a look at the first digits of the test set:"
Some digits are not easily recognized by human eyes... 
"Yeah! :-) We can still see the true target and not the fooling target. That's amazing. But we can also see, that the background has increased intensitiy. Let's visualize the difference between the original true label and the adversarial image for $\epsilon = 16$: "
"### The gradient travel guide - natural fooling targets\n\nI'm happy that it was possible to fool our model but it's still diffuse and unclear where the one-step-gradient guides us through (remember we do not iterate with gradient ascent, we just take one step and size is given by strength of gradient times eta). I assume that some numbers are closer to each other in weight space than to others. As the model training draws decision boundaries dependent on the quality of the input data and flexibility of model architecture, there will be regions where a 3 is not predicted as 3 but as 8. Those regions where the model makes an incorrect prediction. And I think, that there are preffered numbers to be wrong predictions given a digit input image. Perhaps the fooling gradients drives us to those ""natural"" fooling target numbers? "
"### The gradient travel guide - natural fooling targets\n\nI'm happy that it was possible to fool our model but it's still diffuse and unclear where the one-step-gradient guides us through (remember we do not iterate with gradient ascent, we just take one step and size is given by strength of gradient times eta). I assume that some numbers are closer to each other in weight space than to others. As the model training draws decision boundaries dependent on the quality of the input data and flexibility of model architecture, there will be regions where a 3 is not predicted as 3 but as 8. Those regions where the model makes an incorrect prediction. And I think, that there are preffered numbers to be wrong predictions given a digit input image. Perhaps the fooling gradients drives us to those ""natural"" fooling target numbers? "
"Ok, we see that 8 was selected most often as fooling target. But 9, 3, 5 and 2 have high counts as well in contrast to 0, 1, 6 and 7. If our assumption is true that the gradient drives us to targets where the model tends to fail in prediction we should see a similar pattern of counts for wrong predictions:"
"Ok, so out of 16800 samples, the model failed to predict around 1600. That's why our intital accuracy score is close to 90 % (means 10 % failing). Now, which digit was selected as wrong prediction result most often?"
"Yes, that's the same pattern as for the fooling targets. As this is caused by the difficulty of our model to draw good decision boundaries we should see this pattern as well for the true labels of those digits that were wrong predicted:"
"Yes, that's the same pattern as for the fooling targets. As this is caused by the difficulty of our model to draw good decision boundaries we should see this pattern as well for the true labels of those digits that were wrong predicted:"
Now I want to see it in more detail: Which are the natural fooling targets (for successful foolings) for each digit?
"# Preface\nPeople who have read other kernels of mine, would probably be surprised by this one. Indeed, this is **my very first Python kernel**. Am I going to abandon R? Certainly not! I just want to broaden my options and knowledge, and strongly believe that I will continue to use both in the future. My initial thoughts on this are:\n\n* Use R for EDAs. Although visualizations with Python are not bad, ggplot just looks better and I find it quite easy to compose visualizations with ggplot. Also, Rmarkdown offers neat additional features that Python does not have (such as Tabs, and automatic generation of the Table of Content).\n* Use R for statistics\n* Use R for creating interactive web apps (with Shiny)\n* Use Python for machine learning and deep learning\n* Use Python for big datasets. I know that R can handle big datasets too with data.table, but I am a big fan of tidyverse. The readability of code decreases when using data.table, and I chose not to learn it. \n* Chose R or Python depending on the libraries available. CRAN (R) has many more (data science) libraries than Python. I know for instance that there are domain specific packages for Finance. If R or Python offers a specific package that really suits the task at hand, I will likely choose the language with the best libraries on offer.\n\nInsideairbnb.com is a website on which web scraped datasets of ""snapshots"" of cities are published. I have downloaded the files of Amsterdam of the situation on December 6th, 2018. I thought it is a fun dataset to take on. Besides basic data wrangling and plots, I have also added **interactive Folium maps, interactive plotly graphs, and text mining of the review comments.**\n\n\n\n\n\n# Table of contents\n\n* [1. Loading libraries and data](#1.-Loading-libraries-and-data)\n  * [1.1 Loading libraries](#1.1-Loading-libraries)\n  * [1.2 The listings and listing details files](#1.2-The-listings-and-listing-details-files)\n* [2. Data exploration](#2.-Data-exploration)\n  * [2.1 Neighbourhoods](#2.1-Neighbourhoods)\n  * [2.2 Room types and property types](#2.2-Room-types-and-property-types)\n  * [2.3 Accommodates (number of people)](#2.3-Accommodates-(number-of-people))\n* [3. Advice to the municipality of Amsterdam](#3.-Advice-to-the-municipality-of-Amsterdam)\n  * [3.1 Finding possibly illegal hotels](#3.1-Finding-possibly-illegal-hotels)\n  * [3.2 Unwanted effects of professional hosts?](#3.2-Unwanted-effects-of-professional-hosts?)\n* [4. Advice to the tourists](#4.-Advice-to-the-tourists)\n  * [4.1 Average daily price per neighbourhood](#4.1-Average-daily-price-per-neighbourhood)\n  * [4.2 Neighbourhood safety](#4.2-Neighbourhood-safety)\n  * [4.3 Review scores location, and location scores versus price](#4.3-Review-scores-location,-and-location-scores-versus-price)\n  * [4.4 How to use review scores](#4.4-How-to-use-review-scores)\n  * [4.5 Finding a good host](#4.5-Finding-a-good-host)\n  * [4.6 Availability over time](#4.6-Availability-over-time)\n  * [4.7 Average price by date](#4.7-Average-price-by-date)\n* [5. Text mining the Review comments](#5.-Text-mining-the-Review-comments)\n\n# 1. Loading libraries and data\n## 1.1 Loading libraries"
"## 1.2 The listings and listing details files\nThe dataset that I created contains a total of 7 files. The listings file is an overview file that insideairbnb labels as ""good for visualizations"". The unique identifier in the dataset is the ""listing"" id. This is basically the id of the advertisement. Overall, there were 20,030 Airbnb-listings in Amsterdam on December 6th, 2018."
"We see that neighbourhood_group is useless in Amsterdam, as it contains zero non-null objects. Below, I getting rid of this variable, and am showing the head of the dataframe that I am going to work with."
"# 2. Data exploration\n## 2.1 Neighbourhoods\nNeighbourhood ""De Baarsjes"" holds most listings, and altogether eight neigbourhoods have over one thousand listings.\n\n"
"# 2. Data exploration\n## 2.1 Neighbourhoods\nNeighbourhood ""De Baarsjes"" holds most listings, and altogether eight neigbourhoods have over one thousand listings.\n\n"
"Below, you can see that most listings are in the city centre. This map is interactive, and you can zoom-in on the clusters to eventually find the individual locations of the listings.\n\nNote: In a later version I made a map on the average daily price per neighoudhood (section 4.1). I think this map would also be better at this point (as it uses the neighbourhood shapefile), but as my main goal of this kernel is to learn as much as possible I left the map below unchanged as FastMarkerCluster seems useful for the future."
"Below, you can see that most listings are in the city centre. This map is interactive, and you can zoom-in on the clusters to eventually find the individual locations of the listings.\n\nNote: In a later version I made a map on the average daily price per neighoudhood (section 4.1). I think this map would also be better at this point (as it uses the neighbourhood shapefile), but as my main goal of this kernel is to learn as much as possible I left the map below unchanged as FastMarkerCluster seems useful for the future."
"## 2.2 Room types and property types\n### 2.2.1 Room types\nThe room type is very important in Amsterdam, because Amsterdam has a rule that Entire homes/apartments can only be rented out via Airbnb for a maximum of 60 days a year. Below, we can see that this restriction applies to most of the listings."
"## 2.2 Room types and property types\n### 2.2.1 Room types\nThe room type is very important in Amsterdam, because Amsterdam has a rule that Entire homes/apartments can only be rented out via Airbnb for a maximum of 60 days a year. Below, we can see that this restriction applies to most of the listings."
"### 2.2.2 Property types\nIn the dataset, we find a lot of different property types."
"However, many of those property types have very few listings in Amsterdam. In the figure below, I am only displaying property types with at least 100 listings. As we can see, the vast majority of the properties in Amsterdam are apartments."
"## 2.3 Accommodates (number of people)\nAs expected, most listings are for 2 people. In addition, Airbnb uses a maximum of 16 guests per listing."
"## 2.3 Accommodates (number of people)\nAs expected, most listings are for 2 people. In addition, Airbnb uses a maximum of 16 guests per listing."
"However, Amsterdam has an additional restriction. Due to fire hazard considerations and also taking possible noisy group into account, owners are only allowed to rent their property to groups with a maximum of 4 people. This actually means that the listings that indicate that the maximum number of people is above 4 are breaking this rule!"
"# 4. Advice to the tourists\n## 4.1 Average daily price per neighbourhood\nIn order to compare ""apples to apples"" I have only selected the most common type of accommodation, which is accommodation for 2 persons. As expected, accommodation in the city centre is the most expensive. "
"Below you can find the neighbourhoods on a map. This map is **interactive**. Hovering over the polygons shows the name of the neighbourhood, and the average price for 2-persons accommodations."
"Below you can find the neighbourhoods on a map. This map is **interactive**. Hovering over the polygons shows the name of the neighbourhood, and the average price for 2-persons accommodations."
"## 4.2 Neighbourhood safety\nAs there is no data on neighbourhood safety in the Airbnb files, I searched for this online and came across the map below. As you can see, nowadays the western parts of the city are the most dangerous.\n\nPersonal note: Apparently ""de Bijlmer"" (south-eastern areas) is reasonably safe these days. However, I used to know somebody who lived there as a student years ago. At that time it certainly was not a good neighbourhood, and I was warned to ""stay under the balconies"" as people might throw rubbish and old furniture from their balconies. Getting an old couch on your head did not sound like the most pleasant thing in the world ;-).\n\n"
"## 4.3 Review scores location, and location scores versus price\nIn tis section, I am grouping the review scores for the location by neighbourhood (only listings with at least 10 reviews). Although I expect the distance to the city centre to an important factor, these score should also take other things into account. Other factors may include:\n\n* The safety of a location (as displayed in the previous section)\n* Noise. If a listing is centrally located, but surrounded by noisy bars, that should cost points in the location review score.\n* If a listing is located outside the city centre but well connected by public transportation, it should get bonus points for that.\n* Facilities near the listing. Are there any supermarkets, bars and restaurants nearby?\n* Some people may be looking for free parking if they come by car (parking is very expensive in Amsterdam in general).\n\nBelow we see that the central neighbourhoods, which were generally also the most expensive, generally also score higher on location review score.  If I would calculate the distance to the city centre for each listing, I expect to see pretty strong correlations between this distance with both price and location review score.\n\nWhen looking at the average review score, I am surprised to see that the average is above 8/10 for all neighbourhoods! I know that Amsterdam is a small city (much smaller than many people might think!). Therefore, it does not take much time to get to the city centre from anywhere, which might explain the high averages to a certain extend. My personal advice to tourists would be to consider more affordable accommodation outside the city centre, in a safe neighbourhood, and with good public transportation connections to the city centre anyway. However, are the differences between the best locations and outside neighbourhoods really that small? Let's find out in the next section!\n"
"## 4.4 How to use review scores\nIn addition to written reviews, guests can submit an overall star rating and a set of category star ratings. Guests can give ratings on:\n\n* Overall Experience. What was your overall experience?\n* Cleanliness. Did you feel that your space was clean and tidy?\n* Accuracy. How accurately did your listing page represent your space?\n* Value. Did you feel your listing provided good value for the price?\n* Communication. How well did you communicate with your host before and during their stay?\n* Arrival. How smoothly did their check-in go?\n* Location. How did you feel about the neighborhood?\n\nBelow you can see the scores distribution of all those categories. What caught my eye immediately is that scores seem really high across the board! A quick internet search told me that this seems common across Airbnb. It is explained well in this article: [Higher than the average rating? 95% of Airbnb listings rated 4.5 to 5 stars](https://mashable.com/2015/02/25/airbnb-reviews-above-average/?europe=true#1YLfzOC34sqd).\n\nAfter having seen the scores distributions, I would personally consider any score of 8 or lower to be not a good score. If I wanted to use any of these scores in a search for accomodation, I believe the ""Value"" seems most useful. First of all, I always like to get good value for money ;-). However, the number of ""10 averages"" is reasonably small, which makes the indicator a bit more ""distinguishable"" than other indicators."
"## 4.4 How to use review scores\nIn addition to written reviews, guests can submit an overall star rating and a set of category star ratings. Guests can give ratings on:\n\n* Overall Experience. What was your overall experience?\n* Cleanliness. Did you feel that your space was clean and tidy?\n* Accuracy. How accurately did your listing page represent your space?\n* Value. Did you feel your listing provided good value for the price?\n* Communication. How well did you communicate with your host before and during their stay?\n* Arrival. How smoothly did their check-in go?\n* Location. How did you feel about the neighborhood?\n\nBelow you can see the scores distribution of all those categories. What caught my eye immediately is that scores seem really high across the board! A quick internet search told me that this seems common across Airbnb. It is explained well in this article: [Higher than the average rating? 95% of Airbnb listings rated 4.5 to 5 stars](https://mashable.com/2015/02/25/airbnb-reviews-above-average/?europe=true#1YLfzOC34sqd).\n\nAfter having seen the scores distributions, I would personally consider any score of 8 or lower to be not a good score. If I wanted to use any of these scores in a search for accomodation, I believe the ""Value"" seems most useful. First of all, I always like to get good value for money ;-). However, the number of ""10 averages"" is reasonably small, which makes the indicator a bit more ""distinguishable"" than other indicators."
"## 4.5 Finding a good host\nAt Airbnb you can get the status ""Superhost"". From Airbnb:\n* As a Superhost, you‚Äôll have more visibility, earning potential, and exclusive rewards. It's our way of saying thank you for your outstanding hospitality.\n* How to become a Superhost: Every 3 months, we check if you meet the following criteria. If you do, you'll earn or keep your Superhost status.\n    * Superhosts have a 4.8 or higher average overall rating based on reviews from at least 50% of their Airbnb guests in the past year. \n    * Superhosts have hosted at least 10 stays in the past year or, if they host longer-term reservations, 100 nights over at least 3 stays. \n    * Superhosts have no cancellations in the past year, unless there were extenuating circumstances.\n    * Superhosts respond to 90% of new messages within 24 hours.\n\nBelow, we can see that only a small portion of the listings in Amsterdam do have a host who is Superhost."
"## 4.5 Finding a good host\nAt Airbnb you can get the status ""Superhost"". From Airbnb:\n* As a Superhost, you‚Äôll have more visibility, earning potential, and exclusive rewards. It's our way of saying thank you for your outstanding hospitality.\n* How to become a Superhost: Every 3 months, we check if you meet the following criteria. If you do, you'll earn or keep your Superhost status.\n    * Superhosts have a 4.8 or higher average overall rating based on reviews from at least 50% of their Airbnb guests in the past year. \n    * Superhosts have hosted at least 10 stays in the past year or, if they host longer-term reservations, 100 nights over at least 3 stays. \n    * Superhosts have no cancellations in the past year, unless there were extenuating circumstances.\n    * Superhosts respond to 90% of new messages within 24 hours.\n\nBelow, we can see that only a small portion of the listings in Amsterdam do have a host who is Superhost."
"If I were to book accomomodation, I would not necessarily look for a superhost. Actually, I would be afraid that I would pay too much as superhost will likely increase their prices. However, I would also not want to host that responds badly, or cancels a lot.\n\nAs we can see, over 5,000 of the 20,000 listings have at least 10 reviews and respond to at least 90% of the new messages. I would consider those hosts ""proven"" good responders (which does not mean that a listing with less than 10 reviews cannot have good responding hosts; it is just not proven yet). Also, there are very few listings with hosts not replying to new messages within 24 hours."
"If I were to book accomomodation, I would not necessarily look for a superhost. Actually, I would be afraid that I would pay too much as superhost will likely increase their prices. However, I would also not want to host that responds badly, or cancels a lot.\n\nAs we can see, over 5,000 of the 20,000 listings have at least 10 reviews and respond to at least 90% of the new messages. I would consider those hosts ""proven"" good responders (which does not mean that a listing with less than 10 reviews cannot have good responding hosts; it is just not proven yet). Also, there are very few listings with hosts not replying to new messages within 24 hours."
"## 4.6 Availability over time\nThe calendar file holds 365 records for each listing, which means that for each listing the price and availablity by date is specified 365 days ahead."
"## Basic Exploration for five securities\n\nLooking at Close values of some of the stocks. It is observed that sometimes all the stocks fall together but quantum of downside varies. But it is evident that with time, all stocks behave differently."
"## Advance to Decline Ratio\n\nAdvance and Decline ratio is defined as the number of periods a stock has advanced over number of periods it has declined. This is being examined to identify if there is a trend here. For example, if there is an indication that in a particular month, stock tend to generally decline or advance.\n\n**Observation** : There does not appear to be an evidence if Stock advancement is tied to a month."
"## Advance to Decline Ratio\n\nAdvance and Decline ratio is defined as the number of periods a stock has advanced over number of periods it has declined. This is being examined to identify if there is a trend here. For example, if there is an indication that in a particular month, stock tend to generally decline or advance.\n\n**Observation** : There does not appear to be an evidence if Stock advancement is tied to a month."
"## Exploring and finding features\n\n- The next step is to explore features which could be helpful in predictive modelling. Here, we are looking at two variables, Volume and RSI. While the Volume is provided, RSI is a metric which has been found useful while studying movement of stock prices and is calculated.\n- Here, we see that stock could move either way (up or down) when volumes are high, this indicates heavy buying and heavy selling. This indicates that with high volumes, movement of stock is imminent, but the direction is not known.\n- On the other hand, an increasing RSI represents bullishness in a stock, while a decreasing RSI appears to push the prices down."
"## Closer look at RSI\n\nSince RSI appears to correlate better to Stock Prices, here is another way of analyzing this. The RSI values are categorized in Low, Medium and High category. Looking at the RSI categories, it appears that when RSI is in medium range, the price movement is most favourable. RSI in higher range is sometimes an indication of impending correction. \n\nThis looks promising and a strategy can be developed to remain invested in a stock when RSI values are in medium range. Or to be precise - **Do not buy a stock when RSI is in Low category.**\n\n**Note**: This phenomenon is quite pronounced for Security code 1377."
## RSI distribution\n\nA look at how RSI is distributed can also be helpful to understand this variable. This is heartening to see that RSI distribution is similar to the securities which are examined. This appears to be a variable which can be useful in model creation.
## RSI distribution\n\nA look at how RSI is distributed can also be helpful to understand this variable. This is heartening to see that RSI distribution is similar to the securities which are examined. This appears to be a variable which can be useful in model creation.
"## Exploring more of RSI\n\nSince RSI appears to be a good indicator, trying to see if this can help in predicting price increase. Here I am trying to see which RSI period helps in predicting the Target variable. This can be a feature for building models. This is done using calculating correlation between `Target` variable and RSI for various RSI periods between 5 to 20.\n\nThe results show that RSI period 5 has got best correlation followed by 19. This will be used as a feature for Machine Learning models.\n\n**Note: Below code is commented which generates this statistic. This is done to save processing time. Interestd ones can uncomment the code.**"
"## Return Analysis\n\nReturn Analysis gives the information that how much an unit investment is worth. This is an useful feature which tells the investor that how much her/his investment is worth today.\n\nThis helps in comparing the returns on a Normalized scale, since stock prices of various stocks vary, it is difficult to compare them. Calculating a return index gives a more holistic comparison of various securities."
"## Analyzing SMA (Simple Moving Averages) and EMA (Exponential Moving Averages)\n\nSMA and EMA are some well know pointers when it comes to Price Tracking and making decisions based on them. These methods help in identifying trends related to stock prices. While as the name suggests, SMA are jsut the average of a period where as EMA attach weights to the calculation and sensitive to recent price movements."
## Bollinger Bands\n\n- Bollinger Bands are another useful feature when designing strategies.\n- These are indicator of volatitility.\n- These can be used to draw a support curve and a resistance curve.\n- Share values remain within the curve most of the time (likely 95%).\n- This is done based on Moving Average (SMA) for 20 sessions.\n- We see that this behaviour is confirmed by the stocks chosen below. Often the hitting the support and resistance indicates that trend reversal is coming.
## Comparing daily returns of few securities (Volatility)\n\nIt is interesting to see how daily returns of various stocks fare with each other. Distributions of some stocks daily returns are plotted below. \n\n-  Max and Min range of daily returns are around -30 to +30%.\n-  One stock (1435) is more volatile than the others.\n- This is also seen that stocks who rise high also fall higher.
## Comparing daily returns of few securities (Volatility)\n\nIt is interesting to see how daily returns of various stocks fare with each other. Distributions of some stocks daily returns are plotted below. \n\n-  Max and Min range of daily returns are around -30 to +30%.\n-  One stock (1435) is more volatile than the others.\n- This is also seen that stocks who rise high also fall higher.
"## Examining VWAP\n\nVWAP is an important indicator used in trading setups. This is calculated by adding up the value traded for every transaction (price multiplied by the number of shares traded) and then dividing by the total shares traded.\n\nWe are looking to examine the impact of VWAP on Stock Price movement. Since this is being done to evaluate the various indicators as features, here I am looking to see the impact of previous day's VWAP on current day's target. The results are encouraging. "
"## Examining VWAP\n\nVWAP is an important indicator used in trading setups. This is calculated by adding up the value traded for every transaction (price multiplied by the number of shares traded) and then dividing by the total shares traded.\n\nWe are looking to examine the impact of VWAP on Stock Price movement. Since this is being done to evaluate the various indicators as features, here I am looking to see the impact of previous day's VWAP on current day's target. The results are encouraging. "
"This concludes basic analysis of Volume ,RSI and Moving Averages on stock prices. Will be examining more features in due course of time. Stay tuned!"
\nLibraries And Utilities
\nData Loading
\nUnivariate analysis of continuous variables
\nUnivariate analysis of categorical variables
\nPlot a general overview of data
\nTransform numerical variables into categorical variables
\nRelationship between two continuous variables
\nRelationship between two categorical variables and one continuous variable
\nHow to display correlation between variables
\nNon usual plots
\n\n3D plots
\nGeographical plots
\nGeographical plots
\nReferences
\nReferences
[Back to table of content](#table)
"# **16. Visualize feature scores of the features** \n\n[Table of Contents](#0.1)\n\n\nNow, I will visualize the feature scores with matplotlib and seaborn."
"# **17. Build Random Forest model on selected features** \n\n[Table of Contents](#0.1)\n\n\nNow, I will drop the least important feature `doors` from the model, rebuild the model and check its effect on accuracy."
- **child_mort : Economically backward nations have a high infant mortality rate!**
"- **Haiti** has the highest children's deaths. **African countries** have significant positions in this statistic.\n- At the other extreme of **child_mort**, countries from **Asia and Europe** has some solid presence. "
"- **exports : It is a very important factor for building the nation's economy. Higher the exports of a nation, stronger the nation's economy and more is the wealth generated by the nation!**"
"- **exports** of a nation are usually goods and services created domestically but sold to other nations. Goods and services exported depends on factors like the geographical location, natural resources, population size & their preference towards specific skills, etc.\n- Despite **Singapore's** population size not being in the top 100, they have the highest number of **exports**. **Luxembourg** & **Malta** have probably followed the same route.\n- **Afghanistan** & **Nepal** are present in the lower end of **exports**. Geographical locations of these nations have a heavy influence. Countries with lower exports also have small geographical area."
- **health : Citizens of the developed nations have higher income and thus they don't have a problem on spending more on healthy lifestyle!**
- **US** stands at the top when it comes to spending on health with **17%** of the individual GDP contribution.\n- **6%** seems to be the mean values of the citizens spending on their **health**.\n- **Asian** countries dominate the lower end of **health** with less than **3%**. They are the most ignorant citizens when it comes to health.
- **imports : It is another attribute that determines the reliance of the nations on other nations for the goods and services!**
"- **imports** stats of a nation describe the self reliance of a nation to solve their problems irrespective of being handicapped on one of the essential resources.\n- **Singapore**, **Malta**, **Luxembourg** & **Seychelles** are present in the top 5 of **exports** as well as **imports**. This is just an indication that highlight the nation's strategies of probably capitalizing on their resources and creating solid **exports** that gets countered by the heavy **imports** on something else. This just balances the books!\n- **Brazil** has the lowest **imports** out of all the nations with **11%**. **Sudan** is the only African country present in this lower end list with **17%**."
"- **income : Income of the per person is a key indicator about country's economic state. Higher the income of the citizens, more capabale they are to deal with uncertain situations!**"
"- Citizens of the **Qatar** have the highest **income** out of all the countries with a difference of **30k** more than the 2nd placed countries. **Singapore** & **Luxembourg** are again present in the top 5 of another feature.\n- Lower end of the **income** is dominated by the **African** nations. This is influenced by the damage done by colonization out of which the nations have not yet recovered. \n- The difference in the **income** of the nations in the top, middle and lower end is quite significant that will have an effect on every other features."
"- **inflation : It dictates the state of the economy, strength of the currency as well as demand for goods & services!**"
"- Higher **inflation** reduces the purchasing power of the citizens. Countries present at the top end of **inflation** have a devastating economic situation. Having such high inflation is a risk to the existence of the nation.\n- Similarly, the lower end of **inflation** has negative values i.e known as deflation. It signals a economy in a downward spiral leading to a recession or even a depression."
- **life_expec : Higher life expectancy displays citizens with health attributes physically as well as mentally!**
- **life_expec** depends alot on mental state as well as the lifestyle adopted by the citizens. **Singapore** is again present in the top of 5 of a feature.\n- None of the countries with a high **life_expec** are present in the top 5 of **health** that is related to the spending done by the citizen on health.\n- **African** countries are again present in the lower end for another feature.
- **total_fer : Economically backward countries have a high fertility rate!**
- **African** countries dominate the **total_fer** with values **6+**.\n- Mean **total_fer** value is **2** whereas lower end values of **1** concern abit as well.
- **gdpp : It is a feature that provides information about the contribution of a citizen to it's GDP!**
- It is a tricky feature as the population of the nation is a significant factor. One clear cut example of this is **China**. It has a huge population alongwith huge GDP.\n- **Luxembourg** is again present in the top ranks. **Switzerland** & **Qatar** are present in the top 5 similar to **income**.\n- Lower end is again dominated by **African** nations that labels them as the economically backward.
###  Numerical Features :
# Summary EDA
### Correlation Matrix :
"- Many features have relationships with each other.\n- **child_mort** clearly increases when **income**, **gdpp** & **exports** decreases. Rise in **inflation** also leads to high **child_mort** cases. Economic conditions unfortunately act as an important factor!\n- Rise in **exports** clearly increases **gdpp**, **income** & **imports**.\n- Spending on **health** has a small rise in **life_expec** and also decreases the **child_mort**.\n- **income** & **gdpp** display a very high 0.9 correlation value. From the health perspective, high **income** has lead to higher **life_expec** but decreases the **total_fer** by some significant margin.\n- As expected high **inflation** has a negative effect on the financial features. High **inflation** displays a high **total_fer** and **child_mort**. This describes the typical features of a backward nation.\n- According to the data, higher **life_expec** displays a low **total_fer**. Higher **gdpp** has lead more spending on **health**. \n\n**We can clearly see that some features are essentially from the same category and they have the same reaction to other features of different category.**\n- The 3 categories of the features are :\n    - **health** : **child_mort**, **health**, **life_expec**, **total_fer**\n    - **trade** : **imports**, **exports**\n    - **finance** : **income**, **inflation**, **gdpp**\n- Hence, we will dissolve these features into these categories and normalize them!"
- These are the variance values of the each feature present in the dataset.
"- This is a very effective method where we add up the variances of all the features in cummulative format.\n- Typically **eigen values with more than 95% of ratio of variance** are selected.\n- They correspond to the columns of the PCA generated dataframe.\n- In this case, we select the **Eigen Value : 2** as the steps generated have significant variances and thus the other features get dominated by their variances. "
#### Elbow Method & Silhouette Score Method :
"- From the results of the above 2 methods, we select :\n    - **k : Clusters = 3**"
"- From the results of the above 2 methods, we select :\n    - **k : Clusters = 3**"
"- Now we have got the clusters but we don't know which value corresponds to what! \n- Hence, we draw a boxplot of **income** & **child_mort** w.r.t labelled clusters!\n- We know that **low income and high child mortality** is a sign of an **economically backward nation**."
"- Now we have got the clusters but we don't know which value corresponds to what! \n- Hence, we draw a boxplot of **income** & **child_mort** w.r.t labelled clusters!\n- We know that **low income and high child mortality** is a sign of an **economically backward nation**."
- From the above plot we can conclude :\n    - **0 : No Help Needed**\n    - **1 : Help Needed**\n    - **2 : Might Need Help**
- From the above plot we can conclude :\n    - **0 : No Help Needed**\n    - **1 : Help Needed**\n    - **2 : Might Need Help**
### PCA Data 
#### Elbow Method & Silhouette Score Method :
"- From the results of the above 2 methods, we select :\n    - **k : Clusters = 3**        "
"- From the results of the above 2 methods, we select :\n    - **k : Clusters = 3**        "
- We again check the scatterplot of **income** & **child_mort** w.r.t labelled clusters for confirming the cluster values!\n- We know that **low income and high child mortality** is a sign of an **economically backward nation**.
- We again check the scatterplot of **income** & **child_mort** w.r.t labelled clusters for confirming the cluster values!\n- We know that **low income and high child mortality** is a sign of an **economically backward nation**.
- From the above plot we can conclude :\n    - **0 : Might Need Help**\n    - **1 : Help Needed**\n    - **2 : No Help Needed**
- From the above plot we can conclude :\n    - **0 : Might Need Help**\n    - **1 : Help Needed**\n    - **2 : No Help Needed**
"## Density Based Spatial Clustering of Application with Noise Clustering\n\n**DBSCAN Clustering** is a density based clustering algorithm that is used for unsupervised learning problems.\n- In a bid to eliminate the problems of **K-Means Clustering** with nested data and high-dimensional data, **DBSCAN Clustering** eliminates it! It's has 3 important terms & 2 important hyperparameters :\n    - Terms :\n        - **Core Point** : It is the center point that has **minPts** number of data points present in it's area and the points under it's area can extend the cluster.\n        - **Non-Core Point** : It is the center point that does not have **minPts** number of data points present in it's area and it cannot extend the cluster.\n        - **Outliers / Noise** : It is the data points that are not a part of any cluster.\n    - Hyperparameters :\n        - **minPts** : It is the minimum number of data points that need to be present in the area of a point to be considered as a core point.\n        - **Epsilon** : It is the radius of the area of a center point.\n- **DBSCAN Clustering** creates the clusters in the following way :        \n    - Select a random point and consider it as a center point from the data. \n    - Check the other data points that present in the area of this center point with the **Epsilon** value as it's radius.\n    - If this area has **minPts** number of data points in it, then the center point is considered as a **core point**. \n    - The data points in this area are then considered as **core point** that further extends by the same rules creating a cluster. \n    - If any of the point does not satisfy the rules, it is considered as **non-core point** that remains the part of the cluster but it cannot extend the cluster further.\n    - When all the **core points** are done with engulfing the other data points, **non-core points** are included & the cluster is completed.\n    - The next cluster then starts forming by the same rules. Some of the points don't be a part of any of the clusters, such points are known as **outliers / noise**.\n- As you might have noticed, the hyperparameters play a crucical role in this process. Thus, they have certain rules for assigning them values :\n    - **minPts** : If D represents the number of dimensions / features of a dataset, then **minPts** >= D + 1. Typically **minPts** >= 2 * D is selected for smaller or noisy datasets. \n    - **Epsilon** : It's value is usually decided using the **k-distance graph** that is determined from the **KNN model**. The value at which the graph changes sharply is selected."
### Feature Combination : Health - Trade - Finance 
"- We select **minPts** = 8 i.e >= 2 * 3 features \n- The value assigned to the **n_neighbors** : **minPts** - 1.\n- From the graph above, we select :\n    - **eps** : 0.08"
"- Now we have got the clusters but we don't know which value corresponds to what! \n- Hence, we draw a boxplots of **income** & **child_mort** w.r.t labelled clusters!\n- We know that **low income and high child mortality** is a sign of an **economically backward nation**."
- By rule **-1** is associated with **Noise / Outliers**!\n- From the above plot we can conclude :\n    - **-1 : Noise / Outliers**\n    - **0 : Might Need Help**\n    - **1 : No Help Needed**\n    - **2 : Help Needed**
- By rule **-1** is associated with **Noise / Outliers**!\n- From the above plot we can conclude :\n    - **-1 : Noise / Outliers**\n    - **0 : Might Need Help**\n    - **1 : No Help Needed**\n    - **2 : Help Needed**
### PCA Data 
### PCA Data 
"- We select **minPts** = 8 i.e >= 2 * 3 features \n- The value assigned to the **n_neighbors** : **minPts** - 1.\n- From the graph above, we select :\n    - **eps** : 0.08"
- We again draw boxplots of **income** & **child_mort** w.r.t labelled clusters for identifying the assistance required by the nations!\n- We know that **low income and high child mortality** is a sign of an **economically backward nation**.
- From the above plot we can conclude :\n    - **-1 : Noise / Outliers**\n    - **0 : Help Needed**\n    - **1 : Might Need Help**\n    - **2 : No Help Needed**
- From the above plot we can conclude :\n    - **-1 : Noise / Outliers**\n    - **0 : Help Needed**\n    - **1 : Might Need Help**\n    - **2 : No Help Needed**
"## Hierarchical Clustering\n\n**Hierarchical Clustering** is a distanced based algorithm that is used for unsupervised learning problems. \n    \n- It develops the hierarchy of clusters in the form of a tree i.e known as the **dendrogram**. For this problem we are going to use **Agglomerative Clustering** which is a bottom-up approach that considers all the points as clusters and then merges them together based on their distances in the following ways :\n    - Initally all the points are considered as clusters.\n    - Then, clusters that are closer together they start merging as new cluster.\n    - This combined cluster then further gets compared with other clusters and the closest cluster gets merged.\n    - This process continues till a single large cluster is formed.\n- This process of forming clusters can then be viewed with a **dendrogram**. From it, we select the number of clusters by identifying the number of merges present at the penultimate stage. More is the length of the vertical lines, higher the distance between the clusters.\n- To select the number of clusters, we set a threshold value and count the number of vertical lines present above it. This number of vertical lines forms the number of clusters. Other methods like **Silhouette Score Method** and **Elbow method** can also be used.\n- For **Agglomerative Clustering**, it has 2 important hyperparameters :\n    - **linkage** : There are various connecting or linking methods for cluster i.e Single, Centroid, Average , etc.\n    - **affinity** : It is the distance formula that compares the distance before merging the clusters. "
"## Hierarchical Clustering\n\n**Hierarchical Clustering** is a distanced based algorithm that is used for unsupervised learning problems. \n    \n- It develops the hierarchy of clusters in the form of a tree i.e known as the **dendrogram**. For this problem we are going to use **Agglomerative Clustering** which is a bottom-up approach that considers all the points as clusters and then merges them together based on their distances in the following ways :\n    - Initally all the points are considered as clusters.\n    - Then, clusters that are closer together they start merging as new cluster.\n    - This combined cluster then further gets compared with other clusters and the closest cluster gets merged.\n    - This process continues till a single large cluster is formed.\n- This process of forming clusters can then be viewed with a **dendrogram**. From it, we select the number of clusters by identifying the number of merges present at the penultimate stage. More is the length of the vertical lines, higher the distance between the clusters.\n- To select the number of clusters, we set a threshold value and count the number of vertical lines present above it. This number of vertical lines forms the number of clusters. Other methods like **Silhouette Score Method** and **Elbow method** can also be used.\n- For **Agglomerative Clustering**, it has 2 important hyperparameters :\n    - **linkage** : There are various connecting or linking methods for cluster i.e Single, Centroid, Average , etc.\n    - **affinity** : It is the distance formula that compares the distance before merging the clusters. "
### Feature Combination : Health - Trade - Finance 
### Feature Combination : Health - Trade - Finance 
"- In this case, we need to divide the countries into 3 categories. That is why we will select a 3 clusters directly. Dendrogram analysis for this dataset is kind of redundant. \n- Here, we can see that 1 **blue line** alongwith 2 **red lines** are the penultimate clusters that before connecting together.\n- It has 3 branches, thus indicating the **3 clusters** that it creates before merging into 1!"
"- Now we have got the clusters but we don't know which value corresponds to what! \n- Hence, we draw a boxplots of **income** & **child_mort** w.r.t labelled clusters!\n- We know that **low income and high child mortality** is a sign of an **economically backward nation**."
- From the above plot we can conclude :\n    - **0 : No Help Needed**\n    - **1 : Help Needed**\n    - **2 : Might Need Help**
- From the above plot we can conclude :\n    - **0 : No Help Needed**\n    - **1 : Help Needed**\n    - **2 : Might Need Help**
### PCA Data
### PCA Data
"- In this case, we need to divide the countries into 3 categories. That is why we will select a 3 clusters directly. Dendrogram analysis for this dataset is kind of redundant.\n- Here, again, we can see that 1 **blue line** alongwith 2 **red lines** are the penultimate clusters formed before connecting together.\n- It has 3 branches, thus indicating the **3 clusters** that it creates before merging into 1!"
- We again draw boxplots of **income** & **child_mort** w.r.t labelled clusters!\n- We know that **low income and high child mortality** is a sign of an **economically backward nation**.
- From the above plot we can conclude :\n    - **0 : Help Needed**\n    - **1 : Might Need Help**\n    - **2 : No Help Needed**
- From the above plot we can conclude :\n    - **0 : Help Needed**\n    - **1 : Might Need Help**\n    - **2 : No Help Needed**
"# Conclusion\n\n- This is a great dataset that has been applied for a noble cause which highlights the scope of utilizing Data Science practices.\n\n\n- A model's performance is the reflection of the quality of the data feed to it. EDA section of this dataset provides a list of insights. Effect of normalization and standardization is massive on model performance.\n\n\n- Feature engineering is pivotal! For **feature combination & PCA data**, complexity of methods to achieve it is clearly visible. These processes highlight the various options open and thus requires us to be sensible in choosing the methods depending on the data and the problem statement.\n\n\n- **K-Means Clustering**, **Hierarchical Clustering** and **DBSCAN Clustering** are fundamentally different. They are based on different principles, thus displaying the difference in the model performances. Overall the model performances are not as great.    "
"# Conclusion\n\n- This is a great dataset that has been applied for a noble cause which highlights the scope of utilizing Data Science practices.\n\n\n- A model's performance is the reflection of the quality of the data feed to it. EDA section of this dataset provides a list of insights. Effect of normalization and standardization is massive on model performance.\n\n\n- Feature engineering is pivotal! For **feature combination & PCA data**, complexity of methods to achieve it is clearly visible. These processes highlight the various options open and thus requires us to be sensible in choosing the methods depending on the data and the problem statement.\n\n\n- **K-Means Clustering**, **Hierarchical Clustering** and **DBSCAN Clustering** are fundamentally different. They are based on different principles, thus displaying the difference in the model performances. Overall the model performances are not as great.    "
# Please upvote if you like the work!Any sort of feedback is appreciated!Thank You!
## Helper Functions\n
"# Non-Parametric\n\nCounterpart to Parametric models, Parametric models does not make any assumptions about the data generating process‚Äô distribution. For example, in statistical test, Non-Parametric models utilize rank and medians, instead of the mean and variance! On the other hand, they function in a infinite space of parameters, making their name counter-intuitive, but also highlighting their practical approach to representation; enabling them to increase their flexibility indefinitely."
"# Logistic Generalized Additive Model [GAM]\n\nPrediction for this model reqiuires the data range to match, so I am unable to apply to the submission set.\n\nAdditional Resources:\n- [GAM: The Predictive Modeling Silver Bullet](https://multithreaded.stitchfix.com/blog/2015/07/30/gam/)\n- [pyGAM : Getting Started with Generalized Additive Models in Python](https://codeburst.io/pygam-getting-started-with-generalized-additive-models-in-python-457df5b4705f)\n- [pyGAM Github](https://github.com/dswah/pyGAM)"
"These plots showcase the marginal effect of each feature towards the depedent variable in the model. Let me interpret some to clarify:\n- **Sex:** Class 1 (Female) contributes towards the survial rate, while Class 0 (Male) has a negative effect.\n- **Age:** This variable is rescaled, but we can still see an increase rate of survival for the lower and upper age levels.\n- **Fare:** The more people payed, the more likely they are to survive.\n\n# Feedforward Neural Networks\n\nThe only ‚ÄúDeep Model‚Äù out of the mix. This is a characteristic of *Representation Learning*, which effectively grants the model its own feature processing and selection steps, catering to large, complex data with intertwining effects. In computer vision tasks, the convolutional neural networks is able to piece apart corners, colors, patterns and more. Recent research in Style Transfer even suggests that stylistic properties of a picture, such as art, can be extracted, and controlled.  \n**Source:** https://arxiv.org/abs/1611.07865\n\nA common explanation for Neural Networks is that it is a whole bunch of Logistic Regressions. An important thing to remembers is that the hidden-layers are fully connected, meaning that each input variables has a weight to each node (hidden-unit) in the hidden layer, thereby resulting in a black box with a whole lot of parameters, and a whole lot of matrix multiplications. Finally it's, ironic that one of the most intelligible classifiers can be transformed into the least intelligible! In *Computer Age Statistical Inference* authors Bradley Efron and Trevor Hastie are hopeful for the next Ronald Fisher to come and provide statistical intelligibility to modern day machine learning models, many of which are highly developed computationally, but lacking inferential theory.\n\nModel not ideal for such as small dataset."
**Probability Predictions:**
"## Introduction to Receiver Operating Characteristic curve [ROC]\nROC curve, is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. Well said wikipedia. Closer the line is to the top left, the better its predictive ability. Non-Smooth curve suggest important thresholds, effective a cluster of probabilities near a swing point.\n\n\n# Soft and Hard Voting Ensembles of Difference Sizes"
"## Introduction to Receiver Operating Characteristic curve [ROC]\nROC curve, is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. Well said wikipedia. Closer the line is to the top left, the better its predictive ability. Non-Smooth curve suggest important thresholds, effective a cluster of probabilities near a swing point.\n\n\n# Soft and Hard Voting Ensembles of Difference Sizes"
"## Sklearn Voter Pipeline\n\nTalking about pipelines, Sklearn's ensemble voting is structured as one!"
"### Dear all,\nThe objective of this notebook is to shed some light on the question of *How good is my score in the Titanic competition?*\nWe all know the feeling of clicking on the '*Jump to your position on the leaderboard*', and we get a score of 0.7xxx (well, as least I do) then the next thing we do is scroll up and see all these 1.00000 results and we feel that \nwe are doing something wrong. \n\nThe first thing to say is that these *1.00000* submissions are cheating! Basically it works because the *hidden* data against which your submission is tested is actually [*public*](https://en.wikipedia.org/wiki/Passengers_of_the_RMS_Titanic#Passenger_list) data, which can be found on Wikipedia, etc. (it is the *Titanic* after all). In other words, if you hand-craft the submission file you will score a 1, with no machine learning required; hardly any need for python or R code, no need for trying new methods and learning new techniques, etc. etc.\n\nWhat is the point in doing that? **I really don't know...**\n\nOk, so, how good is my score? First we shall load in a snapshot of the leaderboard. This data can be downloaded directly from [the Titanic leaderboard page ](https://www.kaggle.com/c/3136/publicleaderboarddata.zip) where it says Raw Data.\n"
"Firstly, we shall make a frequency plot of the *whole* leaderboard to get an overall feeling for the data:"
"Firstly, we shall make a frequency plot of the *whole* leaderboard to get an overall feeling for the data:"
"We can see that the majority of the scores lie between `0.6` and `0.85`, so we shall zoom in on that area:"
"We can see that the majority of the scores lie between `0.6` and `0.85`, so we shall zoom in on that area:"
"We can clearly see a distribution of scores, and in particular, three interesting peaks.\n### The 0.76555 peak:\nThis is by far the highest peak and corresponds to correctly classifying 320 results. This peak is due to people submitting the default `gender_submission.csv` file provided by the competition. The submission of this file alone represents more than 20% of the results that are seen on the leaderboard. This is not entirely surprising, given that the excellent [Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) by Alexis Cook suggests doing this as an exercise."
"# Table of content\n\n1. Introduction - Loading libraries and dataset\n2. Exploratory analysis, engineering and cleaning features - Bi-variate analysis\n3. Correlation analysis - Tri-variate analysis\n4. Predictive modelling, cross-validation, hyperparameters and ensembling\n5. Submitting results\n6. Credits\n\n### Check other Kaggle notebooks from [Yvon Dalat](https://www.kaggle.com/ydalat):\n* [Titanic, a step-by-step intro to Machine Learning](https://www.kaggle.com/ydalat/titanic-a-step-by-step-intro-to-machine-learning): **a practice run ar EDA and ML-classification**\n* [HappyDB, a step-by-step application of Natural Language Processing](https://www.kaggle.com/ydalat/happydb-what-100-000-happy-moments-are-telling-us): **find out what 100,000 happy moments are telling us**\n* [Work-Life Balance survey, an Exploratory Data Analysis of lifestyle best practices](https://www.kaggle.com/ydalat/work-life-balance-best-practices-eda): **key insights into the factors affecting our work-life balance**\n*  [Work-Life Balance survey, a Machine-Learning analysis of best practices to rebalance our lives](https://www.kaggle.com/ydalat/work-life-balance-predictors-and-clustering): **discover the strongest predictors of work-life balance**\n\n**Interested in more facts and data to balance your life, check the [360 Living guide](https://amzn.to/2MFO6Iy) ![360 Living: Practical guidance for a balanced life](https://images-na.ssl-images-amazon.com/images/I/61EhntLIyBL.jpg)**\n\n**Note:** Ever feel burnt out? Missing a deeper meaning? Sometimes life gets off-balance, but with the right steps, we can find the personal path to authentic happiness and balance.\n[Check out how Machine Learning and statistical analysis](https://www.amazon.com/dp/B07BNRRP7J?ref_=cm_sw_r_kb_dp_TZzTAbQND85EE&tag=kpembed-20&linkCode=kpe) sift through 10,000 responses to help us define our unique path to better living.\n\n# 1. Introduction - Loading libraries and dataset\nI created this Python notebook as the learning notes of my first Machine Learning project.\nSo many new terms, new functions, new approaches, but the subject really interested me; so I dived into it, studied one line of code at a time, and captured the references and explanations in this notebook.\n\nThe algorithm itself is a fork from **Anisotropic's Introduction to Ensembling/Stacking in Python**, a great notebook in itself.\nHis notebook was itself based on **Faron's ""Stacking Starter""**, as well as **Sina's Best Working Classfier**. \nI also used multiple functions from **Yassine Ghouzam**.\nI added many variations and additional features to improve the code (as much as I could) as well as additional visualization.\n\nSome key take away from my personal experiments and what-if analysis over the last couple of weeks:\n\n* **The engineering of the right features is absolutely key**. The goal there is to create the right categories between survived and not survived. They do not have to be the same size or equally distributed. What helped best is to group together passengers with the same survival rates.\n\n* ** I tried many, many different algorightms. Many overfit the training data** (up to 90%) but do not generate more accurate predictions with the test data. What worked better is to use the cross-validation on selected algotirhms. It is OK to select algorithms with various results as there is strenght in diversity. \n\n* **Lastly, the right ensembling was best achieved** with a votingclassifier with soft voting parameter\n\nOne last word: please use this kernel as a first project to practice your ML/Python skills. I shameless ley sotle and learnt from many Kagglers through my learning process, please do the same with the code in this kernel.\n\nI also welcome your comments, questions and feedback.\n\nYvon\n\n## 1.1. Importing Library"
## 1.2. Loading dataset
## 1.4. A very first look into the data
"This is only a quick of the relationships between features before we start a more detailed analysis.\n\n\n# 2. Exploratory Data Analysis (EDA), Cleaning and Engineering features\n\nWe will start with a standard approach of any kernel: correct, complete, engineer the right features for analysis.\n\n## 2.1. Correcting and completing features\n### Detecting and correcting outliers\nReviewing the data, there does not appear to be any aberrant or non-acceptable data inputs.\n\nThere are potential outliers that we will identify (steps from Yassine Ghouzam):\n* It creates firset a function called detect_outliers, implementing the Tukey method\n* For each column of the dataframe, this function calculates the 25th percentile (Q1) and 75th percentile (Q3) values.\n* The  interquartile range (IQR) is a measure of statistical dispersion, being equal to the difference between the 75th and 25th percentiles, or between upper and lower quartiles.\n* Any data points outside 1.5 time the IQR (1.5 time IQR below Q1, or 1.5 time IQR above Q3), is considered an outlier.\n* The outlier_list_col column captures the indices of these outliers. All outlier data get then pulled into the outlier_indices dataframe.\n* Finally, the detect_outliers function will select only the outliers happening multiple times. This is the datadframe that will be returned."
## 2.3 Feature Engineering - Bi-variate statistical analysis\n\nOne of the first tasks in Data Analytics is to **convert the variables into numerical/ordinal values**.\nThere are multiple types of data\n\n**a) Qualitative data: discrete**\n* Nominal: no natural order between categories. In this case: Name\n* Categorical: Sex\n\n**b) Numeric or quantitative data**\n* Discrete: could be ordinal like Pclass or not like Survived.\n* Continuous. e.g.: age\nMany feature engineering steps were taken from Anisotropic's excellent kernel.\n\n### Pclass
Embarked does not seem to have a clear impact on the survival rate. We will analyse it further in the next sections and drop it if we cannot demonstrate a proven relationship to Survived. \n\n### Name_length
Embarked does not seem to have a clear impact on the survival rate. We will analyse it further in the next sections and drop it if we cannot demonstrate a proven relationship to Survived. \n\n### Name_length
"The first graph shows the amount of people by Name_length.\n\nThe second one, their average survival rates.\n\nThe proposed categories are: less than 23 (mostly men), 24 to 28, 29 to 40, 41 and more (mostly women).\nThe categories are sized to group passengers with similar Survival rates."
### Age
The best categories for age are:\n* 0:  Less than 14\n* 1:  14 to 30\n* 2:  30 to 40\n* 3:  40 to 50\n* 4:  50 to 60\n* 5:  60 and more
### Family: SibSp and Parch\n\nThis section creates a new feature called FamilySize consisting of SibSp and Parch.
"IsAlone does not result in a significant difference of survival rate. In addition, the slight difference between men and women go in different direction, i.e. IsAlone alone is not a good predictor of survival. O will drop this feature.\n\n### Fare"
"**Observations**\n* The Fare distribution is very skewed to the left. This can lead to overweigthing the model with very high values.\n* In this case, it is better to transform it with the log function to reduce the skewness and redistribute the data."
**Observations**\nLog Fare categories are:\n* 0 to 2.7: less survivors\n* More than 2.7 more survivors
### Titles
"There are 4 types of titles:\n0. Mme, Ms, Lady, Sir, Mlle, Countess: 100%. \n1. Mrs, Miss: around 70% survival\n2. Master: around 60%\n3. Don, Rev, Capt, Jonkheer: no data\n4. Dr, Major, Col: around 40%\n5. Mr: below 20%"
## 3.1. Correlation analysis with histograms and pivot-tables
"**Observations for Age graph:**\n* 0 or blue represent women; 1 or orange represent men. Gender and age seem to have a stronger influece of the survival rate.\n* We start to find where most survivors are: older women (48 to 64 year old), and younger passengers.\n* What is statistically interesting is that only young boys (Age Category = 0) have  high survival rates, unlike other age groups for men.\n* We will create a new feature called young boys"
"## 3.3. Pearson Correlation Heatmap\n\nThe Seaborn plotting package allows us to plot heatmaps showing the Pearson product-moment correlation coefficient (PPMCC) correlation between features.\nPearson is bivariate correlation, measuring the linear correlation between two features. "
"**Observations from the Pearson analysis:** \n* Correlation coefficients with magnitude between 0.5 and 0.7 indicate variables which can be considered **moderately correlated**.\n* We can see from the red cells that many features are ""moderately"" correlated: specifically, IsAlone, Pclass, Name_length, Fare, Sex.\n* This is influenced by the following two factors: 1) Women versus men (and the compounding effect of Name_length) and 2) Passengers paying a high price (Fare) have a higher chance of survival: there are also in first class, have a title. \n\n\n## 3.4. Pairplots\n\nFinally let us generate some pairplots to observe the distribution of data from one feature to the other.\nThe Seaborn pairplot class will help us visualize the distribution of a feature in relationship to each others."
"**Observations from the Pearson analysis:** \n* Correlation coefficients with magnitude between 0.5 and 0.7 indicate variables which can be considered **moderately correlated**.\n* We can see from the red cells that many features are ""moderately"" correlated: specifically, IsAlone, Pclass, Name_length, Fare, Sex.\n* This is influenced by the following two factors: 1) Women versus men (and the compounding effect of Name_length) and 2) Passengers paying a high price (Fare) have a higher chance of survival: there are also in first class, have a title. \n\n\n## 3.4. Pairplots\n\nFinally let us generate some pairplots to observe the distribution of data from one feature to the other.\nThe Seaborn pairplot class will help us visualize the distribution of a feature in relationship to each others."
"**Observations**\n* The pairplot graph all trivariate analysis into one figure.\n* The clustering of red dots indicates the combination of two features results in higher survival rates, or the opposite (clustering of blue dots = lower survival)\nFor example:\n- Smaller family sizes in first and second class\n- Middle age with Pclass in third category = only blue dot\nThis can be used to validate that we extracted the right features or help us define new ones."
"## 4.10. Model summary\nI found that the picture illustrates the various model better than words.\nThis should be taken with a grain of salt, as the intuition conveyed by these two-dimensional examples does not necessarily carry over to real datasets.\nThe reality os that the algorithms work with many dimensions (11 in our case).\n\nBut it shows how each classifier algorithm partitions the same data in different ways.\nThe three rows represent the three different data set on the right.\nThe plots show training points in solid colors and testing points semi-transparent. The lower right shows the classification accuracy on the test set.\n\nFor instance, the visualization helps understand how RandomForest uses multiple Decision Trees, the linear SVC, or Nearest Neighbors grouping sample by their relative distance to each others.\n\n![image](http://scikit-learn.org/0.15/_images/plot_classifier_comparison_0011.png)\n"
"**Observations**\n* The above models (classifiers) were applied to a split training and x_test datasets.\n* This results in some classifiers (Decision_tree and Random_Forest) over-fitting the model to the training data. \n* This happens when the classifiers use many input features (to include noise in each feature) on the complete dataset, and ends up ‚Äúmemorizing the noise‚Äù instead of finding the signal.\n* This overfit model will then make predictions based on that noise. It performs unusually well on its training data, but will not necessarilyimprove the prediction quality with new data from the test dataset.\n* In the next section, we will cross-validate the models using sample data against each others. We will this by using StratifiedKFold to train and test the models on sample data from the overall dataset.\nStratified K-Folds is a cross validation iterator. It provides train/test indices to split data in train test sets. This cross-validation object is a variation of KFold, which returns stratified folds. The folds are made by preserving the percentage of samples for each class."
"## 4.11. Model cross-validation with K-Fold\n\nThe fitting process applied above optimizes the model parameters to make the model fit the training data as well as possible.\nCross-validation is a way to predict the fit of a model to a hypothetical validation set when an explicit validation set is not available.\nIn simple words, it allows to test how well the model performs on new data.\nIn our case, cross-validation will also be applied to compare the performances of different predictive modeling procedures. \n![Cross-validation process:](https://image.slidesharecdn.com/kagglesharingmarkpeng20151216finalpresented-151216161621/95/general-tips-for-participating-kaggle-competitions-13-638.jpg?cb=1452565877)\n### Cross-validation scores"
"## 4.12 Hyperparameter tuning & learning curves for selected classifiers\n\n**1. Adaboost** is used in conjunction with many other types of learning algorithms to improve performance. The output of the other learning algorithms ('weak learners') is combined into a weighted sum that represents the final output of the boosted classifier. AdaBoost is adaptive in the sense that subsequent weak learners are tweaked in favor of those instances misclassified by previous classifiers. AdaBoost is sensitive to noisy data and outliers.\n\n**2. ExtraTrees** implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting.\n\n**3. RandomForest ** operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set.\n\n**4. GradientBoost ** produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function.\n\n**5. SVMC, or Support Vector Machines.**vGiven a set of training examples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier.\n\nAll descripotion adapted from Wikipedia."
"**Observations to fine-tune our models**\n\nFirst, let's compare their best score after fine-tuning their parameters:\n1. Adaboost: 80\n2. ExtraTrees: 83\n3. RandomForest: 82\n4. GradientBoost: 82\n5. SVC: 83\n\nIt appears that GBC and SVMC are doing the best job on the Train data. This is good because we want to keep the model as close to the training data as possible. But not too close!\nThe two major sources of error are bias and variance; as we reduce these two, then we could build more accurate models:\n\n* **Bias**: The less biased a method, the greater its ability to fit data well.\n* **Variance**: with a lower bias comes typically a higher the variance. And therefore the risk that the model will not adapt accurately to new test data.\nThis is the case here with Gradient Boost: high score but cross-validation is very distant.\n\nThe reverse also holds: the greater the bias, the lower the variance. A high-bias method builds simplistic models that generally don't fit well training data. \nWe can see the red and green curves from ExtraTrees, RandomForest and SVC are pretty close.\n**This points to a lower variance, i.e. a stronger ability to apply the model to new data.**\n\nI used the above graphs to optimize the parameters for Adaboost, ExtraTrees, RandomForest, GradientBoost and SVC.\nThis resulted in a significant improvement of the prediction accuracy on the test data (score).\n\nIn addition, I found out that AdaBoost does not do a good job with this dataset as the training score and cross-validation score are quite far apart. \n\n## 4.13 Selecting and combining the best classifiers\nSo, how do we achieve the best trade-off beween bias and variance?\n1. We will first compare in the next section the classifiers; results between themselves and applied to the same test data.\n2. Then ""ensemble"" them together with an automatic function callled *voting*."
"**Observations:**\n* As indicated before, Adaboost has the lowest correlations when compared to other predictors. This indicates that it predicts differently than the others when it comes to the test data.\n* We will therefore 'ensemble' the remaining four predictors.\n\n## 4.14 Ensembling\nThis is the final step, pulling it together with an amazing 'Voting' function from sklearn.\nAn ensemble is a supervised learning algorithm, that it can be trained and then used to make predictions.\nThe last line applied the ""ensemble predictor"" to the test data for submission."
## 4.15. Summary of most important features
"Nice graphics, but the obsevation is unclear in my opinion:\n* On one side, we hope as analyst that the models come out with similar patterns. An easy direction to follow.\n* At the same time, ""there have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores"". As we say in business, diversity brings better results, this seems to be true with algorithms as well!"
## What we want?\n1. Gathering Data\n2. Analysis the target and understand what is the important features\n3. Looking for missing values\n4. Feature Engineering\n5. Converting categorical to numerical\n6. Modeling
## 1. Gathering Data
"### Are we need a specialist or a broker to know what are the most important features that affect home prices?\nOf course not, we can know the important features by sea. So let's go and explore the data.\n"
"Ok, now as you see the correlation between features.. The colours show to us the strong and weak correlation.\nBut what we really need? we need the highest correlation between features and SalesPrice, so let's do it."
"Ok, now as you see the correlation between features.. The colours show to us the strong and weak correlation.\nBut what we really need? we need the highest correlation between features and SalesPrice, so let's do it."
"#### What we note?\n* It's important to know what you do and how benefit from it. We can see 'OverQual' in the top of highest correlation it's 0.79!\n* 'GarageCars' & 'GarageArea' like each other (correlation between them is 0.88) \n* 'TotalBsmtSF' & '1stFlrSF' also like each other (correlation betwwen them is 0.82), so we can keep either one of them or add the1stFlrSF to the Toltal.\n* 'TotRmsAbvGrd' & 'GrLivArea' also has a strong correlation (0.83), I decided to keep 'GrLivArea' because it's correlation with 'SalePrice' is higher.\n"
#### ok let's focus on the features have highest correlation.
"Now, We explored the data and know the important features."
"# Quora Question-pair classification\n\nThis competition is about modelling whether a pair of questions on Quora is asking the same question. For this problem we have about **400.000** training examples. Each row consists of two sentences and a binary label that indicates to us whether the two questions were the same or not.\n\nInspired by this nice [kernel](https://www.kaggle.com/arthurtok/d/mcdonalds/nutrition-facts/super-sized-we-macdonald-s-nutritional-metrics) from [Anisotropic](https://www.kaggle.com/arthurtok) I've added a few interactive 2D and 3D scatter plots.\nTo get an insight into how the duplicates evolve over the number of words in the questions, I've added a plotly animation that encodes number of words and word share similarity in a scatter plot.\n\n**We will be looking in detail at:**\n\n* question pair TF-IDF encodings\n* basic feature engineering and their embeddings in lower dimensional spaces\n* parallel coordinates visualization\n* model selection and evaluation + sample submission.\n\nIf you like this kernel, please upvote it :D, thanks!\n\n\n----------\n\n\nAdded a final section for cross-validated model selection and evaluation. We will look at standard binary classification metrics, like ROC and PR curves and their AUCs. The best (linear) model that we found then generates a submission."
So we have six columns in total one of which is the label.
We have a fairly balanced dataset here.
# Feature construction\n\nWe will now construct a basic set of features that we will later use to embed our samples with.\n\nThe first we will be looking at is rather standard TF-IDF encoding for each of the questions. In order to limit the computational complexity and storage requirements we will only encode the top terms across all documents with TF-IDF and also look at a subsample of the data.
# Feature construction\n\nWe will now construct a basic set of features that we will later use to embed our samples with.\n\nThe first we will be looking at is rather standard TF-IDF encoding for each of the questions. In order to limit the computational complexity and storage requirements we will only encode the top terms across all documents with TF-IDF and also look at a subsample of the data.
"The subsample still has a very similar label distribution, ok to continue like that, without taking a deeper look how to achieve better sampling than just taking the first rows of the dataset.\n\nCreate a dataframe where the top 50% of rows have only question 1 and the bottom 50% have only question 2, same ordering per halve as in the original dataframe."
## Feature EDA\n\nLet us now construct a few features\n\n* character length of questions 1 and 2\n* number of words in question 1 and 2\n* normalized word share count.\n\nWe can then have a look at how well each of these separate the two classes.
"The distributions for normalized word share have some overlap on the far right hand side, meaning there are quite a lot of questions with high word similarity but are both duplicates and non-duplicates."
"The distributions for normalized word share have some overlap on the far right hand side, meaning there are quite a lot of questions with high word similarity but are both duplicates and non-duplicates."
Scatter plot of question pair character lengths where color indicates duplicates and the size the word share coefficient we've calculated earlier.
Scatter plot of question pair character lengths where color indicates duplicates and the size the word share coefficient we've calculated earlier.
"# Animation over average number of words\n\nFor that we will calculate the average number of words in both questions for each row.\n\nIn the end we want to have a scatter plot, just like the one above, but giving us one more dimension, in that case the average number of words in both questions. That will allow us to see the dependence on that variable. We also expect that as the number of words is increased, the character lengths of Q1 and Q2 will increase."
"# Parallel Coordinates\n\nWe now want to get another perspective on high dimensional data, such as the TF-IDF encoded questions. For that purpose I'll encode the concatenated questions into a set of N dimensions, s.t. each row in the dataframe then has one N dimensional vector associated to it.\nWith this we can then have a look at how these coordinates (or TF-IDF dimensions) vary by label.\n\nThere are many EDA methods to visualize high dimensional data, I'll show parallel coordinates here.\n\nTo make a nice looking plot, I've chosen N to be quite small, much smaller actually than you would encode it in a machine learning algorithm."
In the parallel coordinates we can see that there are some dimensions that have high TF-IDF features values for duplicates and others high values for non-duplicates.
"# Question character length correlations by duplication label\n\nThe pairplot of character length of both questions by duplication label is showing us that, duplicated questions seem to have a somewhat similar amount of characters in them.\n\nAlso we can see something quite intuitive, that there is rather strong correlation in the number of words and the number of characters in a question."
"# Model starter\n\nTrain a model with the basic feature we've constructed so far.\n\nFor that we will use Logisitic regression, for which we will do a quick parameter search with CV, plot ROC and PR curve on the holdout set and finally generate a submission."
"### ROC\n\nReceiver operator characteristic, used very commonly to assess the quality of models for binary classification.\n\nWe will look at at three different classifiers here, a strongly regularized one and two with weaker regularization. The heavily regularized model has parameters very close to zero and is actually worse than if we would pick the labels for our holdout samples randomly."
"# Precision-Recall Curve\n\nAlso used very commonly, but more often in cases where we have class-imbalance. We can see here, that there are a few positive samples that we can identify quite reliably. On in the medium and high recall regions we see that there are also positives samples that are harder to separate from the negatives."
"# Precision-Recall Curve\n\nAlso used very commonly, but more often in cases where we have class-imbalance. We can see here, that there are a few positive samples that we can identify quite reliably. On in the medium and high recall regions we see that there are also positives samples that are harder to separate from the negatives."
# Prepare submission\n\nHere we read the test data and apply the same transformations that we've used for the training data. We also need to scale the computed features again.
"We have >21,000 images! Hopefully, we can develop a highly-predictive, robust, and generalizable model with this dataset. \n\nLet's check the distribution of the different classes:"
"In this case, we have 5 labels (4 diseases and healthy):\n0. Cassava Bacterial Blight (CBB)\n1. Cassava Brown Streak Disease (CBSD)\n2. Cassava Green Mottle (CGM)\n3. Cassava Mosaic Disease (CMD)\n4. Healthy\n\nIn this case label 3, [Cassava Mosaic Disease (CMD)](https://en.wikipedia.org/wiki/Cassava_mosaic_virus) is the most common label. This imbalance may have to be addressed with a weighted loss function or oversampling. I might try this in a future iteration of this kernel or in a new kernel.\n\nLet's check an example image to see what it looks like:"
## Import Libraries
"To start exploring your data, you‚Äôll need to start by actually loading in your data. You‚Äôll probably know this already, but thanks to the Pandas library, this becomes an easy task: you import the package as pd, following the convention, and you use the read_csv() function, to which you pass the URL in which the data can be found and a header argument. This last argument is one that you can use to make sure that your data is read in correctly: the first row of your data won‚Äôt be interpreted as the column names of your DataFrame.\n\nAlternatively, there are also other arguments that you can specify to ensure that your data is read in correctly: you can specify the delimiter to use with the sep or delimiter arguments, the column names to use with names or the column to use as the row labels for the resulting DataFrame with index_col."
"#### Dendrogram\n\nThe dendrogram allows you to more fully correlate variable completion, revealing trends deeper than the pairwise ones visible in the correlation heatmap:"
"The dendrogram uses a hierarchical clustering algorithm (courtesy of scipy) to bin variables against one another by their nullity correlation (measured in terms of binary distance). At each step of the tree the variables are split up based on which combination minimizes the distance of the remaining clusters. The more monotone the set of variables, the closer their total distance is to zero, and the closer their average distance (the y-axis) is to zero.\n\nTo interpret this graph, read it from a top-down perspective. Cluster leaves which linked together at a distance of zero fully predict one another's presence‚Äîone variable might always be empty when another is filled, or they might always both be filled or both empty, and so on. In this specific example the dendrogram glues together the variables which are required and therefore present in every record.\n\nCluster leaves which split close to zero, but not at it, predict one another very well, but still imperfectly. If your own interpretation of the dataset is that these columns actually are or ought to be match each other in nullity , then the height of the cluster leaf tells you, in absolute terms, how often the records are ""mismatched"" or incorrectly filed‚Äîthat is, how many values you would have to fill in or drop, if you are so inclined.\n\nAs with matrix, only up to 50 labeled columns will comfortably display in this configuration. However the dendrogram more elegantly handles extremely large datasets by simply flipping to a horizontal configuration."
### Correlation Heat Map
"The heatmap is the best way to get a quick overview of correlated features thanks to seaborn!\n\nAt initial glance it is observed that there are two red colored squares that get my attention. \n1. The first one refers to the 'TotalBsmtSF' and '1stFlrSF' variables.\n2. Second one refers to the 'GarageX' variables. \nBoth cases show how significant the correlation is between these variables. Actually, this correlation is so strong that it can indicate a situation of multicollinearity. If we think about these variables, we can conclude that they give almost the same information so multicollinearity really occurs. \n\nHeatmaps are great to detect this kind of multicollinearity situations and in problems related to feature selection like this project, it comes as an excellent exploratory tool.\n\nAnother aspect I observed here is the 'SalePrice' correlations.As it is observed that 'GrLivArea', 'TotalBsmtSF', and 'OverallQual' saying a big 'Hello !' to SalePrice, however we cannot exclude the fact that rest of the features have some level of correlation to the SalePrice. To observe this correlation closer let us see it in Zoomed Heat Map "
#### SalePrice Correlation matrix
From above zoomed heatmap it is observed that GarageCars & GarageArea are closely correlated .\nSimilarly TotalBsmtSF and 1stFlrSF are also closely correlated.\n
"Visualisation of 'OverallQual','TotalBsmtSF','GrLivArea','GarageArea','FullBath','YearBuilt','YearRemodAdd' features \nwith respect to SalePrice in the form of pair plot & scatter pair plot for better understanding."
"Although we already know some of the main figures, this pair plot gives us a reasonable overview insight about the correlated features .Here are some of my analysis.\n\n- One interesting observation is between 'TotalBsmtSF' and 'GrLiveArea'. In this figure we can see the dots drawing a linear line, which almost acts like a border. It totally makes sense that the majority of the dots stay below that line. Basement areas can be equal to the above ground living area, but it is not expected a basement area bigger than the above ground living area.\n\n- One more interesting observation is between 'SalePrice' and 'YearBuilt'. In the bottom of the 'dots cloud', we see what almost appears to be a exponential function.We can also see this same tendency in the upper limit of the 'dots cloud' \n- Last observation is that prices are increasing faster now with respect to previous years."
#### Box plot - OverallQual
#### Box plot - Neighborhood
#### Box plot - Neighborhood
#### Count Plot - Neighborhood
#### Count Plot - Neighborhood
Based on the above observation can group those Neighborhoods with similar housing price into a same bucket for dimension-reduction.Let us see this in the preprocessing stage
With qualitative variables we can check distribution of SalePrice with respect to variable values and enumerate them. 
#### Housing Price vs Sales\n\n- Sale Type & Condition\n- Sales Seasonality
#### ViolinPlot - Functional vs.SalePrice
#### FactorPlot - FirePlaceQC vs. SalePrice 
#### FactorPlot - FirePlaceQC vs. SalePrice 
#### Facet Grid Plot - FirePlace QC vs.SalePrice
#### Facet Grid Plot - FirePlace QC vs.SalePrice
#### PointPlot
#### PointPlot
### Missing Value Analysis \n \n #### Numeric Features
#### Missing values for all numeric features in Bar chart Representation
#### Categorical Features
#### Missing values for  Categorical features in Bar chart Representation
### Categorical Feature Exploration
## Date of Recordings
## Birds Seen
## Birds Seen
## Pitch
## Pitch
## Sampling Rate\n\nSampling rate (audio) or sampling frequency defines the number of samples per second.
## Volume
"## Channels\nChannel is the passage way a signal or data is transported.One Channel is usually referred to as mono, while more Channels could either indicate stereo, surround sound and the like."
"## Channels\nChannel is the passage way a signal or data is transported.One Channel is usually referred to as mono, while more Channels could either indicate stereo, surround sound and the like."
## Recordist\nLet us find out the number of people who provided the recordings
## Ratings\nLet us find out the ratings
## Bird Seen by Country
## Bird Seen by Country
# Audio Data Analysis
"## Spectrogram Analysis\n\n![](https://www.researchgate.net/profile/Phillip_Lobel/publication/267827408/figure/fig2/AS:295457826852866@1447454043380/Spectrograms-and-Oscillograms-This-is-an-oscillogram-and-spectrogram-of-the-boatwhistle.png)\n\n**What is a spectrogram?**\nA spectrogram is a visual way of representing the signal strength, or ‚Äúloudness‚Äù, of a signal over time at various frequencies present in a particular waveform.  Not only can one see whether there is more or less energy at, for example, 2 Hz vs 10 Hz, but one can also see how energy levels vary over time.  In other sciences spectrograms are commonly used to display frequencies of sound waves produced by humans, machinery, animals, whales, jets, etc., as recorded by microphones.  In the seismic world, spectrograms are increasingly being used to look at frequency content of continuous signals recorded by individual or groups of seismometers to help distinguish and characterize different types of earthquakes or other vibrations in the earth. \n\n**How do you read a spectrogram?**\n\nSpectrograms are basically two-dimensional graphs, with a third dimension represented by colors. Time runs from left (oldest) to right (youngest) along the horizontal axis. Each of our volcano and earthquake sub-groups of spectrograms shows 10 minutes of data with the tic marks along the horizontal axis corresponding to 1-minute intervals.  The vertical axis represents frequency, which can also be thought of as pitch or tone, with the lowest frequencies at the bottom and the highest frequencies at the top.  The amplitude (or energy or ‚Äúloudness‚Äù) of a particular frequency at a particular time is represented by the third dimension, color, with dark blues corresponding to low amplitudes and brighter colors up through red corresponding to progressively stronger (or louder) amplitudes.\n![](https://s3.amazonaws.com/pnsn-cms-uploads/attachments/000/000/583/original/6dd1240572ba9085af145892a1b4c1eacce3a651)\nAbove the spectrogram is the raw seismogram, drawn using the same horizontal time axis as the spectrogram (including the same tick marks), with the vertical axis representing wave amplitude. This plot is analogous to webicorder-style plots (or seismograms) that can be accessed via other parts of our website.  Collectively, the spectrogram-seismogram combination is a very powerful visualization tool, as it allows you to see raw waveforms for individual events and also the strength or ‚Äúloudness‚Äù at various frequencies. The frequency content of an event can be very important in determining what produced the signal."
"# 4. Audio Features\n## Spectral Centroid\nThe spectral centroid is a measure used in digital signal processing to characterise a spectrum. It indicates where the center of mass of the spectrum is located. Perceptually, it has a robust connection with the impression of brightness of a sound."
# Spotify Music - EDA \n![](https://storage.googleapis.com/pr-newsroom-wp/1/2020/03/Header.png)\nIn continuation of previous kernel about spotify music data extraction -Part 1 \nhttps://www.kaggle.com/pavansanagapati/spotify-music-api-data-extraction-part1\n\nWe now will use the data extracted from Spotify to perform two steps as follows\n\n#### 1. Explore the Audio Features and analyze\n#### 2. Build a Machine Learning Model \n\n## 1. Explore the Audio Features and analyze
#### Let us first analyse at high level the data in the spotify music dataframe that we build by accessing the spotify data as shown in part 1 of this kernel https://www.kaggle.com/pavansanagapati/spotify-music-api-data-extraction-part1.
"The standard deviation of the audio features themselves do not give us much information ( as we can see in the plots below), we can sum them up and calculate the mean of the standard deviation of the lists."
"\n### Correlation Between Variables\n\nWe will correlate the feature **valence** which describes the musical positiveness with **danceability** and **energy**.\n\n\n#### Valence and Energy\nThe correlation between valence and energy shows us that there is a conglomeration of songs with high energy and a low level of valence. This means that many of my energetic songs sound more negative with feelings of sadness, anger and depression ( NF takes special place here haha). whereas when we look at the grays dots we can see that as the level of valence - positive feelings increase, the energy of the songs also increases. Although her data is split , we can identify this pattern which indicates a kind of 'linear' correlation between the variables."
Now let us load a sample audio file using librosa
Now let us visually inspect data and see if we can find patterns in the data
Now let us visually inspect data and see if we can find patterns in the data
As you can see the air conditioner class is shown as random class and we can see its pattern.Let us again see another class by using the same code to randomly select another class and observe its pattern
As you can see the air conditioner class is shown as random class and we can see its pattern.Let us again see another class by using the same code to randomly select another class and observe its pattern
Let us see the class distributions for this problem
## Preparation 
## Discrete and Continuous Variables 
"Here we visualize a PMF of a binomial distribution. You can see that the possible values are all integers. For example, no values are between 50 and 51. \n\nThe PMF of a binomial distribution in function form:\n\n![](http://reliabilityace.com/formulas/binomial-pmf.png)\n\nSee the ""[Distributions](#3)"" sections for more information on binomial distributions."
### PDF (Probability Density Functions)
"The PDF is the same as a PMF, but continuous. It can be said that the distribution has an infinite number of possible values. Here we visualize a simple normal distribution with a mean of 0 and standard deviation of 1.\n\nPDF of a normal distribution in formula form:\n\n![](https://www.mhnederlof.nl/images/normalpdf.jpg)"
### CDF (Cumulative Distribution Function)
"The CDF maps the probability that a random variable X will take a value of less than or equal to a value x (P(X ‚â§  x)). CDF's can be discrete or continuous. In this section we visualize the continuous case. You can see in the plot that the CDF accumulates all probabilities and is therefore bounded between 0 ‚â§ x ‚â§ 1.\n\nThe CDF of a normal distribution as a formula:\n\n![](https://wikimedia.org/api/rest_v1/media/math/render/svg/187f33664b79492eedf4406c66d67f9fe5f524ea)\n\n*Note: erf means ""[error function](https://en.wikipedia.org/wiki/Error_function)"".*"
## Distributions 
"A Binomial Distribution has a countable number of outcomes and is therefore discrete.\n\nBinomial distributions must meet the following three criteria:\n\n1. The number of observations or trials is fixed. In other words, you can only figure out the probability of something happening if you do it a certain number of times.\n2. Each observation or trial is independent. In other words, none of your trials have an effect on the probability of the next trial.\n3. The probability of success is exactly the same from one trial to another.\n\nAn intuitive explanation of a binomial distribution is flipping a coin 10 times. If we have a fair coin our chance of getting heads (p) is 0.50. Now we throw the coin 10 times and count how many times it comes up heads. In most situations we will get heads 5 times, but there is also a change that we get heads 9 times. The PMF of a binomial distribution will give these probabilities if we say N = 10 and p = 0.5. We say that the x for heads is 1 and 0 for tails.\n\nPMF:\n\n![](http://reliabilityace.com/formulas/binomial-pmf.png)\n\nCDF:\n\n![](http://reliabilityace.com/formulas/binomial-cpf.png)\n\n\nA **Bernoulli Distribution** is a special case of a Binomial Distribution.\n\nAll values in a Bernoulli Distribution are either 0 or 1. \n\nFor example, if we take an unfair coin which falls on heads 60 % of the time, we can describe the Bernoulli distribution as follows:\n\np (change of heads) = 0.6\n\n1 - p (change of tails) = 0.4\n\nheads = 1\n\ntails = 0\n\nFormally, we can describe a Bernoulli distribution with the following PMF (Probability Mass Function):\n\n![](https://wikimedia.org/api/rest_v1/media/math/render/svg/a9207475ab305d280d2958f5c259f996415548e9)\n"
### Poisson Distribution
"The Poisson distribution is a discrete distribution and is popular for modelling the number of times an event occurs in an interval of time or space. \n\nIt takes a value lambda, which is equal to the mean of the distribution.\n\nPMF: \n\n![](https://study.com/cimages/multimages/16/poisson1a.jpg)\n\nCDF: \n![](http://www.jennessent.com/images/cdf_poisson.gif)"
### Log-Normal Distribution
A log-normal distribution is continuous. The main characteristic of a log-normal distribution is that it's logarithm is normally distributed. It is also referred to as Galton's distribution.\n\nPDF: \n\n![](https://www.mhnederlof.nl/images/lognormaldensity.jpg)\n\nCDF:\n\n![](https://wikimedia.org/api/rest_v1/media/math/render/svg/29095d9cbd6539833d549c59149b9fc5bd06339b)\n\nWhere Phi is the CDF of the standard normal distribution.
## Summary Statistics and Moments 
Linear Regression can be performed through Ordinary Least Squares (OLS) or Maximum Likelihood Estimation (MLE).\n\nMost Python libraries use OLS to fit linear models.\n\n![](https://image.slidesharecdn.com/simplelinearregressionpelatihan-090829234643-phpapp02/95/simple-linier-regression-9-728.jpg?cb=1251589640)
"Here we observe that the linear model is well-fitted. However, a linear model is probably not ideal for our data, because the data follows a quadratic pattern. A [polynomial regression model](https://en.wikipedia.org/wiki/Polynomial_regression) would better fit the data, but this is outside the scope of this tutorial."
We can also implement linear regression with a bare-bones approach. In the following example we measure the vertical distance and horizontal distance between a random data point and the regression line. \n\nFor more information on implementing linear regression from scratch [I highly recommend this explanation by Luis Serrano](https://aitube.io/video/introduction-to-linear-regression).
The coefficients of a linear model can also be computed using MSE (Mean Squared Error) without an iterative approach. I implemented Python code for this technique as well. The code is in [the second cell of this Github repository](https://github.com/CarloLepelaars/linreg/blob/master/linreg_from_scratch.ipynb)
"Tukey suggested that an observation is an outlier whenever an observation is 1.5 times the interquartile range below the first quartile or 1.5 times the interquartile range above the third quartile. This may sound complicated, but is quite intuitive if you see it visually.\n\nFor normal distributions, Tukey‚Äôs criteria for outlier observations is unlikely if no outliers are present, but using Tukey‚Äôs criteria for other distributions should be taken with a grain of salt.\n\nThe formula for Tukey's method:\n\n![](https://wikimedia.org/api/rest_v1/media/math/render/svg/2a103bbd9233d9f8f711a7c76dfeb9694446f860)\n\nYa is the larger of two means being compared. SE is the standard error of the sum of the means.\n\n[Source](https://en.wikipedia.org/wiki/Tukey%27s_range_test)"
## Overfitting 
A Link Function is used in Generalized Linear Models (GLMs) to apply linear models for a continuous response variable given continuous and/or categorical predictors. A link function that is often used is called the inverse logit or logistic sigmoid function.\n\nThe link function provides a relationship between the linear predictor and the mean of a distribution.
"### Logistic regression\n\nWith logistic regression we use a link function like the inverse logit function mentioned above to model a binary dependent variable. While a linear regression model predicts the expected value of y given x directly, a GLM uses a link function. \n\nWe can easily implement logistic regression with [sklearn's Logistic Regression function.](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)"
## Setup
"Kaggle is a hot spot for what is trending in data science and machine learning.\n\nDue to its competitiveness, the top players are constantly looking for new tools, technologies, and frameworks that give them an edge over others. If a new package or an algorithm delivers actionable value, there is a good chance it receives immediate adoption and becomes popular.\n\nThis post is about 7 of such trendies packages that are direct replacements for many tools and technologies that are either outdated or need urgent upgrades."
A simple GroupBy operation:
# 3. Lazypredict
"If you thought GPUs are deep learning-exclusive, you are *horribly* mistaken.\n\nThe cuDF library, created by the open-source platform RAPIDs, enables you to run tabular manipulation operations on one or more GPUs.\n\nUnlike datatable, cuDF has a very similar API to Pandas, thus offering a less steep learning curve. As it is standard with GPUs, the library is super fast, giving it an edge over datatable when combined with its Pandas-like API.\n\nThe only hassle when using cuDF is its installation‚Ää-‚Ääit requires:\n- CUDA Toolkit 11.0+\n- NVIDIA driver 450.80.02+\n- Pascal architecture or better (Compute Capability >=6.0)\n\nIf you want to try out the library without installation limitations, Kaggle kernels are a great option. Here is a [notebook](https://www.kaggle.com/rohanrao/tutorial-on-reading-large-datasets?scriptVersionId=49328159&cellId=14) to get you started.\n\n### üõ† GitHub and documentation\n- https://docs.rapids.ai/api/cudf/stable/\n- https://github.com/rapidsai/cudf\n\n### üíª Demo\nHere is a snippet from the documentation that shows a simple GroupBy operation on the tips dataset:"
# 7. Automatic EDA libraries
"# Time Series Forecasting Youtube Tutorial\n## Using Machine Learning to Forecast Energy Consumption\n\nThis notebook is accompanied by a Youtube tutorial.\n\n[WATCH THE VIDEO HERE](https://youtu.be/vV12dGe_Fho)\n\n[You can find it on my channel here!](https://www.youtube.com/channel/UCxladMszXan-jfgzyeIMyvw)\n\n![](https://res.cloudinary.com/monday-blogs/w_768,h_384,c_fit/fl_lossy,f_auto,q_auto/wp-blog/2021/05/sales-forecasting-software.jpg)"
## Types of Time Series Data\n\n![](https://miro.medium.com/max/1400/1*V_RKPeIxCB9CS_2SsLyKXw.jpeg)\n\nreference: https://engineering.99x.io/time-series-forecasting-in-machine-learning-3972f7a7a467
# Feature Importance
# Forecast on Test
"# Visualizing the data\n\n## 1. Map of the location and population of housing districts, along with a heatmap to show where homes are the most expensive.\n\nThis plot is almost directly from Aur√©lien G√©ron's recent book 'Hands-On Machine learning with Scikit-Learn and TensorFlow' (Chapter 2).  It gives a good representation of the original dataset and provides a reference prior to our discussion of the engineered features below."
## 2. Below we have a graph showing the size and location of the cities in the dataset.
## 2. Below we have a graph showing the size and location of the cities in the dataset.
"## 3. Engineered Features - Length of vectors between districts and nearest town (Vincenty's formulae)\n\nThe map below provides a visualization of three of the features added to the data. The black lines represent the vectors connecting the districts to the nearest town. Based on these lines, the distance to the nearest town (length of black line), the population of the nearest town (point the black line arrives at) and a categorical for the name of the nearest town were assigned to the district.\n\nThese features help to capture the general trend of houses getting cheaper the further away from downtown one gets (a fairly universal pattern... with plenty of exceptions). They can also aid the model by providing a rough representation of more abstract aspects associated with position in town, such as the commute time for people working in city centers. The nearest town can also capture important effects such as 'who do you pay property taxes to?' and 'what is the infastructure like for this municipality?' which are the types of things that could have impacts of the values of homes."
"## 3. Engineered Features - Length of vectors between districts and nearest town (Vincenty's formulae)\n\nThe map below provides a visualization of three of the features added to the data. The black lines represent the vectors connecting the districts to the nearest town. Based on these lines, the distance to the nearest town (length of black line), the population of the nearest town (point the black line arrives at) and a categorical for the name of the nearest town were assigned to the district.\n\nThese features help to capture the general trend of houses getting cheaper the further away from downtown one gets (a fairly universal pattern... with plenty of exceptions). They can also aid the model by providing a rough representation of more abstract aspects associated with position in town, such as the commute time for people working in city centers. The nearest town can also capture important effects such as 'who do you pay property taxes to?' and 'what is the infastructure like for this municipality?' which are the types of things that could have impacts of the values of homes."
"## 4. Other Engineered Features - Distance to the big city\n\nThis set of features shows which big city (San Francisco, San Jose, San Diego or Los Angeles... those are all really religious spanish names!) a given district is closest to. The cutoff for 'big city' was 500,000 people (this is 1990 data remember). The black lines here represent the same information as the previous plot, and each district was given a value for the distance to the nearest big city (Vincenty) and a categorical for the name of the nearest big city.  I decided to add these features in because the proximity to a large city is somthing I intuitively thought would impact a house price. Access to things like major aiports and major entertainment events/venues could all have a small but important effect of the price of a home.\n\nLooking at the map below, we can see that this clustering also has an interesting side effect of effectively splitting California into quarters. So having the categorical San Francisco as the closest big city is roughly equivalent to being from the northern 1/4 of the state, while the San Diego category is roughly equivalent to the bottom 1/4 of California (I said roughy... don't get out the rulers!). So this feature may be adding some extra structure to the data in ways I had not initially intended."
"## 4. Other Engineered Features - Distance to the big city\n\nThis set of features shows which big city (San Francisco, San Jose, San Diego or Los Angeles... those are all really religious spanish names!) a given district is closest to. The cutoff for 'big city' was 500,000 people (this is 1990 data remember). The black lines here represent the same information as the previous plot, and each district was given a value for the distance to the nearest big city (Vincenty) and a categorical for the name of the nearest big city.  I decided to add these features in because the proximity to a large city is somthing I intuitively thought would impact a house price. Access to things like major aiports and major entertainment events/venues could all have a small but important effect of the price of a home.\n\nLooking at the map below, we can see that this clustering also has an interesting side effect of effectively splitting California into quarters. So having the categorical San Francisco as the closest big city is roughly equivalent to being from the northern 1/4 of the state, while the San Diego category is roughly equivalent to the bottom 1/4 of California (I said roughy... don't get out the rulers!). So this feature may be adding some extra structure to the data in ways I had not initially intended."
"The large block of code below does some cleaning on the existing features, performs a train test split, one-hot encodes the categorical variables, scales the numerical variables and then recombines the data to produce the final, cleaned version of the train and test dataframes"
### Load packages
### Load data
**73 groups**\n**Each group_id is a unique recording session and has only one surface type **
### Target feature - surface and group_id distribution\nLet's show now the distribution of target feature - surface and group_id.\nby @gpreda.
### Target feature - surface and group_id distribution\nLet's show now the distribution of target feature - surface and group_id.\nby @gpreda.
We need to classify on which surface our robot is standing.\n\nMulti-class Multi-output\n\n9 classes (suface)
We need to classify on which surface our robot is standing.\n\nMulti-class Multi-output\n\n9 classes (suface)
"**So, we have 3810 train series, and 3816 test series.\nLet's engineer some features!**\n\n## Example: Series 1\n\nLet's have a look at the values of features in a single time-series, for example series 1  ```series_id=0```\n\nClick to see all measurements of the **first series** "
\n### Correlations (Part I)
"**Correlations test (click ""code"")** "
"**Correlations test (click ""code"")** "
"Well, this is immportant, there is a **strong correlation** between:\n- angular_velocity_Z and angular_velocity_Y\n- orientation_X and orientation_Y\n- orientation_Y and orientation_Z\n\nMoreover, test has different correlations than training, for example:\n\n- angular_velocity_Z and orientation_X: -0.1(training) and 0.1(test). Anyway, is too small in both cases, it should not be a problem."
"**Normal distribution**\n\nThere are obviously differences between *surfaces* and that's good, we will focus on that in order to classify them better.\n\nKnowing this differences and that variables follow a normal distribution (in most of the cases) we need to add new features like: ```mean, std, median, range ...``` (for each variable).\n\nHowever, I will try to fix *orientation_X* and *orientation_Y* as I explained before, scaling and normalizing data.\n\n---\n\n### Now with a new scale (more more precision)"
### Histogram for main features
### Histogram for main features
## Step 0 : quaternions
![](https://d2gne97vdumgn3.cloudfront.net/api/file/UMYT4v0TyIgtyGm8ZXDQ)
"**Euler angles** are really important, and we have a problem with Z.\n\n### Why Orientation_Z (euler angle Z) is so important?\n\nWe have a robot moving around, imagine a robot moving straight through different surfaces (each with different features), for example concrete and hard tile floor. Our robot can can **bounce** or **balance** itself a little bit on if the surface is not flat and smooth, that's why we need to work with quaternions and take care of orientation_Z.\n\n![](https://lifeboat.com/blog.images/robot-car-find-share-on-giphy.gif.gif)"
**Useful functions**
This advanced features based on robust statistics.
## A look at some of the images in the training set
"Images as Arrays \nAn image is nothing but a standard Numpy array containing pixels of data points. You can think of pixels to be tiny blocks of information arranged in the form of a 2 D grid, and the depth of a pixel refers to the colour information present in it"
"**Coloured images** are represented as a combination of Red, Blue, and Green, and all the other colours can be achieved by mixing these primary colours in the correct proportions."
"A **grayscale image** consists of 8 bits per pixel. This means it can have 256 different shades where 0 pixels will represent black colour while 255 denotes white. For example, the image below shows a grayscale image represented in the form of an array. A grayscale image has only 1 channel where the channel represents dimension."
¬†1.1 Flipping with skimage  
¬†1.2 Rotation with skimage  
"# 2. Data Augmentation using OpenCV-Python\n\n![](https://opencv-python-tutroals.readthedocs.io/en/latest/_static/opencv-logo-white.png)\n\n[OpenCV](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_tutorials.html) essentially stands for Open Source Computer Vision Library. Although it is written in optimized C/C++, it has interfaces for Python and Java along with C++. \nOpenCV-Python is the python API for OpenCV. You can think of it as a python wrapper around the C++ implementation of OpenCV. OpenCV-Python is not only fast (since the background consists of code written in C/C++) but is also easy to code and deploy(due to the Python wrapper in foreground). This makes it a great choice to perform computationally intensive programs.\n"
\n¬†2.1 Flipping with opencv  \n\nThe image is flipped according to the value of flipCode as follows:\n\n* flipcode = 0: flip vertically\n* flipcode > 0: flip horizontally\n* flipcode < 0: flip vertically and horizontally
"# 3. Data Augmentation using imgaug  \n  \n\n[imgaug](https://imgaug.readthedocs.io/en/latest/) is a library for image augmentation in machine learning experiments. It supports a wide range of augmentation techniques, allows to easily combine these and to execute them in random order or on multiple CPU cores, has a simple yet powerful stochastic interface and can not only augment images, but also keypoints/landmarks, bounding boxes, heatmaps and segmentation maps.\n![](https://cdn-images-1.medium.com/max/800/1*QT3A5EZIp1EXSVIiB4SlCA.png)\n\n"
 2.1 Flipping with imgaug  
"2.7 Augmentation pipeline  \n\nThe imgaug library provides a very useful feature called **Augmentation pipeline**. Such a pipeline is a sequence of steps that can be applied in a fixed or random order. This also gives the flexibility to apply certain transformations to a few images and other transformations to other images. In the following example, we are applying the flip, sharpen,crop etc transformations on some of the images. The blur and affline transformations will be applied sometimes and all these transformations will be applied in random order."
"# 4. Data Augmentation using Albumentations \n\n![](https://albumentations.readthedocs.io/en/latest/_static/logo.png)\n\n[Albumentations](https://albumentations.readthedocs.io/en/latest/index.html#) is a fast image augmentation library and easy to use wrapper around other libraries.It is based on numpy, OpenCV, imgaug picking the best from each of them.It is written by Kagglers and was used to get top results in many DL competitions at Kaggle, topcoder, CVPR, MICCAI. Read more about it here: https://www.mdpi.com/2078-2489/11/2/125"
"# 4. Data Augmentation using Albumentations \n\n![](https://albumentations.readthedocs.io/en/latest/_static/logo.png)\n\n[Albumentations](https://albumentations.readthedocs.io/en/latest/index.html#) is a fast image augmentation library and easy to use wrapper around other libraries.It is based on numpy, OpenCV, imgaug picking the best from each of them.It is written by Kagglers and was used to get top results in many DL competitions at Kaggle, topcoder, CVPR, MICCAI. Read more about it here: https://www.mdpi.com/2078-2489/11/2/125"
# 5. Data Augmentation using Augmentor \n\n![](https://augmentor.readthedocs.io/en/master/_static/logo.png)\n
# 6. Keras Image Data Generator  \n\n![](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcRgbD4KXC9PBSYSLHojvt-qcu99NCfy4AcN3eEGFM1YTmLIAJFo&usqp=CAU)\nThe Keras library has a built in class created just for the purpose of adding transformations to images.This class is called **ImageDataGenerator** and it generates batches of tensor image data with real-time data augmentations. 
Let's create a directory where the transformed images will be stored. The directory will be called keras_augmentation and its path is as follows:\n
"\n# HEART FAILURE PREDICTION \n \n\n\nCardiovascular diseases (CVDs) are the number 1 cause of death globally, taking an estimated 17.9 million lives each year, which accounts for 31% of all deaths worlwide.\nHeart failure is a common event caused by CVDs and this dataset contains 12 features that can be used to predict mortality by heart failure.\n\nMost cardiovascular diseases can be prevented by addressing behavioural risk factors such as tobacco use, unhealthy diet and obesity, physical inactivity and harmful use of alcohol using population-wide strategies.\n\nPeople with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors such as hypertension, diabetes, hyperlipidaemia or already established disease) need early detection and management wherein a machine learning model can be of great help. \n\n\n\n    \n TABLE OF CONTENTS \n\n### [**1. IMPORTING LIBRARIES**](#title-one)\n    \n### [**2. LOADING DATA**](#title-two)\n\n### [**3. DATA ANALYSIS**](#title-three)\n\n### [**4. DATA PREPROCESSING**](#title-four)\n\n### [**MODEL BUILDING**](#title-MB) \n    \n### [**5. SVM**](#title-five) \n    \n### [**6. ANN**](#title-six)\n\n### [**7. END**](#title-seven)\n    \n\nIMPORTING LIBRARIES "
\nLOADING DATA 
### **Note:** \n* There are 299 non-null values in all the attributes thus no missing values.\n* Datatype is also either 'float64' or 'int64' which works well while feeded to an algorithm.
### **Note:** \n* Target labels are 203 versus 96 thus there is an imbalance in the data. 
"### **Note:** \n* Features ""creatinine_phosphokinase"" & ""serum creatinine"" are significantly skewed. \n* All the other features almost shows the normal distribution, since mean is equal to their respective medians. "
"### **Note:**\n* ""time"" is the most important feature as it would've been very crucial to get diagnosed early with cardivascular issue so as to get timely treatment thus, reducing the chances of any fatality. (Evident from the inverse relationship) \n\n* ""serum_creatinine"" is the next important feature as serum's (essential component of blood) abundancy in blood makes it easier for heart to function. \n\n* ""ejection_fraction"" has also significant influence on target variable which is expected since it is basically the efficiency of the heart. \n\n* Can be seen from the inverse relation pattern that heart's functioning declines with ageing. "
"### **Note:** \n* Few Outliers can be seen in almost all the features \n* Considering the size of the dataset and relevancy of it, we won't be dropping such outliers in data preprocessing which wouldn't bring any statistical fluke. "
"### **Note:**\n* With less follow-up days, patients often died only when they aged more. \n* More the follow-up days  more the probability is, of any fatality."
"# Code Implementation in Tensorflow 2.0\n> Note: The code for this notebook is taken from the [public kernel](https://www.kaggle.com/akensert/bert-base-tf2-0-minimalistic/) posted by [akensert](https://www.kaggle.com/akensert)\n\nThis kernel does not explore the data. For that you could check out some of the great EDA kernels: [introduction](https://www.kaggle.com/corochann/google-quest-first-data-introduction), [getting started](https://www.kaggle.com/phoenix9032/get-started-with-your-questions-eda-model-nn) & [another getting started](https://www.kaggle.com/hamditarek/get-started-with-nlp-lda-lsa). This kernel is an example of a TensorFlow 2.0 Bert-base implementation, using TensorFow Hub. "
"**1. Read data and tokenizer**\n\nRead tokenizer and data, as well as defining the maximum sequence length that will be used for the input to Bert (maximum is usually 512 tokens)"
You can access the raw files with:
"\nYou will see that **each word comes with a slash and a label** and unlike normal text, we see that **punctuations are separated from the word that comes before it**, e.g. \n\n> The/at General/jj-tl Assembly/nn-tl ,/, which/wdt adjourns/vbz today/nr ,/, has/hvz performed/vbn in/in an/at atmosphere/nn of/in crisis/nn and/cc struggle/nn from/in the/at day/nn it/pps convened/vbd ./.\n\n\nAnd we also see that the **each sentence is separated by a newline**:\n\n> There/ex followed/vbd the/at historic/jj appropriations/nns and/cc budget/nn fight/nn ,/, in/in which/wdt the/at General/jj-tl Assembly/nn-tl decided/vbd to/to tackle/vb executive/nn powers/nns ./.\n> \n> The/at final/jj decision/nn went/vbd to/in the/at executive/nn but/cc a/at way/nn has/hvz been/ben opened/vbn for/in strengthening/vbg budgeting/vbg procedures/nns and/cc to/to provide/vb legislators/nns information/nn they/ppss need/vb ./.\n\n\nThat brings us to the next point on **sentence tokenization** and **word tokenization**."
## Visually inspecting our network against unlabeled data ##\n\n
"Alright, so we made a couple mistakes, but not too bad actually! \n\nIf you're happy with it, let's compete!"
# Import the libraries
# Import the dataset
**2.1 Boxplots of all stats**
**2.2 Customized Boxplots**
**2.2 Customized Boxplots**
# 3. Radar charts\nA radar chart is a graphical method of displaying multivariate data in the form of a two-dimensional chart of three or more quantitative variables represented on axes starting from the same point. The relative position and angle of the axes is typically uninformative.\nSource: [Wikipedia](https://en.wikipedia.org/wiki/Radar_chart)
**3.1 Visualizing stats of single pokemon**
**3.2 Comparing the stats of 2 pokemon**
**4.1 2D Scatterplot(Adding colorscale makes it 3D)**
**4.2 3D scatterplots**
**4.2 3D scatterplots**
"# 5. Contour Charts\nContour plots (sometimes called Level Plots) are a way to show a three-dimensional surface on a two-dimensional plane. It graphs two predictor variables X Y on the y-axis and a response variable Z as contours. These contours are sometimes called z-slices or iso-response values.\n\nThis type of graph is widely used in cartography, where contour lines on a topological map indicate elevations that are the same. Many other disciples use contour graphs including: astrology, meteorology, and physics. Contour lines commonly show altitude and density.\n\nSource: [http://www.statisticshowto.com/contour-plots/](http://www.statisticshowto.com/contour-plots/)"
**5.1 Contour chart for distribution of bug pokemon**
"**5.2 Contour chart for depicting density(and distribution) of HP, Speed, Sp. Attack, Sp. Defense of different generations of pokemon based on their Attack and Defense**"
"**5.2 Contour chart for depicting density(and distribution) of HP, Speed, Sp. Attack, Sp. Defense of different generations of pokemon based on their Attack and Defense**"
# 6. Bubble charts\nYou can use a bubble chart instead of a scatter chart if your data has three data series that each contain a set of values. The sizes of the bubbles are determined by the values in the third data series. Bubble charts are often used to present financial data.
"**6.1 Categorical bubble chart with Attack on X-axis, Defense on Y-axis and HP as size for different generations of fire pokemon**"
"# 7. Treemaps\nIn information visualization and computing, treemapping is a method for displaying hierarchical data using nested figures, usually rectangles. **Treemaps are often used in R kernels due to their interactivity. Python offers squarify to do the same. We can combine squarify and pyplot to plot treemaps.**"
"# 7. Treemaps\nIn information visualization and computing, treemapping is a method for displaying hierarchical data using nested figures, usually rectangles. **Treemaps are often used in R kernels due to their interactivity. Python offers squarify to do the same. We can combine squarify and pyplot to plot treemaps.**"
"# 8. Bullet chart\nStephen Few's Bullet Chart was invented to replace dashboard gauges and meters, combining both types of charts into simple bar charts with qualitative bars (ranges), quantitiative bars (measures) and performance points (markers) all into one simple layout. ranges typically are broken into three values: bad, okay, good, the measures are the darker bars that represent the actual values that a particular variable reached, and the points or markers usually indicate a goal point relative to the value achieved by the measure bar. **It is good for seeing if a pokemon's attributes are good or not for given data.**"
"1. **int data type variables:** Pclass, SibSp, Parch, and PassengerId.\n2. **float data type variables:** Fare and Age, *Survived (due to concatenation)*\n3. **object (numbers + strings) data type variables:** Name, Sex, Ticket, Cabin, and Embarked.\n\n# 4.Univariate Analysis \nUnivariate analysis separately explores the distribution of each variable in a data set. It looks at the range of values, as well as the central tendency of the values. Univariate data analysis does not look at relationships between various variables (like bivariate and multivariate analysis) rather it summarises each variable on its own. Methods to perform univariate analysis will depend on whether the variable is categorical or numerical. For numerical variable, we would explore its shape of distribution (distribution can either be symmetric or skewed) using histogram and density plots. For categorical variables, we would use bar plots to visualize the absolute and proportional frequency distribution. Knowing the distribution of the feature values becomes important when you use machine learning methods that assume a particular type of it, most often Gaussian. **Let's starts off with categorical variables:**\n\n## 4.1 Categorical Variables  \nTo analyse categorical variables, let's create a custom function to display bar chart in absolute and relative scale of a variable in a subplot."
###  4.1.1 Survived 
"**Findings:** Parch isn't balanced as levels of Parch(8) are not equally represented in its distribution. Over one thousand passengers were without parents or children, followed by 170 passengers had one parents or children. In other words, over 76.5% passengers were without parents or children while rest of the 23.5% had few parents or children.\n\n## 4.2 Numerical Variables \nWe would like to analyse numerical variables using histogram, density plot, and summary statistics. To analyse numerical variables, we will create 2 custom functions. The 1st one will plot histogram and density plot for each numerical variable. And the 2nd one will calculate summary statistics including skewness."
### 4.2.1 Fare 
"**Findings:** Over 73% passengers had ticket of category N, followed by nearly 7.5% passengers ticket category were S and P. Passengers with W ticket category were as low as 1.45%.\n\n# 6.Outliers Detection \n**How outliers affect the distribution:** If a value of a variable is significantly above the expected range, it will drag the distribution to the right, making the graph right-skewed or positive-skewed (like Fare). Alternatively, If a value is significantly below the expected range, it will drag the distribution to the left, making the graph left-skewed or negative-skewed.\n\nAnother useful plot for visualizing a continuous variable is box plot. Box plot is particularly helpful to understand the spread of the continus data and whether there are potential unusual observations (outliers) in that variable. It presents information of min, 1st quartile, 2nd quartile(median), 3rd quartile, and max of a variable.**We will use IQR method to detect the outliers for variable Age and Fare though we won't remove them.**"
## 6.1 Outliers Detection for Age 
"## 7.2 Impute Age \nTo impute Age with grouped median, we need to know which features are highly correlated with Age. Let's find out the variables correlated with Age."
"**Findings:** \n1. Age distribution seems to be the same in male and female subpopulations of Sex and S, C, Q subpopulations of Embarked. So Sex and Embarked aren't good predictors for Age.\n2. On the other hand, Age distribution seems to be distinct in Pclass's 1, 2 and 3 subpopulations, so Pclass is informative to predict Age.\n3. Finally, Age distribution seems to be distinct in different categories for nameProcessed, familySize, SibSp, Parch, and cabinProcessed. So they might be good predictors for Age as well."
"**Findings:** \n1. Age distribution seems to be the same in male and female subpopulations of Sex and S, C, Q subpopulations of Embarked. So Sex and Embarked aren't good predictors for Age.\n2. On the other hand, Age distribution seems to be distinct in Pclass's 1, 2 and 3 subpopulations, so Pclass is informative to predict Age.\n3. Finally, Age distribution seems to be distinct in different categories for nameProcessed, familySize, SibSp, Parch, and cabinProcessed. So they might be good predictors for Age as well."
"**Findings:** As expected Sex, Embarked, and ticketProcessed have the weakest correlation with Age what we could guess beforehand from boxplot. Parch and familySize are moderately correlated with Age. nameProcessed, Pclass, Cabin, and SibSp have the highest correlation with Age. But we are gonna use nameProcessed and Pclass only in order to impute Age since they have the strongest correlation with Age. So the tactic is to impute missing values of Age with the median age of similar rows according to nameProcessed and Pclass."
"# 8.Bivariate Analysis \nBeing the most important part, bivariate analysis tries to find the relationship between two variables. We will look for correlation or association between our predictor and target variables. Bivariate analysis is performed for any combination of categorical and numerical variables. The combination can be: Numerical & Numerical, Numerical & Categorical and Categorical & Categorical. Different methods are used to tackle these combinations during analysis process. The methods are:\n1. Numerical & Numerical: Pearson's correlation, or Spearman correlation (the later doesn't require normal distribution).\n2. Numerical & Categorical: Point biserial correlation (only  if categorical variable is binary type), or ANOVA test. For this problem, you can use either biserial correlation or ANOVA. But I will perform both test just to learn because ANOVA will come in handy if categorical variable has more than two classes.\n3. Categorical & Categorical: We would use Chi-square test for bivariate analysis between categorical variables.\n\n## 8.1 Numerical & Categorical Variables \nFirst we create a boxplot between our numerical and categorical variables to check if the distribution of numerical variable is distinct in different classes of nominal variables. Then we find the mean of numerical variable for every class of categorical variable. Again we plot a histogram of numerical variable for every class of categorical variable. Finally anova or point biserial correlation (in case of two class categorical variable) is calculated to find association between nominal and numerical variables.   "
### 8.1.1 Fare & Survived 
"**Note:** Choose either biserial correlation (if categorical variable has two groups) or Anova. If anova states main interaction effect(i.e.,p<0.05) and categorical variable has more than two categories ( like good, better, best), then perform tukey test to find out the pair or pairs that cause the difference(i.e., main interaction effect).\n\n**Interpretation of ANOVA result:**\nSince p>0.05, we can say that survival chance is not statistically associated with Age.\n\n## 8.2 Categorical & Categorical Variables \nWe will calculate and plot absolute and relative frequency of output categorical variable by predictor nominal variables. We would calculate the chi square test between target nominal and predictor nominal variables. Finally we will calculate Bonferroni-adjusted P value if the contingency table has dimension more than 2x2."
### 8.2.1 Sex & Survived 
"**Interpretation of chi-square test result**: Since all of the expected frequencies aren't greater than 5, the chi2 test results can't be trusted.\n\n# 9.Multivariate Analysis \nIn multivariate analysis, we try to find the relationship among more than two variables. Number of predictor variable in bivariate analysis was one. On the contrary, number of predictor variables for multivariate analysis are more than one. More specifically, we will try to associate more than one predictor variable with the response variable. We will just visualize the impact of different predictor variables (3 variables) at a time on variable Survived."
"## 9.1 (Pclass, Sex, cabinProcessed) vs Survived "
## 11.2.3  Model Selection \nLet's compare our models according to their accuracy score after tunning hyperparameters with cross validation scores to select the best models for further study on this classification problem.
"**Findings:** Among the classifiers, RF and GBC have the highest accuracy after  tunning hyperparameters. So RF and GBC are perhaps worthy of further study on this classification problem. Hence we choose RF and GBC.\n\n**Note:** Please note that if we chose our classifier based on cross validation scores, we would not get RF and GBC as our best classifiers instead we would end up choosing LR and SVC. So it is recommended to select best classifiers based on accuracy after tunning hyperparameters though it is computationally intensive.\n\n## 11.3 Retrain and Predict Using Optimized Hyperparameters \nSo we have our best classifiers with their best hyperparameters that produces best accuracy out of a model. That means if we retrain the classifiers using their best hyperparameters, we will be able to get the very same score that we got after tunning hyperparameters (see part 14.4). Let's retrain our classifiers and then use cross validation to calculate the accuracy of the trained model. That's how we will have the same accuracy score as after tunning hyperparameters. Let's retrain models with optimized hyperparameters."
## 11.4 Feature Importance \nDo the classifiers give the same priority to every feature? Let's visualize the features importance given by our classifiers.
"**Findings:** RF, DT, ETC, and ABC (in particular) give some features no importance (zero importance). On the other hand, GBC give all the features more or less importance but it doesn't give zero importance to any features. These are the tree based models that have 'feature_importances_' method by default. LR, KNN and SVC don't have this method. In this problem, SVC uses rbf kernel (only possible for linear kernel to plot feature importance), so its not possible to view feature importance given by SVC. Though its trickier, we would try to get the feature importance given by LR."
"**Findings:** RF, DT, ETC, and ABC (in particular) give some features no importance (zero importance). On the other hand, GBC give all the features more or less importance but it doesn't give zero importance to any features. These are the tree based models that have 'feature_importances_' method by default. LR, KNN and SVC don't have this method. In this problem, SVC uses rbf kernel (only possible for linear kernel to plot feature importance), so its not possible to view feature importance given by SVC. Though its trickier, we would try to get the feature importance given by LR."
"**Findings:** We can see some negative values that means that higher value of the corresponding feature pushes the classification more towards the negative class (in our case 0) that is, of course, something we're already aware of. Some features like Family_size_single, Embarked_Q, Embarked_C, and Cabin_F were given zero importance by lr.\n\n## 11.5 Learning Curves  \nLet's plot the learning curves for the optimized classifiers to see their bias-variance tradeoff."
"**We can see precision, recall, f1 score and class count for both class (0 and 1) of our two models.**\n## 12.7 Precision-Recall vs Threshold Curve  \nSometimes we want a high precision and sometimes a high recall depending on our classification problem. The thing is that an increasing precision results in a decreasing recall and vice versa. This is called the precision-recall tradeoff that can be illustrated using precision-recall curve as a function of the decision threshold."
"**We can see for RF, the recall falls quickly at a precision of around 84%. So therefore, we need to select the precision-recall tradeoff before 84% of precision which could be at around 82%. Now, for example, if we want a precision of 80% off RF we would need a threshold of around 0.4**\n\n**On the other hand, for GBC, the recall falls fast at a precision of around 84% and hence we would select precision-recall tradeoff at around 80% of precision. If we want a precision of around 81% off GBC, we would need a threshold of around 0.38**\n\n## 12.8 Precision-Recall Curve  \nWe can also plot precision against recall to get an idea of precision-recall tradeoff where y-axis represents precision and x-axis represents recall. In my plot, I plot recall on y-axis and precision on x-axis."
"**We can see for RF, the recall falls quickly at a precision of around 84%. So therefore, we need to select the precision-recall tradeoff before 84% of precision which could be at around 82%. Now, for example, if we want a precision of 80% off RF we would need a threshold of around 0.4**\n\n**On the other hand, for GBC, the recall falls fast at a precision of around 84% and hence we would select precision-recall tradeoff at around 80% of precision. If we want a precision of around 81% off GBC, we would need a threshold of around 0.38**\n\n## 12.8 Precision-Recall Curve  \nWe can also plot precision against recall to get an idea of precision-recall tradeoff where y-axis represents precision and x-axis represents recall. In my plot, I plot recall on y-axis and precision on x-axis."
"**We can see recall falls rapidly at around a precision of 0.84 for both RF and 0.82 for GBC that we've observed in the previous section.**\n\n## 12.9 ROC  Curve & AUC Score  \nROC (Reicever Operating Characteristic Curve) is a plot of the true positive rate against the false positive rate of a classifier. It shows the tradeoff between sensitivity and specificity (any increase in sensitivity will be accompanied by a decrease in specificity). AUC (Area under the ROC Curve) score is the corresponding score to the AUC Curve. It is simply computed by measuring the area under the ROC curve, which is called AUC. We will plot ROC curve and AUC score together for our two classifiers."
"**We can see recall falls rapidly at around a precision of 0.84 for both RF and 0.82 for GBC that we've observed in the previous section.**\n\n## 12.9 ROC  Curve & AUC Score  \nROC (Reicever Operating Characteristic Curve) is a plot of the true positive rate against the false positive rate of a classifier. It shows the tradeoff between sensitivity and specificity (any increase in sensitivity will be accompanied by a decrease in specificity). AUC (Area under the ROC Curve) score is the corresponding score to the AUC Curve. It is simply computed by measuring the area under the ROC curve, which is called AUC. We will plot ROC curve and AUC score together for our two classifiers."
"This two plots tells few different things:\n\n1. A model that predicts at chance will have an ROC curve that looks like the diagonal red line. That is not a discriminating model.\n\n2. The further the curve is off the diagonal red line, the better the model is at discriminating between positives and negatives in general.\n\n3. There are useful statistics that can be calculated from this curve, like the Area Under the Curve (AUC). This tells you how well the model predicts and the optimal cut point for any given model (under specific circumstances).\n\n**Comparing the two ROC curves, we can see the distance between blue and red line of RF is greater than the distance between blue and red line of GBC. Hence it can safely be said that RF, in general, is better at discriminating between positives and negatives than GBC. Also RF(92.11%) auc score (which is the area under the roc curve) is greater than gbc(91.94%). It seems the higher the area, the further the classifier is off the red diagonal line and vice versa and hence more accurate. Since RF has more area under the ROC curve than GBC, RF is more accurate.**\n\n# 13.Prediction & Submission  \nFirst we will predict using both rf and gbc. Then we will create two prediction files in csv format for kaggle submission."
"**Correlation among Base Models Predictions:** How base models' predictions are correlated? If base models' predictions are weakly correlated with each other, the ensemble will likely to perform better. On the other hand, for a strong correlation of predictions among the base models, the ensemble will unlikely to perform better. To sumarize, diversity of predictions among the base models is inversely proportional to the ensemble accuracy. Let's make prediction for the test set."
"**Findings:** The prediction looks quite similar for the 8 classifiers except when DT is compared to the others classifiers. Now we will create an ensemble with the base models RF, GBC, DT, KNN and LR. This ensemble can be called heterogeneous ensemble since we have three tree based, one kernel based and one linear models. We would use **EnsembleVotingClassifier method from mlxtend module** for both hard and soft voting ensembles. The advantage is it requires lesser codes to plot decision regions and I find it a bit faster than sklearn's voting classifier."
"**Not so much! But considering the number of features we have, its not either too less. Let's visualize our two components (transformed features) in a scatter plot.**"
"**Looking at this plot, one thing we can say that a linear decision boundary will not be a good choice to separate these two classes. Now we would train our models on this 2d transformed samples to visualize decision regions created by them.**\n\n**Note:** PCA gives you an intuition if a linear or non-linear algorithms would be suitable for a problem. For example, if we look at the scatter plot, we see a non-linear trend between the two class that is, of course better seperable by a non-linear decision boundary. So a non-linear model would be a better bet than a linear one. That's why rf(non-linear) performs better than lr(linear model) for this problem."
"**Looking at this plot, one thing we can say that a linear decision boundary will not be a good choice to separate these two classes. Now we would train our models on this 2d transformed samples to visualize decision regions created by them.**\n\n**Note:** PCA gives you an intuition if a linear or non-linear algorithms would be suitable for a problem. For example, if we look at the scatter plot, we see a non-linear trend between the two class that is, of course better seperable by a non-linear decision boundary. So a non-linear model would be a better bet than a linear one. That's why rf(non-linear) performs better than lr(linear model) for this problem."
**Findings:** There seems to be lesser misclassifications made by hard voting decision region compared to both rf and gbc's decision regions. Let's see how and where hard voting ensemble corrects base learners prediction in a data frame together.
"**Findings:** Soft voting ensemble fails to beat our two best models (rf and gbc). In fact, it produces way to inferior results compared to hard voting ensemble (83.95 vs 84.18). So hard voting ensemble, for this problem, seems to be superior to soft voting ensemble method. WE can visualize soft voting ensemble decision region along with base models decision regions."
**Findings:** Soft voting decision region just seems to be creating more misclassification than rf and gbc.
Or in a more illustrative way:
Let's go deeper:
 \n## 5-1 Import
 \n## 5-2 version
 \n## 5-2 version
 \n## 5-3 Setup\n\nA few tiny adjustments for better **code readability**
 \n## 5-3 Setup\n\nA few tiny adjustments for better **code readability**
" \n## 6- EDA\nBy the end of the section, you'll be able to answer these questions and more, while generating graphics that are both insightful and beautiful.  then We will review analytical and statistical operations:\n\n1. Data Collection\n1. Visualization\n1. Data Cleaning\n1. Data Preprocessing\n\n\n ###### [Go to top](#top)"
"Most of the targets almost have the value between +8 or -8,please check the plot below. and some of data have value (-30)"
we should be careful about them!
 \n## 6-3-2  distplot
 \n## 6-3-3 violinplot
 \n## 6-3-3 violinplot
 \n## 6-3-4 Scatter plot\nScatter plot Purpose to identify the type of relationship (if any) between two quantitative variables
 \n## 6-3-5 Box\n
 \n## 6-4 Data Preprocessing\n\n\n>What methods of preprocessing can we run on  Elo?! \n###### [Go to top](#top)
# DataSet & Library Loading
# Making the dataset ready for the model\n\n- let's drop the unnecessary columns\n- encode the categorical (no details)\n- impute the necessary columns (again no details)\n- scale both the train and test data for linear models\n- split the data for the model
# Pytorch Training
# predictions
## 1.3. JOBS
## 1.4. MARITAL
## 1.4. MARITAL
## 1.5. EDUCATION
## 1.5. EDUCATION
"## 1.6. DEFAULT, HOUSING, LOAN"
## 2.1 Duration
"##### PLease note: duration is different from age, Age has 78  values and Duration has 1544 different values"
"PLOTLY TUTORIAL - 1\n***\n\n*Kaggle ML and Data Science Survey was live from August 7th to August 25th. The median time in the survey was 16.4 minutes. Respondents were allowed to complete the survey at any time.*\n\n*Since Kaggle is one of the best data science community, I would like to share main findings of the survey with interactive plotly library. I hope that recommendations of respondents help data enthusiasts.*\n\n*Let's deep dive into the world of data scientists!*\n\n**PLOTLY TUTORIAL - 2 (2015 Flight Delays and Cancellations):** https://www.kaggle.com/hakkisimsek/plotly-tutorial-2\n\n**PLOTLY TUTORIAL - 3 (S&P 500 Stock Data): **\nhttps://www.kaggle.com/hakkisimsek/plotly-tutorial-3\n\n**PLOTLY TUTORIAL - 4 (Google Store Customer Data): https://www.kaggle.com/hakkisimsek/plotly-tutorial-4**\n\n**PLOTLY TUTORIAL - 5 (Kaggle Survey 2018): https://www.kaggle.com/hakkisimsek/plotly-tutorial-5**\n\n\n\nsource: http://www.timqian.com/star-history/#bokeh/bokeh&plotly/dash"
**The tech world seems still a man's world.**
**The tech world seems still a man's world.**
**Top five countries in 2017:**\n1. USA - 4197 participants\n2. India - 2704 participants\n3. Russia - 578 participants\n4. United Kingdom - 535  participants\n5. China - 471 participants\n\n**Top five countries in 2018:**\n1. USA - 4716 participants\n2. India - 4417 participants\n3. China - 1644 participants\n4. Russia - 879 participants\n5. Brazil - 736 participants
**Age Distribution in 2017 (left) vs 2018 (right):**\n\n* 18-21: ** 7.2% - 12.7%**\n* 22-24: ** 14.9% - 21.5%**\n* 25-29: **25.9% - 25.8%**\n* 30-34: **18.5% - 15.8%**\n* 35-39: **12.6% - 9.4%**\n* 40-44: **7.7% - 5.7%**\n* 44+: **12.9% - 8.4%**
**We can say that online courses (MOOC) are mainstream training platforms of data science.**
**We can say that online courses (MOOC) are mainstream training platforms of data science.**
"**Coursera seems the leader of MOOCs thanks to Andrew NG's' [amazing machine learning courses](http://www.coursera.org/instructor/andrewng).**\n* **After learning basics of machine learning, people discover the world of Kaggle so although it is the last first learning platform in the above ranking, it is graded as the best learning platform. **\n* **Online courses and Stack & Overflow are preferred to textbooks and university courses. The changing face of education in the 21st century!**"
"**Coursera seems the leader of MOOCs thanks to Andrew NG's' [amazing machine learning courses](http://www.coursera.org/instructor/andrewng).**\n* **After learning basics of machine learning, people discover the world of Kaggle so although it is the last first learning platform in the above ranking, it is graded as the best learning platform. **\n* **Online courses and Stack & Overflow are preferred to textbooks and university courses. The changing face of education in the 21st century!**"
**Basic laptop is enough to follow data science trends so hardware requirements is no excuse not to discover the world.**
**Basic laptop is enough to follow data science trends so hardware requirements is no excuse not to discover the world.**
"**The most wondering part of the survey is probably salaries. I think that there is no surprise in the ranking. However, before accepting a job offer,  I would recommend you to check [purchasing power parities](http://data.oecd.org/conversion/purchasing-power-parities-ppp.htm).** \n\n**Note: Although I calculate median salary that is more robust to outliers I dropped rate-adjusted monthly salaries less than 100 dollars more than 500.000 dollars for the following parts.**"
"**The most wondering part of the survey is probably salaries. I think that there is no surprise in the ranking. However, before accepting a job offer,  I would recommend you to check [purchasing power parities](http://data.oecd.org/conversion/purchasing-power-parities-ppp.htm).** \n\n**Note: Although I calculate median salary that is more robust to outliers I dropped rate-adjusted monthly salaries less than 100 dollars more than 500.000 dollars for the following parts.**"
"**If you are a newbie, do not be in a hurry about salary, it increases. Learn as many new technologies as possible and be an active member of the data science community. **\n\n**Since there is a big wage gap between US & non-US countries, I analyze them seperately. Do not mix apples & oranges.**"
"**If you are a newbie, do not be in a hurry about salary, it increases. Learn as many new technologies as possible and be an active member of the data science community. **\n\n**Since there is a big wage gap between US & non-US countries, I analyze them seperately. Do not mix apples & oranges.**"
**Correlation does not imply causation but it seems that satisfaction and salary go hand in hand.**
**It seems that there is no gender inequality in terms of salary in data science world.**
"**But wait! After 60, there is something weird it could change all the picture. Maybe gender-gap?? Let's check box-plot!**"
"**But wait! After 60, there is something weird it could change all the picture. Maybe gender-gap?? Let's check box-plot!**"
"**Yes, we are right to being suspicious. Median wage of male is higher than median wage of female.  \nLet's investigate gender inequality in US vs. Non-US.**"
"**Yes, we are right to being suspicious. Median wage of male is higher than median wage of female.  \nLet's investigate gender inequality in US vs. Non-US.**"
"**It seems that women are underpaid after 35 and they are out of labor force especially after their 50s in non-US countries!!**\n\n**Let's investigate majors & titles. As expected, computer science, mathematics, statistics and engineering are leading degree majors in data science.**"
"* **Data scientists and machine learning engineers are not only highly paid, they also have higher job satisfaction.** \n* **Interestingly, even if researchers are underpaid, their job satisfaction is quite high compared to their salaries. That's why it is important to learn new things everyday.**"
"**Although experiences of most respondents in coding are lower than 5 years, employment rate is quite high.**"
"**Although experiences of most respondents in coding are lower than 5 years, employment rate is quite high.**"
**[Social capital](http://www.investopedia.com/terms/s/socialcapital.asp) is probably the most important thing in the professional world regardless of sectors.**
**[Social capital](http://www.investopedia.com/terms/s/socialcapital.asp) is probably the most important thing in the professional world regardless of sectors.**
"* **Experience from work in company related to ML and Kaggle competitions seem most important way to prove knowledge in data science. Although 60% of respondents have master or doctoral degree, degree is not considered as proof of knowledge in data science.** \n* **Github portfolio and certificates of online courses are even graded better than master and doctoral degree. But it is important to keep in mind that most respondents are employees, not employers.**"
"* **Experience from work in company related to ML and Kaggle competitions seem most important way to prove knowledge in data science. Although 60% of respondents have master or doctoral degree, degree is not considered as proof of knowledge in data science.** \n* **Github portfolio and certificates of online courses are even graded better than master and doctoral degree. But it is important to keep in mind that most respondents are employees, not employers.**"
"**It is critical to learn logistic regression, random forests and support vector machine. To learn math behind these algorithms to narrate non-technical audiences is  more critical.**"
"**It is critical to learn logistic regression, random forests and support vector machine. To learn math behind these algorithms to narrate non-technical audiences is  more critical.**"
**2018 will be the year of Deep Learning!**
"**It would be great if we would download and build models on the famous iris data-set everyday but we should spend  our most time (%60-%70) to clean the data, not to build cool algorithms and models.**"
"* **There are two powerful tools and communities in data science: Python and R. Somehow it is hard to select which language a newbie starts in data science. Although  I am a big fan of Python, I would not give many comments in this section. **\n* **These graphs speak for themselves!**"
\n# 1. \n## Import packages
***
"The **data points** are shown in **light blue** on the left hand side of the grey dashed line. The **orange points** represent the **true future values**, and the **solid dark blue line** shows the **prediction** from the data points. \n\n- When the outliers are left in the model, the **model overfits** and is sensitive to these points. Therefore, it predicts values much higher than the true future values. *This is what we want to avoid.*\n- However, when outliers are removed, it **predicts much more accurately** with a generalised model that splits the distribution of the data points evenly.\n- ***This is very important in Machine Learning because our goal is to create robust models that are able to generalise to future situations.*** If we create a model that is very sensitive and tuned to fit outliers, this will result in a model that over or underfits. If we can create models that are able to cancel out the distractions and noise of outliers, this is usually a better situation.\n\nBy referring to the **Ames Housing Dataset** link provided in the **Acknowledgements**, you'll see that the author outlines there are some outliers that must be treated: \n\n*"" Although all known errors were corrected in the data, no observations have been removed due to unusual values and all final residential sales from the initial data set are included in the data presented with this article. There are five observations that an instructor may wish to remove from the data set before giving it to students (a plot of SALE PRICE versus GR LIV AREA will quickly indicate these points). Three of them are true outliers (Partial Sales that likely don‚Äôt represent actual market values) and two of them are simply unusual sales (very large houses priced relatively appropriately). I would recommend removing any houses with more than 4000 square feet from the data set (which eliminates these five unusual observations) before assigning it to students. ""*\n\n- First, let's plot the two features stated against one another, to identify the outliers. Then we will remove them. The chart on the left shows the data before removing the outliers, and the chart on the right shows after."
***
"Here we see that we have 1 remaining feature with missing values, Utilities.\n\n- Let's inspect this closer to see how to treat it."
"This tell us that within the training dataset, Utilities has two unique values: ""AllPub"" and ""NoSeWa"". With ""AllPub"" being by far the most common.\n- However, the test dataset has only 1 value for this column, which means that it holds no predictive power because it is a constant for all test observations.\n\nTherefore, we can drop this column"
"\n# 4. \n## Exploratory Data Analysis\n\n\n### 4.1 - Correlation matrix\n\nNow that missing values and outliers have been treated, I will analyse each feature in more detail. This will give guidance on how to prepare this feature for modeling. I will analyse the features based on the different aspects of the house available in the dataset.\n\n"
"- Using this correlation matrix, I am able to visualise the raw highly influencing factors on SalePrice.\n- I am looking for these because I will create polynomial features from the highly correlating features, in an attempt to capture the complex non-linear relationships within the data.\n\n***\n\n\n### 4.2 - Feature engineering\n\nThis section is quite lengthy, so I have added hyperlinks to each subsection below in case you want to skip through...\n\n- 4.2.1 - [Polynomials](#polynomials)\n- 4.2.2 - [Interior](#interior)\n- 4.2.3 - [Architectural & Structural](#architectural_&_structural)\n- 4.2.4 - [Exterior](#exterior)\n- 4.2.5 - [Location](#location)\n- 4.2.6 - [Land](#land)\n- 4.2.7 - [Access](#access)\n- 4.2.8 - [Utilities](#utilities)\n- 4.2.9 - [Miscellaneous](#miscellaneous)\n\n\n#### 4.2.1 - Polynomials\n\nThe most common relationship we may think of between two variables, would be a straight line or a linear relationship. What this means is that if we increase the predictor by 1 unit, the response always increases by X units. However, not all data has a linear relationship and therefore it may be necessary for your model to fit the more complex relationships in the data. \n\nBut how do you fit a model to data with complex relationships, unexplainable by a linear function? There are a variety of curve-fitting methods you can choose from to help you with this.\n\n- The most common way to fit curves to the data is to include polynomial terms, such as squared or cubed predictors.\n- Typically, you choose the model order by the number of bends you need in your line. Each increase in the exponent produces one more bend in the curved fitted line. It‚Äôs very rare to use more than a cubic term."
\n#### 4.2.2 - Interior\n\n***BsmtQual***\n\n- Evaluates the height of the basement.
"- SalePrice is clearly affected by BsmtQual, with the better the quality being meaning the higher the price. \n- However, it looks as though most houses have either 'Good' or 'Typical' sized basements.\n- Since this feature is ordinal, i.e. the categories represent different levels of order, I will replace the values by hand."
***BsmtCond***\n- Evaluates the general condition of the basement.
"- As the condition of the basement improves, the SalePrice also increases.\n- However, we see some very high SalePrice values for the houses with ""Typical"" basement conditions. This perhaps suggests that although these two features correlate positively, BsmtCond may not have a largely influential contribution on SalePrice.\n- We also see the largest number of houses falling into the ""TA"" category.\n- Since this feature is ordinal, I will replace the values by hand."
***BsmtExposure***\n- Refers to walkout or garden level walls
"- As the amount of exposure increases, so does hte typical SalePrice. Interestingly, the average difference of SalePrice between categories is quite low here, telling me that some houses sold for very high prices, even with no exposure.\n- From this analysis I would say that it is positively correlating with SalePrice, but it isn't massively influential.\n- Since this feature is ordinal, I will replace values by hand."
***BsmtFinType1***\n- Rating of basement finished area
"- This is very interesting, it seems as though houses with an unfinished basement on average sold for more money than houses having up to an average rating...\n- However, houses with a good finish within the basement still demand more money than unfinished ones.\n- This is an ordinal feature, however as you can see this order does not necessarily cause a higher SalePrice. By creating an ordinal variable it was suggest that as the order of the feature increases then the target variable would also. We can see that this is not the case. Therefore, I will create dummy variables from this feature."
***BsmtFinSF1***\n- Type 1 finished square feet.
"- This feature has a positive correlation with SalePrice and the spread of data points is quite large. \n- It is also clear that the local area (Neighborhood) and style of building (BldgType, HouseStyle and LotShape) has a varying effect on this feature.\n- Since this is a continuous numeric feature, I will bin this into several categories and create dummy features."
***BsmtFinType2***\n- Rating of basement finished area (if multiple types)
"- There seems as though there are a lot of houses with unfinished second basements, and this may cause the skew in terms og SalePrice's being relatively high for these...\n- There also looks to be only a few values for each of the other categories, with the highest average SalePrice coming from the second best category.\n- Although this is intended to be an ordinal feature, we can see that the SalePrice does not necessarily increase with order. Hence, I will cerate dummy variables here."
***BsmtFinSF2***\n- Type 2 finished square feet.
"- There are a large number of data points with this feature = 0. Outside of this, there is no significant correlation with SalePrice and a large spread of values.\n- Hence, I will replace this feature with a flag."
***BsmtUnfSF***\n- Unfinished square feet of basement area
"- This feature has a significant positive correlation with SalePrice, with a small proportion of data points having a value of 0. This tells me that most houses will have some amount of square feet unfinished within the basement, and this actually positively contributes towards SalePrice. \n- The amount of unfinished square feet also varies widely based on location and style. \n- Whereas the average unfinished square feet within the basement is fairly consistent across the different lot shapes.\n- Since this is a continuous numeric feature with a significant correlation, I will bin this and create dummy variables. "
***TotalBsmtSF***\n- Total square feet of basement area.
"- This will be a very important feature within my analysis, due to such a high correlation with Saleprice.\n- We can see that it varies widely based on location, however the average basement size has a lower variance based on type, style and lot shape.\n- Due to this being a continuous numeric feature and also being a very significant feature when describing SalePrice, I believe there could be more value to be mined within this feature. Hence, I will create some binnings and dummy variables. "
***1stFlrSF***\n- First floor square feet.
"- Clearly this shows a very high positive correlation with SalePrice, this will be an important feature during modeling.\n- Once again, this feature varies greatly across neighborhoods and the size of this feature varies across building types and styles. \n- This feature does not vary so much across the lot size.\n- Since this is a continuous numeric feature, once again I will bin this feature and create dummy variables."
***2ndFlrSF***\n- Second floor square feet.
"- Interestingly we see a highly positively correlated relationship with SalePrice, however we also see a significant number of houses with value = 0.\n- This is explained with the other visuals, showing that some styles of houses perhaps do not have a second floor, hence cannot have a value for this feature - such as ""1Story"" houses.\n- We also see a high dependance and variation between neighborhoods, building types and lot sizes.\n- It is evident that all the variables related to ""space"" are important in this analysis. Since this feature is a continuous numeric feature, I will bin this and create dummy variables."
***LowQualFinSF***\n- Low quality finished square feet (all floors)
"- We can see that there is a large number of properties with a value of 0 for this feature. Clearly, it does not have a significant correlation with SalePrice.\n- For this reason, I will replace this feature with a flag."
***Bedroom***\n- Bedrooms above grade (does not include basement bedrooms)
"- We see a lot of houses with 2 3 and 4 bedrooms above ground, and a very low number of houses with 6 or above.\n- Since this is a continuous numeric feature, I will leave it how it is."
***Kitchen***\n- Kitchens above grade.
"- Similarly to last previous feature, we see just a small number of houses with a large number of kitchens above grade. This shows that most houses have 1 kitchen above grade.\n- Since this is a continuous numeric feature, I will leave it as it is."
***KitchenQual***\n- Kitchen quality.
"- There is a clear positive correlation with the SalePrice and the quality of the kitchen.\n- There is one value for ""Gd"" that has an extremely high SalePrice however.\n- For this feature, since it is categorical with an order, I will replace these values by hand."
***TotRmsAbvGrd***\n- Total rooms above grade (does not include bathrooms)
"- Generally we see a positive correlation, as the number of rooms increases, so does the SalePrice.\n- However due to low frequency, we do see some unreliable results for the very large and small values for this feature.\n- Since this is a continuous numeric feature, I will leave it as it is."
***Fireplaces***\n- Number of fireplaces.
"- Once again we have a positive correlation with SalePrice, with most houses having just 1 or 0 fireplaces.\n- I will leave this feature as it is."
***FireplaceQu***\n- Fireplace quality.
"- We also see a positive correlation and the fireplace quality increases. Most houses have either ""TA"" or ""Gd"" quality fireplaces. \n- Since this is a categorical feature with order, I will replace the values by hand."
***GrLivArea***\n- Above grade ground living area in square feet.
"- We see a very high positive correlation with SalePrice.\n- We also see the values varying very highly between styles of houses and neigborhood.\n- Since this will be an important feature in our modeling, I will create bins and dummy features."
\n#### 4.2.3 - Architectural & Structural\n\n***MSSubClass***\n- Identifies the type of dwelling involved in the sale.
"- Each of these classes represents a very different style of building, as shown in the data description. Hence, we can see large variance between classes with SalePrice. \n- This is a numeric feature, but it should actually be categorical. I could cluster some of these categories together, but for now I will create a dummy feature for each category."
***BldgType***\n- Type of dwelling.
"- The different categories exhibit a range of average SalePrice's. The class with the most observations is ""1Fam"". \n- We can also see that the variance within classes is quite tight, with only a few extreme values in each case.\n- There could be a possibility to cluster these classes, however for now I am going to create dummy features."
***HouseStyle***\n- Style of dwelling.
"- Here we see quite a few extreme values across the categories and a large weighting of observations towards the integer story houses.\n- Although the highest average SalePrice comes from ""2.5Fin"", this has a very high standard deviation and therefore more reliably, the ""2Story"" houses are also very highly priced on average.\n- Since there are some categories with very few values, I will cluster these into another category and create dummy variables."
***OverallQual***\n- Rates the overall material and finish of the house.
"- This feature although being numeric is actually categoric and ordinal, as the value increases so does the SalePrice. Hence, I will keep it as a numeric feature.\n- We see here a nice positive correlation with the increase in OverallQual and the SalePrice, as you'd expect."
***OverallCond***\n- Rates the overall condition of the house.
"- Interestingly, we see here that it does follow a positive correlation with SalePrice, however we see a peak at a value of 5, along with a high number of observations at this value.\n- The highest average SalePrice actually comes from a value of 5 as opposed to 10, which may be a reasonable assumption.\n- For this feature, I will leave it as being numeric and ordinal.\n\n***YearRemodAdd***\n- Remodel date (same as construction date if no remodeling or additions)."
"- Interestingly, we see here that it does follow a positive correlation with SalePrice, however we see a peak at a value of 5, along with a high number of observations at this value.\n- The highest average SalePrice actually comes from a value of 5 as opposed to 10, which may be a reasonable assumption.\n- For this feature, I will leave it as being numeric and ordinal.\n\n***YearRemodAdd***\n- Remodel date (same as construction date if no remodeling or additions)."
"- Here we can see that the newer the remodelling of a house, the higher the SalePrice.\n- From the data description, I believe that creating a new feature describing the difference in number of years between remodeling and construction may be a good choice."
"- Here we can see that the newer the remodelling of a house, the higher the SalePrice.\n- From the data description, I believe that creating a new feature describing the difference in number of years between remodeling and construction may be a good choice."
"- Clearly we can see that there are some values which have a much higher SalePrice than others. I will leave this feature as it is, without any binnings."
***YearBuilt***\n- Original construction date.
"- Here we can see a fairly consistent upward trend for the SalePrice as houses are more modern. \n- For this feature, I am going to create bins and dummy features"
***Foundation***\n- Type of foundation.
"- We have 3 classes with high frequency, however we have 3 of low frequency.\n- Due to the large difference in median and mean SalePrice's across the 3 lower frequent classes, I am not going to cluster these together. \n- Also since this feature is not ordinal, labelling does not make sense. Instead I will create dummy variables."
***Functional***\n- Home functionality.
"- This categorical feature shows that most houses have ""Typ"" functionality, and looking at the data description leads me to believe that there is an order within these categories, ""Typ"" being of the highest order.\n- Therefore, I will replace the values of this feature by hand with numbers."
\n#### 4.2.4 - Exterior\n\n***RoofStyle***\n- Type of roof.
"- This feature has two highly frequent categories but the values of SalePrice differ between each.\n- Since this is a categorical feature without order, I will create dummy variables."
***RoofMatl***\n- Roof material.
"- Interestingly, there are very few observations in the training data for several classes. However, these will be dropped during feature reduction if they turn out to be insignificant.\n- Hence, I will create dummy variables."
***MasVnrType***\n- Masonry veneer type.
"- Each class has quite a unique range of values for SalePrice, the only class that stands out is ""BrkCmn"", which has a low frequency.\n- Clearly ""Stone"" demands the highest SalePrice on average, although there are some extreme values within ""BrkFace"".\n- Since this is a categorical feature without order, I will create dummy variables here."
***MasVnrArea***\n- Masonry veneer area in square feet.
"- From this we can see that this feature has negligible correlation with SalePrice, and the values for this feature vary widely based on house type, style and size. \n- Since this feature is insignificant in regards to SalePrice, and it also correlates highly with ""MasVnrType"" (if ""MasVnrType = ""None"" then it has to be equal to 0), I will drop this feature."
***ExterQual***\n- Evaluates the quality of the material on the exterior.
"- We can see here that this feature shows a clear order and has a positive correlation with SalePrice. As the quality increases, so does the SalePrice. \n- We see the largest number of observations within the two middle classes, and the lowest observations within the lowest class.\n- Since this is a categorical feature with order, I will replace these values by hand."
***ExterCond***\n- Evaluates the present condition of the material on the exterior. 
"- Interestingly we see the largest values of SalePrice for the second and third best classes. This is perhaps because of the large frequency of values within these classes, whereas we only see 3 observations within ""Ex"" from the training data.\n- Since this categorical feature has an order, but thr SalePrice does not necessarily correlate with this order... I will create dummy variables."
***GarageType***\n- Garage location.
"- Here we see ""BuiltIn"" and ""Attched"" having the 2 highest average SalePrices, with only a few extreme values within each class.\n- Since this is categorical without order, I will create dummy variables."
***GarageYrBlt***\n- Year garage was built.
- We can see a slight upward trend as the garage building year becomes more modern.\n- For this feature I am going to create bins and the dummy variables.
***GarageFinish***\n- Interior finish of the garage.
"- Here we see a nice split between the 3 classes, with ""Fin"" producing the highest SalePrice's on average.\n- I will create dummy variables for this feature."
***GarageCars***\n- Size of the garage in car capacity.
"- We generally see a positive correlation with an increasing garage car capacity. However, we see a slight dip for 4 cars I believe due to the low frequency of houses with a 4 car garage."
***GarageArea***\n- Size of the garage in square feet.
"- This has an extremely high positive correlation with SalePrice, and it is highly dependant on Neighborhood, building type and style of the house.\n- This could be an important feature in the analysis, so I will bin this feature and create dummy variables."
***GarageQual***\n- Garage quality.
"- We see a lot of homes having ""TA"" quality garages, with very few homes having high quality and low quality ones.\n- I am going to cluster the classes here, and then create dummy variables."
***GarageCond***\n- Garage condition.
"- We see a fairly similar pattern here with the previous feature. We see a slight positive correlation and then a dip, I believe due to the low number of houses that have ""Ex"" or ""Gd"" garage conditions. \n- Similarly to before, I am going to cluster and then dummy this feature."
***WoodDeckSF***\n- Wood deck area in SF.
"- This feature has a high positive correlation with SalePrice.\n- We can also see that it varies widely with location, building type, style and size of the lot.\n- There is a significant number of data points with a value of 0, so I will create a flag to indicate no Wood Deck. Then, since this is a continuous numeric feature, and I believe it to be an important one, I will bin this and then create dummy features. "
***PoolArea***\n- Pool area in square feet. 
"- We see almost 0 correlation due to the high number of houses without a pool.\n- Hence, I will create a flag here."
***PoolQC***\n- Pool quality.
"- Due to not many houses having a pool, we see very low numbers of observations for each class.\n- Since this does not hold much information this feature, I will simply remove it."
***Fence***\n- Fence quality.
"- Here we see that the houses with the most privacy have the highest average SalePrice.\n- There seems to be a slight order within the classes, however some of the class descriptions are slightly ambiguous, therefore I will create dummy variables here from this categorical feature. "
\n#### 4.2.5 - Location\n\n***MSZoning***\n- Identifies the general zoning classification of the sale. 
"- Since this a categorical feature without order, and each of the classes has a very different range and average for SalePrice, I will create dummy features here."
***Neighborhood***\n- Physical locations within Ames city limits.
"- Neighborhood clearly has an important contribution towards SalePrice, since we see such high values for certain areas and low values for others.\n- Since this is a categorical feature without order, I will create dummy features."
#### 4.2.6 - Land\n\n***LotFrontage***\n- Linear feet of street connected to property.
"- This feature seems to be fairly randomly distributed against SalePrice without any significant correlation.\n- LotFrontage doesn't seem to vary too much based on ""Neighborhood"", but the ""BldgType"" does seem to have a affect on the average LotFrontage.\n- Since this feature doesn't seem to show any significance to bin into groupings, I will leave this feature as it is until I scale the features."
***LotArea***\n- Lot size in square feet.
"- This feature shows a high correlation but it is very positively skewed. \n- Hence, I will create quantile bins and dummy features. Quantile bins are not based on approximately equal sized bins, instead creating bins with a similar frequency of data points within each bin."
***LotShape***\n- General shape of property.
"- Clearly we see some extreme values for some categories and a varying SalePrice across classes.\n- ""Reg"" and ""IR1"" have the highest frequency of data points within them.\n- Since this is a categorical feature without order, I will create dummy features."
***LandContour***\n- Flatness of the property
"- Most houses are indeed on a flat contour, however the houses with the highest SalePrice seem to come from properties on a hill interestingly.\n- Since this a categorical feature without order, I will create dummy features."
***LotConfig***\n- Lot configuration.
"- Cul de sac's seem to boast the highest average prices within Ames, however most houses are positioned inside or on the corner of the lot.\n- To simplify this feature I wil cluster ""FR2"" and ""FR3"", then create dummy features."
***LandSlope***\n- Slope of property.
"- We see that most houses have a gentle slope of land and overall, the severity of the slope doesn't appear to have much of an impact on SalePrice.\n- Hence, I am going to cluster ""Mod"" and ""Sev"" to create one class, and create a new flag to indicate a gentle slope or not."
\n#### 4.2.7 - Access\n\n***Street***\n- Type of road access to the property.
"- With such a lower number of observations being assigned to the class ""Grvl"" it is redundant within the model.\n- Hence, I will drop this feature."
***Alley***\n- Type of alley access to the property.
"- Here we see a fairly even split between to two classes in terms of frequency, but a much higher average SalePrice for Paved alleys as opposed to Gravel ones.\n- Hence, this seems as though it could be a good predictor. I will create dummy features from this."
***PavedDrive***\n- Paved driveway.
"- Here we see the highest average price being demanded from houses with a paved driveway, and most houses in this srea seem to have one.\n- Since this is a categorical feature without order, I will create dummy variables."
\n#### 4.2.8 - Utilities\n\n***Heating***\n- Type of heating.
"- We see the highest frequency and highest average SalePrice coming from ""GasA"" and a very low frequency from all other classes.\n- Hence, I will create a flag to indicate whether ""GasA"" is present or not."
***HeatingQC***\n- Heating quality and condition.
"- Here we see a positive correlation with SalePrice as the heating quality increases. With ""Ex"" bringing the highest average SalePrice.\n- We also see a high number of houses with this heating quality too, which means most houses had very good heating!\n- This is a categorical feature, however because it exhibits an order, I will replace the values by hand with numbers."
***CentralAir***\n- Central air conditioning.
"- We see that houses with central air conditioning are able to demand a higher average SalePrice than ones without.\n- For this feature, I will simply replace the categories with numbers 0 and 1."
***Electrical***\n- Electrical system.
"- We see the highest average SalePrice coming from houses with ""SBrkr"" electrics, and these are also the most frequent electrical systems installed in the houses from this area. \n- We have 2 categories in particular that have very low frequencies, ""FuseP"" and ""Mix"".\n- I am going to cluster all the classes related to fuses, and the ""Mix"" class will probably be removed during feature reduction."
\n#### 4.2.9 - Miscellaneous\n\n***MiscFeature***\n- Miscellaneous feature not covered in other categories.
"- We can see here that only a low number of houses in this area with any miscalleanous features. Hence, I do not believe that this feature holds much.\n- Therefore I will drop this feature along with MiscVal."
***MoSold***\n- Month sold (MM).
"- Although this feature is a numeric feature, it should really be a category. \n- We can see that there is no real indicator as to any months that consistetly sold houses of a higher price, however there does seem to be a fairly even distribution of values between classes.\n- I will create dummy variables from each category."
***YrSold***\n- Year sold (YYYY).
"- Here we see just a 5 year time period of which the houses in this dataset were sold.\n- There is a n even distribution of values between each class, and each year has a very similar average SalePrice.\n- Even though this is numeric, it should be categorical. Therefore I will create dummy variables."
***SaleType***\n- Type of sale.
"- Most houses were sold under the ""WD"" category, being a conventional sale, however the highest SalePrice was seen from houses that were sold as houses that were brand new and just sold.\n- For this feature, I will cluster some categories together and then create dummy features."
***SaleCondition***\n- Condition of sale.
"- Here we see the largest average SalePrice being associated with partial sales, and the most frequent sale seems to be the normal sales.\n- Since this is a categorical feature without order, I will create dummy features."
"\n### 4.3 - Target Variable\n\n- Unlike classification, **in regression we are predicting a continuous number**. Hence, the prediction could be any number along the real number line.\n- Therefore, it is always useful to check the distribution of the target variable, and indeed all numeric variables, when building a regression model. Machine Learning algorithms work well with features that are **normally distributed**, a distribution that is symmetric and has a characteristic bell shape. If features are not normally distributed, you can transform them using clever statistical methods.\n- First, let's check the target variable."
"The distribution of the target variable is **positively skewed**, meaning that the mode is always less than the mean and median. \n\n- In order to transform this variable into a distribution that looks closer to the black line shown above, we can use the **numpy function log1p** which applies log(1+x) to all elements within the feature."
"The distribution of the target variable is **positively skewed**, meaning that the mode is always less than the mean and median. \n\n- In order to transform this variable into a distribution that looks closer to the black line shown above, we can use the **numpy function log1p** which applies log(1+x) to all elements within the feature."
"We can see from the skewness and the plot that it follows much more closely to the normal distribution now. **This will help the algorithms work most reliably because we are now predicting a distribution that is well-known, i.e. the normal distribution**. If the distribution of your data approximates that of a theoretical distribution, we can perform calculations on the data that are based on assumptions of that well-known distribution. \n\n- ***Note:*** Now that we have transformed the target variable, this means that the prediction we produce will also be in the form of this transformation. Unless, we can revert this transformation..."
"\n### 4.4 - Treating skewed features\n\nAs touched on earlier, skewed numeric variables are not desirable when using Machine Learning algorithms. The reason why we want to do this is move the models focus away from any extreme values, to create a generalised solution. We can tame these extreme values by transforming skewed features."
"Clearly, we have a variety of positive and negative skewing features. Now I will transform the features with skew > 0.5 to follow more closely the normal distribution.\n\n- **Note**: I am using the Box-Cox transformation to transform non-normal variables into a normal shape. Normality is an important assumption for many statistical techniques; if your data isn't normal, applying a Box-Cox means that you are able to run a broader number of tests."
\n**Import Libraries**\n\nThere are basically 4 type of libraries which you have to import\n\n1. Pandas :- For reading / writing data\n2. Matplotlib to display images\n3. Tensorflow Keras models :- Need a model to predict right !! \n4. Tensorflow Keras layers :- Every NN needs layers and CNN needs well a couple of layers.\n\nLayers needed by CNN\n1. Conv2D :- Basic Convolutional layer . Here we will be using a 64 neuron layer\n2. Dense :- Dense layer is needed by every neural network to finally output the result however every once in while using a Dense layer helps in making model learn.\n3. MaxPooling :- CNN has a concept of max pooling. After every convoulution we get some values in a kernel. However in max pooling we select max kernel value.\n4. Flatten:- Conv2D layer returns doesn't return a flatten data hence we need Flatten layer before feeding it into final Dense layer\n
"We need to train a model first so we will check training data In the below code we are iterating through all images in train folder and then we will split image name with deliminiter ""."" We have names like dog.0, dog.1, cat.2 etc.. Hence after splitting we are gonna get results like ""dog', ""cat"" as category value of the image. To make this example more easy we will consider dog as ""1"" and cat as ""0""\n\nNow every image is actually a set of pixels so how to get our computer know that. Its simple convert all those pixels into an array. So we are going to use here a **cv2** library to read our image into an array and also it will read as a gray scale image.\n\n>cv2.imread(os.path.join(path,p),cv2.IMREAD_GRAYSCALE)\n\nNow we have got here images of all sizes . We have landscape, portrait etc etc.. We need to make them all of a single size so it can be analysed pretty easily. How to do that very very simple again. Use cv2\n\n>cv2.resize(img_array, dsize=(80, 80))\n\nOk so we have got image array and its resized but do you believe whatever I just did was correct. Was the resizing of 80 X 80 good or is it bad. Should check it. How can we do that. There is one answer matplotlib. Using the below code we can display the image.\n\n>plt.imshow(new_img_array,cmap=""gray"")\n\nPlease run the below the code to get better understanding. I have applied break here to just display 1 image. You can try out with 50 X 50 or 100 X100 to see the difference.\n\n\n"
"We need to train a model first so we will check training data In the below code we are iterating through all images in train folder and then we will split image name with deliminiter ""."" We have names like dog.0, dog.1, cat.2 etc.. Hence after splitting we are gonna get results like ""dog', ""cat"" as category value of the image. To make this example more easy we will consider dog as ""1"" and cat as ""0""\n\nNow every image is actually a set of pixels so how to get our computer know that. Its simple convert all those pixels into an array. So we are going to use here a **cv2** library to read our image into an array and also it will read as a gray scale image.\n\n>cv2.imread(os.path.join(path,p),cv2.IMREAD_GRAYSCALE)\n\nNow we have got here images of all sizes . We have landscape, portrait etc etc.. We need to make them all of a single size so it can be analysed pretty easily. How to do that very very simple again. Use cv2\n\n>cv2.resize(img_array, dsize=(80, 80))\n\nOk so we have got image array and its resized but do you believe whatever I just did was correct. Was the resizing of 80 X 80 good or is it bad. Should check it. How can we do that. There is one answer matplotlib. Using the below code we can display the image.\n\n>plt.imshow(new_img_array,cmap=""gray"")\n\nPlease run the below the code to get better understanding. I have applied break here to just display 1 image. You can try out with 50 X 50 or 100 X100 to see the difference.\n\n\n"
"Okay so the above code was more for understanding purpose. Nowe we will get to the real part of coding here.\n\nDeclare your training array X and your target array y. Here X will be the array of pixels and y will be value 0 or 1 indicating its a dog or cat\nWrite convert function to map category ""dog"" or ""cat"" into 1 and 0\n\nCreate a function create_test_data which takes all training images into a loop. Converts into image array.Resize image into 80 X80. Append image into X array. Append category value into y array."
# 2. | Importing Libraries üìö\n\n    üëâ Importing libraries that will be used in this notebook.\n\n
# 3. | Color Palettes üé®\n\n    üëâ This section will create some color palettes that will be used in this notebook.\n
# 3. | Color Palettes üé®\n\n    üëâ This section will create some color palettes that will be used in this notebook.\n
"# 4. | Reading Dataset üëì\n\n    üëâ After importing libraries, the dataset that will be used will be imported.\n\n"
### 5.1.2 | Item_Fat_Content
"\n    üëâ It can be seen that most of the products are categorized as ""Low Fat"" products with a percentage of 59.71%.\n    üëâ There are inconsistent values, means that some values have same meaning but in different form. (Ex: ""LF"" and ""low fat"" for Low Fat"", and ""reg"" for Regular).\n"
### 5.1.4 | Outlet_Identifier
"\n    üëâ There are ten outlets, with almost all the outlets have the same number of outlets (around 11%).\n    üëâ However, ""OUT10"" and ""OUT19"" have the least number of outlets (around 6%).\n"
### 5.1.5 | Outlet_Size
"\n    üëâ There are 3 types of outlet size, small, medium, and high size.\n    üëâ Medium size becomes the outlet size with the most number (45.69%).\n    üëâ However, the smallest number outlet size is High size (only 15.25%).\n"
### 5.1.6 | Outlet_Location_Type
"\n    üëâ There are 3 levels of outlet location, tier 1, tier 2, and tier 3.\n    üëâ Tier 3 becomes the outlet location with the most number (39.31%).\n    üëâ However, the smallest number outlet location is Tier 1 (only 28.02%).\n"
### 5.1.7 | Outlet_Type
"\n    üëâ There are 4 outlet types namely grocery store, supermarket type 1, supermarket type 2, and supermarket type 3.\n    üëâ Supermarket type 1 becomes the outlet type with the most number (65.43%).\n    üëâ However, the smallest number outlet type is supermarket type 2 (only 10.89%).\n"
### 5.2.2 | Continuous Column Distribution üìà\n\n    üëâ This section will show the continuous column distribution using histograms and box plots.\n
"\n    üëâ Item_Weight, Item_MRP, and Outlet_Establishment_Year distribution is normal (no outliers detected in these columns). \n    üëâ For Item_Visibility and Item_Outlet_Sales distribution, the distribution is right-skewed distributions (has a long right tail, the mean position is on the right side of the data). These outliers will be pre-processed in the next section.\n"
## 5.3 | Missing Values Exploration ‚ùì\n\n    üëâ This section will show missing values exploration for all columns.\n
"# 6. | Data Pre-processing ‚öô\n\n    üëâ Data pre-processing will be performed in this section to ensure high-quality data by cleaning dirty data, imputing missing values, and handling outliers.\n\n\n    "
"### 6.2.3 | After Imputation üî®\n\n    üëâ After imputation for ""Outlet_Size"" and ""Item_Weight"" columns, a final check will be done to check if there is still missing values left in the dataset.\n"
"## 6.3 | Handling Outliers üîß\n\n    üëâ As mentioned in previous section, ""Item_Visibility"" and ""Item_Outlet_Sales"" have outliers.\n    üëâ In this section, those columns will be transformed using log or square root transformation. After that, those technique will be compared first before applied into real dataset.\n"
## 7.1 | Heatmap üî•
## 7.2 | Item Fat Content based on Item Type ü•ß
## 7.2 | Item Fat Content based on Item Type ü•ß
## 7.3 | Outlet Location Type based on Outlet Type üìâ
## 7.3 | Outlet Location Type based on Outlet Type üìâ
## 7.4 | Outlet Identifier based on Outlet Size üíπ
## 7.4 | Outlet Identifier based on Outlet Size üíπ
## 7.5 | Outlet Identifier based on Outlet Type üì¶
## 7.5 | Outlet Identifier based on Outlet Type üì¶
# 8. | Hypothesis Tests üß™\n\n    üëâ This section will perform hypothesis testing on pre-processed data set.\n\n\n    
"## 8.1 | Hypothesis 1 1Ô∏è‚É£\n\n    \n      \n          H0: There is no heteroscedasticity between ""Item_MRP"" and ""Item_Weight"".\n          H1: There is heteroscedasticity between ""Item_MRP"" and ""Item_Weight"". \n      \n    \n"
\n    \n        ‚ñ∂ Conclusion: H0 accepted\n    \n
"## 8.2 | Hypothesis 2 2Ô∏è‚É£\n\n    \n      \n          H0: Tier 3 does not have the most numbers in ""High"" size outlets.\n          H1: Tier 3 has the most numbers in ""High"" size outlets. \n      \n    \n"
\n    \n        ‚ñ∂ Conclusion: H0 accepted\n    \n
## 8.3 | Hypothesis 3 3Ô∏è‚É£\n\n    \n      \n          H0: There was a decrease in the number of outlets from 1985 to 1998.\n          H1: There was an increase in the number of outlets from 1985 to 1998. \n      \n    \n
\n    \n        ‚ñ∂ Conclusion: H0 accepted\n    \n
"### **Imputation of Age variable**\n\n- `Age` is a continuous variable. First, we will check the distribution of `age` variable."
"- We can see that the `age` distribution is skewed. So, we will use the median imputation."
"## **Age**\n\n\n### **Original distribution**\n\n\n- We can visualise the distribution of the `Age` variable, by plotting a histogram and the Q-Q plot."
"- The variable `Age` is almost normally distributed, except for some observations on the lower value tail of the distribution. Note the slight skew to the left in the histogram, and the deviation from the straight line towards the lower values in the Q-Q- plot. \n\n- In the following cells, I will apply the above mentioned transformations and compare the distributions of the transformed `Age` variable."
"- We can observe the buckets into which each Age observation was placed. For example, age 27 was placed into the 20-40 bucket."
- We can see that there are different passengers in each age bucket label.
"### **Outliers in continuous variables**\n\n- We can see that `Age` and `Fare` are continuous variables. So, first I will cap the outliers in those variables."
- Both Age and Fare contain outliers. Let's find which valuers are the outliers.
- Both Age and Fare contain outliers. Let's find which valuers are the outliers.
"Age is quite Gaussian and Fare is skewed, so I will use the Gaussian assumption for Age, and the interquantile range for Fare."
"Importing the Essential Libraries, Metrics"
Loading the Data
***Visualizing the correlations between numerical variables***
Feature Selection
***Visualizing the Correlation between the numerical variables using pairplot visualization***
***Visualizing the Correlation between each column and the target variable using jointplot visualization***
***Visualizing the Correlation between each column and the target variable using jointplot visualization***
"X, y Split"
This simple histogram shows the count of digits in the training data for each number. This graphic is used to visualize if there is an unequal sample size among the digits. The sample size for each digit appear to be comparable. There is no issue of unequal sampling. 
"This graphic plots the first few digits in the training set to show how this pixel data is representing digits, and to also show how the handwriting varies. Not all digits are written the same. For example, there is a lot of variation in how people write 4s and 9s. "
"This graphic plots the first few digits in the training set to show how this pixel data is representing digits, and to also show how the handwriting varies. Not all digits are written the same. For example, there is a lot of variation in how people write 4s and 9s. "
"# PCA\nThere are many features in this data resulting in high dimensionality. PCA is used to compress the features into a small but informative set of features before using the data in a machine learning model. Data is normalized before PCA is applied. This is so the scale of the data does not throw of the PCA, and so the 0's are represented meaningfully.  There is unequal variance in this data, and features with larger variance will influence the PCA more, creating bias. This is why the data is normalized. "
"This plot shows the separation of classes (digits) based on the first PCAs. Theoretically, these PCs should explain most of the variance in the data, enough to show separation in the groups of digits."
#Neural Network
# Wordcloud of Top N words in each topic
# Sentence Coloring of N Sentences
# Prepare for data analysis\n\n## Load packages
## Check the data\n\nWe verify what data is available.
Let's show now distribution of combinations of features. We create a function to show a heatmap.
Let's see first what consonant diacritics and vowel diacritics appears together.
Let's plot the audio frames
Let's zoom in on first 1000 frames
Let's zoom in on first 1000 frames
\n### Audio Length\n\nWe shall now analyze the lengths of the audio files in our dataset
"The number of categories is large, so let's check the frame distributions of top 25 categories."
We observe:  \nThe distribution of audio length across labels is non-uniform and has high variance as the previous competition.  \n\nLet's now analyze the frame length distribution in train and test.
We observe:  \nThe distribution of audio length across labels is non-uniform and has high variance as the previous competition.  \n\nLet's now analyze the frame length distribution in train and test.
We observe:\n- Majority of the audio files are short.\n- There are an `abnormal` length in the train histogram. Let's analyze them.
#### Some sssential imports
\n#### Configuration
"# ART BY GAN\n\n\n\nIn this Notebook, I will build a Generative Adversarial Network  (GAN) to illustrate the workings of a Generative Adversarial Network and to generate images. Generative modelling is an unsupervised learning task in machine learning that involves automatically discovering and learning the regularities or patterns in input data. As GANs work by identifying the patterns in the data, I will be using oil painted portraits. However, glancing over the dataset gives me an idea that it is going to be a long shot. The orientation and poses in the dataset vary vastly. Keeping that in mind I am still willing to give it a try. Only because portraits are my jam. I basically love oil painted portraits. \n\n\n\n TABLE OF CONTENTS   \n    \n* [1. IMPORTING LIBRARIES](#1)\n    \n* [2. DATA LOADING & PREPREPROCESSING](#2)\n    \n* [3. BUILDING GAN](#3)\n    * [3.1 The Generator](#3.1)\n    * [3.2 The Discriminator](#3.2)\n    \n    \n* [4. GAN COMPILATION](#4)  \n    \n* [5. TRAINING THE MODEL](#5) \n      \n* [6. EVALUATING THE MODEL](#6)\n    \n* [7. CONCLUSION](#7)\n    \n* [8. END](#8)\n\n\n\n# IMPORTING LIBRARIES\n    \nThe following Libraries will be used in the project\n    "
"##### \n# DATA LOADING & PREPREPROCESSING\n\nFor this project, I am using .jpg files of images of portraits. The dataset includes various artists. I am loading data as TensorFlow.Dataset,, with a batch size of 64. I have reduced the image size to (64,64), presuming, it will be computationally less taxing on the GPU.\n\nLoading the data"
"Now that I have the dataset loaded, let us have a look at a few images."
"Most of the images are portraits. A portrait is a painting representation of a person, The face is predominantly depicted portraits along with expressions and postures. To represent the personality of the subject. Since our model is relative a smaller GAN we have reduced the size of the image. \n\nPreprocessing the data\n\n**Normalization:** For the data normalization, I will convert the data in the range between 0 to 1. This helps in fast convergence and makes it easy for the computer to do calculations faster. \nEach of the three RGB channels in the image can take pixel values ranging from 0 to 256. Dividing it by 255 converts it to a range between 0 to 1. By doing this we "
"Now that the Generator is framed, let us see what random output our untrained Generator produces to get an idea of the process. "
"Clearly, the output is a random seed containing noise as the Generator is not trained yet. \n\n\n# The Discriminator\n\nIn GANs the Generator works along with the Discriminator. \n\nThe Discriminator network decided whether the data is fake aka created by the Generator or real i.e. from the original input data. To do so it applies a binary classification method using a sigmoid function to get an output in the range of 0 to 1.\n\nBuilding a Discriminator"
"\n# EVALUATING THE MODEL\n\nNow that I have my model trained, let us see how it performs.\nHaving a look at the performance of the model via Learning Curves\n\nPloting the Learning Curves"
This looks alright-ish! \n\nLet us get some portraits done by the GAN and appreciate the art created by this AI. \nTo get the art output I will create a function that saves the output portraits generated. We will be plotting the generated Portraits\n\nAI makes Artwork
"# How To Recommend Anything?\n\n**To support people best possible on their way through life, it is necessary to have an optimal recommendation on hand.**\nWhether you want to introduce people among themselves in your social network, try to recommend a suitable supplement for the shopping basket of your customers or need a hint for yourself which movie to watch in the evening, there are unlimited possibilities to apply recommendation engines/systems around us.\n\nIn this notebook I will explore and compare different algorithms and approaches to recommend anything. I am using the **[netflix movie-dataset](https://www.kaggle.com/netflix-inc/netflix-prize-data/home)** and the **[movies-dataset](https://www.kaggle.com/rounakbanik/the-movies-dataset/home)** for this purpose.\n\nFeel free to suggest suggestions or to comment comments.\n\n+ [1. Import Libraries](#1)\n+ [2. Load Movie-Data](#2)\n+ [3. Load User-Data And Preprocess Data-Structure](#3)\n+ [4. When Were The Movies Released?](#4)\n+ [5. How Are The Ratings Distributed?](#5)\n+ [6. When Have The Movies Been Rated?](#6)\n+ [7. How Are The Number Of Ratings Distributed For The Movies And The Users?](#7)\n+ [8. Filter Sparse Movies And Users](#8)\n+ [9. Create Train- And Testset](#9)\n+ [10. Transform The User-Ratings To User-Movie-Matrix](#10)\n+ [11. Recommendation Engines](#11)\n + [11.1. Mean Rating](#11.1)\n + [11.2. Weighted Mean Rating](#11.2)\n + [11.3. Cosine User-User Similarity](#11.3)\n + [11.4. Cosine TFIDF Movie Description Similarity](#11.4)\n + [11.5. Matrix Factorisation With Keras And Gradient Descent](#11.5)\n + [11.6. Deep Learning With Keras](#11.6)\n + [11.7. Deep Hybrid System With Metadata And Keras](#11.7)\n+ [12. Exploring Python Libraries](#12)\n + [12.1. Surprise Library](#12.1)\n + [12.2. Lightfm Library](#12.2)\n+ [13. Conclusion](#13)\n\n***\n## 1. Import Libraries"
***\n## 2. Load Movie-Data
There are about **24.000.000 different ratings**.\nI loaded only a single file of four to reduce memory footprint and accelerate computation. Keep in mind that this approach could introduce biases in the data.\n\n***\n## 4. When Were The Movies Released?
Many movies on Netflix have been released in this millennial. Whether Netflix prefers young movies or there are no old movies left can not be deduced from this plot.\nThe decline for the rightmost point is probably caused by an **incomplete last year.**\n\n***\n## 5. How Are The Ratings Distributed?
Many movies on Netflix have been released in this millennial. Whether Netflix prefers young movies or there are no old movies left can not be deduced from this plot.\nThe decline for the rightmost point is probably caused by an **incomplete last year.**\n\n***\n## 5. How Are The Ratings Distributed?
"Netflix movies rarely have a rating lower than three. **Most ratings have between three and four stars.**\nThe distribution is probably biased, since only people liking the movies proceed to be customers and others presumably will leave the platform.\n\n***\n## 6. When Have The Movies Been Rated?"
"Netflix movies rarely have a rating lower than three. **Most ratings have between three and four stars.**\nThe distribution is probably biased, since only people liking the movies proceed to be customers and others presumably will leave the platform.\n\n***\n## 6. When Have The Movies Been Rated?"
With beginning of november 2005  a strange decline in ratings can be observed. Furthermore two unnormal peaks are in january and april 2005.\n\n***\n## 7. How Are The Number Of Ratings Distributed For The Movies And The Users?\n\n
With beginning of november 2005  a strange decline in ratings can be observed. Furthermore two unnormal peaks are in january and april 2005.\n\n***\n## 7. How Are The Number Of Ratings Distributed For The Movies And The Users?\n\n
The ratings per movie as well as the ratings per user both have nearly a perfect **exponential decay**. Only very few \nmovies/users have many ratings. \n\n***\n## 8. Filter Sparse Movies And Users\n\nTo reduce the dimensionality of the dataset I am filtering rarely rated movies and rarely rating users out.
"***\n## 11. Recommendation Engines\n### 11.1. Mean Rating\n\nComputing the **mean rating for all movies** creates a ranking. The recommendation will be the same for all users and can be **used if there is no information on the user.**\nVariations of this approach can be separate rankings for each country/year/gender/... and to use them individually to recommend movies/items to the user.\n\nIt has to be noted that this approach is **biased and favours movies with fewer ratings**, since large numbers of ratings tend to be less extreme in its mean ratings."
### 11..2. Weighted Mean Rating\n\nTo tackle the problem of the unstable mean with few ratings **e.g. IDMb uses a weighted rating.** Many good ratings outweigh few in this algorithm. \n
### 11..2. Weighted Mean Rating\n\nTo tackle the problem of the unstable mean with few ratings **e.g. IDMb uses a weighted rating.** Many good ratings outweigh few in this algorithm. \n
"The variable **""m"" can be seen as regularizing parameter.** Changing it determines how  much weight is put onto the movies with many ratings.\nEven if there is a better ranking the RMSE decreased slightly. There is a **trade-off between interpretability and predictive power.**\n\n### 11.3. Cosine User-User Similarity\n\nInterpreting each row of the matrix as a vector, a similarity between all user-vectors can be computed. This enables us to find all similar users and to work on user-specific recommendations. **Recommending high rated movies of similar users** to a specific user seems reasonable.\nSince there are still empty values left in the matrix, we have to use a reliable way to impute a decent value. A simple first approach is to **fill in the mean of each user into the empty values.**\nAfterwards the **ratings of all similar users will be weighted with their similarity score and the mean will be computed.** Filtering for the unrated movies of a user reveals the best recommendations.\nYou can easily adapt this process to find similar items by computing the item-item similarity the same way. Since the matrix is mostly sparse and there are more users than items, this could be better for the RMSE score."
"The variable **""m"" can be seen as regularizing parameter.** Changing it determines how  much weight is put onto the movies with many ratings.\nEven if there is a better ranking the RMSE decreased slightly. There is a **trade-off between interpretability and predictive power.**\n\n### 11.3. Cosine User-User Similarity\n\nInterpreting each row of the matrix as a vector, a similarity between all user-vectors can be computed. This enables us to find all similar users and to work on user-specific recommendations. **Recommending high rated movies of similar users** to a specific user seems reasonable.\nSince there are still empty values left in the matrix, we have to use a reliable way to impute a decent value. A simple first approach is to **fill in the mean of each user into the empty values.**\nAfterwards the **ratings of all similar users will be weighted with their similarity score and the mean will be computed.** Filtering for the unrated movies of a user reveals the best recommendations.\nYou can easily adapt this process to find similar items by computing the item-item similarity the same way. Since the matrix is mostly sparse and there are more users than items, this could be better for the RMSE score."
"### 11.4. Cosine TFIDF Movie Description Similarity\n\nIf there is no historical data for a user or there is reliable metadata for each movie, it can be useful to **compare the metadata of the movies to find similar ones.**\nIn this approch I will use the **movie description to create a TFIDF-matrix**, which counts and weights words in all descriptions, and compute a cosine similarity between all of those sparse text-vectors. This can easily be extended to more or different features if you like.\nUnfortunately it is impossible for this model to compute a RMSE score, since the model does not recommend the movies directly.\nIn this way it is possible to **find movies closly related to each other**, but it is **hard to find movies of different genres/categories.**"
"### 11.4. Cosine TFIDF Movie Description Similarity\n\nIf there is no historical data for a user or there is reliable metadata for each movie, it can be useful to **compare the metadata of the movies to find similar ones.**\nIn this approch I will use the **movie description to create a TFIDF-matrix**, which counts and weights words in all descriptions, and compute a cosine similarity between all of those sparse text-vectors. This can easily be extended to more or different features if you like.\nUnfortunately it is impossible for this model to compute a RMSE score, since the model does not recommend the movies directly.\nIn this way it is possible to **find movies closly related to each other**, but it is **hard to find movies of different genres/categories.**"
"### 11.5. Matrix Factorisation With Keras And Gradient Descent\n\nThe **user-movie rating matrix is high dimensional and sparse**, therefore I am going to reduce the dimensionality to represent the data in a dense form.\n**Using matrix factorisation a large matrix can be estimated/decomposed into two long but slim matrices.** With gradient descent it is possible to adjust these matrices to represent the given ratings. The **gradient descent algorithm finds latent variables which represent the underlying structure** of the dataset. Afterwards these latent variables can be used to reconstruct the original matrix and to predict the missing ratings for each user.\nIn this case the model has not been trained to convergence and is not hyperparameter optimized."
"# OpenVaccine: mRNA Vaccine Degradation Prediction\n\n\n\n\n\nIn this competition, you will be predicting the degradation rates at various locations along RNA sequence.\n\nThere are multiple ground truth values provided in the training data. While the submission format requires all 5 to be predicted, only the following are scored: `reactivity`, `deg_Mg_pH10`, and `deg_Mg_50C`."
## Data Exploration\nLets take a look at the data. It's provided in json format.
Lets plot this data for 25 examples.
"# Sample Submission\n\nLets quickly look at the sample submission format. Even though we submit for addional solumns, only three columns are scored: `reactivity`, `deg_Mg_pH10`, and `deg_Mg_50C`"
## signal_to_noise feature
## seq_length\n\nTrain data consists of only 107 sequence length. The test data contains mostly 130 sequence lengths.
## seq_length\n\nTrain data consists of only 107 sequence length. The test data contains mostly 130 sequence lengths.
# Baseline Submission [0.47840 LB]\n## Predict the average value for each target column\nLets first calculate the average value for the target columns. And then create a 91 length vector as a baseline submission.
# Baseline Submission [0.47840 LB]\n## Predict the average value for each target column\nLets first calculate the average value for the target columns. And then create a 91 length vector as a baseline submission.
## Fill in predictions with the mean value\n
# Relationship between targets\nColored by `SN_filter` although I'm not clear from the data description what this column represents.\n
"# Simple Baseline Using Simple LightGBM [0.47706 LB]\n\nThis model still predicts the same value for each id in the test set, but the predicted value is based off of the sequence data."
# Improve Baseline by adding: **structure** and **predicted_loop_type** features [0.47520 LB]
"# Modeling approach. Fit Line for Reactivity?\nLets test and see what a regression line looks like for some example samples. Since we are only given 68 values in the training set and will predict 93 in the test, this might be a good idea for extending the trend beyond 93. "
# Exploratory Data Analysis\n\n### Heatmap to check null/missing values
" Let's have a closer look at the distribution of temperature and ph.\n    \nIt is symmetrical and bell shaped, showing that trials will usually give a result near the average, but will occasionally deviate by large amounts. It's also fascinating how these two really resemble each other!"
" Let's have a closer look at the distribution of temperature and ph.\n    \nIt is symmetrical and bell shaped, showing that trials will usually give a result near the average, but will occasionally deviate by large amounts. It's also fascinating how these two really resemble each other!"
" A quick check if the dataset is balanced or not. If found imbalanced, we would have to downsample some targets which are more in quantity but so far everything looks good! "
" A quick check if the dataset is balanced or not. If found imbalanced, we would have to downsample some targets which are more in quantity but so far everything looks good! "
 A very important plot to visualize the diagonal distribution between two features for all the combinations! It is great to visualize how classes differ from each other in a particular space.
 A very important plot to visualize the diagonal distribution between two features for all the combinations! It is great to visualize how classes differ from each other in a particular space.
"#### During rainy season, average rainfall is high (average 120 mm) and temperature is mildly chill (less than 30'C).\n\n#### Rain affects soil moisture which affects ph of the soil. Here are the crops which are likely to be planted during this season. \n\n-  Rice needs heavy rainfall (>200 mm) and a humidity above 80%. No wonder major rice production in India comes from East Coasts which has average of 220 mm rainfall every year!\n-  Coconut is a tropical crop and needs high humidity therefore explaining massive exports from coastal areas around the country."
"#### During rainy season, average rainfall is high (average 120 mm) and temperature is mildly chill (less than 30'C).\n\n#### Rain affects soil moisture which affects ph of the soil. Here are the crops which are likely to be planted during this season. \n\n-  Rice needs heavy rainfall (>200 mm) and a humidity above 80%. No wonder major rice production in India comes from East Coasts which has average of 220 mm rainfall every year!\n-  Coconut is a tropical crop and needs high humidity therefore explaining massive exports from coastal areas around the country."
#### This graph correlates with average potassium (K) and average nitrogen (N) value (both>50). \n#### These soil ingredients direcly affects nutrition value of the food. Fruits which have high nutrients typically has consistent potassium values.
#### This graph correlates with average potassium (K) and average nitrogen (N) value (both>50). \n#### These soil ingredients direcly affects nutrition value of the food. Fruits which have high nutrients typically has consistent potassium values.
Let's try to plot a specfic case of pairplot between `humidity` and `K` (potassium levels in the soil.)\n\n#### `sns.jointplot()` can be used for bivariate analysis to plot between humidity and K levels based on Label type. It further generates frequency distribution of classes with respect to features
Let's try to plot a specfic case of pairplot between `humidity` and `K` (potassium levels in the soil.)\n\n#### `sns.jointplot()` can be used for bivariate analysis to plot between humidity and K levels based on Label type. It further generates frequency distribution of classes with respect to features
#### We can see ph values are critical when it comes to soil. A stability between 6 and 7 is preffered
#### We can see ph values are critical when it comes to soil. A stability between 6 and 7 is preffered
#### Another interesting analysis where Phosphorous levels are quite differentiable when it rains heavily (above 150 mm).
#### Another interesting analysis where Phosphorous levels are quite differentiable when it rains heavily (above 150 mm).
"#### Further analyzing phosphorous levels.\n\nWhen humidity is less than 65, almost same phosphor levels(approx 14 to 25) are required for 6 crops which could be grown just based on the amount of rain expected over the next few weeks."
"#### Further analyzing phosphorous levels.\n\nWhen humidity is less than 65, almost same phosphor levels(approx 14 to 25) are required for 6 crops which could be grown just based on the amount of rain expected over the next few weeks."
# DATA PRE-PROCESSING\n\n### Let's make the data ready for machine learning model
**Correlation visualization between features. We can see how Phosphorous levels and Potassium levels are highly correlated.**
"# FEATURE SCALING\n**Feature scaling is required before creating training data and feeding it to the model.**\n\nAs we saw earlier, two of our features (temperature and ph) are gaussian distributed, therefore scaling them between 0 and 1 with MinMaxScaler."
### Confusion Matrix
### Let's try different values of n_neighbors to fine tune and get better results
### Let's try different values of n_neighbors to fine tune and get better results
## Classification using Support Vector Classifer (SVC)\n
### Let's visualize the import features which are taken into consideration by decision trees.
## Classification using Random Forest.\n
###  Classification report \n\n#### **Let's use yellowbrick for classification report as they are great for visualizing in a tabular format**
## Classification using Gradient Boosting\n
Machine Learning Class\n
Up to outlines
Up to outlines
### Thanks for reading
"Also note that one does not have to use only words. In some cases, it is possible to generate N-grams of characters. This approach would be able to account for similarity of related words or handle typos."
"Adding onto the Bag of Words idea: words that are rarely found in the corpus (in all the documents of this dataset) but are present in this particular document might be more important. Then it makes sense to increase the weight of more domain-specific words to separate them out from common words. This approach is called TF-IDF (term frequency-inverse document frequency), which cannot be written in a few lines, so you should look into the details in references such as [this wiki](https://en.wikipedia.org/wiki/Tf%E2%80%93idf). The default option is as follows:\n\n$$ \large idf(t,D) = \log\frac{\mid D\mid}{df(d,t)+1} $$\n\n$$ \large tfidf(t,d,D) = tf(t,d) \times idf(t,D) $$\n\nAnalogs of Bag of Words can be found outside of text problems e.g. bag of sites in the [Catch Me If You Can competition](https://inclass.kaggle.com/c/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking), [bag of apps](https://www.kaggle.com/xiaoml/talkingdata-mobile-user-demographics/bag-of-app-id-python-2-27392), [bag of events](http://www.interdigital.com/download/58540a46e3b9659c9f000372), etc.\n\n![image](https://habrastorage.org/webt/r7/sq/my/r7sqmyj1nmqmzltaftt40zi7-gw.png)\n\nUsing these algorithms, it is possible to obtain a working solution for a simple problem, which can serve as a baseline. However, for those who do not like the classics, there are new approaches. The most popular method in the new wave is Word2Vec, but there are a few alternatives as well (GloVe, Fasttext, etc.).\n\nWord2Vec is a special case of the word embedding algorithms. Using Word2Vec and similar models, we can not only vectorize words in a high-dimensional space (typically a few hundred dimensions) but also compare their semantic similarity. This is a classic example of operations that can be performed on vectorized concepts: king - man + woman = queen.\n\n![image](https://cdn-images-1.medium.com/max/800/1*K5X4N-MJKt8FGFtrTHwidg.gif)\n\nIt is worth noting that this model does not comprehend the meaning of the words but simply tries to position the vectors such that words used in common context are close to each other. If this is not taken into account, a lot of fun examples will come up.\n\nSuch models need to be trained on very large datasets in order for the vector coordinates to capture the semantics. A pretrained model for your own tasks can be downloaded [here](https://github.com/3Top/word2vec-api#where-to-get-a-pretrained-models).\n\nSimilar methods are applied in other areas such as bioinformatics. An unexpected application is [food2vec](https://jaan.io/food2vec-augmented-cooking-machine-intelligence/). You can probably think of a few other fresh ideas; the concept is universal enough."
This part is inspired by: https://www.analyticsvidhya.com/blog/2016/02/time-series-forecasting-codes-python/\nVery goodjob with the ARIMA models ! It is more simple when we have directly a stationary Time series. It is not our case...\n\nWe will use the Dickey-Fuller Test. More informations here: https://en.wikipedia.org/wiki/Dickey%E2%80%93Fuller_test
Our Time Series is stationary ! it is a good news ! We can to apply the ARIMA Model without transformations.
Good job ! We have a Time Series Stationary ! We can apply our ARIMA Model !!!
"We expose the naive decomposition of our time series (More sophisticated methods should be preferred). They are several ways to decompose a time series but in our example we take a simple decomposition on three parts.\nThe additive model is Y[t] = T[t] + S[t] + e[t]\nThe multiplicative model is Y[t] = T[t] x S[t] x e[t]\nwith:\n\n 1. T[t]: Trend \n 2. S[t]: Seasonality \n 3. e[t]: Residual\n\nAn additive model is linear where changes over time are consistently made by the same amount. A linear trend is a straight line. A linear seasonality has the same frequency (width of cycles) and amplitude (height of cycles).\nA multiplicative model is nonlinear, such as quadratic or exponential. Changes increase or decrease over time. A nonlinear trend is a curved line.A non-linear seasonality has an increasing or decreasing frequency and/or amplitude over time.\nIn ou example we can see it is not a linear model. So it is the reason why we use a multiplicative model."
" 2.1.2 Correlation Heatmap \n\nCorrelation heatmap measures nullity correlation between columns of the dataset. It shows how strongly the presence or absence of one feature affects the other.\n\nNullity correlation ranges from(-1 to 1):\n- -1 means if one column(attribute) is present, the other is almost certainly absent.\n- 0 means there is no dependence between the columns(attributes).\n- 1 means if one column(attributes) is present, the other is also certainly present.\n\nUnlike in a familiar correlation heatmap, if you see here, many columns are missing. Those columns which are always full or always empty have no meaningful correlation and are removed from the visualization.\n\nThe heatmap is helpful for identifying data completeness correlations between attribute pairs, but it has the limited explanatory ability for broader relationships and no special support for really big datasets."
From above visualization we can easily interpret missingness of attribute rate_of_interest and upfront_charges is dependent on each other(correlation value = 1) means if one will be present another will be present. 
# Importing libraries:
# Importing Data:
# 2.Ngram Analysis: 
## Bi-gram Plots:
# 3. Histogram plot of Number of words
We can observe from above histogram plot that the number of words in train text and test text ranges from 1 to 30.Selected text words mostly fall in range of 1-10. 
# 4.Histogram plots of Number of characters
From above plot we can see that number of characters in test and train set was in same range.In selected text the range flows from 3 to 138 Characters.
From above plot we can see that number of characters in test and train set was in same range.In selected text the range flows from 3 to 138 Characters.
We can see that number of unique words in train and test sets range from 1 to 26. In selected text most number  
## **Null Values**\n\nüìå There are no null values in the dataset
## **Numerical Feature Distribution**
## **Numerical Feature Distribution**
## **Categorical Feature Distribution**
## **Categorical Feature Distribution**
## **Target Distribution**
## **Target Distribution**
## **Correlation of Features**
## **Correlation of Features**
# **Preprocessing**
"# Undersampling and oversampling imbalanced data\n\n## Introduction\n\nOftentimes in practical machine learning problems there will be significant differences in the rarity of different classes of data being predicted. For example, when detecting cancer we can expect to have datasets with large numbers of false outcomes, and a relatively smaller number of true outcomes. \n\nThe overall performance of any model trained on such data will be constrained by its ability to predict rare points. In problems where these rare points are only equally important or perhaps less important than non-rare points, this constraint may only become significant in the later ""tuning"" stages of building the model. But in problems where the rare points are important, or even the _point_ of the classifier (as in a cancer example), dealing with their scarcity is a first-order concern for the mode builder.\n\nTangentially, note that the relative importance of performance on rare observations should inform your choice of error metric for the problem you are working on; the more important they are, the more your metric should penalize underperformance on them. See my previous [Model Fit Metrics](https://www.kaggle.com/residentmario/model-fit-metrics/) and [Log Loss](https://www.kaggle.com/residentmario/log-loss-with-new-york-city-building-sales/) notebooks for slightly more detail on which error metrics do and don't care about this problem.\n\nSeveral different techniques exist in the practice for dealing with imbalanced dataset. The most naive class of techniques is **sampling**: changing the data presented to the model by undersampling common classes, oversampling (duplicating) rare classes, or both.\n\n## Motivation\n\nWe'll motivate why under- and over- sampling is useful with an example. The following visualization shows the radical effect that the relative number of points-per-class in a dataset has on classification as performed by a (linear-kernel) Support Vector Machine (for a quick primer on SVMs check [here](https://www.kaggle.com/residentmario/primer-on-support-vector-machines)), shamelessly stolen from the `imbalanced-learn` documentation [here](http://contrib.scikit-learn.org/imbalanced-learn/stable/auto_examples/over-sampling/plot_comparison_over_sampling.html)."
"As you can see, when a dataset is dominated by one or few classes, to the exclusion of some other classes, the optimal solution can break down to collapse: a model which simply classifies all or most points in the majority class (as in the first and second visualizations in this grid). However, as the number of observations per point approaches an equal split, the classifier becomes less and less biased.\n\nRe-sampling points that are being fed into the model is the simplest way to fix model errors like this one stemming from rare class problems. Not all datasets have this issue, but for those that due, dealing with this issue is an important early step in modeling the data."
## Getting some sample data\n\nWe'll use the following data for the sake of illustration (taken from the `sklearn` documentation):
"## Raw over- and under- sampling\n\nA group of researchers implemented the full suite of modern data sampling techniques with the `imbalance-learn` contrib module for `sklearn`. This submodule is installed as part of the base `sklearn` install by default, so it should be available to everyone. It comes with its own documentation as well; that is available [here](http://contrib.scikit-learn.org/imbalanced-learn/stable/).\n\n`imblearn` implements over-sampling and under-sampling using dedicated classes."
"In the first graph we have oversampled the dataset, duplicating points from the rare class 2 and the ultra rare class 3 in order to match the common class 1. This results in many points getting ""pasted over"" a huge number of times, as there are just 64 distinct points in class 2, but 4700 of them in class 1.\n\nIn the second graph we have undersampled the dataset. This goes the other way: classes 1 and 2 are reduced in numeric volume until they reach the same number of observations as class 3, at 64.\n\nWhich of these two fields of points are you better off training your classifier on? In extreme cases where the number of observations in the rare class(es) is *really* small, oversampling is better, as you will not lose important information on the distribution of the other classes in the dataset. For example, if there were just five observations in class 3, we'd have an awful time training a classifier on just 15 (5 times 3) undersampled points!\n\nOutside of this case however, the performance of the one or the other will be most indistinguishable. Remember, sampling doesn't introduce new information in the dataset, it (hopefully) merely shifts it around so as to increase the ""numerical stability"" of the resulting models."
"By default the number of observations will be balanced, e.g. each class will appear equally many times. There's also a `ratio` parameter, which allows you to choose the number of observations per class (in terms of integer absolute numbers, e.g. ""60 class 1 observations"") to push into your sample dataset. For example:"
"By default the number of observations will be balanced, e.g. each class will appear equally many times. There's also a `ratio` parameter, which allows you to choose the number of observations per class (in terms of integer absolute numbers, e.g. ""60 class 1 observations"") to push into your sample dataset. For example:"
"This resampling technique is useful if the distribution of the classes in your training data differs from the distribution of the classes in the ""real world"" (and you have a sufficiently large amount of data to have the luxury of ""slimming down""). In theory you should not try to work with such data, but in practice this scenario arises often. For example, suppose that you have a dataset of kidney health checks taken from a health clinic, and we are classifying them as health and unhealthy according to some metric. We would naturally expect kidneys from the general population to be healthier, on average, than the one represented in clinical data. For us to succeed in building a generalizable model we would need to resample as a step one!"
"## Sample size learning curves\n\nIn the context of the bias-variance tradeoff, what we hope to achieve by resampling data is to reduce bias, or underfit (recall the clearly one-class-is-the-only-class model from the illustrative example, which is a) more than we increase variance, or overfit (which goes up when decrease the number of input observations or copy-paste points). A way of quantifying this hope is to look at a **learning curve**.\n\nA learning curve illustrates the level of fit of a model as we increase the number of observations we show it. I gave a very brief demo on learning curves in [a previous notebook](https://www.kaggle.com/residentmario/learning-curves-with-zillow-economics-data/), and if you're totally unfamiliar with them I suggest taking a look at that first.\n\nWe can generate a learning curve for any sampling method by training a simple model on an incrementally larger number of sampled points, then scoring the resulting model on the full test set. When the performance of the model becomes stable with respect to the number of observations it was trained on, then we have evidence that that's the number of observations we need to include in the sample in order to represent ""enough"" of the data distribution to not hurt accuracy much.\n\nHere is a simple learning curve built on sampling the data from our synthetic dataset:"
"Here we see that the performance of the model becomes stable around 30 observations per class. This means that the decision boundaries on the Support Vector Machine we train on this sampled data stops moving around (much) some time after the 30-point mark.  which is in turn good evidence that the 64-point `RandomUnderSampler` we've been working with won't introduce significant variance to the model by virtue of having too few points. Hence if under-sampling helps significantly with our rare-point bias problem, it is well worth doing!\n\nBuilding and checking the sample learning curve is easy and well worth doing. Note that for two-dimensional, normally distributed data like this the number of observations necessary to achieve stability will of course be quite small. In the general case, except stability to require potentally quite a lot of points, certainly more than this."
"## Ensemble samplers\n\nThe two basic fitting classes we've introduced so far may be combined to generate arbitrarily sized samples. If you want to oversample one class, then undersample some other class, then by running one sampler after the other you can do just that (using the `ratio` argument to fine-tune).\n\nHowever this can be nitty-gritty. For production `sklearn` provides abstractions on top of these two classes called ""ensemble samplers"". \n\nThe first and more straightforward of these is `EasyEnsemble`. `EasyEnsemble` can be used to resample features in the dataset naively. An application to our synthetic dataset looks like this:"
"An interesting feature is that `EasyEnsemble` will perform this resampling operation ten times, resulting in ten different sample sets. The plot here is actually showing just the first of these ten different samples. You can change the number of times this is done using the `n_subsets` parameter.\n\nAnother thing `EasyEnsemble` allows is sampling with replacement.\n\nBy default `EasyEnsemble` is the same as `RandomUnderSampler`, it reduces the counts of each of the classes down to the count of the smallest constituent class. Since class 3 had just 64 observations, we ended up with a sample which has 64 observations per class. This reduces the shape of our dataset from `(5000, 2)` way down to `(192, 2)` per sample. The API for adjusting this is the same: use the `ratio` parameter, passing in a `dict` with the desired number of observations per class.\n\nHowever, I find the `EasyEnsemble` API hard to use, because it doesn't allow automatically combining oversampling with undersampling, e.g. oversampling one category and undersampling another. There is however one `EasyEnsemble` feature that is not available in the raw sampler methods, which is sampling with replacement (`replacement=True`). I'm dubious as to the utility of this feature, but it exists.\n\nThe other method you can use is `BalanceCascade`. `BalanceCascade` is a computational adaptation of `EasyEnsemble`. You feed it an estimator, and that estimator is used to predict output classes. When observations from the dataset are sampled, observations which are misclassified are replaced in the dataset, and those which aren't are not. The end result is a set of samples biased towards (duplicated) poorly classifiable observations.\n\n`EasyEsnemble` allows you to over/under-sample with or without replacement, `BalanceCascade` allows you to over/under-sample with smart replacement (at least that's what the `sklearn` documentation calls this technique). This is an interesting resampling technique if you are interested in working on problem point performance while working with rare classes. It allows you to, in a sense, test the numerical stability of the classifier you have built, by re-training it on a set of points that it is known to have trouble with (and then checking the difference).\n\nI again find this API difficult-to-use, however."
"An interesting feature is that `EasyEnsemble` will perform this resampling operation ten times, resulting in ten different sample sets. The plot here is actually showing just the first of these ten different samples. You can change the number of times this is done using the `n_subsets` parameter.\n\nAnother thing `EasyEnsemble` allows is sampling with replacement.\n\nBy default `EasyEnsemble` is the same as `RandomUnderSampler`, it reduces the counts of each of the classes down to the count of the smallest constituent class. Since class 3 had just 64 observations, we ended up with a sample which has 64 observations per class. This reduces the shape of our dataset from `(5000, 2)` way down to `(192, 2)` per sample. The API for adjusting this is the same: use the `ratio` parameter, passing in a `dict` with the desired number of observations per class.\n\nHowever, I find the `EasyEnsemble` API hard to use, because it doesn't allow automatically combining oversampling with undersampling, e.g. oversampling one category and undersampling another. There is however one `EasyEnsemble` feature that is not available in the raw sampler methods, which is sampling with replacement (`replacement=True`). I'm dubious as to the utility of this feature, but it exists.\n\nThe other method you can use is `BalanceCascade`. `BalanceCascade` is a computational adaptation of `EasyEnsemble`. You feed it an estimator, and that estimator is used to predict output classes. When observations from the dataset are sampled, observations which are misclassified are replaced in the dataset, and those which aren't are not. The end result is a set of samples biased towards (duplicated) poorly classifiable observations.\n\n`EasyEsnemble` allows you to over/under-sample with or without replacement, `BalanceCascade` allows you to over/under-sample with smart replacement (at least that's what the `sklearn` documentation calls this technique). This is an interesting resampling technique if you are interested in working on problem point performance while working with rare classes. It allows you to, in a sense, test the numerical stability of the classifier you have built, by re-training it on a set of points that it is known to have trouble with (and then checking the difference).\n\nI again find this API difficult-to-use, however."
"## Application\n\nSo far we've motivated the case for sampling your data and introduced under- and over- sampling facilities in `sklearn`. Next let's look at an application. \n\nFor ease of visualization let's initially treat the first two predictor columns, `V1` and `V2`, only."
"If we resampled this data, a more meaningful pattern emerges. It appears that transactions that load heavily in `V2` are almost always fradulent:"
"If we train the SVM on this resampled data, the algorithm will now ""realize"" this and pick a separating hyperplane that takes this fact into account:"
"If we train the SVM on this resampled data, the algorithm will now ""realize"" this and pick a separating hyperplane that takes this fact into account:"
"Suppose you are designing an ""early warning"" system that quarantines transactions which might be fradulent for additional checks in the system. Suppose that after market research and cost analysis, your analysts have determined that a 50:1 ratio of false positives to true positives is ""worth it"". We could implement a custom classification metric to reflect this. We would find that in this (admittedly made-up, but reasonably realistic) cost environment, the price of the post-sampling model is near that of the pre-processing model, even though we're misclassifying a hella lot of legit records:"
## 5.1 | Disease Distribution based on Chest Pain Type in Each Gender\n
"\n    From the butterfly chart above and as previously mentioned, typical angina chest pain and female patients have a greater number in the dataset. When viewed more detail, atypical angina, non-anginal pain, and asymptomatic chest pain have more sick patients than healthy male and female patients. In addition, for patients with typical angina chest pain, the ratio of male and female patients with heart disease is almost the same. However, the number of healthy female patients in that chest pain category is higher than healthy male patients.\n\n"
## 5.2 | Maximum Heart Rate vs. Age based on Patients Sickness\n
"\n    The scatter plot above shows that patients with and without heart disease are aged between 40 to 70 years old. In addition, the spread of max. patient's heart rate in the dataset ranges from 140 to 180. When viewed in more detail, patients who tend to get heart disease have max. heart rate over 149 and under 54 years of age. In the scatter plot above, it can also be seen that age and max. heart rate has a negative correlation, especially in patients with heart disease. In addition, heart disease patients have more numbers than healthy ones.\n\n"
## 5.3 | Fasting Blood Sugar Distribution by Resting Electrocardiographic Results\n
"\n    The donut chart above shows resting electrocardiograph types 0 and 1 have almost the same number of patients. However, inversely proportional to resting electrocardiograph type 2, where the number of patients is only four. In addition, resting electrocardiograph types 0 and 1 have patients with fasting blood sugar over 120 mg/dl. Although, resting electrocardiograph type 2 had no patients with fasting blood sugar over 120 mg/dl.\n\n"
## 5.4 | Number of Major Vessles Distribution based on Exercise Induced Angina\n
\n    The waffle charts above show that the proportion between patients who do and do not do exercise-induced angina is almost the same. This can be seen by comparing the total number of patients between major vessels in each exercise.\n\n
## 5.5 | Resting Blood Pressure Distribution based on Slope\n
"\n    The distribution plot and Q-Q plots above show that each slope type's distribution is moderately right-skewed. This is due to outliers (distribution tail) on the right side of the plot. In addition, the skewness value and gap at the upper of Q-Q plots with a 45-degree line also show that the distribution in this column is not normal.\n\n"
"# 7. | Model Implementation üõ†Ô∏è\n\n    This section will implement various machine learning models as mentioned in Introduction section. In addition, explanation for each models also will be discussed.\n"
"## 7.1 | Logistic Regression\n\n    \n        Logistic regression is a statistical method that is used for building machine learning models where the dependent variable is dichotomous: i.e. binary. Logistic regression is used to describe data and the relationship between one dependent variable and one or more independent variables. The independent variables can be nominal, ordinal, or of interval type.\n    The name ""logistic regression"" is derived from the concept of the logistic function that it uses. The logistic function is also known as the sigmoid function. The value of this logistic function lies between zero and one.\n        \n            \n            üñº Logistic Function by Simplilearn\n        \n    \n\n"
# Load Malware Train and Test Data
# Append Timestamps to Data
"# Define EDA Python Functions\nI wrote two EDA Python functions. To see the code, click the show code button. One function visualizes overall density and detection rate per category value. The other visualizes density and detection rate over time. Feel free to fork my kernel and use my functions to explore the data. I've also made my timestamp database public so you can import timestamps."
"In the plots below, solid lines are density and use the left y-axis. Dotted lines are detection rate and use the right y-axis. You can see that the majority of train data are observations in August and September 2018 while test data is October and November 2018. It is interesting that the malware infection rate is correlated with observation density. Perhaps infected computers send more reports to Microsoft. Or Microsoft chose to give us more infected samples during this time interval of interest."
"# Training ROC Curve\nThis competiton's metric is AUC, area under ROC. Let's plot our training ROC and calculate our training AUC. (We should really calculate validation AUC, but since our model is linear, training AUC should be similar.)"
# Predict and Submit to Kaggle
### a. Target column details:-
### b. Passenger class and gender:-
### b. Passenger class and gender:-
### c. Age:-
### c. Age:-
### d. Ticket fare:-
### e. Null valued columns:-
### f. Cabin null inference in training set:-
### Categorical Features :\n\n#### Distribution of Categorical Features :
- All the categorical features are **Normally Distributed**.
### Numerical Features :\n\n#### Distribution of Numerical Features :
"- **Age**,**Creatinine_Phosphokinase**,**Ejaction_Fraction** and **Serum_creatinine** have a **rightly** or **positively skewed** data distribution.\n- **Platelets** and **Serum_Sodium** are near about **normally distributed**.\n- **Time's** data distribution is similar to a typical **Time Series Analysis** graph with irregularities present in it. "
### Target Variable Visualization (DEATH_EVENT) : 
"- The dataset is **unbalanced** with very low data points (299)!\n- **2 : 1** ratio for **No Death Event cases : Death Event cases!**\n- Due to this, predictions will be biased towards **No Death Event** cases.\n- Visualizations will also display this bias, thus making it difficult to gain insight."
### Categorical Features vs Target Variable (DEATH_EVENT) :
"- All the graphs near about share the same pattern.\n- According to the graphs, patients with negative cases of **anaemia**, **diabetes**, **high_blood_pressure** and **smoking**  leads to **DEATH_EVENT** more than the positive cases of these medical conditions.\n- There are more cases cases of **male** population confronting a **DEATH_EVENT** due to heart failure than **female** population."
### Numerical Features vs Target Variable (DEATH_EVENT) :
"- Cases of **DEATH_EVENT** initiate from the age of **45**. Some specific peaks of high cases of **DEATH_EVENT** can be observed at 45, 50, 60, 65, 70, 75 and 80.\n- High cases of **DEATH_EVENT** can be observed for **ejaction_fraction** values from **20 - 60**.\n- **serum_creatinine** values from **0.6** to **3.0** have higher probability to lead to **DEATH_EVENT**.\n- **serum_sodium** values **127 - 145** indicate towards a **DEATH_EVENT** due to heart failure."
#### anaemia vs Numerical Features :
"- Irrespective of **anaemia**, **age** group of **55 - 75** and **ejaction_fraction** values of **20 - 40** are prone to **DEATH_EVENT**.\n- Similarly, **serum_creatinine** levels between **1 - 2** and **serum_sodium** levels of **130 - 140** display a higher chance of confronting a **DEATH_EVENT**."
#### diabetes vs Numerical Features :
"- For **creatinine_phosphokinase**, values from **0 - 500** and **platelets** range from **2x10^5 - 3x10^5** detect more cases of heart failure.\n- Similarly, **serum_creatinine** levels between **1 - 2** and **time** feature's values from **0 - 100** highlight more heart failure cases."
#### high_blood_pressure vs Numerical features :
"- Due to **high_blood_pressure**, **age** at which the **DEATH_EVENT** occurs for heart failure increases its range of values. The lower threshold of age limit drops just **below 55** and upper limit extends **over 70**.\n- Chances of confronting a **DEATH_EVENT** due to **high_blood_pressure** lowers the values of **time** feature's values and increases the chances of heart failure."
#### sex vs Numerical Features :
"- For female(0) population, **age** group **50 - 70** and male(1) population's **age** group **60 - 75** are more prone to heart failure leading to **DEATH_EVENT**.\n- **ejaction_fraction** values for female(0) population of **30 - 50** and **20 - 40** for male(1) population leads to cases of **DEATH_EVENT**.\n- **serum_sodium** values indicating **DEATH_EVENT** due to heart failure is different for male and female."
#### smoking vs Numerical Features :
"- **age** group of **60 - 70** dominates the cases for **DEATH_EVENT** due to **smoking**. However, range of values ,**50 - 75** , increases for cases of **DEATH_EVENT** that do not **smoke**.\n- **Smoking** reduces the range of values for feature **time** to **0 - 75** that someone might face a **DEATH_EVENT**."
### Numerical features vs Numerical features w.r.t Target variable(DEATH_EVENT) :
"- For **time vs age** plot, **DEATH_EVENT** peaks can be found at **age** values of 50, 60, 70 and 80 for **time** value range between **50 - 100**.\n- **creatinine_phosphokinase** values between 0 - 500 are dominant in recording **DEATH_EVENT** irrespective of other features.\n- Similarly, **ejaction_fraction** values between **20 - 40** record high number of cases of **DEATH_EVENT**.\n- **platelets** range of values between **2x10^5 - 4x10^5** and **time** between **0 - 50** is a strong indicator for **DEATH_EVENT**.\n- Another indicator for **DEATH_EVENT** is **serum_creatinine** values from **0 - 2** with **time** values from **0 - 50**.\n- **serum_sodium** range of values from **130 - 140** record high number of cases for **DEATH_EVENT**."
### Correlation Matrix :
- It is a huge matrix with too many features. We will check the correlation only with respect to **DEATH_EVENT**. 
- It is a huge matrix with too many features. We will check the correlation only with respect to **DEATH_EVENT**. 
"- Features like **high_blood_pressure**, **anaemia**, **creatinine_phosphokinase**, **diabetes**, **sex**, **smoking**, and **platelets** do not display any kind of correlation with **DEATH_EVENT**."
#### Mutual Information Test :
#### Chi Squared Test :
#### Chi Squared Test :
"- According to the above tests, none of the features should be selected for modeling."
#### ANOVA Test :
"- According to the test, **platelets** and **creatinine_phosphokinase** need to be left out for modeling. \n\n\n- We will create 2 models : 1) Based on the statistical test 2) Based on the domain information. \n\n\n- According to the statistical tests, we will drop the following features : **anaemia, diabetes, high_blood_pressure, sex, smoking, creatinine_phosphokinase, platelets.**\n\n\n- According to the Domain Information, we will drop the following features : **sex, platelets.**"
- Selecting the features from the above conducted tests and splitting the data into **85 - 15 train - test** groups.
### 1] XGBoostClassifier :
"## 4. Regularization of Linear Regression \n\nThere are situations where we might intentionally increase the bias of the model for the sake of stability i.e. to reduce the variance of the model $\text{Var}\left(\widehat{f}\right)$. One of the conditions of the Gauss-Markov theorem is the full column rank of matrix $\textbf{X}$. Otherwise, the OLS solution $\textbf{w} = \left(\textbf{X}^\text{T} \textbf{X}\right)^{-1} \textbf{X}^\text{T} \textbf{y}$ does not exist since the inverse matrix $\left(\textbf{X}^\text{T} \textbf{X}\right)^{-1}$ does not exist. In other words, matrix $\textbf{X}^\text{T} \textbf{X}$ will be singular or degenerate. This problem is called an  ill-posed problem. Problems like this must be corrected, namely, matrix $\textbf{X}^\text{T} \textbf{X}$ needs to become non-degenerate, or regular (which is why this process is called regularization). Often we observe the so-called multicollinearity in the data: when two or more features are strongly correlated, it is manifested in the matrix $\textbf{X}$ in the form of ""almost"" linear dependence between the columns. For example, in the problem of predicting house prices by their parameters, attributes ""area with balcony"" and ""area without balcony"" will have an ""almost"" linear relationship. Formally, matrix $\textbf{X}^\text{T} \textbf{X}$ for such data is reversible, but, due to multicollinearity, some of its eigenvalues will be close to zero. In the inverse matrix $\textbf{X}^\text{T} \textbf{X}$, some extremely large eigenvalues will appear, as eigenvalues of the inverse matrix are $\frac{1}{\lambda_i}$. The result of this vacillation of eigenvalues is an unstable estimate of model parameters, i.e. adding a new set of observations to the training data will lead to a completely different solution. \nOne method of regularization is Tikhonov regularization, which generally looks like the addition of a new member to the mean squared error:\n\n$$\Large \begin{array}{rcl} \n\mathcal{L}\left(\textbf{X}, \textbf{y}, \textbf{w} \right) &=& \frac{1}{2n} \left\| \textbf{y} - \textbf{X} \textbf{w} \right\|_2^2 + \left\| \Gamma \textbf{w}\right\|^2\\\n\end{array}$$\n\nThe Tikhonov matrix is often expressed as the product of a number by the identity matrix: $\Gamma = \frac{\lambda}{2} E$. In this case, the problem of minimizing the mean squared error becomes a problem with a restriction on the $L_2$ norm. If we differentiate the new cost function with respect to the model parameters, set the resulting function to zero, and rearrange for  $\textbf{w}$, we get the exact solution of the problem.\n\n$$\Large \begin{array}{rcl} \n\textbf{w} &=& \left(\textbf{X}^{\text{T}} \textbf{X} + \lambda \textbf{E}\right)^{-1} \textbf{X}^{\text{T}} \textbf{y}\n\end{array}$$\n\nThis type of regression is called ridge regression. The ridge is the diagonal matrix that we add to the $\textbf{X}^\text{T} \textbf{X}$ matrix to ensure that we get a regular matrix as a result.\n\n\n\nSuch a solution reduces dispersion but becomes biased because the norm of the vector of parameters is also minimized, which makes the solution shift towards zero. On the figure below, the OLS solution is at the intersection of the white dotted lines. Blue dots represent different solutions of ridge regression. It can be seen that by increasing the regularization parameter $\lambda$, we shift the solution towards zero.\n\n\n"
## Support course creators\n\n\nYou can make a monthly (Patreon) or one-time (Ko-Fi) donation ‚Üì\n\n\n\n\n\n\n\n\n\n\n\n    \n
"# Analysis Time!\n\nOk the short inspection at the beginning give us some hints how should we move from here. I'm going to play with the data we have while analysing the data at the same time. With this way I hope we can get the data in better shape while digging deeper into it.\n\nWe're going to start with basic correlation table here. I dropped the top part since it's just mirror of the other part below. With this table we can understand some linear relations between different features.\n\n#### Observations:\n- There's strong relation between overall quality of the houses and their sale prices.\n- Again above grade living area seems strong indicator for sale price.\n- Garage features, number of baths and rooms, how old the building is etc. also having effect on the price on various levels too.\n- There are some obvious relations we gonna pass like total square feet affecting how many rooms there are or how many cars can fit into a garage vs. garage area etc.\n- Overall condition of the house seems less important on the pricing, it's interesting and worth digging.\n"
- **I'm going to merge the datasets here before we start editing it so we don't have to do these operations twice. Let's call it features since it has features only. So our data has 2919 observations and 79 features to begin with...**
"- **That's quite a lot! No need to panic though we got this. If you look at the data description given to us we can see that most of these missing data actually not missing, it's just means house doesn't have that specific feature, we can fix that easily...**"
"### **Ok this is how we gonna fix most of the missing data:**\n\n1. First we fill the NaN's in the columns where they mean 'None' so we gonna replace them with that,\n2. Then we fill numerical columns where missing values indicating there is no parent feature to measure, so we replace them with 0's.\n3. Even with these there are some actual missing data, by checking general trends of these features we can fill them with most frequent value(with mode).\n4. MSZoning part is little bit tricky I choose to fill them with most common type of the related MSSubClass type. It's not perfect but at least we decrease randomness a little bit.\n5. Again we fill the Lot Frontage with similar approach."
Now the same separately for Alice and everybody else.
"Now we definitely see that Alice mostly prefers 4-5 pm for browsing. So let's create features 'morning', 'day' and 'evening' and 'night'. Separators between these times of the day will be almost arbitrary: 0 am, 7 am, 12 am, and 7 pm. However, you can tune this."
## Preparing the Data
"The dataset is divided into separate files, one per weather attribute. To illustrate the encoding of cyclical features we will only be using the temperature data of a single city: Montreal."
"Great, it appears the absolute difference an `hour_sin` before, at and after midnight is now the same! However, if we look at the plot of `hour_sin` (following any flat line intersection with the graph), we can see there is a problem. If we consider just the one dimension, there are two records with exactly the same `hour_sin` values, e.g. records 11 and 25.\n\nThis is why we also need the cosine transformation, to separate these records from each other.\n\nIndeed, if we plot both features together in two dimensions we get the following:"
Exactly what we want: our cyclical data is encoded as a cycle. Perfect for presenting to our deep learning algorithm.
"Immediately evident is a dramatic improvement in our model's convergence rate. After only one epoch, the model using the encoded features has a validation loss comparable to the unencoded model's final validation loss."
How do the models compare on the validation data?
"Examining the raw data: \n\nTaking the raw data we were given of the 2017 and 2018 seasons, and examining each *unique* NFL athlete, I have compiled their college attended previous to their entry into the NFL, and ranked in order of the density of graduates that are in the NFL.  You see some colleges absolutely dominate.  Which means its EVEN harder to get into the NFL than the statistics shown above, because it looks like your best shot is by attending certain certain dominant colleges, which in turn have less and less room on their roster, lowering the player's chance of getting into the NFL.  Research also seems to indicate that approximately 85% of the NFL players were former **Division I college** students.  \n\nDoes it make more sense now why **101,821** people attended the LSU vs Alabama game (November 9th) ?  \n\nBetter yet, the front runner Heisman Trophy candidate [Joe Burrow](https://twitter.com/LSUfootball/status/1194754755123277824) ?  He attends LSU...\n(UPDATE:  Joe did go on to win the Heisman in an overwhelming fashion)\n\nPlot is interactive, click on a college and you will see its NFL player count and ranking..."
"> It gets worse:\n  * Even if you enter the NFL draft, it also depends on what **position** you play.  They may not need certain players depending on how the season before had gone and other external factors. \n\nThe 256 players that entered the NFL in the year 2018:  ¬† *Source: Wikipedia*\n\n"
"Summary of our dataset:\nIt is always important to look at our entire dataset and examine the descriptive statistics:\n\n‚ÄÇ **Number of football teams in the NFL:** ‚ÄÇ 32  \n‚ÄÇ **Number of unique NFL players in our dataset:** ‚ÄÇ 2,231  \n‚ÄÇ **Number of 2017 Season players:** ‚ÄÇ 1,788  \n‚ÄÇ **Number of 2018 Season players:** ‚ÄÇ 1,783   \n‚ÄÇ **Number of players playing both yrs:** ‚ÄÇ 1,340    \n‚ÄÇ **Number of players allowed per team:** ‚ÄÇ 53    \n‚ÄÇ **Number of games a team plays in a NFL season:** ‚ÄÇ 16      \n‚ÄÇ **Number of weeks in a NFL season:** ‚ÄÇ 17   \n‚ÄÇ **Total unique NFL games played per season:** ‚ÄÇ 256  \n‚ÄÇ **Number of NFL seasons in the dataset:** ‚ÄÇ 2  \n‚ÄÇ **Dataset NFL season years:** ‚ÄÇ 2017 and 2018 Seasons    \n‚ÄÇ **Dataset total number of unique NFL games:** ‚ÄÇ 512  \n‚ÄÇ **Number of unique run plays in our dataset:** ‚ÄÇ 23,171  \n‚ÄÇ **Number of 2017 Season run plays:** ‚ÄÇ 11,900  \n‚ÄÇ **Number of 2018 Season run plays:** ‚ÄÇ 11,271  \n‚ÄÇ **Number of unique NFL jersey numbers:** ‚ÄÇ 99  \n‚ÄÇ **Number of players on roster that never played:** ‚ÄÇ 11  \n‚ÄÇ **Size of a typical NFL field (in acres):** ‚ÄÇ 1.32"
It's Gametime...\n
"How much meaning are you really getting out of this visualization ??    \n\nThis seems to happen a lot when data is spread over a large range. The highest values 'drown' out the ability to see the relationships in the heights of the smaller values...  \n\nData in the field of data science seems to include data distributions like this a lot, perhaps we can fix this visualization issue.   You see plots like the above a fair amount. "
"There is nothing wrong with keeping the original plot, but a subplot should be created for the area 'east' of the Cut, to see the RELATIONSHIP between the data points.   \n\nI will now plot the datapoints east of the cut in its own subplot, for visualization clarity..."
"There is nothing wrong with keeping the original plot, but a subplot should be created for the area 'east' of the Cut, to see the RELATIONSHIP between the data points.   \n\nI will now plot the datapoints east of the cut in its own subplot, for visualization clarity..."
**Concept Reference**  -  You will see [this guy](https://mgoblue.com/roster.aspx?rp_id=19098) in the NFL some day\n  * Formulated during the viewing of [this](https://raw.githubusercontent.com/tombresee/Temp/master/ENTER/michvsnd.png) game (there was some beer involved)
**Concept Reference**  -  You will see [this guy](https://mgoblue.com/roster.aspx?rp_id=19098) in the NFL some day\n  * Formulated during the viewing of [this](https://raw.githubusercontent.com/tombresee/Temp/master/ENTER/michvsnd.png) game (there was some beer involved)
"Side Note:\n* I did considerable analysis to see if the windspeed would have any factor in the outcome of the running play, and found **no real evidence based on the data that it made any type of difference**\n* I would posit that windspeed and direction would have a considerable impact on passing plays though (go outside and play catch with someone when its windy, a football is not a baseball, even if you put zip on it a football will drift a bit under high wind conditions\n* I wish I had more temp and humidity data, my guess is that it would make a difference in yards gained as defensive linemen started getting fatigued, but then again the offensive line would get fatigued as well possibly the same amount, hard to say without the data \n* Personal Opinion:  IF one is introduced in doing a serious analysis of this data, and diving beyond the surface, I think its important to constantly keep your eye out for the types of distributions seen (age/height/weight may be gaussian for instance, but there are MANY features/attributes that are inherently skewed, even the actual yards gained).  Where am I going with this ?  You cannot apply the same advanced statistics principles on skewed data that you can on gaussian distributions.  But either way, a general guideline I follow here is that when a feature is skewed in some form, it is IMPORTANT to realize that a better measure of central tendency of the data is median over mean (average)...\n* I am guilty of initial bias:\n  * I was **convinced** that turf versus grass would make a difference in the running back performance, and yet I found no real evidence that running backs perform better on one surface versus the other.  I have no included this analysis but will probably post at some point.  \n  * I will wager a guess:  Based on my experience playing rugby, a *muddy* field seems to slow down even the fastest runners, and offers an advantage to the 'forwards' (think linemen), but the quality and conditions of NFL playing fields never really result in a truly muddy field, and thus to a certain extent it is likely that a well-kept grass field and a turf field are both going to allow the runner to run as fast as he desires... \n  * I read online extensively on this 'turf vs grass' debate, and there seems to be the understanding that neither field type really offers a substantial quantifiable advantage...so for now we will drop this line of discussion...\n"
"Let's now begin:  Initial Examination of overall running (rushing) yards per play:\n* Let's take a look at the most important feature, the yards, which we will need to be able to predict going forward after our machine learning model is created\n    "
"\nHistogram Hit:  \n\nIts important to understand that a histogram is a great starting point to examine the data's distribution, but there is some data that is smoothed out by doing this (as it is inherently a binning process).  It is understood that when examining extremely large datasets it is important to start somewhere, and the histogram in general has many many positives, but I'm creating the term `Histogram Hit` so it is understood it is somewhat of a smoothing process AND depending on the bin size chosen can steer the visualization in many different directions.  I am aiming to create an equation to quantity the actual 'hit' one takes when creating histogrames, but for now I'll introduce the term and come back to this at some later point.  Please understand that a histogram is an A-, its great, it is a quick way of visualizing data, but it is not flawless..."
2018 - Top 10 Longest Rushes:\n* Let's take a look at the top ten most spectacular rushes:\n
"The first was NOT a kickoff return, it was a **handoff**.  ¬† Now look at HOW far back he is in the endzone !   \nThe guy ran probably 107 yards on that play (but gets credit for 99 yards from line of scrimmage at their own 1 yard line; that day he ran for a total of a staggering 238 yards).  This tied the NFL record for longest rush, set 35 years earlier.   \n\n\nDerrick Henry from the Tennessee Titans is a [*beast*](https://twitter.com/NFL/status/1070863698791550976?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1070863698791550976&ref_url=https%3A%2F%2Fwww.theguardian.com%2Fsport%2F2018%2Fdec%2F06%2Fderrick-henry-touchdown-titans-jaguars-nfl-99-yards).  He is so money you could hold him sideways, swipe him across an ATM machine, and money would just keep streaming out for hours...\nThe best part:  He celebrated by striking the Heisman pose, which is perfectly fine, since he won it in 2015.  \n\n1. Derrick Henry - Tennessee Titans\n1. Lamar Miller	- Houston Texans \n1. Nick Chubb	- Cleveland Browns\n1. Adrian Peterson	- Washington Redskins	\n"
"The first was NOT a kickoff return, it was a **handoff**.  ¬† Now look at HOW far back he is in the endzone !   \nThe guy ran probably 107 yards on that play (but gets credit for 99 yards from line of scrimmage at their own 1 yard line; that day he ran for a total of a staggering 238 yards).  This tied the NFL record for longest rush, set 35 years earlier.   \n\n\nDerrick Henry from the Tennessee Titans is a [*beast*](https://twitter.com/NFL/status/1070863698791550976?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1070863698791550976&ref_url=https%3A%2F%2Fwww.theguardian.com%2Fsport%2F2018%2Fdec%2F06%2Fderrick-henry-touchdown-titans-jaguars-nfl-99-yards).  He is so money you could hold him sideways, swipe him across an ATM machine, and money would just keep streaming out for hours...\nThe best part:  He celebrated by striking the Heisman pose, which is perfectly fine, since he won it in 2015.  \n\n1. Derrick Henry - Tennessee Titans\n1. Lamar Miller	- Houston Texans \n1. Nick Chubb	- Cleveland Browns\n1. Adrian Peterson	- Washington Redskins	\n"
"Yards vs Down:\n* Plotting the distribution of yards by series 'Down'.  Note that I have configured many of my plots to show granular ultra-precise 1-yard increments ! \n* In this case, I have removed a few outliers so as to see the general trend of the data \n* I am doing this for a very specific reason, you will see in a sec    ‚ÄÇ - *Tom Bresee*\n    "
"* >Note: ¬† I am creating a new term I will call 'unit grid'. When creating plots where the y or x axis, depending on the plot, is in a relatively short magnitiude range (lets say approximately 20 units or below), I find it helps to actually use the grid lines to expand to the plot on a somewhat granular basis.  When that is needed or helpful, spacing the grid at 'unit' levels shall be known as 'unit grid', i.e. the grids are every 1 unit on the scale.  I think it helps the viewer quickly quantify actual values, to the point where it approaches the information transfer of a barplot..."
"* First and second down we see almost an identical distribution of runs (i.e. In the first two quarters, the teams run about the same distribution of runs in the quarters)\n  * What i find suprising is that in the first two quarters, examining the plots we see that 25% of the plays generated **less** than 1 yard total gained. Running the ball has risk, it does not always end with yards gained.   \n* In the third down, we see a slight drop in the number of yards gained, and the median yards gained has dropped a solid yard.  In a game of inches, this is a big deal. \n* Fourth down performance is relatively poor, but then again, most of the time a team does NOT run the ball on 4th down, due to the risk. "
"\nWarning: It is important to list the sample size for each of the histograms, because one may draw the erroneous conclusion that the number of times the ball was run was the 'same' for each of the downs, when in fact it wasn't..."
"![](http://)> **INSIGHT**: ¬† For a sample size of all NFL rush plays over the course of two entire years, only **7.83%** of the runs were on 3rd down, and less than **1%** were rushes on 4th down. "
"Yards vs Quarter of the Game:\n* Plotting the distribution of yards by game quarter, where 5Q symbolizes overtime..."
Yards Gained vs Box Defender Count:\n* Plotting the distribution of yards gained vs number of defenders in the box.  We will call this the defensive 'density' count...\n* A helpful reference image I created is shown below. 
"* One of course would see how the more 'dense' the box is, the more difficult it would be for a rusher to gain considerable yards\n* And now you know why the NFL tracks this stat:\n  * **8+ Defenders in the Box (8+D%):** ¬†  'On every play, Next Gen Stats calculates how many defenders are stacked in the box at snap. Using that logic, DIB% calculates how often does a rusher see 8 or more defenders in the box against them.'\n  * And thus we can insert a fairness factor, where rushers should be judged by how often they had a larger defensive 'density' employed against them..."
"* When there are nine defensive players in the box, 25% of the runs gained LESS than 0 yards, and half the runs were for LESS than 2 yards. \n* It is very rare for defenses to line up with four or less players, but when they do, the Offense seems to gain a fair amount of yards."
* Yards Gained vs Defensive Personnel 'Groupings':\n* Plotting the distribution of yards gained vs Defensive Formation...\n* First lets start by looking at the combined 2017/2018 dataset formations by play count (i.e. how many times in the two year season data that particular Defensive Schema were run)\n* We will then look exclusively look at 2018 stats
"Let's examine the same values, but broken out by percentage, i.e. what percentage of the time did the run play go against a particular DefensePersonnel Schema, and lets grab the top 10, since after that there is an extremely small percentage of plays incorporating that style: "
"Let's examine the same values, but broken out by percentage, i.e. what percentage of the time did the run play go against a particular DefensePersonnel Schema, and lets grab the top 10, since after that there is an extremely small percentage of plays incorporating that style: "
"**The Top Five Formations:**\n\n1. 4 DL, 2 LB, 5 DB\n  * 4 linemen, 2 linebackers, and 5 defensive backs (6 in the 'box')\n2. 4 DL, 3 LB, 4 DB	\n  * your conventional *4:3* type defense (7 in the 'box')\n3. 3 DL, 4 LB, 4 DB\n  * your conventional *3:4* type defense  (7 in the 'box')\n4. 2 DL, 4 LB, 5 DB	\n  * four linebackers, with only 2 guys on the line  (6 in the 'box')\n5. 3 DL, 3 LB, 5 DB\n  * a type of formation build to stop the pass (6 in the 'box')\n "
"**The Top Five Formations:**\n\n1. 4 DL, 2 LB, 5 DB\n  * 4 linemen, 2 linebackers, and 5 defensive backs (6 in the 'box')\n2. 4 DL, 3 LB, 4 DB	\n  * your conventional *4:3* type defense (7 in the 'box')\n3. 3 DL, 4 LB, 4 DB\n  * your conventional *3:4* type defense  (7 in the 'box')\n4. 2 DL, 4 LB, 5 DB	\n  * four linebackers, with only 2 guys on the line  (6 in the 'box')\n5. 3 DL, 3 LB, 5 DB\n  * a type of formation build to stop the pass (6 in the 'box')\n "
"**Very very interesting...**  \n* This analysis is important to understanding how defense is set up, its critical to understanding how to predict run productivity\n* The `#1` used defensive scheme actually resulted in slightly longer yardage plays, but also slightly higher yards lost for the offense\n* You can see how the first and second scheme is relatively good at containing the run, and as you get lower on the y-axis, you are giving up higher and higher yards\n* The most common is a 4-2-5, which is a good coverage against the pass (you have 5 DB). And since roughly 35-40% of NFL plays are runs, and the other percentage are pass, you can see why this is common. \n* With a median yardage gain allowed of 4 yards, its pretty good against the run, AND you can see that in some cases you can get losses of up to -6 yards. 75% of the runs against this defense are held to 6 yards or less. \n* The next most popular is your typical 4-3 defense, where you can see it holds runs to a bit shorter yardage, obviously since you have an extra linebacker involved in the tackling. But what i find interesting is that the third most common defense (3-4) has almost PRECISELY characteristics based on the data, look at the boxplots.  The 3-4 is run less than the 4-3 and the 4-2, but it seems to hold up pretty well against the run. The 3-4 in a flexible defense, and provides some great advantages when it comes to rushing the quarterback and defending against the pass. The 3-4 can be confusing for opposing quarterbacks, who find that wherever they look downfield there is a linebacker.  IF one could argue that the 3-4 is a better defense than the 4-3 in terms of rushing the QB, AND it holds up relatively well against the run (as it appears it does), then it would appear more teams SHOULD be running the 3-4 !  \n* The `#2` and `#3` most occurring run defense resulted in almost precisely the same running yards allowed distribution\n* Having 6 men on the line may appear to be a great idea against the run (and it does seems to 'squash' the run), you see that although it lowers the potential yards a runner could get, it offers no real ability to gain you negative yards on run plays, and its penetration ability are limited.  It does work well against runs, BUT if the play is a pass, you are devasted as you have very few DB to stop the pass. \n* When there are nine defensive players in the box, 25% of the runs gained LESS than 0 yards, and half the runs were for LESS than 2 yards. \n* It is very rare for defenses to line up with four or less players, but when they do, the Offense seems to gain a fair amount of yards."
"Number of run plays called per NFL game per team:\n* Histogram plot of the total number of run plays called per game per season.  \n* This takes into consideration every game played, where each team takes turns calling run plays (and contains both 2017 and 2018 data)\n"
"* The median number of times there is a run play per team in a game is 22, i.e. if a single running back was used, he would be running roughly 22 plays per game, but there is a fairly wide variation here from 10 up to about 40 plays in a game.  30 is considered a fair number of plays for a running back, beyond 40 is considered *extreme* for a single player...\n* Distribution appears to be bi-modal, where there is a peak at 20 and a peak at about 28 carries.  One could argue this could even be the difference between teams that run the ball a fair amount (as part of their offensive strategy), and those that choose to prefer the pass with a balance of some running plays to keep the defense off guard...\n* This does bring up the fact that to play in the NFL, as a premier running back you will be getting the ball many times, and **durability** becomes a major factor as the season goes on ! \n* Also, remember that the RB does not run every run play, sometimes there are substitutes made"
"Diving Deeper:  \n\nThe below plot is an exact count visualization of the number of run plays that occurred in a game, specifically in the entire 2018 season \n* By using the swarmplot, we see the precise distribution - and this gives a better representation of the distribution of values (where 1:1 viz, i.e. one dot is one game that had a specific count of run plays)\n* We also can **quickly** see the second, third, and fourth most run play count in a random game"
"* What we find interesting is the peaks are not **that** pronouced though, i.e. there are many teams that will run the ball 17, 18, 19, 20, up to 22 times in a single game, and also a fair amount of teams that will run the ball 24, 25, 26, up to 27 times in a single game...\n* It should be noted that this is an intriguing factor:\n  * In a single game, there are not a tremendous number of run plays either way, meaning our sample size per team per game of run plays is somewhat limited, so deriving a predictive model will contain many factors with a number of samples that is relatively small, offering a challenge..."
"Total Rushing Yards per NFL Team:\n* The following shows the total run yards per team, over the course of two individual seasons.  \n* I specifically use the total over two years to show the effect the running game can have on a team's performance.  I will eventually plot the yards on a per game average basiss, but but the point here is to show the vast amount of offensive yards that the top teams had over the others.  \n* The New England Patriots won the 2018 season superbowl (against the LA Rams).  I believe the running offense was a major factor in that. \n* **Note:** I include a new plotting term called **`icicles`** to enhance the visualization of barplots.  Using `icicles`, one can not clutter the plot excessively but still relay x-axis values superimposed onto the chart.  Thus it is not necessary to cover the entire plot with a grid, but rather only the section that specifically needs it and where it is pertinent.  \n  * *This term does not currently exist in mainstream visualization, I'm creating it.*"
"Correlation between PlayerWeight and JerseyNumber:\n*  If you run a correlation between generalized player weight and jersey number and position, you see high correlation, but why ? \n*  We accidentally have a good feature to use, which is on the surface jersey number shouldn't matter in any of this, **but** if we change JerseyNumber into a categorical bin (1-19), (20-29), we see that it can be quite helpful.  Because only certain positions are actually allowed to wear certain ranges of jersey numbers.  Thus during our modelling we will in fact include jersey number into bins.  \n"
Offense Formation Strategies:\n* We will now examine which offense formation strategies appeared to result in the best yardage gained
"* I would say there was a relatively large difference in the yards gained based on offensive scheme\n* **Wildcat** - doesn't appear to be that effective.  But it should be noted that it is not run very often in the NFL.  But when it is, it performs pretty poorly.\n* **Shotgun** - performs surprisingly low compared to the other offensive schemes.  One could argue it is a kinda pass play, but more offense* **Empty** - is the clear winner. 'Empty' simply means there is no back in the backfield.  All five eligible receivers are assembled at the line of scrimmage in some fashion.\n* The **I-formation** is one of the more tried and true offensive formations seen in football, and you will likely see it used in short-yardage running siturations.  The I-formation places the runningback 6 - 8 yards behind the line of scrimmage with the quarterback under the center and a fullback splitting them in a three-point stance;  which also means that it is highly likely the defense can see where the runningback is going, but then again, he will probably have a fair amount of speed by the time he hits the line of scrimmage.  "
Weight Distribution:\n* Let's dive into examining player weight information \n
"**Read very carefully:**\n* I specifically am *not* looking at the unique player distribution here and plotting their weights.  That is not what I am doing here.  I am gathering up the total number of running plays of all of the combined teams over the entire 2018 seasons, and I am creating a distribution of the weight of the runner who made the play (in orange), and also during those SAME plays, gathering up the weight distribution of those who did NOT run the ball.  I believe this thus gives us a very good idea of the weights of the players that were **on** the field during the season (broken out by rushing player versus non-rushing player), and starts to paint a picture of being able to predict the expected yards gained during running plays. \n  * I care about **who** is on the playing field here, that is the key for future prediction models. \n  * As long as the weights of the players are updated throughout the season, this also is an extremely granular way of determining kinetic energy on the field as well.\n  * I guess my real point is this - if they aren't on the field, or aren't on the field much, do I really care what their weight is when i figure out my model ? \n* Thus, of those that ran the ball in the 2018 NFL season, they had an average weight of **217 lbs**, and a median weight of 220 lbs. \n* Non-Runners had a pretty wide distribution, obviously depends on position they played...\n  * There is a pronounced peak at 310lbs, which is our linemen..."
"* I like this view (in order of descending median weight, by position).  You immediately see that all of the linemen are just over 300lbs. And they make up a LARGE distribution of the players on the field, i.e. there are some BIG BOYS on that field.  \n* I find it suprising that FB (fullbacks) are as heavy as they are.  I would imagine one could argue that two things are pretty critical to determining the performance of a running back: \n  * How big are the offensive linemen ???  (ideally we knew how strong they were as well, but no information contained about that)\n  * How big is the fullback ?  A fullback with some size would really help blocking for the running back and I believe would be directly proportional to the success of the runningback.  \n  * Look at how large the OTs (offensive tackles) are.  One would imagine a run off the OT being a smart play, IF the defensive linebacker at that area was smaller as well..."
"Collisions:\n* Collisions between dynamic bodies can either be elastic or inelastic\n* We will defined an `angle of attack`, similar to airfoil design.  This angle of attack will be the angle at which contact is made from the defender onto the offensive runningback.  We will then able able to also break out momentum and force into components with one common reference frame\n  * Assumption is that runner is running from left to right m\n* Every runningback should PREFER inelastic collisions !  \n  * Why ? **Because it creates the separation that is their advantage**\n  * How ? Ideally with an alpha less than 45 degrees - this way they bounce off and keep going somewhat in the x-axis direction, but ideally it is not an inelastic collision with alpha of 0 or near it, that is going to completely stop runningback momentum\n  * Effectively a runningback wants to maneuver, and when maeuvering is no longer much of an option, to 'bounce' off the tackler\n"
"* There is something here but I can't put my finger on it.  Inherent Cauchy distribution ?  Some version of Lorentz distribution ?  I think drinking might help here. \n* I think if we were able to dive deeper we would see this is something like a fire distribution\n   * I'm making the term up, but something to the effect of I make a fire and it has an intensity centerfied, but rising embers, or else there is nothing really here and its actually two sep distinct distributions superimposed\n   * But it seems to mimic this [click](https://www.shutterstock.com/video/clip-4927760-large-fire-burning-night-smoke-sparks-rising)"
Kinetic Energy:\n* An interesting thing occurs when we look at Kinetic Energy
Machine Learning Model
"*I'm going to have to spin up a second jupyter notebook to cover my machine learning model here, I still plan on adding a signficant amount of stuff above for EDA-related visuals...*  \n\n*I will include the link to the second notebook, but as it stands I still want to do more EDA here, so this notebook will be dedicated to EDA exclusively, but you see the Feature Engineering perspective as well here*"
# Preparation & peek at the data structure \n\n## Loading packages and setting 
### Settings
# Exploratory analysis \n\n## What do we know about our data? 
### Insights\n\n1. The number of image patches per patient varies a lot! **This leads to the questions whether all images show the same resolution of tissue cells of if this varies between patients**. \n2. Some patients have more than 80 % patches that show IDC! Consequently the tissue is full of cancer or only a part of the breast was covered by the tissue slice that is focused on the IDC cancer. **Does a tissue slice per patient cover the whole region of interest?**\n3. The **classes of IDC versus no IDC are imbalanced**. We have to check this again after setting up a validation strategy and find a strategy to deal with class weights (if we like to apply them).
### Cancer patches
### Healthy patches
### Healthy patches
### Insights\n\n* Sometimes we can find artifacts or incomplete patches that have smaller size than 50x50 pixels. \n* Patches with cancer look more violet and crowded than healthy ones. Is this really typical for cancer or is it more typical for ductal cells and tissue?\n* Though some of the healthy patches are very violet colored too!\n* Would be very interesting to hear what criteria are important for a pathologist.\n* I assume that the wholes in the tissue belong to the mammary ducts where the milk can flow through. 
### Binary target visualisation per tissue slice \n\nBefore we will take a look at the whole tissue let's keep it a bit simpler by looking at the target structure in the x-y-space for a handful of patients:
### Insights\n\n* Sometimes we don't have the full tissue information. It seems that tissue patches have been discarded or lost during preparation. \n* Reading the paper (link!) that seems to be related to this data this could also be part of the preprocessing.
Let's use an example patient with id 13616: 
"### Insights\n\n* The tissue on the left is shown without target information.\n* The image on the right shows the same tissue but cancer is stained with intensive red color. \n* Comparing both images it seems that darker, more violet colored tissue has a higher chance to be cancer than those with rose color. \n* But as one can see it's not always the case. So we need to ask ourselves if violet tissue patches have more mammary ducts than rose ones. If this is true we have to be careful. Our model might start to learn that mammary ducts are always related to cancer! \n\nSometimes it's not possible to load an image patch as the path is ill defined. But in our case, we were able to load them all:"
## Target distributions \n\nLet's take a look at the target distribution difference of the datasets: 
We can see that the test data has more cancer patches compared to healthy tissue patches than train or dev. We should keep this in mind!
Let's take a look at the augmentations:
For validation we have only used the image resizing.
## Building the training loop 
## Searching for an optimal cyclical learning rate \n\nThe learning rate is one of the most important hyperparameters for tuning neural networks. A rate that is too high will lead to jumps to higher values in the training loss during optimization. If it's too small the learning process is too slow and will probably stop too early in the case we have defined a minimum required loss change. Take a look at the paper [Cyclical Learning Rates for Training Neural Networks](https://arxiv.org/abs/1506.01186).
Importing the librarys
Importing the data
Let's start exploring the Funded and and Loan Amount
Cool. We have a normal distribuition to the both values.
Another interesting numerical values is the lender number and the term in months.\n\nI will start exploring further the Lenders_count column
We have a interesting distribuition...Or you have 1 lender or you will have a great chance to chave through 4 ~ 12 lenders in the project
We have a interesting distribuition...Or you have 1 lender or you will have a great chance to chave through 4 ~ 12 lenders in the project
"Curious... In a ""normal"" loan the term almost ever is 6 ~ 12 ~ 24 and so on. \nIt's the first time that I see 8 ~ 14 ~ 20; Very curious.\n"
Let's look through the Sectors to known them
"Very cool graph. It show us that the highest mean values ins't to the most frequent Sectors.... Transportation, Arts and Servies have a low frequency but a high mean. "
Now I will look some values through the sectors.
"The values have an equal distribution through all data, being little small to Personal Use, that make sense.\n\nThe highest Term months is to Agriculture, Education and Health.\n\n"
" Taking advantage of sectors, let's look the Acitivities'"
We can see that the activities with highest mean loan amount aren't the same as more frequent...\nThis is an interesting distribution of Acitivies but it isn't so meaningful... Let's further to understand this.
Now I will explore the activies by the top 3 sectors
Looking the activities by Sector is more insightful than just look through the sectors or activities
Another important value on this Dataset is the Repayment Interval.... \nMight it is very meaningful about the loans to sectors  
Humm.. An high number of loans have a Irregular Repayment... It's very interesting.... \nLet's explore further the Sector's
 I will plot the distribuition of loan by Repayment Interval to see if they have the same distribuition 
And with the Lender Count? How is the distribuition of lenders over the Repayment Interval
And with the Lender Count? How is the distribuition of lenders over the Repayment Interval
"Intesresting behavior of Irregular Payments, this have a little differenc... The first peak in lenders distribuition is about the zero values that I add 1."
Let's take a better look on Sectors and Repayment Intervals in this heatmap of correlation
"We have 3 sector's that have a high number of Irregular Repayments. Why this difference, just because is the most frequent ? "
And what's the most frequent countrys? 
"The highest mean value without the filter is Cote D'Ivoire is 50000 because have just 1 loan to this country, that was lended by 1706 lenders, how we can look below. "
"If you want to see the heatmap correlations below, click on Show Output"
On this heatmap correlation above we can see that just Kenya have Weekly payments and Kenya have the highest number of Irregular payments
On this heatmap correlation above we can see that just Kenya have Weekly payments and Kenya have the highest number of Irregular payments
We can look a lot of interesting values on this heatmap.\n- Philipines have high number of loan in almost all sectors\n- The USA is the country with the highest number of Entertainment loans\n- Cambodia have the highest number of Loans to Personal Use\n- Paraguay is the country with highest Education Loan request\n- Pakistan have highest loan requests to Art and Whosale\n- Tajikistan have highest requests in Education and Health\n- Kenya and Philipines have high loan requests to Construction\n- Kenya also have high numbers to Services Loans
Let's verify the most frequent currency's 
PHP THAT IS THE CURRENCY PHILIPINE\nUSD EVERYONE KNOWS\nKES THAT IS THE CURRENCY OF KENYA
"# Bank Customer Segmentation\n\nIn this kernel I will perform segmentation of German bank customers. The first step is to read necessary libraries. We will use: \n* [pandas](https://pandas.pydata.org/) - to manipulate data frames\n* [numpy](http://www.numpy.org/) - providing linear algebra\n* [seaborm](https://seaborn.pydata.org/) - to create nice visualizations\n* [matplotlib](https://matplotlib.org/) - basic tools for visualizations\n* [scikit-learn](https://scikit-learn.org/stable/) - machine learning library\n\nFrom sklearn, I will import necessary pre-processing tools and two clustering algorithms: [KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) and [Affinity Propagation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AffinityPropagation.html).\n"
Reading the raw data 
**Exploratory Data Analysis**\n\nBelow I will define a function which will generate plots for three numeric variables with stratification by selected categorical column.
At the beginning let‚Äôs look at scatter plots our 3 numerical variables stratified by sex.
"The general impression is that women tend to be younger than men, however, the top plot shows that there is no clear difference between men and women in terms of amount and duration of the credit. From visual inspection, it seems that there is some positive correlation between duration and amount of credit, what makes sense. \n\nLet‚Äôs check the linear correlation between credit amount and duration"
"The plot above shows a linear correlation with Pearson value of 0.62 and very small p-value. That make‚Äôs sense because usually, people take bigger credits for longer periods.  Below I will analyse linear regression plots with various categorisations."
"The plot above shows a linear correlation with Pearson value of 0.62 and very small p-value. That make‚Äôs sense because usually, people take bigger credits for longer periods.  Below I will analyse linear regression plots with various categorisations."
The plot above indicates that there is no significant difference between men and women.
The plot above indicates that there is no significant difference between men and women.
The plot above shows similarly that there is no diference betwen housing categories.\n\nBelow I will show ‚Äúbusiness‚Äù area where granted the biggest amount of credits. 
"In terms of job category once again there is no difference between men and women, but we can see that job category 3 tends to take bigger credit amounts for longer duration.  \n\nAnd at the end if someone likes 3D plots here you go."
"**Clustering with KMeans**\n\nFor clustering, I will create a subset containing only numerical variables (Age, Credit amount, Duration). "
I will create a function which plots three histograms - one for each variable. 
Let's look at the histograms.
I will check how inertia changes for various number of clusters.
The plot above shows that inertia decreases with increasing number of clusters. \n\nThis method allows for assessment of cluster separations and fitting of each observation in its own cluster.  The highest score the better.  I will perform this analysis for various seeds as well.
The plot above shows that inertia decreases with increasing number of clusters. \n\nThis method allows for assessment of cluster separations and fitting of each observation in its own cluster.  The highest score the better.  I will perform this analysis for various seeds as well.
The heatmap above shows silhouette scores for various combinations of random state and number of clusters. The highest scores are for 2 and 3 clusters and they are relatively insensitive to seed. \n\nI will chose 3 clusters to get more insight into data.
Below I will create silhouette graph for 3 clusters in order to visually depict fit of each point within its own cluster (modified code from scikit-learn doc).
I will define a function showing clusters on the scatter plot.  
"In this algorithm there are two relevant parameters: preference and dumping. It means that we don‚Äôt define upfront number of clusters, algorithm itself chooses their number. I will fix dumping and check number of clusters in function of preference parameter.  "
Together with decreasing value of preference parameter number of clusters goes down as well and levels for very small preference values. I will check four clusters option. 
### Import required libraries
### Load models
### Image Label Groups by No. of Images
\n\nBasic Image Exploration
We shall check if the provided images have the same dimension or not. I'll use Dask to parallelize the operation and speed things up.
### K-means Clustering
This is unusual as we most of the time see data splits as 80:20 but in our case it is 1:16 which indicates that the competition creators are encouraging participants to generate their own data.
"The target labels; 1 if the data contains the presence of a gravitational wave, 0 otherwise. (Please note the presence of a small number of files labeled -1. Physicists are currently unable to determine the status of these files.)"
## Frequencies Distribution
## Time stamps Distribution
## Time stamps Distribution
"## Simulating gravitational waves and evaluating their detectability in Python\n\nRiroriro is a Python package to simulate the gravitational waveforms of binary mergers of black holes and/or neutron stars, and calculate several properties of these mergers and waveforms, specifically relating to their observability by gravitational wave detectors.""\n\n""The gravitational waveform simulation of Riroriro is based upon the methods of Buskirk and Babiuc-Hamilton (2019), a paper which describes a computational implementation of an earlier theoretical gravitational waveform model by Huerta et al. (2017), using post-Newtonian expansions and an approximation called the implicit rotating source to simplify the Einstein field equations and simulate gravitational waves. Riroriro's calculation of signal-to-noise ratios (SNR) of gravitational wave events is based on the methods of Barrett et al. (2018), with the simpler gravitational wave model Findchirp (Allen et al. (2012)) being used for comparison and calibration in these calculations.""\n\nLink for the paper\n\n#### cc. of GW simulation: this part was written by Geir Drange and presented by Mar√≠lia Prata\n\n\n\n\n""Riroriro is a set of Python modules containing functions to simulate the gravitational waveforms of mergers of black holes and/or neutron stars, and calculate several properties of these mergers and waveforms, specifically relating to their observability by gravitational wave detectors. Riroriro combines areas covered by previous gravitational wave models (such as gravitational wave simulation, SNR calculation, horizon distance calculation) into a single package with broader scope and versatility in Python, a programming language that is ubiquitous in astronomy. Aside from being a research tool, Riroriro is also designed to be easy to use and modify, and it can also be used as an educational tool for students learning about gravitational waves.""\n\n""The modules ‚Äúinspiralfuns‚Äù, ‚Äúmergerfirstfuns‚Äù, ‚Äúmatchingfuns‚Äù, ‚Äúmergersecondfuns‚Äù and ‚Äúgwexporter‚Äù, in that order, can be used to simulate the strain amplitude and frequency of a merger gravitational waveform. The module ‚Äúsnrcalculatorfuns‚Äù can compare such a simulated waveform to a detector noise spectrum to calculate a signal-to-noise ratio (SNR) for that signal for that detector. The module ‚Äúhorizondistfuns‚Äù calculates the horizon distance of a merger given its waveform, and the module ‚Äúdetectabilityfuns‚Äù evaluates the detectability of a merger given its SNR.""\n\nMore information on the pip installation can be found here: https://pypi.org/project/riroriro/\n\nTutorials for Riroriro can be found here: https://github.com/wvanzeist/riroriro_tutorials\n\nFull documentation of each of the functions of Riroriro can be found here: https://wvanzeist.github.io/\n\nhttps://github.com/wvanzeist/riroriro\n"
"### How Gravitational waves get detected\n\n""When a gravitational wave passes by Earth, it squeezes and stretches space. LIGO can detect this squeezing and stretching. Each LIGO observatory has two ‚Äúarms‚Äù that are each more than 2 miles (4 kilometers) long. A passing gravitational wave causes the length of the arms to change slightly. The observatory uses lasers, mirrors, and extremely sensitive instruments to detect these tiny changes."""
### Resample the signal to 2048Hz (only the orthogonal part)
### Resample the signal to 2048Hz (only the orthogonal part)
### Visualizing Frequency Vector
### Visualizing Frequency Vector
### Visualizing spectrum in frequency domain using Constant-Q transform
### Visualizing spectrum in frequency domain using Constant-Q transform
### Amplitude vs. Distance(Inverse square law verification)
### Amplitude vs. Distance(Inverse square law verification)
"### Why the surprising results?\n\nSurprisingly we can notice that gravitational waves amplitude doesn't follow the inverse square law but why?\n\nIn order to answer this question we have to explain the difference between monopolic, diapolic and quadrapolic signals.\nFirst off, there are fundamental ways that light and gravitational waves are the same. They both:\n\n* do carry energy,\n* do reach infinite distances,\n* do spread out over space (in roughly a sphere) as you move farther away,\n* and will be detectable, at a certain distance, in proportion to the magnitude of the signal.\n\nBecause the geometry of space is the same for both light and gravitation, the difference between these two behaviors must lie in the nature of the signal that we can detect.\n\nTo understand that, we need to understand how gravity is a fundamentally different kind of force than electromagnetism. This will lead us to better understand how gravitational radiation (our gravitational waves) behave differently than electromagnetic radiation (light) when we allow it to propagate across the vast distances of intergalactic space.\n\n\n\nIf you want to create electromagnetic or gravitational radiation, how could you do it? The simplest way you could imagine ‚Äî which (spoiler) doesn't work ‚Äî would be to spontaneously create or destroy charge in a region of space. Having a charge pop into (or out of) existence would create radiation of a very specific type: monopole radiation. Monopole radiation is what happens when you have a change in the amount of charge that's present.\n\n\n\nWe cannot do this for either electromagnetism or gravitation, however. In electromagnetism, electric charge is conserved; in gravitation, mass/energy is conserved. The fact that we don't get monopole radiation is important for the stability of our Universe. If charge or mass could spontaneously be created or destroyed, existence would be extremely different!\n\nIf charge and mass/energy are conserved, then the next step is to either move your charges (or masses) rapidly back-and-forth, or to take charges of opposite signs and change the distance between them. This would create what we call dipole radiation, which changes the distribution of charge without changing the total amount of charge.\n\nIn electromagnetism, this creates radiation, because moving an electric charge back-and-forth changes the electric and magnetic fields together. This matters, because changing electric and magnetic fields that are mutually perpendicular to each other and in-phase if wis what an electromagnetic wave actually is. This is the simplest way to make light, and it radiates just like you're familiar with. The light carries energy, and the energy is what we detect, which is why objects appear dimmer as 1/r2 the farther away they are.\n\n\n\nIn gravity, however, freely moving a mass doesn't make gravitational radiation, because there's a conservation rule about masses in motion: the conservation of momentum. Similarly, separating masses doesn't make gravitational radiation either, because the center of mass remains constant. There's also a conservation rule about masses moving at a certain distance from the center of mass: the conservation of angular momentum.\n\nBecause energy, momentum, and angular momentum are conserved, you have to go past both monopole and dipole moments; you need a specific change in how the masses are distributed around their mutual center of mass. The simplest way to imagine this is to take two masses and have them mutually rotate around their center of mass, which results in what we call quadrupole radiation.\n\n\n\nThe amplitude of gravitational quadrupolar radiation falls off as 1/r, meaning the total energy falls off as 1/r2, just as it did for electromagnetic radiation. But this is where the fundamental difference between gravitation and electromagnetism comes in. There's a big difference between what you can physically detect for quadrupole and dipole radiation.\n\nFor electromagnetic (dipole) radiation, when the photons hit your detectors, they get absorbed, causing a change in the energy levels, and that change in energy ‚Äî which remember, falls off as 1/r2 ‚Äî is the signal you observe. That's why objects appear to dim according to an inverse square law.\n\nFor gravitational (quadrupole) radiation, however, it doesn't get directly absorbed in a detector. Rather, it causes objects to move towards or apart from one another in proportion to the amplitude of the wave. Even though the energy falls off as 1/r2, the amplitude only falls off as 1/r. That's why gravitational waves fall off according to a different law than electromagnetic waves.\n\n\n\nBut the amplitude, as we received it, compressed and expanded the entire Earth by about the diameter of three protons. The energy is huge and falls off as 1/r2, but we cannot detect energy for gravitational waves. We can only detect amplitude, which (thankfully) only falls off as 1/r, which is a very good thing. The amplitudes may be tiny, but if we can detect any signal at all, it's only a small step forward to detecting that same magnitude signal at any distance.\n\n For more check this! "
## Binary Cross Entropy Loss Curve
"## Notes about Frequency Domain modeling\n\n* As expected the CNN model could converge faster 3x than the ViT model as the available dataset size isn't the ideal for such architecture(we's talking about 600 instances here) which is based on vision transformers which in turn needs more data than CNN to work properly. \n\n* The model loss curve is descending but it doesn't mean that the model is learning well, that's why further INVESTIGATION and IMPROVEMENTS shall be done on this work to find out if the imbalanced classes is the main issue here or if we need to try feature extraction techniques(noise cancelation filters, etc.) other than using the SFT's available. One of the possible improvements here is to use pre-trained model and only fine tune it, this shall overcome data limitation  and other issues."
"\n  Background\n\nSince 1920 the National Football League has developed the model for a successful modern sports league. The NFL enterprise includes national and international distribution, extensive revenue sharing, competitive excellence, and strong franchises across the country. Last year, 29 of the NFL's 32 teams appeared in the Forbes Top 50 most valuable sports franchises in the world. Football is also regarded as the United States' most popular sport. A 2018 Gallup poll found that among US adults, 37% name football as their favorite sport to watch. The number tops basketaball(11%), baseball(9%) and soccer(7%) by a wide margin. \n\nPlayer safety has always been a concern for football. Over the past few years, concussions sustained during play have become one of the most visible issues affecting players. Concussion incidents rose in 2017 even as the NFL implemented several safety measures. The figure below shows the incidence of concussions since 2012. \n\n Hover over the points to see exact numbers. "
"\nKick and punt plays have historically posed the highest risk of concussion to players. During the 2015-2017 seasons, the kickoff represented only six percent of plays but 12 percent of concussions, making the risk four times greater than running or passing plays. In response, the NFL revised its kicking rules for 2018 to reduce risk during kickoffs. \n\nThe NFL is now increasing the attention given to punt plays. According to NFL executive Jeff Miller, concussion risk during punts is twice that of running or passing plays. In response the NFL is sponsoring this competition as part of an overall effort to make punt plays safer. The goal of the competition is to discover specific rule modifications, supported by data, that may reduce the occurrence of concussions during punt plays."
"The figures and table below show that returned punts are by far the most common outcome for both the absolute number of concussions and the percent of concussions for that play type. Fair catches, by contrast are low on both counts. \n\n\n Hover over the bars to see exact numbers. "
The NFL would most likely see fewer concussions if some returned punts are shifted to other outcomes by changing the rules. This is consistent with my findings in the next section that over half of the recorded concussons occurred near the punt returner.\n\nAnother important consideration here was to see yards gained from returned punts. The figure below shows yards gained on each punt when a returner gets control of the ball with the intent to move forward. It does not include muffed catches. The vertical line in the figure indicates the median of all returns.
" Opportunity Area 2. Less Harmful Contact.\n\nFor this issue I looked at the types of collisions that occurred, where they occurred,  and the players involved. The figure below shows the numerous play flows that result in concussion. Players receiving a concussion appear to the left and primary partners appear to the right. The width of the flowpaths indicates the number of concussions occurring with that combination. For example, offensive linemen from the punting team received 9 concussions while tackling. 8 of those were while the returner was tackled, as one might expect. The 9th was from a defensive lineman blocking the offensive lineman during a tackle.\n\n Hover over flowpaths to see exact values. "
"The above diagram shows a variety of scenarios in which concussions occur. One can see by the green box to the left that 20 offensive linemen sustained concussions from tackling and being blocked. Although punt returners received the most concussions as a single-person role, offensive linemen collectively received over half the concussions during the two-season period. Gunners (typically two per play) also collectively received more concussions than most roles."
## Import Python libraries
### Check file size
"## Plot explained variance ratio with number of dimensions\n\n- An alternative option is to plot the explained variance as a function of the number of dimensions.\n\n- In the plot, we should look for an elbow where the explained variance stops growing fast.\n\n- This can be thought of as the intrinsic dimensionality of the dataset.\n\n- Now, I will plot cumulative explained variance ratio with number of components to show how variance ratio varies with number of components."
### Comment\n\nThe above plot shows that almost 90% of variance is explained by the first 12 components.
# State Feature\n- I will start looking the state column distribuition that might will be our key to understand this dataset
"Very interesting distribution ! \nwe can see that only 35,38% of all projects got sucess.\nMore than 60% have failed or canceled; \n\nI think that the most important category's here is failed and canceled;\n\nMaybe we can build an model to predict if a project would obtain the money or not;\n\n\n\n# Let's start looking our Project values\n- I will start exploring the distribuition logarithmn of these values"
# Goal Distribution\n- Let's plot and analyze the distribution of goal that failed and successful projects.
"Interesting difference between Pledged and Goal distribuition! \n\nGoal seems a normal distribution, so it would be good if we test the normality of it and also, if exists some statistical difference between failed and success projects. \n\nLet start with the normality test of goal"
"# Normality Test\n- Although our data seems normal distributed, its good to do a test and be sure that it is true\n- So, before we go further, lets test if the distribution of or goal is normal distributed"
"Nice. \nBased on the result, we can see that the data isn't normal distributed;"
Looking the State variable\n- pledge log by state\n- goal log by state\n- goal log x pledged log
"Cool. As we saw in your stastical test, the difference between "
We have a very interesting distribuition in goal values.
## I will take a further look at top 10 sucessful and failed categorys.\nI will look at:\n- Goal\n- Pledged\n- diff_pleded_goal 
## I will take a further look at top 10 sucessful and failed categorys.\nI will look at:\n- Goal\n- Pledged\n- diff_pleded_goal 
We can see that almost all categorys in sucessful have the same distribuition of values but some video games projects have the highest values in % difference of Pledged by Goal 
Now I will start to Investigating the 3 top sucess and fail projects\n
Main Category
Main Category
"In the musics with sucess the most frequent is Indie, and fails is Rock and Hip Hop! \n\nAnother interesting thing, is that Documentary is a significant value in both states... "
# Months to Campaign
The most part of projects have 1 month of campaign. We can see that the ratio of successful one month campaigns is better than projects with 1.5 or 2 months of campaign
## Launched Year Distributions
Cool. We can note that 
## Launched Months Distributions
We can note that all months are very similar. 
# Goals 
"Humm... Its an very interesting information.\nOn the first chart, we can clearly see that projects with more than 30 to 60 days have highest vales pledged, what make many sense. \nAlso, we can see that the median of goal reached is like to 120;  Let's s"
# Models Pipeline
"Cool Gradient Boosting, XGB have the best results so I will select them. \nAlso, I will seelect the Logistic Regression too. \n\nThe Decision tree and Extra trees are the models with the lowest roc_auc scores."
### Importing Libraries
### Importing Dataset
NOTE:\n- Higher class passengers (low `Pcass`) have better average survival than the low class(high `Pclass`) passengers.
### 2) Sex vs. Survival
"### 4) Pclass, Sex & Embarked vs. Survival"
"NOTE:From the above plot, it can be seen that:\n- Almost all females from Pclass 1 and 2 survived.\n- Females dying were mostly from 3rd Pclass.\n- Males from Pclass 1 only have slightly higher survival chance than Pclass 2 and 3."
### 8) Age vs. Survival
"NOTE:1) From *`Pclass`* violinplot, we can see that:\n- 1st Pclass has very few children as compared to other two classes.\n- 1st Plcass has more old people as compared to other two classes.\n- Almost all children (between age 0 to 10) of 2nd Pclass survived.\n- Most children of 3rd Pclass survived.\n- Younger people of 1st Pclass survived as compared to its older people.\n\n2) From *`Sex`* violinplot, we can see that:\n- Most male children (between age 0 to 14) survived.\n- Females with age between 18 to 40 have better survival chance."
## Correlating Features
"NOTE:Heatmap of Correlation between different features:\n\n>Positive numbers = Positive correlation, i.e. increase in one feature will increase the other feature & vice-versa.\n>Negative numbers = Negative correlation, i.e. increase in one feature will decrease the other feature & vice-versa.\n\nIn our case, we focus on which features have strong positive or negative correlation with the *Survived* feature."
"> In the example code below, we plot a confusion matrix for the prediction of ***`Random Forest Classifier`*** on our training dataset. This shows how many entries are correctly and incorrectly predicted by our classifer."
## Comparing Models\n> Let's compare the accuracy score of all the classifier models used above.
**Import libraries**\n====================
**Load train & test data**\n====================
**Heatmap**\n-----------
As we saw above there are few feature which shows high multicollinearity from heatmap.\nLets focus on yellow squares on diagonal line and few on the sides.\n\nSalePrice and OverallQual\n\nGarageArea and GarageCars\n\nTotalBsmtSF and 1stFlrSF\n\nGrLiveArea and TotRmsAbvGrd\n\nYearBulit and GarageYrBlt\n\nWe have to create a single feature from them before we use them as predictors.\n
*Univariate Analysis*\n--------------------\n\nHow 1 single variable is distributed in numeric range.\nWhat is statistical summary of it.\nIs it positively skewed or negatively.
Prices are right skewed and  graph shows some peakedness.
*Alley*\n-------
All missing value indicate that particular house doesn't have an alley access.we can replace it with 'None'.
*Fireplaces*\n------------
Having 2 fireplaces increases house price and fireplace of Excellent quality is a big plus. 
*Pool*\n-----------------------
*Fence*\n-----------------------
*Fence*\n-----------------------
Fence has got 1179 null values.\nWe can safely assume that those houses doesn't have a Fence and replace those values with None.
*MSZoning*\n-----------
*1st Floor in square feet*\n--------------------------
*1st Floor in square feet*\n--------------------------
*Ground Living Area w.r.t SalePrice*\n--------------------
*Ground Living Area w.r.t SalePrice*\n--------------------
*SalePrice per square foot*\n--------------------
*Garage Area*\n-------------
"*Building , remodelling years and age of house*\n----------------------------------------"
*Heating and AC arrangements*\n-----------------------------
Having AC definitely escalates price of house.
*Total rooms above grade*\n-------------------------
*Kitchen Quality*\n=================
*Kitchen Quality*\n=================
Having 1 Kitchen of Excellent quality hikes house price like anything.
*Neighbourhood*\n--------------
*Overall Quality*\n-----------------
*Overall Quality*\n-----------------
*2nd Floor with SalePrice*\n--------------------------
*2nd Floor with SalePrice*\n--------------------------
*Street*\n--------
*Street*\n--------
More to come .. Watch this space.
\n\nIMPORT NECESSARY LIBRARIES
\n\nLOAD DATASETS
\n\n Kurtosis grater than 3\n    \nIt is leptokurtic. It will signify that it produces outliers rather than a normal distribution.
"\n\n The graph shows positive skewness\n    \nWe see positive skewness from the graph above. As the graphs shows, more weight is on the left side of the distribution. We will fix it using 'norm' function and ""log1p"" function of numpy"
## Import necessary libraries
## Data loading\nLoad all provided datasets and get a feel of the data provided to us
"# >> 5. Using a linear regression. \n\nBased on the existing data, one can calculate the best fit line between two variables, from checking correlations.\n\nIt is worth mentioning that linear regression models are sensitive to outliers."
- Interesting corelation!
Other Related Notebooks
References
# Training Function
# Validation Function
### The tennis matches data 
"The dataset ""atp_data.csv"" is directly built from the data you can find on http://tennis-data.co.uk/data.php. I selected the columns that will be useful for the betting model, and added the elo rankings of the players at the beginning of each match.\n\nWe have all the matches played on the ATP World Tour from January 2000 to March 2018.\n\nOne row per match. And we have some information about each match :"
# Assessment of some basic strategies 
## Betting on all matches 
"According to the curve above, betting on 10% of the matches leads to an average ROI of 58%.\n\nWe decide to bet on **10%** of the matches : the 10% with the highest confidence between 2013 and 2018 (~1100 matches).\n\nWe have to check first that these matches are well spread between 2013 and 2018."
The matches we are the most confident in are well spread accross the study period.\n\nThat indicates a certain stability in the betting market.\n\nNow let's see our ROI for the consecutive sections of 100 matches between 2013 and 2018.
The matches we are the most confident in are well spread accross the study period.\n\nThat indicates a certain stability in the betting market.\n\nNow let's see our ROI for the consecutive sections of 100 matches between 2013 and 2018.
"There is a great variability but the process seems quite stationary. \n\nTo sum up, a strategy that gives a good ROI in 2013 still gives a good ROI in 2018, which is quite reassuring.\n\nBut such a variability would force us to wait much more that 117 matches to guarantee the 58% ROI...\n\n**Now let's bet on 35% of the matches (~3850 matches). The average ROI over our study period in 20%.**"
**Import the Libraries**
**Import Data**
Correlation matrix between numerical values:
Correlations between numerical variables and Survived aren't so high but it doesn't mean that the other features are not useful.
Embarked vs Survived
##**Missing values**
Let's check how the distribution of survival variable  depending on the title.
People with 'Master' have the highest survival rate. Maybe because people with the master are mainly boys under 13 years old.
"## Dataset\n\nBefore beginning the analysis, we will load and view the dataset, and perform some initial cleaning.\n\n* View the dataset info:"
* Clean up column names\n* Transform selected columns to numeric format:\n    - `Income` to float
"#### Outliers\n\n* Identify features containing outliers:\n    - Findings: Multiple features contain outliers (see boxplots below), but the only that likely indicate data entry errors are `Year_Birth <= 1900`"
* Remove rows where `Year_Birth <= 1900`:
* Remove rows where `Year_Birth <= 1900`:
## Are there any variables that warrant transformations?\n\n* View data types:\n    - Findings: The `Dt_Customer` column should be transformed to datetime format
"### Do you notice any patterns or anomalies in the data? Can you plot them?\n\n* To identify patterns, we will first identify feature correlations. Positive correlations between features appear red, negative correlations appear blue, and no correlation appears grey in the clustered heatmap below.\n* From this heatmap we can observe the following clusters of correlated features:\n    - The **""High Income""** cluster:\n        - Amount spent ('TotalMnt' and other 'Mnt' features) and number of purchases ('TotalPurchases' and other 'Num...Purchases' features) are positively correlated with 'Income'\n        - Purchasing in store, on the web, or via the catalog ('NumStorePurchases', 'NumWebPurchases', 'NumCatalogPurchases') is positively correlated with 'Income'\n    - The **""Have Kids & Teens""** cluster:\n        - Amount spent ('TotalMnt' and other 'Mnt' features) and number of purchases ('TotalPurchases' and other 'Num...Purchases' features) are negatively correlated with 'Dependents' (with a stronger effect from kids *vs.* teens)\n        - Purchasing deals ('NumDealsPurchases') is positively correlated with 'Dependents' (kids and/or teens) and negatively correlated with 'Income'\n    - The **""Advertising Campaigns""** cluster:\n        - Acceptance of the advertising campaigns ('AcceptedCmp' and 'Response') are strongly positively correlated with each other\n        - Weak positive correlation of the advertising campaigns is seen with the ""High Income"" cluster, and weak negative correlation is seen with the ""Have Kids & Teens"" cluster\n* Anomalies:\n    - Surprisingly, the number of website visits in the last month ('NumWebVisitsMonth') does not correlate with an increased number of web purchases ('NumWebPurchases')\n    - Instead, 'NumWebVisitsMonth' is positively correlated with the number of deals purchased ('NumDealsPurchases'), suggesting that  suggesting that deals are an effective way of stimulating purchases on the website"
"* Plot illustrating the effect of high income on spending:\n\nNote: For the purposes of this plot, limiting income to < 200000 to remove outlier"
"* Plot illustrating the effect of high income on spending:\n\nNote: For the purposes of this plot, limiting income to < 200000 to remove outlier"
* Plot illustrating negative effect of having dependents (kids & teens) on spending:
* Plot illustrating negative effect of having dependents (kids & teens) on spending:
* Plot illustrating positive effect of having dependents (kids & teens) on number of deals purchased:
* Plot illustrating positive effect of having dependents (kids & teens) on number of deals purchased:
"* Plots illustrating the positive effect of income and negative effect of having kids & teens on advertising campaign acceptance:\n\nNote: For the purposes of the following plot, limiting income to < 200000 to remove outlier"
"# Section 02: Statistical Analysis\n\nPlease run statistical tests in the form of regressions to answer these questions & propose data-driven action recommendations to your CMO. Make sure to interpret your results with non-statistical jargon so your CMO can understand your findings.  \n\n### What factors are significantly related to the number of store purchases?  \n\n* We will use use a linear regression model with `NumStorePurchases` as the target variable, and then use machine learning explainability techniques to get insights about which features predict the number of store purchases\n* Begin by plotting the target variable:"
"* Drop uninformative features\n    - `ID` is unique to each customer\n    - `Dt_Customer` will be dropped in favor of using engineered variable `Year_Customer`\n* Perform one-hot encoding of categorical features, encoded data shown below:"
"* Explore the directionality of these effects, using SHAP values:\n    - Findings:\n        - The number of store purchases increases with higher number of total purchases ('TotalPurchases')\n        - The number of store purchases decreases with higher number of catalog, web, or deals purchases ('NumCatalogPurchases', 'NumWebPurchases', 'NumDealsPurchases')\n    - Interpretation:\n        - Customers who shop the most in stores are those who shop less via the catalog, website, or special deals"
"### Does US fare significantly better than the Rest of the World in terms of total purchases?\n\n* Plot total number of purchases by country:\n    - Findings: \n        - Spain (SP) has the highest number of purchases\n        - US is second to last, therefore the US does not fare better than the rest of the world in terms of the total number of purchases"
"### Does US fare significantly better than the Rest of the World in terms of total purchases?\n\n* Plot total number of purchases by country:\n    - Findings: \n        - Spain (SP) has the highest number of purchases\n        - US is second to last, therefore the US does not fare better than the rest of the world in terms of the total number of purchases"
"* Plot total amount spent by country: \n    - Findings: \n        - Spain (SP) has the highest total amount spent on purchases\n        - US is second to last, therefore the US does not fare better than the rest of the world in terms of the total amount spent on purchases"
"* Plot total amount spent by country: \n    - Findings: \n        - Spain (SP) has the highest total amount spent on purchases\n        - US is second to last, therefore the US does not fare better than the rest of the world in terms of the total amount spent on purchases"
"### Your supervisor insists that people who buy gold are more conservative. Therefore, people who spent an above average amount on gold in the last 2 years would have more in store purchases. Justify or refute this statement using an appropriate statistical test\n\n* Plot relationship between amount spent on gold in the last 2 years (`MntGoldProds`) and number of in store purchases (`NumStorePurchases`):\n    - Findings: There is a positive relationship, but is it statistically significant?"
"### Your supervisor insists that people who buy gold are more conservative. Therefore, people who spent an above average amount on gold in the last 2 years would have more in store purchases. Justify or refute this statement using an appropriate statistical test\n\n* Plot relationship between amount spent on gold in the last 2 years (`MntGoldProds`) and number of in store purchases (`NumStorePurchases`):\n    - Findings: There is a positive relationship, but is it statistically significant?"
* Perform Kendall correlation analysis (non-parametric test since `MntGoldProducts` is not normally distributed and contains outliers):\n    - Findings: There is significant positive correlation between `MntGoldProds` and `NumStorePurchases`
* Perform Kendall correlation analysis (non-parametric test since `MntGoldProducts` is not normally distributed and contains outliers):\n    - Findings: There is significant positive correlation between `MntGoldProds` and `NumStorePurchases`
"### Fish has Omega 3 fatty acids which are good for the brain. Accordingly, do ""Married PhD candidates"" have a significant relation with amount spent on fish? \n\n* We will compare `MntFishProducts` between Married PhD candidates and all other customers:\n    - Findings: Married PhD candidates spend significantly less on fish products compared to other customers."
"### What other factors are significantly related to amount spent on fish?\n\n* Like with the analysis of `NumStorePurchases` above, we will use use a linear regression model with `MntFishProducts` as the target variable, and then use machine learning explainability techniques to get insights about which features predict the amount spent on fish\n* Begin by plotting the target variable:"
"* Fit linear regression model to training data (70% of dataset)\n* Evaluate predictions on test data (30% of dataset) using RMSE:\n    - Findings: The RMSE is exceedingly small compared to the median value of the target variable, indicating good model predictions"
"* Explore the directionality of these effects, using SHAP values:\n    - Findings:\n        - The amount spent on fish increases with higher total amount spent ('TotalMnt')\n        - The amount spent on fish decreases with higher amounts spent on wine, meat, gold, fruit, or sweets ('MntWines', 'MntMeatProducts', 'MntGoldProds', 'MntSweetProducts', 'MntFruits')\n    - Interpretation:\n        - Customers who spend the most on fish are those who spend less on other products (wine, meat, gold, fruit, and sweets)"
### Is there a significant relationship between geographical regional and success of a campaign?\n\n* Plot success of campaigns by region:\n    - Findings:\n        - The campaign acceptance rates are low overall\n        - The campaign with the highest overall acceptance rate is the most recent campaign (column name: `Response`)\n        - The country with the highest acceptance rate in any campaign is Mexico\n    - Is the effect of region on campaign success statistically significant? See below.
### Is there a significant relationship between geographical regional and success of a campaign?\n\n* Plot success of campaigns by region:\n    - Findings:\n        - The campaign acceptance rates are low overall\n        - The campaign with the highest overall acceptance rate is the most recent campaign (column name: `Response`)\n        - The country with the highest acceptance rate in any campaign is Mexico\n    - Is the effect of region on campaign success statistically significant? See below.
"* Statistical summary of regional effects on campaign success:\n    - Methodology: Performed logistic regression for Campaign Accepted by Country, reporting Chisq p-value for overall model.\n    - Findings: The regional differences in advertising campaign success are statistically significant."
Please plot and visualize the answers to the below questions.\n\n### Which marketing campaign is most successful?\n\n* Plot marketing campaign overall acceptance rates:\n    - Findings: The most successful campaign is the most recent (column name: `Response`)
"### What does the average customer look like for this company?\n\n* Basic demographics: The average customer is...\n    - Born in 1969\n    - Became a customer in 2013\n    - Has an income of roughly \$52,000 per year\n    - Has 1 dependent (roughly equally split between kids or teens)\n    - Made a purchase from our company in the last 49 days"
"### Which products are performing best?\n\n* The average customer spent...\n    - \$25-50 on Fruits, Sweets, Fish, or Gold products\n    - Over \$160 on Meat products\n    - Over \$300 on Wines\n    - Over \$600 total\n* Products performing best:\n    - Wines\n    - Followed by meats"
"### Which channels are underperforming?\n\n* Channels: The average customer...\n    - Accepted less than 1 advertising campaign\n    - Made 2 deals purchases, 2 catalog purchases, 4 web purchases, and 5 store purchases\n    - Averaged 14 total purchases\n    - Visited the website 5 times\n* Underperforming channels:\n    - Advertising campaigns\n    - Followed by deals, and catalog"
"### Which channels are underperforming?\n\n* Channels: The average customer...\n    - Accepted less than 1 advertising campaign\n    - Made 2 deals purchases, 2 catalog purchases, 4 web purchases, and 5 store purchases\n    - Averaged 14 total purchases\n    - Visited the website 5 times\n* Underperforming channels:\n    - Advertising campaigns\n    - Followed by deals, and catalog"
# Conclusion
# Imports
# Loading the data
### üéóÔ∏è Main Training Function\n\n> How does it work:\n
### üéóÔ∏è Experimenting\n\n> üìå **Note**: This cell below was ran locally on my ZBook Studio (as the Kaggle environment is too slow for all the data). I am sharing with you the logs that I've got :)
We'll be taking our data visualization skills to the next level with plotly.  \nplotly is a great package that allows you to create dynamic data visualizations in Python.  \nHere's an example of what we'll be able to create by the end of this tutorial.
The prerequisites for this tutorial are intermediate knowledge of Python and beginner knowledge of pandas and numpy.
The prerequisites for this tutorial are intermediate knowledge of Python and beginner knowledge of pandas and numpy.
"Shoutout to Derek Banas, I created this Kaggle Notebook in part by following his [tutorial on YouTube](https://www.youtube.com/watch?v=GGL6U0k8WYA).  \n"
"Let's say that we want to show both value counts and the probability distribution (value_count / total).  \nWe'll need to generate 2 plots. We'll do that by using subplots.  \nIn our imports we imported subplots from plotly, we'll be using that module."
"Since the 2 distributions are the same but with different y-axis scaling, we can consolidate the 2 subplots into 1 by using the secondary_y attribute."
"A Density Heatmap is kinda like a 2D Histogram, where the x and y axis are dedicated to 2 variables and the ""z axis"" (or rather the color of the Heatmap cell) is dedicated to count/sum of occurences."
"We want to use the count aggregation on our data, so we'll have to create a new row for each passenger.  \nSo the first record will be mapped to 112 records, the second one to 118 records etc.  \nWe'll be flexing our pandas skills to do this."
We'll be be showing the marginal distributions along x and y
Next let's overlay the number of passengers in the Heatmap cells and show the Year over Year growth as a Bar Chart.  \nWe'll be doing some data transformation to get the YoY growth.
3D Plots allow you to add a 3rd axis to your plots and rotate/pan/zoom on the plot with your mouse and keyboard.  \nLet's see them in action on the flights dataset.
## 3D Scatter Plot
## 3D Scatter Plot
## 3D Line Plot
## 3D Line Plot
## 3D Surface Plot
A scatter matrix creates multiple 2d scatter plots to explore relationships there might be between column variables.  \n$$n = \# columns \implies \# plots = n^2$$
You can highlight a certain variable with the color parameter.
You can highlight a certain variable with the color parameter.
This tool is very useful for quickly exploring correlation between variables
## Scatter Polar
## Line Polar
We can define an additional facet with the facet_row parameter
Last example
# Imports
# TPU or GPU detection
# Configuration\nThe Flowers dataset is availabe in multiple image sizes. 331x331px is the default. 512x512px will OOM on GPU but works on TPU.
"## Visualization utilities\n\ndata -> pixels, nothing of much interest for the machine learning practitioner in this section."
"## Visualization utilities\n\ndata -> pixels, nothing of much interest for the machine learning practitioner in this section."
# Read images and labels from TFRecords
"\n\n# 1. Introduction üìú\n\nThis notebook is just me being frustrated on **deep learning** and trying to understand in ""baby steps"" what is going on here. For somebody that starts in this area with no background whatsoever it can be very confusing, especially because I seem to be unable to find code with many explanations and comments.\n\nSo, if you are frustrated just like I was when I started this stuff I hope the following guidelines will help you. I am by no means a teacher, but in this notebook I will:\n1. Share articles/videos I watched that TRULY helped\n2. Explain code along the way to the best of my ability\n\n \nNote: Deep learning coding is VERY different in structure than the usual sklearn for machine learning. In addition, it usually works with images and text, while ML usually works with tabular data. So please, be patient with yourself and if you don't understand something right away, continue reading/ coding and it will all make sense in the end.\n\n\n\n\n# 2. Before we start üìù\n\n> This is my third notebook in the ""series"": **How I taught myself Deep Learning**.\n1. **[How I taught myself Deep Learning: Vanilla NNs](https://www.kaggle.com/andradaolteanu/how-i-taught-myself-deep-learning-1-pytorch-fnn)**\n        * PyTorch and Tensors\n        * Neural Network Basics, Perceptrons and a Plain Vanilla Neural Net model\n        * MNIST Classification using FNN\n        * Activation Functions\n        * Forward Pass\n        * Backpropagation (Loss and Optimizer Functions)\n        * Batching, Iterations and Epochs\n        * Computing Classification Accuracy\n        * Overfitting: Data Augmentation, Weight Decay, Learning Rate, Dropout() and Layer Optimization   \n2. **[Convolutional Neural Nets (CNNs) Explained](https://www.kaggle.com/andradaolteanu/convolutional-neural-nets-cnns-explained)**\n        * Why ConvNets\n        * Convolutions Explained\n        * Computing Activation Maps\n        * Kernels, Padding, Stride\n        * AlexNet\n        * MNIST Classification using Convolutions"
"# 3. RNN with 1 Layer üìò\nRecurrent Neural Networks are very different from [FNNs](https://www.kaggle.com/andradaolteanu/how-i-taught-myself-deep-learning-vanilla-nns) or [CNNs](https://www.kaggle.com/andradaolteanu/how-i-taught-myself-deep-learning-convnet-cnns). \n\nRNNs model **sequential data**, meaning they have **sequential memory**. An RNN takes in different kind of inputs (text, words, letters, parts of an image, sounds, etc.) and returns different kinds of outputs (the next word/letter in the sequence, paired with an FNN it can return a classification etc.).\n\n\n\n**How RNN works**:\n1. It uses previous information to affect later ones\n2. There are 3 layers: *Input*, *Output* and *Hidden* (where the information is stored)\n3. The loop: passes the input forward sequentialy, while *retaining information* about it\n4. This info is stored in the *hidden state*\n5. There are only 3 matrixes (U, V, W) that contain weights as parameters. These *DON'T change* with the input, they stay the same through the entire sequence."
### Target Variable Visualization (Churn) : 
"- The dataset is **unbalanced** in a near about **3 : 1** ratio for **Not-Churn : Churn** customers!\n- Due to this, predictions will be biased towards **Not-Churn** customers.\n- Visualizations will also display this bias!"
#### Group 1 : Customer Information : \n#### gender | SeniorCitizen | Partner | Dependents |
"- Customer churning for **male** & **female** customers is very similar to each other!\n- Similarly, number of **SeniorCitizen** customers is pretty low! Out of that, we can observe a near about 40% churn of **SeniorCitizen** customers. It accounts for a total of 476 customers out of 1142 **Senior Citizen** customers.\n- Customers who are housing with a **Partner** churned less as compared to those not living with a **Partner**.\n- Similary, churning is high for the customers that don't have **Dependents** with them!"
"#### Group 2: Services Subscribed by the Customer :\n\n- **For visualization purposes, we will create 2 groups!**\n\n#### PhoneService | MultipleLines | InternetService | StreamingTV | StreamingMovies |"
"- For **PhoneService**, despite having no phone service, more customers were retained as compared to the number of customers who dropped the services.\n- In case of **MultipleLines**, churn rate in when the **Multiplelines** are present or not is the same. \n- A high number of customers have displayed their resistance towards the use of **Fiber optic** cables for providing the **InternetService**. On the contrary, from the above graph, customers prefer using **DSL** for their **InternetService**!\n- **StreamingTV** and **StreamingMovies** display an identical graph. Irrespective of being subscribed to **StreamingTV** & **StreamingMovies**, a lot of customers have been churned. Looks like the streaming content was not entirely at fault!"
#### Group 2: Services Subscribed by the Customer : \n#### OnlineSecurity | OnlineBackup | DeviceProtection | TechSupport |
"- When it comes down to catering the customers, services w.r.t **OnlineSecurity**, **OnlineBackup**, **DeviceProtection** & **TechSupport** are crucial from the above visualizations! \n- A high number of customers have switched their service provider when it comes down poor services with the above mentioned features."
#### Group 3 : Contract | PaperlessBilling | PaymentMethod |
"- Customer churning for a **Month-to-Month** based **Contract** is quite high. This is probably because the customers are testing out the varied services available to them and hence, in order to save money, 1 month service is tested out!\n- Another reason can be the overall experience with the internet service, streaming service and phone service were not consistent. Every customer has a different priority and hence if one of the 3 was upto par, the entire service was cutoff!\n- **PaperlessBilling** displays a high number of customers being churned out. This is probably because of some payment issue or receipt issues.\n- Customers clearly resented the **Electronic check** **PaymentMethod**. Out of the **2365** number of bills paid using **Electronic check**, a staggering 1071 customers exited the pool of service due to this payment method. Company definitely needs to either drop **Electronic check** method or make it hassle-free and user-friendly."
### Numerical Features :\n\n#### Distribution of Numerical Features :
- **tenure** and **MonthlyCharges** kind of create a **bimodal distribution** with peaks present at **0 - 70** and **20 - 80** respectively.\n- **TotalCharges** displays a **positively or rightly skewed distribution**.
### Numerical Features w.r.t Target Variable (Outcome) :
"- Considering **tenure**, a high number of customers have left after the 1st month. This high cancellation of services continues for **4 - 5** months but the churn customers have reduced since the 1st month. As the **tenure** increases, customers dropping out decreases. \n- This results in low customer churning as the **tenure** increases. It displays a symmetrical graph with the left side dominating with churn numbers and right side dominating with low churn numbers.  \n- Because of too many unique data points in **MonthlyCharges** & **TotalCharges**, it is difficult to gain any type of insight. Thus, we will scale these numerical features for understandable visualization and gaining insights purposes. This brings the varied data points to a constant value that represents a range of values.\n- Here, we divide the data points of the numerical features by 5 or 500 and assign its quotient value as the representative constant for that data point. The scaling constants are decided by looking into the data & intuition."
"- Considering **tenure**, a high number of customers have left after the 1st month. This high cancellation of services continues for **4 - 5** months but the churn customers have reduced since the 1st month. As the **tenure** increases, customers dropping out decreases. \n- This results in low customer churning as the **tenure** increases. It displays a symmetrical graph with the left side dominating with churn numbers and right side dominating with low churn numbers.  \n- Because of too many unique data points in **MonthlyCharges** & **TotalCharges**, it is difficult to gain any type of insight. Thus, we will scale these numerical features for understandable visualization and gaining insights purposes. This brings the varied data points to a constant value that represents a range of values.\n- Here, we divide the data points of the numerical features by 5 or 500 and assign its quotient value as the representative constant for that data point. The scaling constants are decided by looking into the data & intuition."
"- For **MonthlyCharges** group, churn rate is high for the values between **65** (13x5) - **105** (21x5). This **MonthlyCharges** range of values caused the customers to switch.\n- A very high number of customers opted out of the services for the **TotalCharges** below **500**. This customer churning continues for a **TotalCharges** range of values from **0** (0x500) - **1000** (2x500). "
- ### tenure vs Categorical Features :\n\n#### tenure vs Group 1 : Customer Information : gender | SeniorCitizen | Partner | Dependents |
"- **Male** & **Female** customer churn graphs are very similar. \n- **SeniorCitizen** opted out from the services for a tenure values of **0 - 35** months. **20 - 35** months is the kind of decision making period about whether to continue or swtich for **SeniorCitizen**.\n- Similarly, customers with partners continued with the service for a **tenure** of **5 - 45** months."
#### tenure vs Group 2: Services Subscribed by the Customer : PhoneService | MultipleLines | InternetService | StreamingTV | StreamingMovies |
"- Presence of **MutipleLines** pushes the median **MonthlyCharges** irrespective if the customers opt out of the services or not.\n- For the graph of **tenure vs PhoneService**, availability of **PhoneService** or not display a mirroring visuals. Customers were probably not heavy phone (call - message) users.\n- For **InternetService**, customers seem to be very skeptical about the usage of **Optic Fibre** cables as the churning lasted for about **30 - 35** months before either carrying it forward or switching to a new one!\n- Similary for **StreamingTV** & **StreamingMovies**, a churn tenure period of about **10 - 40** months can be observed! "
#### tenure vs Group 2: Services Subscribed by the Customer : OnlineSecurity | OnlineBackup | DeviceProtection | TechSupport |
"- For **OnlineSecurity**, **OnlineBackup**, **DeviceProtection** & **TechSupport**, median churn tenure value is of **25** months. The highest value of this churn tenure is of around **45** months.\n- **30 - 35** month period is where the customers take a call about whether to continue with the current services or switch w.r.t above features!"
#### tenure vs Group 3 : Contract | PaperlessBilling | PaymentMethod |
"- When customers sign **One year** and **Two year** contracts for the services, they seem to continue with the services for about **25** and **45** months respectively! However, they start questioning the services and think about switching from the **35** month and **55** month mark respectively!\n- Irrespective of the **PaperlessBilling**, customers think of switching right from the 1st month.\n- When it comes to **PaymentMethod**, median churn tenure of **Bank Transfer (automatic)** & **Credit Card (automatic)**, **above 20 months**, is nearly double than that of **Electronic check** & **Mailed check**, **around 10 months** & **around 5 months** respectively."
- ### MonthlyCharges vs Categorical Features :\n\n#### MonthlyCharges vs Group 1 : Customer Information : gender | SeniorCitizen | Partner | Dependents |
"- For all the features mentioned above, the median value of the **not-churn** customers is very close to the lower limit of the **churn** customers. \n- **Male** & **Female** customers have the same median **MonthlyCharges** of around **60**. For **SeniorCitizen**, this value is pushed to **80**.\n- Customers living with **Partner** have a higher lower limit of churning, **MonthlyCharges** of **70**, than those living alone, **MonthlyCharges** of **just below 60**!  "
#### MonthlyCharges vs Group 2: Services Subscribed by the Customer : PhoneService | MultipleLines | InternetService | StreamingTV | StreamingMovies |
"- **MonthlyCharges** of **Fiber Optic** cables is very high. Thus, it might be the reason for such high churn of customers.\n- Similarly, **MonthlyCharges** of **StreamingTV** & **StreamingMovies** is quite high.\n- Range of **MonthlyCharges** for **PhoneService** is from **25 - 85** but customers think of unsubscribing from **75** value of **MonthlyCharges**."
#### MonthlyCharges vs Group 2: Services Subscribed by the Customer : OnlineSecurity | OnlineBackup | DeviceProtection | TechSupport |
"- For **OnlineSecurity**, **OnlineBackup**, **DeviceProtection** & **TechSupport**, range of values is from **around 50 to 100**.\n- Customers who subscribe who to these services, probably don't think about cancelling the subscription due to **MonthlyCharges** as the range of values of customers who unsubscribe & continue is near about the same!"
#### MonthlyCharges vs Group 3 : Contract | PaperlessBilling | PaymentMethod |
- Lower limit of the **MonthlyCharges** is higher for **Month-to-Month** contract than **One year** & **Two year** contracts. However the lower limit of the customers who discontinue the services is lower for **Month-to-Month** contract.\n- Lower limit of the **Electronic check** is very high and it can be a huge factor due to which customers resent using it! Whereas **Mailed check** has the lowest starting values of customers who left and continued.
- ### TotalCharges vs Categorical Features :\n\n#### TotalCharges vs Group 1 : Customer Information : gender | SeniorCitizen | Partner | Dependents |
- **TotalCharges** for **male** & **female** customers is quite the same! **SeniorCitizen** that continued with the services have a higher starting and closing values of **TotalCharges**.\n- Customers housing with their **Partner** have a higher median value of **TotalCharges** as compared to those living alone!
#### TotalCharges vs Group 2: Services Subscribed by the Customer : PhoneService | MultipleLines | InternetService | StreamingTV | StreamingMovies |
"- **TotalCharges** of **PhoneService** range from **0 - 4000**. However, customers start getting 2nd thoughts about **PhoneService** due to **TotalCharges** from **around 1000**.\n- Similarly, customers start to hesitate to pay **around 2000** for **MultipleLines**. However, some customers seem to be desperate for **MultipleLines** as they paid a value of **around 6000** for it!\n- When it comes to paying for **Fiber Optic** cables, customers test out the products by paying **around 2000**!\n- Similar to **Fiber Optic**, **StreamingTV** & **StreamingMovies**, customers that continue with the services pay from **3000 - 6000**."
#### TotalCharges vs Group 2: Services Subscribed by the Customer : OnlineSecurity | OnlineBackup | DeviceProtection | TechSupport |
"- For all the features mentioned above, customers become skeptical about paying for them around the **2000** mark. This median value of churn customers is very close to the lower limit of the customers that carry on with this service.\n- Customers that do not churn out are ready to pay from **2000 - 6000** of **TotalCharges**."
#### TotalCharges vs Group 3 : Contract | PaperlessBilling | PaymentMethod |
"- Median values of customers that decide to opt out from the services that have **One year** & **Two year** contracts is high at **around 4000 & 6000**. Some of the customers with **Two year** contracts even paid around **7000**.\n- For **PaymentMethod**, customers are skeptical to pay using **Electronic check** for a shorter range of **0 - 2000** whereas for **Bank transfer (automatic)** & **Credit Card (automatic)** this range is around **0 - 4000**. "
### Numerical features vs Numerical features w.r.t Target variable (Churn) :
"- For **tenure** of **0 - 20 months** period, churning of customers quite at any **MonthlyCharges** values. For a **tenure** period from **20 - 60** months, customers at the top end of the **MonthlyCharges** values, **70 - 120**, start to drop out from the services.\n- For **TotalCharges vs tenure**, as **tenure** increases, **TotalCharges** increase as well! Customers opting out from their plans are the ones who are charged the highest of their **tenure** period alongwith a few customers whose **Total Charges** rank in the middle!\n- Customers seemed to have decided to cancel their subscriptions when the **MonthlyCharges** reach **70 and above**."
### Correlation Matrix :
- It is a huge matrix with too many features. We will check the correlation only with respect to **Churn**. 
- It is a huge matrix with too many features. We will check the correlation only with respect to **Churn**. 
"- **MulipleLines**, **PhoneService**, **gender**, **StreamingTV**, **StreamingMovies** and **InternetService** does not display any kind of correlation. We drop the features with correlation coefficient between **(-0.1,0.1)**.\n- Remaining features either display a significant **positive or negative correlation**."
#### Chi-Squared Test :
"- **PhoneService**, **gender**, **StreamingTV**, **StreamingMovies**, **MultipleLines** and **InternetService** display a very low relation with **Churn**."
#### ANOVA Test :
"- According to the **ANOVA test**, **higher the value of the ANOVA score, higher the importance of the feature**.\n- From the above results, we need to include all the numerical features for modeling."
- Selecting the features from the above conducted tests and splitting the data into **80 - 20 train - test** groups.
#### 1] Xgboost Classifier :
# Setup
# Introduction
\n
"Thus, if we have a model that predicts everything as a background pixels (i.e. y = 0 for any input), we will get an accuracy of **0.5** or **50%**.\n\nThat's a good illustration of the accuracy metric.\n\nWhat if we have less object pixels? \n\n\nIn fact, that's what happens **99% of the time**: the backgournd is the vast majority of the image and the object\nto predict represents only a tiny percentage of the whole (let's say between few percents to 10s of percents).\n\n\nLet's take a more realistic example then and see what happens. "
"Thus, if we have a model that predicts everything as a background pixels (i.e. y = 0 for any input), we will get an accuracy of **0.5** or **50%**.\n\nThat's a good illustration of the accuracy metric.\n\nWhat if we have less object pixels? \n\n\nIn fact, that's what happens **99% of the time**: the backgournd is the vast majority of the image and the object\nto predict represents only a tiny percentage of the whole (let's say between few percents to 10s of percents).\n\n\nLet's take a more realistic example then and see what happens. "
"Here you can see that we have only few object pixels (in white). \nAround 10 * 20 + 20 * 20 = **600** pixels out of a total of 256 * 256 = **65536**.\n\nThat's **0.90%**, i.e. less than 1%."
"To have a confusion matrix, we need some predictions and some ground truths. As said in the beginning,\nwe will only focus on binary perdictions for now but notice that the concept extends easily to multiclasses.\n\nLet's generate a confusion matrix using [scikit-learn](https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py): "
"We can see that we have 4 quadrants: \n    \n    \n- A: 0 vs B: 0\n- A: 1 vs B: 0\n- A: 0 vs B: 1\n- A: 1 vs B: 1\n\n\nIf A are the true labels and B are the predicted labels and if 1 is the positive class (object of interest), then we get the following: \n\n\n- A: 0 vs B: 0 => **true negative** tha will be shortened to TN\n- A: 1 vs B: 0 => **false negative** that will be shortened to FP\n- A: 0 vs B: 1 => **false positive** that will be shortened to FN\n- A: 1 vs B: 1 => **true positive**  that will be shortened to TP\n\nWe can place these labels in the confusion matrix.\n\nNow that we have these 4 numbers (TP, FP, TN, FN) from the **confusion matrix**, we can build some useful metrics. \nLet's start with **precision**."
"To check that our dataset pipeline is working, let's plot some of the images and masks as an overaly."
"As for the loss, we will use a symmetric version of [Lovasz](https://arxiv.org/pdf/1705.08790.pdf) (more on this loss in the final section): "
"One other question to ask is: how to move from **binary semantic** segmentation to **multiclass segmentation**?\n\nThere are few things we need to change: \n\n\n- the metric computation of course.\n- the dataset processing, especially the mask.\n- the choice of the loss\n\n\nAs for the metric computation, let's assume it is IoU. A simple extension is to compute this \nmetric for each class then take the mean of everything, this gives the mIoU. \nOften, it is better to exclude the background class from the mIoU computation.\n\nAs for the loss, if the different classes are balanced, we can use categorical cross  entropy.\nIf the classes are unbalanced, we can use the focal loss for example.\n\n\nFinally, for the mask, we can encode each class as an integer from 0 to n-1 where n is the number of classes. For example, if we have 3 classes, let's say background, car, and pedestrian, we can have the following mapping: \n\n- 0 => background\n- 1 => car\n- 2 => pedestrian\n\nThe mask can look like this (we will reuse the same code from the first part but adapted): \n\n"
"Nice. That's enough with multiclass segmentation, let's move to metrics libraries."
"First let's start by importing the essential libraries to work with dataframes (**pandas**), numeric values (**numpy**) and visualization (**matplotlib.pyplot**)."
Now let's import the csv file with the training dataset. You can download it from [here](https://www.kaggle.com/c/titanic/data).  The explanation of the features (each column from the dataset) is also presented in this link. 
"You may wonder why are we still keeping the **""Name""** column. In fact the name does not seem to have influence, it does not matter if a person is named Owen or William, however this column has the title located after the Surname and the comma (""Mr"", ""Mrs"", ""Miss"", etc.) which can be useful.  \n\nIf we take a look at the table X displayed previously we can see many missing values for the **""Age""** column. Removing these rows with missing values would involve removing 177 rows (which is quite a lot!) and we would have less information to create the model. In some cases, it is acceptable to take the average of the column and replace the null values, nonetheless in this case, it is possible to estimate the age of the person by their title, present in the **""Name""** column.   \n\nTherefore, I will first identify the different titles presented and then average the Age for each title. We can provide this averaged Age found for each title to the people with missing Age values, accordingly to their title in **""Name""**. \n\nAfter using the information in **""Name""** we can drop this column. "
"We can also make feature transformation. For example, we could transform the **""Age""** feature in order to simplify it. We could distinguish the youngsters (age less than 18 years) from the adults.  \n\n"
"# Introduction \n\nThe issue of keeping one's employees happy and satisfied is a perennial and age-old challenge. If an employee you have invested so much time and money leaves for ""greener pastures"",  then this would mean that you would have to spend even more time and money to hire somebody else. In the spirit of Kaggle, let us therefore turn to our predictive modelling capabilities and see if we can predict employee attrition on this synthetically generated IBM dataset. \n\nThis notebook is structured as follows:\n\n 1. **Exploratory Data Analysis** : In this section, we explore the dataset by taking a look at the feature distributions, how correlated one feature is to the other and create some Seaborn and Plotly visualisations\n 2. **Feature Engineering and Categorical Encoding** : Conduct some feature engineering as well as encode all our categorical features into dummy variables\n 3. **Implementing Machine Learning models** : We implement a Random Forest and a Gradient Boosted Model after which we look at feature importances from these respective models"
# 1. Exploratory Data Analysis\n\nLet us load in the dataset via the trusty Pandas package into a dataframe object and have a quick look at the first few rows
"### Correlation of Features\n\nThe next tool in a data explorer's arsenal is that of a correlation matrix. By plotting a correlation matrix, we have a very nice overview of how the features are related to one another. For a Pandas dataframe, we can conveniently use the call **.corr** which by default provides the Pearson Correlation values of the columns pairwise in that dataframe.\n\nIn this correlation plot, I will use the the Plotly library to produce a interactive Pearson correlation matrix via the Heatmap function as follows:"
"**Takeaway from the plots**\n\nFrom the correlation plots, we can see that quite a lot of our columns seem to be poorly correlated with one another. Generally when making a predictive model, it would be preferable to train a model with features that are not too correlated with one another so that we do not need to deal with redundant features. In the case that we have quite a lot of correlated features one could perhaps apply a technique such as Principal Component Analysis (PCA) to reduce the feature space."
### Pairplot Visualisations\n\nNow let us create some Seaborn pairplots and set it against the target variable which is our Attrition column to get a feel for how the various features are distributed vis-a-vis employee attrition
"# 2. Feature Engineering & Categorical Encoding\n\nHaving carried out a brief exploration into the dataset, let us now proceed onto the task of Feature engineering and numerically encoding the categorical values in our dataset. Feature engineering in a nutshell involves creating new features and relationships from the current features that we have. Feature engineering has been quite \n\nTo start off, we shall segregate numerical columns from categorical columns via the use of the dtype method as follows:"
However just by a quick inspection of the counts of the number of 'Yes' and 'No' in the target variable tells us that there is quite a large skew in target as shown
"Therefore we have to keep in mind that there is quite a big imbalance in our target variable. Many statistical techniques have been put forth to treat imbalances in data (oversampling or undersampling). In this notebook, I will use an oversampling technique known as SMOTE to treat this imbalance."
### Feature Ranking via the Random Forest \n\nThe Random Forest classifier in Sklearn also contains a very convenient  attribute **feature_importances_** which tells us which features within our dataset has been given most importance through the Random Forest algorithm. Shown below is an Interactive Plotly diagram of the various feature importances.
### Visualising Tree Diagram with Graphviz\n\nLet us now visualise how a single decision tree traverses the features in our data as the DecisionTreeClassifier object of sklearn comes with a very convenient **export_graphviz** method that exports the tree diagram into a .png format which you can view from the output of this kernel.
"### Feature Ranking via the Gradient Boosting Model\n\nMuch like the Random Forest, we can invoke the feature_importances_ attribute of the gradient boosting model and dump it in an interactive Plotly chart"
"### CONCLUSION\n\nWe have constructed a very simple pipeline of predicting employee attrition, from some basic Exploratory Data Analysis to feature engineering as well as implementing two learning models in the form of a Random Forest and a Gradient Boosting classifier. This whole notebook takes less than a minute to run and it even returns a 89% accuracy in its predictions.\n\nThat being said, there is quite a lot of room for improvement. For one, more features could be engineered from the data.  Furthermore one could squeeze performance out of this pipeline by perhaps using some form of blending or stacking of models. I myself am quite keen to implement a classifier voting where a handful of classifiers votes on the outcome of the predictions and we take the majority vote. "
\n## 1. Import the required libraries
\n## 2. Do everything we can to make our results reproducible\n\nThumb rule: **Always set the seed!**
We will read the `monkey_labels.txt` file to extract the information about the labels. We can store this information in a list which then can be converted into a `pandas` dataframe.  
"The labels are `n0, n1, n2, ...`. We will create a mapping of these labels where each class will be represented by an integer starting from 0 to number of classes. We will also create a mapping for the names corresponding to a class. We will be using `Common Name` for the last part"
"### Check the distribution of variables\n\n\nNow, I will plot the histograms to check variable distributions to find out if they are normal or skewed. "
We can see that all the variables in the dataset are positively skewed. 
"### Interpretation \n\n- The correlation coefficient ranges from -1 to +1. \n\n- When it is close to +1, this signifies that there is a strong positive correlation. So, we can see that there is a strong positive correlation between `Class` and `Bare_Nuclei`, `Class` and `Uniformity_Cell_Shape`, `Class` and `Uniformity_Cell_Size`.\n\n- When it is clsoe to -1, it means that there is a strong negative correlation. When it is close to 0, it means that there is no correlation. \n\n- We can see that all the variables are positively correlated with `Class` variable. Some variables are strongly positive correlated while some variables are negatively correlated."
### Correlation Heat Map
### Correlation Heat Map
"### Interpretation\n\n\nFrom the above correlation heat map, we can conclude that :-\n\n1. `Class` is highly positive correlated with `Uniformity_Cell_Size`, `Uniformity_Cell_Shape` and `Bare_Nuclei`. (correlation coefficient = 0.82).\n\n2. `Class` is positively correlated with `Clump_thickness`(correlation coefficient=0.72), `Marginal_Adhesion`(correlation coefficient=0.70), `Single_Epithelial_Cell_Size)`(correlation coefficient = 0.68) and `Normal_Nucleoli`(correlation coefficient=0.71).\n\n3. `Class` is weekly positive correlated with `Mitoses`(correlation coefficient=0.42).\n\n4. The `Mitoses` variable is weekly positive correlated with all the other variables(correlation coefficient < 0.50)."
"### Comment\n\n\nSo, kNN Classification model with k=7 shows more accurate predictions and less number of errors than k=3 model. Hence, we got performance improvement with k=7."
# **18. Classification metrices** \n\n[Table of Contents](#0.1)
"# **19. ROC-AUC** \n\n[Table of Contents](#0.1)\n\n\n\n### ROC Curve\n\n\nAnother tool to measure the classification model performance visually is **ROC Curve**. ROC Curve stands for **Receiver Operating Characteristic Curve**. An **ROC Curve** is a plot which shows the performance of a classification model at various \nclassification threshold levels. \n\n\n\nThe **ROC Curve** plots the **True Positive Rate (TPR)** against the **False Positive Rate (FPR)** at various threshold levels.\n\n\n\n\n**True Positive Rate (TPR)** is also called **Recall**. It is defined as the ratio of **TP to (TP + FN)**.\n\n\n\n\n\n**False Positive Rate (FPR)** is defined as the ratio of **FP to (FP + TN)**.\n\n\n\n\nIn the ROC Curve, we will focus on the TPR (True Positive Rate) and FPR (False Positive Rate) of a single point. This will give us the general performance of the ROC curve which consists of the TPR and FPR at various threshold levels. So, an ROC Curve plots TPR vs FPR at different classification threshold levels. If we lower the threshold levels, it may result in more items being classified as positve. It will increase both True Positives (TP) and False Positives (FP).\n\n\n"
ROC curve help us to choose a threshold level that balances sensitivity and specificity for a particular context.
"## 3. Preparing the data\n\nBefore we build our model using machine learning algorithms, let's prepare our dataset. We will go through the following stages:\n1. Importing the data\n2. Selecting the feature matrix `X` and the target column `y` (*supervised learning*)\n3. Scaling numeric features\n4. Encoding categorical features\n5. Splitting the whole dataset into training and testing datasets\n\nIn the very beginning don't forget to import the required libraries. =)"
For the purpose of this demonstration we will only use the file `train.csv` that contains the target column.
"Since SGD calculates the gradient only of the random part of the dataset (hence the name ""stochastic""), the accuracy score might decrease after some of the iterations. Nevertheless, the score should be converging. This convergence should always be checked to ensure that the optimum point is reached. On the plot below the model converged after 1500 iterations so this or higher value should be used for the maximum number of iterations. Above we used 2000 iterations so it should suffice.\n\n**Try it yourself:** Scaling of the numeric features helps a lot with the convergence of the SGD classifier. Try to comment the code that does scaling and rerun all the cells. You will see that the convergence doesn't occur. In this case you need to set the parameter `learning_rate='constant'` and manually decrease the parameter `eta0` until the convergence occurs again."
"Now that the convergence is confirmed, let's calculate the confusion matrix of our model. These values were used in Part 1 to explain the problem:\n\n* Correct predictions: 177 (survivors 61, victims 116)\n* Wrong predictions: 46 (survivors 23, victims 23)"
"Let's plot the Precision-Recall curve and find a point with a better precsion. Note that precision, recall and corresponding thresholds are obtained from the training dataset, not the testing dataset, to avoid biased scores."
"The best precision of 1.0 is obtained for thresholds of 0.973211, 0.974678 and 1.0 (see the last 3 rows in the dataframe below). Let's pick up the threshold 0.973211 that corresponds to the highest recall among them. The higher the recall, the more true positives (or survived passengers) are predicted and the more insurances we will sell."
"### When we plot character counts we can see some linear relation there. With increasing number of character counts our target score decreases, interesting..."
"## Word Counts\n\n### Word Counts distribution looks more evenly (not normally!) not sure what it means for now, let's take a closer look with scatterplot..."
"## Word Counts\n\n### Word Counts distribution looks more evenly (not normally!) not sure what it means for now, let's take a closer look with scatterplot..."
"# Yeah... Doesn't look much meaningful to me, there's slight decrease in scores when the word count increses. Well let's check other meta features then..."
"# Yeah... Doesn't look much meaningful to me, there's slight decrease in scores when the word count increses. Well let's check other meta features then..."
## Word Lenghts\n\n### Does length of the words affect readability score? Let's check how's the distribution: It looks like decently spread with little bit of right skew...
### Oh that's interesting... There's strong linear relation between mean word lenghts and readability. Looks like longer/complicated words decreases the score.
## Number of Digits\n\n### We can observe that having lots of digits slightly decreasing readability score...
## Number of Digits\n\n### We can observe that having lots of digits slightly decreasing readability score...
# Common N-Grams\n\n### Here we check some common words. They're not directly giving us info but they can help us summarizing what kind of texts we have here and understand them.
## Unigrams
## Bigrams
"# Wordcloud\n\n## Again most common words in more visual format, not giving us extra feature but looks cool right? :)"
# Getting Sentence Embeddings\n\n### We're given small number of training instances. With huge models especially neural networks we're always in risk of overfitting. For this and baseline purposes I implemented a simple sentence embedding model based on fine tuned roberta. After getting embedding matrix we are going to train that using Bayesian Ridge Regression...
# Proportion of Kagglers for each Job Title\n\nFirst let's create a metric to enable us compare those two different datasets and also different countries. The following radar charts express the number of Kagglers and job listings proportionally to the total of each country. See the example below for better understanding.\n\n#### Example Country\nData Scientists = 30 answers (or job listings if Glassdoor)\nDevelopers = 90 answers \nTotal = 120 answers\n\n**We will be plotting:**\nData Scientists = 25%\nDevelopers = 75%\n\n\n  Warning! Having a higher value doesn't mean a greater absolute value of answers or jobs listings. But a higher proportion when compared to the rest in the same country.\n
# USA vs China
"Most countries follows the same proportion of Kagglers as United States, with proportionally more data scientists than other professions. And it does make sense because Kaggle is focused on Data Science. But we do have some outliers. Starting with China, where software engineers and Data Engineers are massively present in Kaggle. **Are they trying to learn data science and change their careers? Or is it just a knowledge they want to add up to their current roles?**"
"Looking at Glassdoor jobs listings we see that China has much more jobs in Software Engineering than in Data Science. \n\n\n ¬†Warning! If you live and China, are a software engineer and are thinking about migrating to Data Science, then reconsider that. You'll likely have a much harder time finding a job in this new career.\n\n\n  USA wants data analysts! If you live in the United States and want to be a Data Scientist in a few years, then consider getting a job as data analyst. It will probably be much easier as there is proportionally more jobs in that area than in DS.  "
"Looking at Glassdoor jobs listings we see that China has much more jobs in Software Engineering than in Data Science. \n\n\n ¬†Warning! If you live and China, are a software engineer and are thinking about migrating to Data Science, then reconsider that. You'll likely have a much harder time finding a job in this new career.\n\n\n  USA wants data analysts! If you live in the United States and want to be a Data Scientist in a few years, then consider getting a job as data analyst. It will probably be much easier as there is proportionally more jobs in that area than in DS.  "
"Looking at salaries reported by Kagglers we see that China is paying better for Statisticians and Research Scientists when compared to other positions. However, unfortunately, there aren't many jobs available in this area as the previous chart shows. In USA the highest salaries are in Data Science, followed by Data Engineering and Software Engineering. And of course... USA pays more than any other country for almost all positions."
# Brazil vs India
"We see that Brazil and India have very similar proportions of Kagglers across all job titles. However Brazil has a higher proportion of Statistician and Research Scientists, it might be an indication that in Brazil there are more people doing DS in the academia than in India."
"We see that Brazil and India have very similar proportions of Kagglers across all job titles. However Brazil has a higher proportion of Statistician and Research Scientists, it might be an indication that in Brazil there are more people doing DS in the academia than in India."
"Looking at jobs listings, we find out that Brazil has a similar shape to China: huge demand for Software Engineers. \n\n\n  Data Scientists from Brazil that are struggling to find a job! Consider learning software engineering and joining a company as junior developer.\n  \n\n  Kagglers from India! There is huge opportunities to start your career as Business Analyst.\n"
"Looking at jobs listings, we find out that Brazil has a similar shape to China: huge demand for Software Engineers. \n\n\n  Data Scientists from Brazil that are struggling to find a job! Consider learning software engineering and joining a company as junior developer.\n  \n\n  Kagglers from India! There is huge opportunities to start your career as Business Analyst.\n"
"Look at that! Salaries in Brazil are looking great for Data Engineering! Not so easy to get a DE job though (remember that Software Engineers are in much higher demand). In India, you'll better paid as Data Scientist. If you want to earn even more, then consider moving to another country."
# Europe
"The Netherlands have more professionals interested in Kaggle, consequently in Data Science. For other countries we see a lack of Data Engineers and Business Analysts. **Are they happy in their current positions?**"
"The Netherlands have more professionals interested in Kaggle, consequently in Data Science. For other countries we see a lack of Data Engineers and Business Analysts. **Are they happy in their current positions?**"
"In Europe the jobs listings are almost equalised (almost same proportion for all titles) which means it won't make much difference, in terms of job offerings, if you wanna be a Data Scientist or a Business Analyst. But in terms of competition, there will be probably more candidates per listing for data science if we extrapolate the Kaggle survey to the real world.\n\n\n  Software Engineers from Europe! Might be easier for you to find a job in The Netherlands!\n  "
"In Europe the jobs listings are almost equalised (almost same proportion for all titles) which means it won't make much difference, in terms of job offerings, if you wanna be a Data Scientist or a Business Analyst. But in terms of competition, there will be probably more candidates per listing for data science if we extrapolate the Kaggle survey to the real world.\n\n\n  Software Engineers from Europe! Might be easier for you to find a job in The Netherlands!\n  "
"Finally when we compare salaries in Europe we find out that they are at almost the same range (a little bit higher for UK, maybe because of stronger currency compared to Euro). And now we can tell that Business Analysts are indeed quite happy in Europe, they are earning on average even more than technical folks.\n\n\n  Software Engineers, Data Engineers and Data Scientists from UK and Germany! Stop coding now and become Business Analysts!\n\n  I'm joking. Don't do that. Or do."
# How does years of experience coding impact salaries?
"We did expect a increase in salaries with the increase in experience, and there it is! However do you see that people who don't code at all earn a little bit more than those who are starting to code? This is probably due to managers sitting there on the first group. Once you get some momentum coding, your salary will consistently increase. \n\nCan we test this hypothesis that managers (without coding experience) are driving increase in the salaries? Well... kind of... We expect managers to earn more and also to be... older, right? Let's see if that is true."
"We did expect a increase in salaries with the increase in experience, and there it is! However do you see that people who don't code at all earn a little bit more than those who are starting to code? This is probably due to managers sitting there on the first group. Once you get some momentum coding, your salary will consistently increase. \n\nCan we test this hypothesis that managers (without coding experience) are driving increase in the salaries? Well... kind of... We expect managers to earn more and also to be... older, right? Let's see if that is true."
"It is reasonable to infer that those who have no coding experience are earning more than those with little experience because they are in managerial positions. Except for Data Engineering, where we have younger people, without coding experience, earning more than those who are starting to code."
# How does company size impact salaries?
The average salary usually increases with the company size. Data Scientists and Statisticians/Research Scientists see the biggest difference in their payslips when moving from a startup to a big corp. Business Analysts can't expect to much increase in salary just by moving to a bigger company.\n\nSalary isn't everything. Consider the pros and cons before going after jobs in large corporations.\n\n![gif career choice](https://media.giphy.com/media/gfV5GoEAaHwnC/giphy.gif)
"Python has been consolidated as the main language used by people with all job titles, about 3/4 of all respondents use it on a regular basis. R Language is much less popular (and almost no Software Engineers use with it)."
"If you want to find a job in Data Science, then Python is the language of choice. More than 75% of all Data Science positions on Glassdoor mention Python in their job descriptions.\n\n Python is the prefered language for Data Science by the market!\n\n\nStatisticians and Research Scientists use less SQL than other professions. **In fact only 1/4 of then use SQL!** This is a sign that academia is much more used to flat files than to databases. Unfortunately the real world is not served in `.csv` files.\n\n Hey you that are doing a Masters or PhD and want to find a job! Look at those two charts above! Learn SQL!\n\nIf you want to start working with data, then knowing SQL will help you get jobs as Business and Data Analyst. Now if you want to go down the Data Engineering path, SQL and Python are musts. Software engineers will find jobs more easily by knowing Java.\n\nTo get a job in the data world, the recommended language to learn is SQL! "
# Programming Languages: Kaggle vs Glassdoor
"We see that Kagglers are well prepared when it comes to Python usage! Although many respondents use Bash and R, job descriptions don't mention those languages frequently. SQL is not frequently required for Business Analysts, but knowing it (as Kagglers do) will put you one step ahead. In general Software Engineers and Data Engineers from Kaggle might find a hard time to find jobs, as Java is frequently required and many of them don't posses that skill.\n\n\nLearning Java will help you to move into Software Engineering or Data Engineering "
# How many programming languages should you learn?
"Learning (and using) multiple languages does not look like a competitive advantage in terms of salary. Pick up to three and be a real *Pro* when it comes to using them. Limit your languages especially if you don't have experience in working with data!\n\nStop learning multiple languages! Python, SQL and one more language of your choice should be enough."
# Who should learn and use Cloud Computing Platforms?
"We see a lot of Kagglers Data Scientists who are used to working with Cloud Platforms! This is good news for productionizing Machine Learning models, as it is much easier go live with a model if data scientists are not doing everything in their local environments."
"We see a lot of Kagglers Data Scientists who are used to working with Cloud Platforms! This is good news for productionizing Machine Learning models, as it is much easier go live with a model if data scientists are not doing everything in their local environments."
I can confirm that this is true! Since I migrated from Data Science to Data Engineering I started to work heavily with AWS products. Cloud computing platforms have a lot of managed services for the whole data pipeline. Knowing where to apply (and how to use) them is of prime importance for any data engineer.\n\nBe aware! To work in Data and Software Engineering you'll need a lot of knowledge about cloud computing platforms!
"Kagglers from academia! Again, working with relational databases should be done more frequently if you wish to be prepared for the marketplace"
"Although they don't appear frequently in the job descriptions, we see that listings for Data Engineers and Software Engineers often require knowledge of database engines such as PostgreSQL, MySQL and SQL Server. Data Analysts with SQL Server Knowledge might benefit from it when looking for a job."
\n\nDistribution Plots
## **Cell Type Distribution**
\n\nImage View
## **Training Images**
"![Next Generation Weapon](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/02/pytorch-logo-flat-300x210.png)\n\n\n---\n10 Minute of Pytorch\n---\n---\n* [Pytorch Introduction](#Pytorch-Introduction)\n    * [PyTorch provides two high-level features](#PyTorch-provides-two-high-level-features)\n* [Environment Configuration](#Environment-Configuration)\n    * [Install pytorch on Windows Snapshop Tutorial](https://www.superdatascience.com/pytorch/)\n* [**Section : 1. Pytorch Basic Foundation**](#Section-1-:-Pytorch-Basic-Foundation)\n    * [Tensor](#Tensor)\n        * [Construct a 5x3 matrix uninitialized](#Construct-a-5x3-matrix-uninitialized)\n        * [Convert to numpy](#Convert-to-numpy)\n        * [Size of tensor](#Size-of-tensor)\n        * [From Numpy to tensor](#From-Numpy-to-tensor)\n    * [Tensor Operation](#Tensor-Operation)\n        * [Random similar to numpy](#Random-similar-to-numpy)\n        * [Construct a matrix filled zeros and of dtype long](#Construct-a-matrix-filled-zeros-and-of-dtype-long)\n        * [Construct a tensor directly from data](#Construct-a-tensor-directly-from-data)\n        * [Create tensor based on existing tensor](#Create-tensor-based-on-existing-tensor)\n        * [Basic Tensor Operation](#Basic-Tensor-Operation)\n    * [Variable](#Variable)\n    * [Activation Function](#Activation-Function)\n        * [Generate Fake Data](#Generate-Fake-Data)\n        * [Popular Activation Function](#Popular-Activation-Function)\n        * [Activation Function plot from data](#Activation-Function-plot-from-data)\n        \n* [**Section : 2. Neural Network**](#Section-:-2.-Neural-Network)\n     * [Linear Regression](#Linear-Regression)\n     * [Relationship Fitting Regression Model](#Relationship-Fitting-Regression-Model)\n     * [Distinguish type classification](#Distinguish-type-classification)\n     * [Easy way to Buid Neural Network](#Easy-way-to-Buid-Neural-Network)\n     * [Save and Reload Model](#Save-and-Reload-Model)\n     * [Train on Batch](#Train-on-batch)\n     * [Optimizers](#Optimizers)\n \n* [**Section : 3. Advance Neural Network**](#Section-:-3.-Advance-Neural-Network)\n    * [CNN](#CNN)\n    * [RNN-Classification](#RNN-Classification)\n    * [RNN-Regression](#RNN-Regression)\n    * [AutoEncoder](#AutoEncoder)\n    * [DQN Reinforcement Learning](#DQN-Reinforcement-Learning)\n    * [A3C Reinforcement Learning](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2) [(Code)](https://github.com/nailo2c/a3c/blob/master/tutorial.ipynb)\n    * [Generative Adversarial Network](#Generative-Adversarial-Network)\n    * [Conditional GAN](#Conditional-GAN)\n    \n    \n    \n\n\n---\nPytorch Introduction\n---\n---\nPyTorch is an open source machine learning library for Python, based on [Torch](https://en.wikipedia.org/wiki/Torch_(machine_learning),used for applications such as natural language processing.It is primarily developed by Facebook's artificial-intelligence research group, and Uber's ""Pyro"" software for probabilistic programming is built on it.\n\n#### PyTorch provides two high-level features:\n\n1. *Tensor computation (like NumPy) with strong GPU acceleration*\n1. *Deep Neural Networks built on a tape-based autodiff system*\n\n---\nEnvironment Configuration\n---\n---\n![](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/02/3-768x368.png)\n\n* [Install pytorch on Windows Snapshop Tutorial](https://www.superdatascience.com/pytorch/)\n"
Section-1 : Pytorch Basic Foundation\n---
### Activation Function\n---\n* Activation functions is used for a Artificial Neural Network to learn and make sense of something really complicated and Non-linear complex functional mappings between the **inputs and response variable.** They introduce non-linear properties to our Network.\n* Their main purpose is to convert a ***input signal of a node in a A-NN to an output signal.** That **output signal** now is used as a **input in the next layer** in the stack.\n![](https://cdn-images-1.medium.com/max/1200/1*ZafDv3VUm60Eh10OeJu1vw.png)
#### Generate Fake Data
#### Activation Function plot from data
Section : 2. Neural Network\n---
![](https://morvanzhou.github.io/static/results/torch/1-1-3.gif)
### Generate Fake Data
### Generate Fake Data
### Model design
"**Neural Network** - Neural networks, a beautiful biologically-inspired programming paradigm which enables a computer to learn from observational data.\n\n![](https://www.researchgate.net/profile/Reza_Rezaee8/publication/230905093/figure/fig1/AS:300580867198976@1448675471259/a-Schematic-diagram-of-an-artificial-neural-network-structure-which-consists-of-an.png)"
### Design Model
### Model Saving Function in Pickle Format
### Load save model
### Load save model
### Parameter Restore for net1 to net3
### Parameter Restore for net1 to net3
### Plot the Models
### Generate Fake Data
### Put Dataset in torch dataset
### Model Training
# Section : 3. Advance Neural Network\n---
### Show data
### Data Convert into Variable
"![](https://i.stack.imgur.com/WSOie.png)\n\n### Sequence Modeling\n\n![](https://morvanzhou.github.io/static/results/ML-intro/rnn2.png)\n* We imagine that there is now a set of sequence data **data 0,1,2,3.** When **predicting result0**, we are based on **data0, and when predicting other data,** we are only based on a single data. The neural networks are all the same NN. However, the data is in an ascending order, just like cooking in the kitchen. Sauce A is placed earlier than Sauce B, otherwise it will be scented. So the ordinary neural network structure can not let NN understands the association between these data.\n\n### Neural network for processing sequence\n\n![](https://morvanzhou.github.io/static/results/ML-intro/rnn3.png)\n\n* So how do we let the association between data be analyzed by NN? Think about how humans analyze the ***relationship between things.*** The most basic way is to remember *what happened before.* Then we let the neural network also have this. ***The ability to remember what happened before***. \n* When analyzing **Data0, we store the analysis results in memory. Then when analyzing data1, NN will generate new memories, but new memories and old memories are unrelated. Simply call the old memory and analyze it together. If you continue to analyze more ordered data, RNN will accumulate the previous memories and analyze them together.**\n\n![](https://morvanzhou.github.io/static/results/ML-intro/rnn4.png)\n\n* Let's repeat the process just now, but this time to add some mathematical aspects. Every time RNN is done, it will produce a description of the **current state**. We replace it with a **shorthand S(t), then the RNN starts Analyze x(t+1), which produces s(t+1) from x(t+1), but y(t+1) is created by s(t) and s(t+1)** So the RNN we usually see can also be expressed like this.\n\n### Application of RNN\n\n* The form of RNN is not limited to this. His structure is very free. If it is used for classification problems, for example, **if a person says a sentence, the emotional color of this sentence is positive or negative. Then we will You can use the RNN that outputs the judgment result only at the last time.**\n* Or this is the picture description RNN, we only need an X to replace the input picture, and then generate a paragraph describing the picture.\n* Or the **RNN of the language translation, give a paragraph of English, and then translate it into Any Langauge.**\n* With these different forms of RNN, RNN becomes powerful. There are many interesting RNN applications. **For example, let RNN describe the photo. Let RNN write the academic paper, let RNN write the program script, let RNN compose. We The average person can't even tell if this is written by the machine.**"
### Hyper Parameters
### Training and Testing
### Predicted and Actual Value match
"## RNN-Regression\n---\nNote : **Regression Concept I have already explaned above, so here I have applied direct RNN on Regression.**"
###  Hyper Parameters
### Show data
### Model Design
"## AutoEncoder\n---\n\n* ***Nueral Network Unsupervised form is Known as AutoEncoder.***\n\n* Autoencoders (AE) are neural networks that aims to copy their inputs to their outputs. They work by compressing the input into a latent-space representation, and then reconstructing the output from this representation. This kind of network is composed of two parts :\n\n* **1) Encoder:** This is the part of the network that compresses the input into a latent-space representation. It can be represented by an encoding function h=f(x).\n* **2) Decoder:** This part aims to reconstruct the input from the latent space representation. It can be represented by a decoding function r=g(h).\n\n![](https://cdn-images-1.medium.com/max/880/1*V_YtxTFUqDrmmu2JqMZ-rA.png)\n\n### What are autoencoders used for ?\n* Data denoising and Dimensionality reduction for data visualization are considered as two main interesting practical applications of autoencoders. With appropriate dimensionality and sparsity constraints, autoencoders can learn data projections that are more interesting than PCA or other basic techniques.\n\n### Types of autoencoder :\n* **Vanilla autoencoder**: In its simplest form, the autoencoder is a three layers net, i.e. a neural net with one hidden layer. The input and output are the same, and we learn how to reconstruct the input, for example using the adam optimizer and the mean squared error loss function.([Code](https://gist.githubusercontent.com/nathanhubens/604fd9cfc2d7f3d022e3ef0cf4b787de/raw/bc437b49f974b767e488a4a896b0e869d87a39d6/vanilla%20autoencoder))\n\n* **Multilayer autoencoder** :  If one hidden layer is not enough, we can obviously extend the autoencoder to more hidden layers.([Code](https://gist.githubusercontent.com/nathanhubens/219ab6efcfbab95508495eb6d6e41884/raw/d50fe4cb3b5d361da68156c789d5bd25f5dad321/multilayer%20autoencoder))\n* **Convolutional autoencoder**: We may also ask ourselves: can autoencoders be used with Convolutions instead of Fully-connected layers ?\nThe answer is yes and the principle is the same, but using images (3D vectors) instead of flattened 1D vectors. The input image is downsampled to give a latent representation of smaller dimensions and force the autoencoder to learn a compressed version of the images. ([Code](https://gist.github.com/nathanhubens/2f11dd9257263874b94966eb48e42922/raw/48f545769116607f713cded00c82d698ce9fb25a/convolutional%20autoencoder))\n\n* **Regularized autoencoder**:  There are other ways we can constraint the reconstruction of an autoencoder than to impose a hidden layer of smaller dimension than the input. Rather than limiting the model capacity by keeping the encoder and decoder shallow and the code size small, regularized autoencoders use a loss function that encourages the model to have other properties besides the ability to copy its input to its output. In practice, we usually find two types of regularized autoencoder: the sparse autoencoder and the denoising autoencoder.\n\n    * **Sparse autoencoder**: Sparse autoencoders are typically used to learn features for another task such as classification. An autoencoder that has been regularized to be sparse must respond to unique statistical features of the dataset it has been trained on, rather than simply acting as an identity function. In this way, training to perform the copying task with a sparsity penalty can yield a model that has learned useful features as a byproduct.([Code](https://gist.github.com/nathanhubens/c6000eee8d6f919d01465183f79a62b6/raw/2e5085740299cf1d9f0a28ddfb438eee4bfe5903/Sparse%20autoencoder))\n\n    * **Denoising autoencoder :** Rather than adding a penalty to the loss function, we can obtain an autoencoder that learns something useful by changing the reconstruction error term of the loss function. This can be done by adding some noise of the input image and make the autoencoder learn to remove it. By this means, the encoder will extract the most important features and learn a robuster representation of the data.([Code](https://gist.github.com/nathanhubens/2c2a7cc138e3d170956c109b10f5a7f7/raw/3b44ff899fe23f2224edf782e8dd068551d83ec5/denoising%20ae))\n"
### Hyper Parameters
### Model Training
### Visualize in 3D plot
### Visualize in 3D plot
"## DQN Reinforcement Learning\n---\n\n*  **Intensive learning**, Deep Q Network is referred to as DQN for short. The Google Deep Mind team is relying on this DQN to make computers play more powerful than us.\n\n![](https://cdn-images-1.medium.com/max/1600/1*M8RWevLxhus56RABFEGYYQ.png)\n\n### Reinforcement learning and neural networks\n---\n\n* **The reinforcement learning methods are more traditional methods.** Nowadays, with the various applications of **machine learning in daily life**, various machine learning methods are also integrated, merged and upgraded. The intensive study explored is a method that combines **neural network and Q learning , called Deep Q Network.** Why is this new structure proposed? Originally, traditional form-based reinforcement learning has such a bottleneck.\n\n### The role of neural networks\n---\n![](https://morvanzhou.github.io/static/results/ML-intro/DQN2.png)\n* We use a table to store each state state, and the Q value that each behavior action has in this state. The problem is that it is too complicated, and the state can be more than the stars in the sky (such as Go). Using tables to store them, I am afraid that our computer has not enough memory, and it is time consuming to search for the corresponding state in such a large table. However, in machine learning, there is a way It is very good for this kind of thing, that is the neural network. \n* We can use the state and action as the input of the neural network, and then the neural network analysis to get the Q value of the action, so we do not need to record the Q value in the table, and It is directly using the neural network to generate Q values. Another form is that we can only input the state value, output all the action values, and then directly select the action with the maximum value as the next step according to the principle of Q learning. \n* The action. We can imagine that the neural network accepts external information, which is equivalent to collecting information from the eyes, nose and ears, and then outputting each action through brain processing. The value, finally select the action by means of reinforcement learning.\n\n### Q-Learning [Reference](https://medium.com/@jonathan_hui/rl-dqn-deep-q-network-e207751f7ae4)\n---\n* Q-learning learns the action-value function Q(s, a): how good to take an action at a particular state. For example, for the board position below, how good to move the pawn two steps forward. Literally, we assign a scalar value over the benefit of making such a move.\n\n![](https://cdn-images-1.medium.com/max/800/1*srmv0GScAs6vObPfPj0-uQ.png)\n\n* In Q-learning, we build a memory table Q[s, a] to store Q-values for all possible combinations of s and a. If you are a chess player, it is the cheat sheet for the best move. In the example above, we may realize that moving the pawn 2 steps ahead has the highest Q values over all others. (The memory consumption will be too high for the chess game. But let‚Äôs stick with this approach a little bit longer.)\n\n* Technical speaking, we sample an action from the current state. We find out the reward R (if any) and the new state s‚Äô (the new board position). From the memory table, we determine the next action a‚Äô to take which has the maximum Q(s‚Äô, a‚Äô).\n\n![](https://cdn-images-1.medium.com/max/800/1*yh8Z2t41HBvY5gwBPTqVVQ.jpeg)\n\n* In a video game, we score points (rewards) by shooting down the enemy. In a chess game, the reward is +1 when we win or -1 if we lose. So there is only one reward given and it takes a while to get it.\n\n* We can take a single move a and see what reward R can we get. This creates a one-step look ahead. R + Q(s‚Äô, a‚Äô) becomes the target that we want Q(s, a) to be. For example, say all Q values are equal to one now. If we move the joystick to the right and score 2 points, we want to move Q(s, a) closer to 3 (i.e. 2 + 1).\n\n![](https://cdn-images-1.medium.com/max/800/1*9CdBkaFzFRyACvj5P2Af2Q.png)\n\n* As we keep playing, we maintain a running average for Q. The values will get better and with some tricks, the Q values will converge.\n\n---\n### Q-Learning Algorithm\n\n---\n![](https://cdn-images-1.medium.com/max/800/1*5ffOxpSgIJCYn0XccfFYUQ.png)\n\n\n* However, if the combinations of states and actions are too large, the memory and the computation requirement for Q will be too high. To address that, we switch to a deep network Q (DQN) to approximate Q(s, a). This is called Deep Q-learning. With the new approach, we generalize the approximation of the Q-value function rather than remembering the solutions.\n\n![](https://rubenfiszel.github.io/posts/rl4j/qmodeling.png)\n\n---\n### Deep Q-Network Algorithm with experience replay\n\n---\n\n**Algorithms**\n![](https://cdn-images-1.medium.com/max/800/1*8coZ4g_pRtfyoHmsuzMH6g.png)\n"
"## Generative Adversarial Network\n---\n![](https://cdn-images-1.medium.com/max/2000/1*AZ5-3WdNdYyC2U0Aq7RhIg.png)\n\n* In **2014, Ian Goodfellow and his colleagues at the University of Montreal published** a [stunning paper](https://arxiv.org/pdf/1406.2661.pdf) introducing the world to **GANs, or generative adversarial networks.** Through an innovative combination of computational graphs and game theory they showed that, given enough modeling power, two models fighting against each other would be able to co-train through plain old backpropagation.\n\n![](https://cdn-images-1.medium.com/max/800/1*-gFsbymY9oJUQJ-A3GTfeg.png)\n\n\n* The models play **two distinct (literally, adversarial) roles.** Given some real data set **R, G** is the **generator**, trying to **create fake data** that looks just like **the genuine data,** while **D** is the **discriminator**, getting data from either the real set or **G** and labeling the difference. **Goodfellow‚Äôs metaphor** (and a fine one it is) was that **G** was like a **team of forgers trying to match real paintings** with their output, while **D** was the team of **detectives trying to tell the difference**. (Except that in this case, the forgers G never get to see the original data‚Ää‚Äî‚Ääonly the judgments of D. They‚Äôre like blind forgers.)\n\n* **GANs** or **Generative Adversarial Networks** are a kind of neural networks that is composed of **2** separate deep neural networks competing each other: the **generator** and **the discriminator**.\n\n* **GAN** to generate various things. It can **generate realistic images, 3D-models, videos, and a lot more.Like this Example below**\n![](https://cdn-images-1.medium.com/max/800/1*NmRWSaTpBydHKnGIEAyWMw.png)\n\n---\n### Idea of GAN\n\n---\n#### Architecture of GAN\n![](https://cdn-images-1.medium.com/max/2000/1*39Nnni_nhPDaLu9AnTLoWw.png)\n**1) Generator**\n![](https://cdn-images-1.medium.com/max/1000/1*7i9iCdLZraZkrMy1-KADrA.png)\n**2) Discriminator:**\n![](https://www.researchgate.net/profile/Sinan_Kaplan2/publication/319093376/figure/fig20/AS:526859935731712@1502624605127/Architecture-of-proposed-discriminator-network-which-is-part-of-GAN-based-on-CNN-units.png)\n\n### Conceptual Diagram\n\n![](https://cdn-images-1.medium.com/max/1200/1*M2Er7hbryb2y0RP1UOz5Rw.png)\n\n> * **""The generator will try to generate fake images that fool the discriminator into thinking that they‚Äôre real. And the discriminator will try to distinguish between a real and a generated image as best as it could when an image is fed.‚Äù**\n\n* They both get stronger together until the **discriminator** cannot **distinguish** between the **real and the generated images anymore**. It could do nothing more than **predicting real or fake with only 50% accuracy.** This is no more useful than **flipping a coin and guess.** \n* This inaccuracy of the **discriminator occurs because the generator generates really realistic face images** that it seems like they are actually real. So, it is normally expected that it wouldn‚Äôt be able to distinguish them. When that happens, the most educated guess would be as equally useful as an uneducated random guess.\n\n---\n### The optimal generator\n---\n* Intuitively, the Code vector that I shown earlier in the generator will represent things that are abstract. For example, if the Code vector has 100 dimensions, there might be a dimension that represents ‚Äúface age‚Äù or ‚Äúgender‚Äù automatically.\n* Why would it learn such representation? Because knowing people ages and their gender helps you draw their face more properly!\n\n---\n### The optimal discriminator\n---\n* When given an image, the discriminator must be looking for components of the face to be able to distinguish correctly.\n* Intuitively, some of the discriminator‚Äôs hidden neurons will be excited when it sees things like eyes, mouths, hair, etc. These features are good for other purposes later like classification!"
### Hyper Parameters
"## 2. Simplifying the Observation Space\n\nLux S2 is fully observable which means you can see everything on the map, the opponents units etc. However, this is very high dimensional and not necessarily easy to learn from due to the curse of dimensionality (again!). We want to simplify this observation space in a way that contains sufficient information to learn a good policy but is also easy to learn from.\n\nFor this tutorial, we will create a state-based observation space (no image like features e.g. the rubble, ice, ore maps) with some feature engineering that includes useful information such as the distance to the closest factory and ice tile. The wrapper we provide below will use the `gym.ObservationWrapper` interface. Note that since we are focusing on just controlling one heavy robot, the observation wrapper is written to only support one heavy robot (and returns 0 if there are none).\n\n\nMore advanced solutions can look into using the full set of observations and designing the appropriate neural net architecture to process them. One idea would be to use convolutional neural networks to process board features like images. See [Season 1's solution by ToadBrigade](https://www.kaggle.com/competitions/lux-ai-2021/discussion/294993) and our previous [research paper: Emergent Collective Intelligence from Massive-Agent Cooperation and Competition](https://arxiv.org/abs/2301.01609) for example architectures and feature engineering choices.\n"
"## 3. Transforming Lux S2 into a Single Phase\n\nNormally RL frameworks like Stable Baselines 3, RLlib, Tianshou etc. expect the action space and observation space to be consistent throughout an episode. Lux S2 does not conform to this as we add some additional complexity like bidding and factory placement phases. A simple way to get around this is to **upgrade the reset function.**\n\nPreviously we saw that `env.reset()` resets an environment to a clean slate. We will upgrade this function by building a environment wrapper that not only resets to the clean slate, but also handles the bidding and factory placement phases so effectively agents that are learning start from game states with factories already placed.\n\nBelow will build a wrapper that works with the SB3 package. To do this, we want to provide the wrapper a bidding policy and factory placement policy which will be used by all teams to handle the first two phases in the reset function. The code below does just that by overriding the environment's reset function in the wrapper. \n\nFurthermore, we want to use the Controller we defined earlier, so that is also an argument to the SB3Wrapper and we use it to transform actions inside the `env.step` function"
"## 3. Transforming Lux S2 into a Single Phase\n\nNormally RL frameworks like Stable Baselines 3, RLlib, Tianshou etc. expect the action space and observation space to be consistent throughout an episode. Lux S2 does not conform to this as we add some additional complexity like bidding and factory placement phases. A simple way to get around this is to **upgrade the reset function.**\n\nPreviously we saw that `env.reset()` resets an environment to a clean slate. We will upgrade this function by building a environment wrapper that not only resets to the clean slate, but also handles the bidding and factory placement phases so effectively agents that are learning start from game states with factories already placed.\n\nBelow will build a wrapper that works with the SB3 package. To do this, we want to provide the wrapper a bidding policy and factory placement policy which will be used by all teams to handle the first two phases in the reset function. The code below does just that by overriding the environment's reset function in the wrapper. \n\nFurthermore, we want to use the Controller we defined earlier, so that is also an argument to the SB3Wrapper and we use it to transform actions inside the `env.step` function"
"### Defining a Bid and Factory Placement policy\n\nTo test the code above, we can program some heuristic bid and factory placement policies"
"So **without the wrapper**, when we reset the environment it looks like this:"
"**With the wrapper**, when we reset the environment it looks like this:"
"**With the wrapper**, when we reset the environment it looks like this:"
"Success! Our upgraded reset function makes the environment now start from the start of the normal game phase, meaning the action space can be consistently the same throughout the game."
"## 3. Training with RL\n\nIn the previous tutorial, we saw how to train an agent with SB3 in single-agent environments. Handling true multi-agent via training separate or shared policies to control all agents requires a few extra things so instead, for the purpose of a tutorial we will treat Lux S2 like a single agent environment by training a policy for one team and letting the other team simply do nothing.\n\nMoreover, we want to define our own reward function to encourage our robots to seek ice, dig it, and return to a factory so it can generate water and survive longer. To do this all, we will just create a custom environment wrapper.\n\n\n"
### 3.1 Defining the Environment and using Wrappers
\n\nLibraries
\n\nGlobal Config
## **Combining Image with Mask** 
---
## **Organ** 
## **Data Source** 
## **Data Source** 
## **Age** 
## **Age** 
## **Sex** 
## **Sex** 
\n\nReferences
Check the distribution of features below to try to fill not so big NULL valued columns
"Let's fill the Embarked column with more frequently value ""S"".   \nThe column Fare fill with a mean value"
# 2. | Importing Libraries üìö\n\n    üëâ Installing and importing libraries that will be used in this notebook.\n
# 3. | Color Palettes üé®\n\n    üëâ This section will create some color palettes that will be used in this notebook.\n
# 3. | Color Palettes üé®\n\n    üëâ This section will create some color palettes that will be used in this notebook.\n
"# 4. | Reading Dataset üëì\n\n    üëâ After importing libraries, the datasets that will be used will be imported.\n"
## 6.1 | Total Survival based on Gender and Title üöªüéì\n
"\n    üëâ The majority of Titanic passengers were males. In more detail, the titanic male passengers are dominated by men over 18. Meanwhile, titanic female passengers are dominated by females aged less than 18 years.\n    üëâ From the plot above, it is clear that most Titanic survivor are dominated by female passengers.\n    üëâ Men over 18 years old (Mr.) had more survivors than men under 18. The opposite happened for women, where the number of female survivors under 18 years old (Miss) has more digits than women over 18 years old.\n    üëâ Two males and one female out of 7 passengers with ""Dr."" title survived the events of the Titanic.\n    üëâ None of the men who have the title ""Rev"" survived üò¢.\n\n"
## 6.2 | Cabin and Class Survival Distributions ü™ôüõ≥Ô∏èüòá\n
"\n    üëâ From the chart, majority of passengers have an unidentified cabin code.\n    üëâ Passengers in the middle cabin (B, C, D, and E) had more survivors than passengers in other cabins.\n\n"
"\n    üëâ From the chart, majority of passengers have an unidentified cabin code.\n    üëâ Passengers in the middle cabin (B, C, D, and E) had more survivors than passengers in other cabins.\n\n"
## 6.4 | Embarked Survival Rate and Fare Distributions ‚öìüí∞\n
## 6.4 | Passengers Family Sizes Distributions üë®‚Äçüë©‚Äçüëß‚Äçüë¶\n
## 6.5 | Age Distribution based on Survival üßìüòá\n
## 6.5 | Age Distribution based on Survival üßìüòá\n
"\n    üëâ From the chart, it can be seen that not survived distribution is leptokurtic, while other is platikurtic. Furthermore, there are outliers on the right side of both plots.\n\n"
"# 8. | Model Implementation ‚öô\n\n    üëâ This section will implement various machine learning models as mentioned in Introduction section. In addition, explanation for each models will be provided.\n"
"## 8.1 | Logistic Regression\n\n    \n    Logistic regression is a statistical method that is used for building machine learning models where the dependent variable is dichotomous: i.e. binary. Logistic regression is used to describe data and the relationship between one dependent variable and one or more independent variables. The independent variables can be nominal, ordinal, or of interval type.\n    The name ""logistic regression"" is derived from the concept of the logistic function that it uses. The logistic function is also known as the sigmoid function. The value of this logistic function lies between zero and one.\n    \n    \n    üñº Logistic Function by Simplilearn\n    \n    \n\n"
Let's see how scores for both Top20% and Bottom 80% are distributed in test data?
"We see on the above chart that the two classes are very well defined, and distinct from each other. This is confirmed when we plot the ROC curve.\n\n> In a Receiver Operating Characteristic (ROC) curve the true positive rate (Sensitivity) is plotted in function of the false positive rate (100-Specificity) for different cut-off points. Each point on the ROC curve represents a sensitivity/specificity pair corresponding to a particular decision threshold. A test with perfect discrimination (no overlap in the two distributions) has a ROC curve that passes through the upper left corner (100% sensitivity, 100% specificity). Therefore the closer the ROC curve is to the upper left corner, the higher the overall accuracy of the test (Zweig & Campbell, 1993)."
"We see on the above chart that the two classes are very well defined, and distinct from each other. This is confirmed when we plot the ROC curve.\n\n> In a Receiver Operating Characteristic (ROC) curve the true positive rate (Sensitivity) is plotted in function of the false positive rate (100-Specificity) for different cut-off points. Each point on the ROC curve represents a sensitivity/specificity pair corresponding to a particular decision threshold. A test with perfect discrimination (no overlap in the two distributions) has a ROC curve that passes through the upper left corner (100% sensitivity, 100% specificity). Therefore the closer the ROC curve is to the upper left corner, the higher the overall accuracy of the test (Zweig & Campbell, 1993)."
## Probability of Being in the Top 20% per Score\nNow let's calculate the probability of belonging to the Top 20% given a certain score. To do that we will create score ranges. We calculate the probability based on how the model performed on test data. Below we show the probability for each bin.
"### üìå Notes:\n\n> - `cp` {Chest Pain} : People with cp equl to 1, 2, 3 are more likely to have heart disease than people with cp equal to 0.\n> - `restecg` {resting electrocardiographic results} : People with value 1 (signals non-normal heart beat, can range from mild symptoms to severe problems) are more likely to have heart disease.\n> - `exang` {exercise induced angina} : People with value 0 (No ==> exercice induced angina) have heart disease more than people with value 1 (Yes ==> exercice induced angina)\n> - `slope` {the slope of the peak exercise ST segment} : People with slope value equal to 2 (Downslopins: signs of unhealthy heart) are more likely to have heart disease than people with slope value equal to 0 (Upsloping: better heart rate with excercise) or 1 (Flatsloping: minimal change (typical healthy heart)).\n> - `ca` {number of major vessels (0-3) colored by flourosopy} : the more blood movement the better so people with ca equal to 0 are more likely to have heart disease.\n> - `thal` {thalium stress result} : People with thal value equal to 2 (fixed defect: used to be defect but ok now) are more likely to have heart disease."
### üìå Notes:\n> - `trestbps` : resting blood pressure (in mm Hg on admission to the hospital) anything above 130-140 is typically cause for concern\n> - `chol` {serum cholestoral in mg/dl} : above 200 is cause for concern.\n> - `thalach` {maximum heart rate achieved} : People how acheived a maximum more than 140 are more likely to have heart disease.\n> - `oldpeak` ST depression induced by exercise relative to rest looks at stress of heart during excercise unhealthy heart will stress more
# 1.2 - Null Values\n\nLet's explore the issue of missing values in the dataset to see if there are systemic problems with data representation.
The good news is that we don't have any null values when it comes to CSF observations with the peptide and protein data. We do however see null values when it comes to the clinical data. Let's see what we are missing and why. We'll start with the clinical data.
The good news is that we don't have any null values when it comes to CSF observations with the peptide and protein data. We do however see null values when it comes to the clinical data. Let's see what we are missing and why. We'll start with the clinical data.
"We are starting to see some interesting trends. Let's break this down by null counts.\n\n### Rows with 1 Null Value\n\nWhen there is a single null value in the row, it usually corresponds to the feature `upd23b_clinical_state_on_medication`. Valid responses are one of `On` or `Off`. Null values within the field are of interest, as it is uncertain whether they indicate that the patient was `Off` of medication, or if the assessment failed to capture the medication status of the patient. In the other two instances of null value counts, they occur 7 times in `updrs_3` and 21 times in `updrs_4`. According to Goetz et al (2008), part 3 of the UPDRS assessment concerns motor assessment, and has a minimum score of 0. Part 4 of the UPDRS assessment concerns motor complications, and again has a minimum score of 0. Null values in either of those columns suggest that the assessment was not performed. This is important, as a score of 0 indicates that the patient was assessed and was deemed to have normal responses. \n\n### Rows with 2 Null Values\n\nWhen there are two null values in the row, they usually correspond to `updrs_4` and `upd23b_clinical_state_on_medication`. As mentioned previously, valid responses are `On` or `Off`, thus a null value here is of interest as we cannot be certain whether the assessment failed to capture medication status. The majority of the other null value fields occur with `updrs_4`, which concerns motor complications. Other null values occur infrequently in the `updrs_3`, and `updrs_2` fields. Again, UPDRS part 3 concerns motor assessment, and null values here cannot assumed to be 0 scores, given that 0 indicates normal function. With UPDRS part 2, the assessment concerns motor experiences of daily living, and again, null values here may indicate that the assessment was not performed. \n\n### Rows with 3 Null Values\n\nThere are 10 instances where rows contain 3 null values. In each instance, the row is missing information from `updrs_3`, `updrs_4`, and `upd23b_clinical_state_on_medication`. Again missing values cannot be assumed to be 0.\n\n### Rows with 4 Null Values\n\nOnly a single instance of a row with 4 null values occurs. It appears that only the UPDRS part 3 assessment was performed at the visit. Again, null values cannot be interpreted as being 0, given that 0 based scores indicate a normal response. "
We need to also look at the supplemental information for null values.
"Again, similar patterns emerge. When there is a single null value in a row, it either appears in `updrs_4` and `upd23b_clinical_state_on_medication`. The same trend continues when there are two null values. When there are three null values, the trend differs slightly from the clinical data. In this case, the supplemental data is more likely to be missing `updrs_1` and `updrs_2` when compared to the clinical data. Finally, when there are four missing values, we see the same missing values in `updrs_1`, `updrs_2`, `updrs_4` and `upd23b_clinical_state_on_medication`. There is a significantly higher number of 4 null value rows in the supplemental data than in the clinical data.\n\n### Key Observations About Null Values\n\n* No null values are missing from peptide and protein data.\n* Null values occur in the clinical data and supplemental data. General trends are:\n    * Single null values occur most frequently in the `updrs_4` and `upd23b_clinical_state_on_medication` features.\n    * Two null values occur most frequently in the `updrs_4` and `upd23b_clinical_state_on_medication` features.\n    * Three null values occur most frequently in the:\n        * `updrs_3`, `updrs_4`, and `upd23b_clinical_state_on_medication` features for the clinical data.\n        * `updrs_1`, `updrs_2`, and `upd23b_clinical_state_on_medication` features for the supplemental data.\n    * Four null values occur most frequently in the `updrs_1`, `updrs_2`, `updrs_4`, and `upd23b_clinical_state_on_medication` features.\n* The supplemental data has many more examples of rows with four null values when compared to the clinical data.\n* Care must be taken when dealing with null values:\n    * It is not apparent whether the null values should be used to indicate a missed assessment, or whether they can be set to another value.\n        * For UPDRS assessments, it may be erroneous to set the value to 0 as that would indicate a ""normal"" result.\n        * For the `upd23b_clinical_state_on_medication` feature, the only valid settings are `On` or `Off`, thus the impact of null is undefined."
"# 1.3 - Duplicated Rows\n\nWe should next check to see if we have any duplicated values in our various datasets. Duplicates may impact our learning methods, resulting in prediction bias toward the duplicate information. "
"### Key Observations About Duplicated Rows\n\n* In terms of raw duplicates:\n    * In the clinical data:\n        * There are 2 instances where the same row of data appears twice.\n        * Duplicates account for 0.15% of all clinical data.\n    * In the supplemental data:\n        * There are 7 instances where the same row of data appears twice.\n        * Duplicates account for 0.63% of all supplemental data.\n    * In the protein data:\n        * There are 400 instances where the same row of data appears twice.\n        * Duplicates account for 0.35% of all protein data.\n    * In the peptide data:\n        * There are 1,765 instances where the same row of data appears twice.\n        * There are 2 instances where the same row of data appears 3 times.\n        * Duplicates account for 0.36% of all peptide data.\n* Overall, with the clinical and supplemental data, duplicates are likely to have little or no impact."
"# 1.4 - Statistical Breakdown\n\nLet's take a closer look at some of the statistical properties of the continuous features. \n\n## 1.4.1 - Clinical vs Supplemental Data\n\nTo begin, let's compare the clinical data to the supplemental data to see what kind of differences we have.\n\n### Clinical Data"
### Supplemental Data
### Supplemental Data
"Supplemental data appears to have visits that occur mainly between 0 and 36 months, while clinical data shows visits occurring between 0 and 108 months. We can confirm this by looking at kernel density estimates for the months of the various visits."
"Supplemental data appears to have visits that occur mainly between 0 and 36 months, while clinical data shows visits occurring between 0 and 108 months. We can confirm this by looking at kernel density estimates for the months of the various visits."
"As we can see, the supplemental data is focused around 0 month visits, and ends at 36 months, while the clinical data spans a much longer time-frame. We can also do a quick visual check to see if there are differences between the clinical and supplemental data when it comes to UPDRS scores. For the figures below, the trend lines are kernel density estimates, thus differences in raw counts are taken into consideration with the trend lines."
"As we can see, the supplemental data is focused around 0 month visits, and ends at 36 months, while the clinical data spans a much longer time-frame. We can also do a quick visual check to see if there are differences between the clinical and supplemental data when it comes to UPDRS scores. For the figures below, the trend lines are kernel density estimates, thus differences in raw counts are taken into consideration with the trend lines."
There are a few interesting observations:\n\n* UPDRS Part 1 and 4 scores appear to be fairly similar in their distribution between the clinical and supplemental data sources.\n* UPDRS Part 2 and 3 scores have a much higher proportion of 0 based scores in the clinical data when compared to the supplemental data source.
"As a final check, we can get a rough measure of the differences between the clinical data and supplemental data by performing an adversarial validation. The goal with adversarial validation is to see if a classifier can tell the two datasets apart. We'll use ROC AUC score to inform us of differences. If the two sets appear to be very similar, the classifier will not be able to tell them apart, and thus will have an ROC AUC score of 0.5. If they are easy to tell apart - and thus are very dissimilar - then the ROC AUC score will approach 1. "
"With an AUC ROC score of 0.939, we can see that the classifier can easily tell the two datasets apart. This suggests that they are very dissimilar in nature, as was indicated as part of the competition. Caution should be used when mixing these two datasets, as they are very different in nature."
## 1.4.2 - Protein Data\n\nLet's take a look at the values we have for protein data.
Protein expression frequency values appear to have a wide range of values. We'll use a quick kernel density estimate to get an idea of where frequencies are clustered. We'll use a logarithmic scale due to the large values and potential variability involved in the expression frequencies.
Protein expression frequency values appear to have a wide range of values. We'll use a quick kernel density estimate to get an idea of where frequencies are clustered. We'll use a logarithmic scale due to the large values and potential variability involved in the expression frequencies.
"As we can see, there is a lot of variability regarding the actual protein expression frequencies. We'll look more into the distribution of various proteins and their association to the UPDRS scores in section 2 below. For now, the key observation we have is that normalized protein expression is highly variable, as indicated by the min, max, and standard deviation of the feature."
## 1.4.3 - Peptide Data\n\nLet's take a look at what we have for the peptide data.
"Again, we see a wide variation in the abundance of peptides. The min, max, and standard deviation tell us that peptide abundances will likely vary greatly depending on the particular peptide we are looking at. Again, we can plot kernel density estimates to give us an idea of where the bulk of our values exist."
"Again, we see a wide variation in the abundance of peptides. The min, max, and standard deviation tell us that peptide abundances will likely vary greatly depending on the particular peptide we are looking at. Again, we can plot kernel density estimates to give us an idea of where the bulk of our values exist."
"Once again, we'll look at peptide data - specifically peptide sequences and how they relate to UPDRS scores - in section 2 below."
"# 2 - Feature Exploration\n\nIn this section, we will examine each of the features we have to work with in more detail.\n\n# 2.1 - Visit Month\n\nThe visit month has an impact across all of the different datasets, and subsequently, through many different features that we have. Let's take a look at them in turn.\n\n# 2.1.1 - Visit Month vs UPDRS \n\nFor each visit month, we have observations about the target features - UPDRS scores. According to Holden et al (2018), the findings in each part of the UPDRS were dependent on whether or not the patient was taking medication. We should sub-divide the UPDRS score observations into groups that were taking medication, and those that were not. For the purposes of this exploration, a null value found in clinical data regarding medication state will be considered to be `Off`."
"Some general observations when OFF medication:\n    \n* There is a large amount of variance and outliers across each of the UPDRS parts and their respective visit months.\n* In general across UPDRS Parts 1 - 3, the trendline of score remains relatively flat.\n    * With UPDRS Part 4, we see a gradual increase in score."
"Some general observations when OFF medication:\n    \n* There is a large amount of variance and outliers across each of the UPDRS parts and their respective visit months.\n* In general across UPDRS Parts 1 - 3, the trendline of score remains relatively flat.\n    * With UPDRS Part 4, we see a gradual increase in score."
"Some general observations when ON medication:\n    \n* There is a large amount of variance and outliers across each of the UPDRS parts and their respective visit months.\n* In general across UPDRS Parts 1, 2, and 4, the trendline of score remains relatively flat.\n    * With UPDRS Part 3, we see a gradual increase in score.\n\nAs mentioned by Holden et al (2018), the maximum score of the UPDRS is 272. In prior versions of the UPDRS, there was a linear progression of UPDRS score as time progressed. We should look at the sum total of the UPDRS scores to see if there is a score increase over time within this data."
"Some general observations when ON medication:\n    \n* There is a large amount of variance and outliers across each of the UPDRS parts and their respective visit months.\n* In general across UPDRS Parts 1, 2, and 4, the trendline of score remains relatively flat.\n    * With UPDRS Part 3, we see a gradual increase in score.\n\nAs mentioned by Holden et al (2018), the maximum score of the UPDRS is 272. In prior versions of the UPDRS, there was a linear progression of UPDRS score as time progressed. We should look at the sum total of the UPDRS scores to see if there is a score increase over time within this data."
"With the sum of all UPDRS scores while OFF medication, we see an upwards trend as the visit month increases, which indicates that overall, disease progression is occurring. While ON medication, the trendline remains relatively flat until months > 96, which sees an increase in overall score. Again, this indicates that disease progression is occurring. If we combine both ON and OFF medication status:"
"With the sum of all UPDRS scores while OFF medication, we see an upwards trend as the visit month increases, which indicates that overall, disease progression is occurring. While ON medication, the trendline remains relatively flat until months > 96, which sees an increase in overall score. Again, this indicates that disease progression is occurring. If we combine both ON and OFF medication status:"
"Overall, we see a trend of increasing UPDRS scores. This observation is important, as it suggests that our scores should likely see increases as time progresses, rather than decreases. This can be used as a post-processing check to ensure predictions being made by our machine learning algorithms make sense."
"# 2.1.2 - Visit Month vs Protein Data\n\nWithout diving too much into the actual protein data, we should check to see if there are general trends regarding the breakdown of protein data by month."
"Unsurprisingly, we see stable amounts of protein expressions across each month category. There is a large amount of variance regarding the protein expression frequencies, but overall, we're seeing the same mean total NPX values repeated across the months. This likely means that there will be differences in the actual proteins expressed, rather than their absolute numbers. \n\nWe can check to see if there are any significant increases or decreases in `UniProt` proteins across the months. Looking at all 227 proteins is going to be challenging. For this, we'll look at proteins that have significant increases or decreases across the months. We'll examine protein counts for all 227 proteins, and then pick out ones that appear to have very large standard deviations compared to their mean. For this EDA, we'll look at any protein expression data that has a standard deviation of more than 25% of the mean value."
"There are a lot of proteins to examine with the correlation matrix. Let's start by defining what we would consider to be a somewhat significant correlation (positive or negative). Values that are 0.1 or below are likely to have little correlation to the UPDRS target scores, and are likely just noise. A quick scan reveals that there are several candidates that may not be useful in our regression:\n\n* `O00533`\n* `O14498`\n* `O15240`\n* `O15394`\n* `O43505`\n* `O60888`\n* `P00738`\n* `P01034`\n* `P01042`\n* `P01717`\n* `P02452`\n* `P02649`\n* `P02751`\n* `P02753`\n* `P02787`\n* `P04075`\n* `P04156`\n* `P04180`\n* `P04216`\n* `P05060`\n* `P05067`\n* `P05155`\n* `P05408`\n* `P05452`\n* `P06396`\n* `P07195`\n* `P07225`\n* `P07602`\n* `P07711`\n* `P07858`\n* `P08133`\n* `P08253`\n* `P08571`\n* `P09104`\n* `P09486`\n* `P09871`\n* `P10645`\n* `P11142`\n* `P13521`\n* `P13591`\n* `P13611`\n* `P13987`\n* `P14313`\n* `P14618`\n* `P17174`\n* `P19021`\n* `P23083`\n* `P23142`\n* `P39060`\n* `P40925`\n* `P43121`\n* `P49908`\n* `P54289`\n* `P55290`\n* `P61278`\n* `P61769`\n* `P61916`\n* `P98160`\n* `Q02818`\n* `Q06481`\n* `Q08380`\n* `Q12907`\n* `Q13332`\n* `Q14118`\n* `Q14508`\n* `Q14515`\n* `Q15904`\n* `Q16610`\n* `Q6UXB8`\n* `Q7Z3B1`\n* `Q8NBJ4`\n* `Q92520`\n* `Q92823`\n* `Q96KN2`\n* `Q99435`\n* `Q99674`\n* `Q9BY67`\n* `Q9NQ79`\n* `Q9NYU2`\n* `Q9UHG2`\n* `P01594`\n* `Q13449`\n* `Q99829`\n\nThere are some proteins that are weak correlates _only_ to `updrs_4`. These are:\n\n* `P00746`\n* `P02749`\n* `P02774`\n* `P04211`\n* `P04217`\n* `P05155`\n* `P06681`\n* `P19827`\n* `P20774`\n* `P31997`\n* `P61626`\n* `Q96BZ4`\n* `Q96PD5`\n\nThe challenge is going to be how we use this knowledge. Our correlation analysis only worked because we were able to ignore values that were missing. For machine learning regression to work, we'll need to satisfy one of the following conditions to use the data:\n\n* Have complete records for every protein type for every visit.\n* Figure out a way of imputing missing values.\n* Use a machine learning algorithm that implicitly handles missing data.\n\nLet's take a look at how our proteins appear across visits. Specifically, we know that there are are 2,615 unique visits. The question is how much of each protein we see given the total number of visits we have. If we only see a protein three or four times, even if it is correlated with an UPDRS score, it's likely not going to help out too much."
"It appears that for all of the protein data, proteins measurement data exists for at most 40% of the visits we have on record. This is going to be somewhat problematic to track trends. The instances where we don't have measurements for a specific protein are going to overwhelm examples where we do have protein measurements, creating a confounding effect. Even more problematic is the visit months where those measurements come from. "
We should also look at it in terms of patients at each month. What percentage of patients are lacking protein data? Do we have representation of proteins at every month for at least some of our patients?
"As we can see, at months 3, 9, 18, 30, 42, 54, and 96, we are lacking protein data for nearly all of the patients in the study."
"# 3 - Research Papers\n\nThere is a body of real-world research that may provide some targeted insights into the data that we have at hand.\n\n# 3.1 - Cerebrospinal Fluid Peptides as Potential Parkinson Disease Biomarkers (2015)\n\nThe research by Shi et al (2015) looks specifically at CSF proteins and peptides that are potential indicators of Parkinson's Disease. Of those identified, the following are available in the training data that we have available:\n\n* Proteins:\n    * `P00450` - Ceruloplasmin (CP)\n    * `P07333` - Macrophage colony-stimulating factor 1 receptor (CSF1R)\n    * `P10451` - Osteopontin (SPP1)\n    * `P01033` - Metalloproteinase inhibitor 1 (TIMP1)\n    * `P01008` - Antithrombin-III (SERPINC1)\n    * `P02647` - Apolipoprotein A-I (APOA1)\n    * `P01024` - Complement C3 (C3)\n    * `Q92876` - Kallikrein-6 (KLK6)\n* Peptides:\n    * `GAYPLSIEPIGVR` - associated with protein Ceruloplasmin (CP)\n    * `EPGLC(UniMod_4)TWQSLR` - associated with protein Metalloproteinase inhibitor 1 (TIMP1)\n    * `WQEEMELYR` - associated with protein Apolipoprotein A-I (APOA1)\n    * `QPSSAFAAFVK` - associated with protein Complement C3 (C3)\n    * `GLVSWGNIPC(UniMod_4)GSK` - associated with protein Kallikrein-6 (KLK6)\n    \nWe should check to see how these protein levels impact UPDRS scores. Let's start by checking correlation of these peptide and protein levels against various UPDRS scores."
"Some interesting observations here. None of the peptides or proteins have any significant positive or negative correlation to UPDRS scores except for:\n\n* Protein `P07333` has a weak negative correlation to `updrs_3`\n* Peptide `GLVSWGNIPC(UniMod_4)GSK` has a weak negative correlation to `updrs_1`. \n\nThese observations fall in line with the findings from the paper. In brief, the authors noted that no single protein or peptide in isolation had a clear correlation to UPDRS scores - it was only in combination with other peptides and proteins that a stronger signal was present. The paper outlines a 2-peptide model that had strong correlations with overall disease severity. Unfortunately, the training dataset does not have the correct combination of proteins and peptides as described in the paper. Nonetheless, these two findings above may be enough to provide minimal lift."
"# 4.2 - Constant UPDRS 4\n\nOverall, our model currently cannot learn enough information about UPDRS 4 due to missing and null values. A simple way to handle this is to zero it out. This isn't very good from a clinical standpoint, but it will likely generate lift for us, since we won't get penalized for the many times we're likely to predict a value where there should be none. For example, see discussion [here](https://www.kaggle.com/competitions/amp-parkinsons-disease-progression-prediction/discussion/393104)."
"As we can see, simply setting the UPDRS 4 value to 0 improves our SMAPE score. There is obviously room here to make improvements, especially if we could leverage information to make better UPDRS 4 predictions. For models moving forward, we'll continue to use the zeroed out UPDRS score."
"# 4.3 - Supplemental Data\n\nIf we are going to continue to ignore protein and peptide data, then we can add in the additional clinical data to the mix. This is likely to generate lift for us, since we give the regressor more information to work with."
We see a little improvement over using the clinical data alone. This is likely going to help us in instances where the hidden test data contains no protein information.
"# 4.4 - Medication State\n\nWhile the medication state of a patient isn't available on our test data, there may still be some interest in it, as we saw that the medication state did have a direct impact on UPDRS scores over time. Let's take a look and see if it could potentially provide lift if we had it."
"We can see that there is lift to our model if we take into account medication state. This suggests that maybe a classifier capable of soft-labeling medication state may be applicable, assuming there is signal in the protein data to provide context for the classifier to work with."
"# 4.5 - Protein Data\n\nUp until now, we haven't looked at all at protein or peptide information. Let's see what happens when we add in raw numbers."
# 4.x - Model Comparisons\n\nWe can compare the performance of each of our models to see which one performs the best. The red dashed line indicates baseline performance that we are looking to beat.
# 4.x - Model Comparisons\n\nWe can compare the performance of each of our models to see which one performs the best. The red dashed line indicates baseline performance that we are looking to beat.
"# 5 - Conclusions\n\nThere are several conclusions based on our observations:\n\n* The size of the dataset means that memory pressure will likely not be too great. \n* There are a total of 248 patients in the clinical data, and 771 patients in the supplemental data.\n* In terms of missing information:\n    * There are no null entries in the protein or peptide data.\n    * There are null entries in the clinical data and supplemental data.\n        * We cannot assume that null values indicate a 0 value for features such as UPDRS assessment parts, as values of 0 indicate a ""normal"" response.\n        * Null values in the medication state of the patient cannot be assumed to be either `On` or `Off`.\n* Duplicated data appears very infrequently, and is unlikely to impact machine learning models if not explicitly filtered out.\n* Clinical data appears to have a greater range in `visit_month` when compared to supplemental data (0 - 108 months compared to 0 - 36 months).\n    * In general, clinical and supplementary data is very different in terms of data distributions, as confirmed statistically, and through adversarial validation.\n* In reference to research by Shi et al (2015):\n    * The protein and peptide samples obtained do not not occur in the combinations needed to act as clear indicators as disease progression or severity.\n    * Our correlation analysis suggested that protein `P07333` has a weak negative correlation to `updrs_3`, and peptide `GLVSWGNIPC(UniMod_4)GSK` has a weak negative correlation to `updrs_1`."
\n## Table of Contents\n* [Introduction](#top_section)\n    - [Well... What do we have here?](#section1)\n* [Exploring the Data](#section2)\n    - [Categorical Features](#section3)\n    - [Numerical Features](#section4)\n    - [Missing Values](#section5)    \n* [Building the Feature Engineering Machine](#section6)\n    - [Data Merger](#section7)\n    - [Family Assembler](#section8)\n    - [Family Survival Detector](#section9)   \n    - [Title Extractor](#section10)\n    - [Title Encoder](#section11)\n    - [Age Filler](#section12)\n    - [Age Grouper](#section13)\n    - [Fare Imputer](#section14)\n    - [Fare Encoder](#section15)\n    - [Scaler](#section16)\n    - [Embarked Processor](#section17)\n    - [Deck Finder](#section18)\n    - [Gender Mapper](#section19)\n    - [Pclass Sorter](#section20)\n    - [Ticket Cleaner](#section21)\n    - [Housekeeping](#section22)\n    - [Feeding the Machine](#section23)\n* [Double Check](#section24)\n    - [Correlation Matrix](#section25)\n* [Modelling](#section26)\n    - [Model Selection](#section27)\n    - [Cross-Validate Models](#section28)\n    - [Model Results](#section29)\n    - [ROC'S of the Models](#section30)\n    - [Learning Curves of the Models](#section31)\n* [Feature Selection](#section31.1)\n    - [Feature Importances](#section32)\n    - [Decision Trees](#section33)    \n    - [Feature Selection by Recursive Feature Elimination](#section34)\n    - [Dimension Reduction by Principal Component Analysis](#section35)\n    - [Reduced Dimension Model Results with Cross-Validation](#sectioncv)\n* [Plotting Decision Boundaries](#section36)\n* [Plotting Decision Regions](#section37)\n* [Submission & Some Last Words](#sectionlst)\n\n\n
"\n# Well... What do we have here?\n\nOk we have two sets(train and test) data and in total 1301 observations 11 features. Our target is Survived column which is not present on the test set(duh!)... With basic inspection I'd say 'PassengerId' has effect on survival, but... Maybe we can use it on feature engineering part? For the rest we gonna inspect them individually soon but  generally speaking they look mostly categorical data with some continuous values like Fare and Age.\n\nI'm going to use different sets of variables for visualizing and feature engineering so let's start with assigning visualizing ones. I'm also saving idx variable for future use.\n\n### [Back To Table of Contents](#toc_section)\n"
"\n## Missing Values\n\nOn both datasets Cabin feature is missing a lot, it looks this feature not useful for modelling but we might give it a chance with feature engineering later.\n\nAgain, Age feature has many missing values, we can impute them with some logical way to use later...\n\nThere are little number of missing values on Embarked and Fare, I think we can impute them without taking much risk.\n\n### [Back To Table of Contents](#toc_section)"
"\n# Building the Feature Engineering Machine\n\nHere comes to part where we are going to play with the data we have. I wanted to build functions instead of moving one by one, I feel like with this way I can change my build up for the model more easily. Since there are some changes might get better results but there are some changes might badly effect outcome of the model by increasing complexity. This is a trial and error process so with this approach I feel like I can control them easily.\n\nI called this part machine but if you are more traditional you can say this is our toolbox too :)\n\n### [Back To Table of Contents](#toc_section)"
"\n## Title Extractor\n\nThis function extracts prefixes from passenger names and groups them in more generalized titles. We know male survival is low but we also know women and children first, right? So we can extract 'Master' title which is given for young males and their survival ratio might differ.\n\n### [Back To Table of Contents](#toc_section)"
"\n## Title Encoder\n\nThis function is for getting dummy variables for titles, might or might not use it.\n\n### [Back To Table of Contents](#toc_section)"
"\n## Scaler\n\nAnother function for getting rid of outliers in Fare values with using boxcox. I wasn't sure about if I use this or binning, but model did little better at the end with binning.\n\n### [Back To Table of Contents](#toc_section)"
\n## Embarked Processor\n\nA function for filling missing values and then hot encoding for each value.\n\n### [Back To Table of Contents](#toc_section)
\n## Ticket Cleaner\n\nThis function extract prefixes from ticket and assign them to specific group based on their prefix. Again I found this approach adding complexity only.\n\n### [Back To Table of Contents](#toc_section)
"\n## Housekeeping\n\nThis function is for cleaning the redundant features after extracting useful information from them. The cabin, passengerId, last name, ticket and ofc our target survived is going to be dropped. These features did their job on feature engineering and we can leave them in peace now.\n\n### [Back To Table of Contents](#toc_section)"
"\n## Correlation Matrix\n\nAlright one last table I'm going to check is correlation matrix. So we can spot linear relations between features. Especially the ones which effects survival rate. It seems Sex, the Mr. title and family survival ratio is most related features to survival.\n\nWell... That's it then let's continue with modelling now!\n\n### [Back To Table of Contents](#toc_section)"
\n# Modelling\n\nSince preprocessing done we are ready for training our models. We start with loading packages and splitting our transformed data so we have 22 features and and 891 observations to train our estimators. Our test set has 418 observations to make predictions.\n\n### [Back To Table of Contents](#toc_section)
"\n# Decision Trees\nSince we were talking about decision tree based models I wanted to visualize how they work. For that I'm going to choose random forest classifier. RF has multiple decision trees in it, we clas display them all but I think first one should be enough for this notebook. You can learn more about decision tree plotting in this great notebook [here](https://www.kaggle.com/willkoehrsen/visualize-a-decision-tree-w-python-scikit-learn). \n\nYou can see the tree is somewhat matching our feature importances ranking just above, again this is just for gaining some knowledge and you might not need it for the competition...\n\n### [Back To Table of Contents](#toc_section)"
"\n# Feature Selection by Recursive Feature Elimination\n\nAgain a part where my main motivation is learning and applying what I know. Here I'm gonna reduce dimensionality with two sklearn tools. First one is recursive feature elimination (RFE). The goal of RFE is to select features by recursively considering smaller and smaller sets of features. I'm going to prune half of the features we have, again this is just for experience and might not be needed for Titanic competition.\n\n### [Back To Table of Contents](#toc_section)"
"- The code below is edited version of [this example from sklearn official page](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html#sphx-glr-auto-examples-classification-plot-classifier-comparison-py) I really enjoyed while tinkering it, kudos to creators!"
\n# Plotting Decision Boundaries\n\nI just wanted to visualise how different models act on given 2D data. I can say that good fitting models produced similar results with small differences. I take this as a good theoretical information...\n\n### [Back To Table of Contents](#toc_section)
# Topic 2. Visual data analysis in Python\n## Part 1. Visualization: from Simple Distributions to Dimensionality Reduction
## Article outline\n\n1. [Dataset](#1.-Dataset)\n2. [Univariate visualization](#2.-Univariate-visualization)\n    * 2.1 [Quantitative features](#2.1-Quantitative-features)\n    * 2.2 [Categorical and binary features](#2.2-Categorical-and-binary-features)\n3. [Multivariate visualization](#3.-Multivariate-visualization)\n    * 3.1 [Quantitative vs. Quantitative](#3.1-Quantitative-vs.-Quantitative)\n    * 3.2 [Quantitative vs. Categorical](#3.2-Quantitative-vs.-Categorical)\n    * 3.3 [Categorical vs. Categorical](#3.3-Categorical-vs.-Categorical)\n4. [Whole dataset visualizations](#4.-Whole-dataset-visualizations)\n    * 4.1 [Naive approach](#4.1-A-naive-approach)\n    * 4.2 [Dimensionality reduction](#4.2-Dimensionality-reduction)\n    * 4.3 [t-SNE](#4.3-t-SNE)\n5. [Assignments](#5.-Assignments)\n6. [Useful resources](#6.-Useful-resources)
"## 1. Dataset\n\nBefore we get to the data, let's initialize our environment:"
"In the first article, we looked at the data on customer churn for a telecom operator. We will reload the same dataset into a `DataFrame`:"
"## 2. Univariate visualization\n\n*Univariate* analysis looks at one feature at a time. When we analyze a feature independently, we are usually mostly interested in the *distribution of its values* and ignore other features in the dataset.\n\nBelow, we will consider different statistical types of features and the corresponding tools for their individual visual analysis.\n\n#### 2.1 Quantitative features\n\n*Quantitative features* take on ordered numerical values. Those values can be *discrete*, like integers, or *continuous*, like real numbers, and usually express a count or a measurement.\n\n##### Histograms and density plots\n\nThe easiest way to take a look at the distribution of a numerical variable is to plot its *histogram* using the `DataFrame`'s method [`hist()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.hist.html)."
"A histogram groups values into *bins* of equal value range. The shape of the histogram may contain clues about the underlying distribution type: Gaussian, exponential, etc. You can also spot any skewness in its shape when the distribution is nearly regular but has some anomalies. Knowing the distribution of the feature values becomes important when you use Machine Learning methods that assume a particular type (most often Gaussian).\n\nIn the above plot, we see that the variable *Total day minutes* is normally distributed, while *Total intl calls* is prominently skewed right (its tail is longer on the right).\n\nThere is also another, often clearer, way to grasp the distribution: *density plots* or, more formally, *Kernel Density Plots*. They can be considered a [smoothed](https://en.wikipedia.org/wiki/Kernel_smoother) version of the histogram. Their main advantage over the latter is that they do not depend on the size of the bins. Let's create density plots for the same two variables:"
"A histogram groups values into *bins* of equal value range. The shape of the histogram may contain clues about the underlying distribution type: Gaussian, exponential, etc. You can also spot any skewness in its shape when the distribution is nearly regular but has some anomalies. Knowing the distribution of the feature values becomes important when you use Machine Learning methods that assume a particular type (most often Gaussian).\n\nIn the above plot, we see that the variable *Total day minutes* is normally distributed, while *Total intl calls* is prominently skewed right (its tail is longer on the right).\n\nThere is also another, often clearer, way to grasp the distribution: *density plots* or, more formally, *Kernel Density Plots*. They can be considered a [smoothed](https://en.wikipedia.org/wiki/Kernel_smoother) version of the histogram. Their main advantage over the latter is that they do not depend on the size of the bins. Let's create density plots for the same two variables:"
"It is also possible to plot a distribution of observations with `seaborn`'s [`distplot()`](https://seaborn.pydata.org/generated/seaborn.distplot.html). For example, let's look at the distribution of *Total day minutes*. By default, the plot displays both the histogram with the [kernel density estimate](https://en.wikipedia.org/wiki/Kernel_density_estimation) (KDE) on top."
"It is also possible to plot a distribution of observations with `seaborn`'s [`distplot()`](https://seaborn.pydata.org/generated/seaborn.distplot.html). For example, let's look at the distribution of *Total day minutes*. By default, the plot displays both the histogram with the [kernel density estimate](https://en.wikipedia.org/wiki/Kernel_density_estimation) (KDE) on top."
The height of the histogram bars here is normed and shows the density rather than the number of examples in each bin.\n\n##### Box plot\n\nAnother useful type of visualization is a *box plot*. `seaborn` does a great job here:
The height of the histogram bars here is normed and shows the density rather than the number of examples in each bin.\n\n##### Box plot\n\nAnother useful type of visualization is a *box plot*. `seaborn` does a great job here:
"Let's see how to interpret a box plot. Its components are a *box* (obviously, this is why it is called a *box plot*), the so-called *whiskers*, and a number of individual points (*outliers*).\n\nThe box by itself illustrates the interquartile spread of the distribution; its length is determined by the $25th \, (\text{Q1})$ and $75th \, (\text{Q3})$ percentiles. The vertical line inside the box marks the median ($50\%$) of the distribution. \n\nThe whiskers are the lines extending from the box. They represent the entire scatter of data points, specifically the points that fall within the interval $(\text{Q1} - 1.5 \cdot \text{IQR}, \text{Q3} + 1.5 \cdot \text{IQR})$, where $\text{IQR} = \text{Q3} - \text{Q1}$ is the [interquartile range](https://en.wikipedia.org/wiki/Interquartile_range).\n\nOutliers that fall outside of the range bounded by the whiskers are plotted individually as black points along the central axis.\n\nWe can see that a large number of international calls is quite rare in our data.\n\n##### Violin plot\n\nThe last type of distribution plots that we will consider is a *violin plot*.\n\nLook at the figures below. On the left, we see the already familiar box plot. To the right, there is a *violin plot* with the kernel density estimate on both sides."
"Let's see how to interpret a box plot. Its components are a *box* (obviously, this is why it is called a *box plot*), the so-called *whiskers*, and a number of individual points (*outliers*).\n\nThe box by itself illustrates the interquartile spread of the distribution; its length is determined by the $25th \, (\text{Q1})$ and $75th \, (\text{Q3})$ percentiles. The vertical line inside the box marks the median ($50\%$) of the distribution. \n\nThe whiskers are the lines extending from the box. They represent the entire scatter of data points, specifically the points that fall within the interval $(\text{Q1} - 1.5 \cdot \text{IQR}, \text{Q3} + 1.5 \cdot \text{IQR})$, where $\text{IQR} = \text{Q3} - \text{Q1}$ is the [interquartile range](https://en.wikipedia.org/wiki/Interquartile_range).\n\nOutliers that fall outside of the range bounded by the whiskers are plotted individually as black points along the central axis.\n\nWe can see that a large number of international calls is quite rare in our data.\n\n##### Violin plot\n\nThe last type of distribution plots that we will consider is a *violin plot*.\n\nLook at the figures below. On the left, we see the already familiar box plot. To the right, there is a *violin plot* with the kernel density estimate on both sides."
"The difference between the box and violin plots is that the former illustrates certain statistics concerning individual examples in a dataset while the violin plot concentrates more on the smoothed distribution as a whole.\n\nIn our case, the violin plot does not contribute any additional information about the data as everything is clear from the box plot alone.\n\n##### describe()\n\nIn addition to graphical tools, in order to get the exact numerical statistics of the distribution, we can use the method [`describe()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html) of a `DataFrame`:"
##### Bar plot\n\nThe bar plot is a graphical representation of the frequency table. The easiest way to create it is to use the `seaborn`'s function [`countplot()`](https://seaborn.pydata.org/generated/seaborn.countplot.html). There is another function in `seaborn` that is somewhat confusingly called [`barplot()`](https://seaborn.pydata.org/generated/seaborn.barplot.html) and is mostly used for representation of some basic statistics of a numerical variable grouped by a categorical feature.\n\nLet's plot the distributions for two categorical variables:
"While the histograms, discussed above, and bar plots may look similar, there are several differences between them:\n1. *Histograms* are best suited for looking at the distribution of numerical variables while *bar plots* are used for categorical features.\n2. The values on the X-axis in the *histogram* are numerical; a *bar plot* can have any type of values on the X-axis: numbers, strings, booleans.\n3. The *histogram*'s X-axis is a *Cartesian coordinate axis* along which values cannot be changed; the ordering of the *bars* is not predefined. Still, it is useful to note that the bars are often sorted by height, that is, the frequency of the values. Also, when we consider *ordinal* variables (like *Customer service calls* in our data), the bars are usually ordered by variable value.\n\nThe left chart above vividly illustrates the imbalance in our target variable. The bar plot for *Customer service calls* on the right gives a hint that the majority of customers resolve their problems in maximum 2-3 calls. But, as we want to be able to predict the minority class, we may be more interested in how the fewer dissatisfied customers behave. It may well be that the tail of that bar plot contains most of our churn. These are just hypotheses for now, so let's move on to some more interesting and powerful visual techniques."
"## 3. Multivariate visualization\n\n*Multivariate* plots allow us to see relationships between two and more different variables, all in one figure. Just as in the case of univariate plots, the specific type of visualization will depend on the types of the variables being analyzed.\n\n#### 3.1 Quantitative vs. Quantitative\n\n##### Correlation matrix\n\nLet's look at the correlations among the numerical variables in our dataset. This information is important to know as there are Machine Learning algorithms (for example, linear and logistic regression) that do not handle highly correlated input variables well.\n\nFirst, we will use the method [`corr()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.corr.html) on a `DataFrame` that calculates the correlation between each pair of features. Then, we pass the resulting *correlation matrix* to [`heatmap()`](https://seaborn.pydata.org/generated/seaborn.heatmap.html) from `seaborn`, which renders a color-coded matrix for the provided values:"
"From the colored correlation matrix generated above, we can see that there are 4 variables such as *Total day charge* that have been calculated directly from the number of minutes spent on phone calls (*Total day minutes*). These are called *dependent* variables and can therefore be left out since they do not contribute any additional information. Let's get rid of them:"
##### Scatter plot\n\nThe *scatter plot* displays values of two numerical variables as *Cartesian coordinates* in 2D space. Scatter plots in 3D are also possible.\n\nLet's try out the function [`scatter()`](https://matplotlib.org/devdocs/api/_as_gen/matplotlib.pyplot.scatter.html) from the `matplotlib` library:
"We get an uninteresting picture of two normally distributed variables. Also, it seems that these features are uncorrelated because the ellipse-like shape is aligned with the axes.\n\nThere is a slightly fancier option to create a scatter plot with the `seaborn` library:"
"We get an uninteresting picture of two normally distributed variables. Also, it seems that these features are uncorrelated because the ellipse-like shape is aligned with the axes.\n\nThere is a slightly fancier option to create a scatter plot with the `seaborn` library:"
"The function [`jointplot()`](https://seaborn.pydata.org/generated/seaborn.jointplot.html) plots two histograms that may be useful in some cases.\n\nUsing the same function, we can also get a smoothed version of our bivariate distribution:"
"The function [`jointplot()`](https://seaborn.pydata.org/generated/seaborn.jointplot.html) plots two histograms that may be useful in some cases.\n\nUsing the same function, we can also get a smoothed version of our bivariate distribution:"
"This is basically a bivariate version of the *Kernel Density Plot* discussed earlier.\n\n##### Scatterplot matrix\n\nIn some cases, we may want to plot a *scatterplot matrix* such as the one shown below. Its diagonal contains the distributions of the corresponding variables, and the scatter plots for each pair of variables fill the rest of the matrix."
"Sometimes, such a visualization may help draw conclusions about data; but, in this case, everything is pretty clear with no surprises.\n\n#### 3.2 Quantitative vs. Categorical\n\nIn this section, we will make our simple quantitative plots a little more exciting. We will try to gain new insights for churn prediction from the interactions between the numerical and categorical features.\n\nMore specifically, let's see how the input variables are related to the target variable Churn.\n\nPreviously, you learned about scatter plots. Additionally, their points can be color or size coded so that the values of a third categorical variable are also presented in the same figure. We can achieve this with the `scatter()` function seen above, but, let's try a new function called [`lmplot()`](https://seaborn.pydata.org/generated/seaborn.lmplot.html) and use the parameter `hue` to indicate our categorical feature of interest:"
"It seems that our small proportion of disloyal customers lean towards the top-right corner; that is, such customers tend to spend more time on the phone during both day and night. But this is not absolutely clear, and we won't make any definitive conclusions from this chart.\n\nNow, Let's create box plots to visualize the distribution statistics of the numerical variables in two disjoint groups: the loyal customers (`Churn=False`) and those who left (`Churn=True`)."
"It seems that our small proportion of disloyal customers lean towards the top-right corner; that is, such customers tend to spend more time on the phone during both day and night. But this is not absolutely clear, and we won't make any definitive conclusions from this chart.\n\nNow, Let's create box plots to visualize the distribution statistics of the numerical variables in two disjoint groups: the loyal customers (`Churn=False`) and those who left (`Churn=True`)."
"From this chart, we can see that the greatest discrepancy in distribution between the two groups is for three variables: *Total day minutes*, *Customer service calls*, and *Number vmail messages*. Later in this course, we will learn how to determine feature importance in classification using *Random Forest* or *Gradient Boosting*; there, we will see that the first two features are indeed very important for churn prediction.\n\nLet's look at the distribution of day minutes spoken for the loyal and disloyal customers separately. We will create box and violin plots for *Total day minutes* grouped by the target variable."
"From this chart, we can see that the greatest discrepancy in distribution between the two groups is for three variables: *Total day minutes*, *Customer service calls*, and *Number vmail messages*. Later in this course, we will learn how to determine feature importance in classification using *Random Forest* or *Gradient Boosting*; there, we will see that the first two features are indeed very important for churn prediction.\n\nLet's look at the distribution of day minutes spoken for the loyal and disloyal customers separately. We will create box and violin plots for *Total day minutes* grouped by the target variable."
"In this case, the violin plot does not contribute any additional information about our data as everything is clear from the box plot alone: disloyal customers tend to talk on the phone more.\n\n**An interesting observation**: on average, customers that discontinue their contracts are more active users of communication services. Perhaps they are unhappy with the tariffs, so a possible measure to prevent churn could be a reduction in call rates. The company will need to undertake additional economic analysis to find out whether such measures would be beneficial.\n\nWhen we want to analyze a quantitative variable in two categorical dimensions at once, there is a suitable function for this in the `seaborn` library called [`catplot()`](https://seaborn.pydata.org/generated/seaborn.factorplot.html). For example, let's visualize the interaction between *Total day minutes* and two categorical variables in the same plot:"
"In this case, the violin plot does not contribute any additional information about our data as everything is clear from the box plot alone: disloyal customers tend to talk on the phone more.\n\n**An interesting observation**: on average, customers that discontinue their contracts are more active users of communication services. Perhaps they are unhappy with the tariffs, so a possible measure to prevent churn could be a reduction in call rates. The company will need to undertake additional economic analysis to find out whether such measures would be beneficial.\n\nWhen we want to analyze a quantitative variable in two categorical dimensions at once, there is a suitable function for this in the `seaborn` library called [`catplot()`](https://seaborn.pydata.org/generated/seaborn.factorplot.html). For example, let's visualize the interaction between *Total day minutes* and two categorical variables in the same plot:"
"From this, we could conclude that, starting with 4 calls, *Total day minutes* may no longer be the main factor for customer churn. Perhaps, in addition to our previous guess about the tariffs, there are customers that are dissatisfied with the service due to other problems, which might lead to fewer number of day minutes spent on calls.\n\n#### 3.3 Categorical vs. Categorical\n\nAs we saw earlier in this article, the variable *Customer service calls* has few unique values and, thus, can be considered either numerical or ordinal. We have already seen its distribution with a *count plot*. Now, we are interested in the relationship between this ordinal feature and the target variable *Churn*.\n\nLet's look at the distribution of the number of calls to customer service, again using a *count plot*. This time, let's also pass the parameter `hue=Churn` that adds a categorical dimension to the plot:"
"From this, we could conclude that, starting with 4 calls, *Total day minutes* may no longer be the main factor for customer churn. Perhaps, in addition to our previous guess about the tariffs, there are customers that are dissatisfied with the service due to other problems, which might lead to fewer number of day minutes spent on calls.\n\n#### 3.3 Categorical vs. Categorical\n\nAs we saw earlier in this article, the variable *Customer service calls* has few unique values and, thus, can be considered either numerical or ordinal. We have already seen its distribution with a *count plot*. Now, we are interested in the relationship between this ordinal feature and the target variable *Churn*.\n\nLet's look at the distribution of the number of calls to customer service, again using a *count plot*. This time, let's also pass the parameter `hue=Churn` that adds a categorical dimension to the plot:"
"**An observation**: the churn rate increases significantly after 4 or more calls to customer service.\n\nNow, let's look at the relationship between *Churn* and the binary features, *International plan* and *Voice mail plan*."
"**An observation**: the churn rate increases significantly after 4 or more calls to customer service.\n\nNow, let's look at the relationship between *Churn* and the binary features, *International plan* and *Voice mail plan*."
"**An observation**: when *International Plan* is enabled, the churn rate is much higher; the usage of the international plan by the customer is a strong feature. We do not observe the same effect with *Voice mail plan*.\n\n##### Contingency table\n\nIn addition to using graphical means for categorical analysis, there is a traditional tool from statistics: a *contingency table*, also called a *cross tabulation*. It shows a multivariate frequency distribution of categorical variables in tabular form. In particular, it allows us to see the distribution of one variable conditional on the other by looking along a column or row.\n\nLet's try to see how *Churn* is related to the categorical variable *State* by creating a cross tabulation:"
and plot it:
"Let's color this t-SNE representation according to the churn (blue for loyal customers, and orange for those who churned)."
"Let's color this t-SNE representation according to the churn (blue for loyal customers, and orange for those who churned)."
"We can see that customers who churned are concentrated in a few areas of the lower dimensional feature space.\n\nTo better understand the picture, we can also color it with the remaining binary features: *International Plan* and *Voicemail*. Orange dots here indicate instances that are positive for the corresponding binary feature."
"We can see that customers who churned are concentrated in a few areas of the lower dimensional feature space.\n\nTo better understand the picture, we can also color it with the remaining binary features: *International Plan* and *Voicemail*. Orange dots here indicate instances that are positive for the corresponding binary feature."
"Now it is clear that, for example, many dissatisfied customers who canceled their subscription are crowded together in one cluster representing the people with the international plan but no voice mail.\n\nFinally, let's note some disadvantages of t-SNE:\n- High computational complexity. The [implementation](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) in `scikit-learn` is unlikely to be feasible in a real task. If you have a large number of samples, you should try [Multicore-TSNE](https://github.com/DmitryUlyanov/Multicore-TSNE) instead.\n- The plot can change a great deal depending on the random seed, which complicates interpretation. [Here](http://distill.pub/2016/misread-tsne/) is a good tutorial on t-SNE. In general, you shouldn't make any far-reaching conclusions based on such graphs because it can equate to plain guessing. Of course, some findings in t-SNE pictures can inspire an idea and be confirmed through more thorough research down the line, but that does not happen very often.\n\nOccasionally, using t-SNE, you can get a really good intuition for the data. The following is a good paper that shows an example of this for handwritten digits: [Visualizing MNIST](https://colah.github.io/posts/2014-10-Visualizing-MNIST/).\n\n"
"We will plot the Recency Distribution and QQ-plot to identify substantive departures from normality, likes outliers, skewness and kurtosis."
"From the first graph above we can see that sales recency distribution is ***skewed***, has a **peak** on the left and a long tail to the right. It **deviates from normal distribution** and is **positively biased**.\n\nFrom the **Probability Plot**, we could see that **sales recency** also does **not align with the diagonal  red line** which represent normal distribution. The form of its distribution confirm that is a skewed right. \n\nWith ***skewness positive of 1.25***, we confirm the **lack of symmetry** and indicate that sales recency  are **skewed right**, as we can see too at the Sales Distribution plot, skewed right means that the right tail is **long relative to the left tail**. The skewness for a normal distribution is zero, and any symmetric data should have a skewness near zero. A distribution, or data set, is symmetric if it looks the same to the left and right of the center point.\n\n**Kurtosis** is a measure of whether the data are heavy-tailed or light-tailed relative to a normal distribution. That is, data sets with high kurtosis tend to have heavy tails, or outliers, and **positive** kurtosis indicates a **heavy-tailed distribution** and **negative** kurtosis indicates a **light tailed distribution**. So, with 0.43 of positive kurtosis **sales recency** are heavy-tailed and has some **outliers**."
"### Clustering for Segments\n#### K-Means Clustering\nThe K-means clustering belongs to the partition based\centroid based hard clustering family of algorithms, a family of algorithms where each sample in a dataset is assigned to exactly one cluster.\n\nBased on this Euclidean distance metric, we can describe the k-means algorithm as a simple optimization problem, an iterative approach for minimizing the within-cluster sum of squared errors (SSE), which is sometimes also called cluster inertia. So, the objective of K-Means clustering is to minimize total intra-cluster variance, or, the squared error function: \n![image](https://www.saedsayad.com/images/Clustering_kmeans_c.png)\n\nThe steps that happen in the K-means algorithm for partitioning the data are as given follows:\n1. The algorithm starts with random point initializations of the required number of centers. The ‚ÄúK‚Äù in K-means stands for the number of clusters.\n2. In the next step, each of the data point is assigned to the center closest to it. The distance metric used in K-means clustering is normal Euclidian distance.\n3. Once the data points are assigned, the centers are recalculated by averaging the dimensions of the points belonging to the cluster.\n4. The process is repeated with new centers until we reach a point where the assignments become stable. In this case, the algorithm terminates.\n\n##### K-means++\n- Place the initial centroids far away from each other via the k-means++ algorithm, which leads to better and more consistent results than the classic k-means.\n- To use k-means++ with scikit-learn's KMeans object, we just need to set the init parameter to k-means++ (the default setting) instead of random.\n\n#### The Elbow Method\n  \nUsing the elbow method to find the optimal number of clusters. The idea behind the elbow method is to identify the value of k where the distortion begins to increase most rapidly. If k increases, the distortion will decrease, because the samples will be closer to the centroids they are assigned to. \n\nThis method looks at the percentage of variance explained as a function of the number of clusters. More precisely, if one plots the percentage of variance explained by the clusters against the number of clusters, the first clusters will add much information (explain a lot of variance), but at some point the marginal gain will drop, giving an angle in the graph. The number of clusters is chosen at this point, hence the ""elbow criterion"". This ""elbow"" cannot always be unambiguously identified.Percentage of variance explained is the ratio of the between-group variance to the total variance, also known as an F-test. A slight variation of this method plots the curvature of the within group variance."
"Note that by the Elbow method from a K equal to 3 we already observed low rates of gain in the decay of the distortions with the decrease of K reaching the limit of 10% with the K equal to 7. With this in mind, we will begin to evaluate the options more deeply with 3, and 7, starting with the silhouette analysis."
"#### Silhouette analysis on K-Means clustering\n\nSilhouette analysis can be used to study the separation distance between the resulting clusters, as a strategy to quantifying the quality of clustering via graphical tool to plot a measure of how tightly grouped the samples in the clusters are. The silhouette plot displays a measure of how close each point in one cluster is to points in the neighboring clusters and thus provides a way to assess parameters like number of clusters visually. \n\nIt can also be applied to clustering algorithms other than k-means\n\nSilhouette coefficients has a range of \[-1, 1\], it calculated by:\n1. Calculate the cluster cohesion a( i )as the average distance between a sample x( i )   and all other points in the same cluster.\n2. Calculate the cluster separation b( i ) from the next closest cluster as the average distance between the sample x( i ) and all samples in the nearest cluster.\n3. Calculate the silhouette s( i )  as the difference between cluster cohesion and separation divided by the greater of the two, as shown here:\n![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/3d80ab22fb291b347b2d9dc3cc7cd614f6b15479)\nWhich can be also written as:\n![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/ab5579a6c7150579af8a0d432b6630ba529376f0)\n\nWhere:\n- If near +1, it indicate that the sample is far away from the neighboring clusters. \n- a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. \n- If most objects have a high value, then the clustering configuration is appropriate. \n- If many points have a low or negative value, then the clustering configuration may have too many or too few clusters.\n- A value of 0 indicates that the sample is on or very close to the decision boundary between two neighboring clusters\n- Negative values indicate that those samples might have been assigned to the wrong cluster.\n\nThe silhouette plot can shows a bad K clusters pick for the given data due to the presence of clusters with below average silhouette scores and also due to wide fluctuations in the size of the silhouette plots. A good k clusters can found when all the plots are more or less of similar thickness and hence are of similar sizes.\n\nAlthough we have to keep in mind that in several cases and scenarios, sometimes we may have to drop the mathematical explanation given by the algorithm and look at the business relevance of the results obtained.\n\nLet's see below how our data perform for each K clusters groups (3, 5 and 7) in the silhouette score of each cluster, along with the center of each of the cluster discovered in the scatter plots, by amount_log vs recency_log and vs frequency_log."
"When we look at the results of the clustering process, we can infer some interesting insights:\n\n- First notice that all K clusters options is valid, because they don't have presence of clusters with below average silhouette scores. \n- In the other hand, all options had a some wide fluctuations in the size of the silhouette plots. \n\nSo, the best choice may lie on the option that gives us a simpler business explanation and at the same time target customers in focus groups with sizes closer to the desired. \n\n#### Clusters Center:\nLet's look at the cluster center values after returning them to normal values from the log and scaled version. "
"#### Clusters Insights:\n\nWith the plots and the center in the correct units, let's see some insights by each clusters groups:\n\n***In the three-cluster:***\n- The tree clusters appears have a good stark differences in the Monetary value of the customer, we will confirm this by a box plot.\n- Cluster 1 is the cluster of high value customer who shops frequently and is certainly an important segment for each business.\n- In the similar way we obtain customer groups with low and medium spends in clusters with labels 0 and 2, respectively.\n- Frequency and Recency correlate perfectly to the Monetary value based on the trend (High Monetary-Low Recency-High Frequency).\n\n***In the five-cluster:***\n- Note that clusters 0 and 1 are very similar to their cluster in the configuration with only 3 clusters.\n- The cluster 1 appears more robust on the affirmation of those who shop often and with high amount.\n- The cluster 2 are those who have a decent spend but are not as frequent as the cluster 1\n- The cluster 4 purchases medium amounts, with a relatively low frequency and not very recent\n- The cluster 3 makes low-cost purchases, with a relatively low frequency, but above 1, and made their last purchase more recently. This group of customers probably response to price discounts and can be subject to loyalty promotions to try increase the medium-ticket, strategy that can be better defined when we analyzing the market basket. \n- The silhouette score matrix says that the  five cluster segments are less optimal then the three cluster segments. \n\n***In the five-cluster:***\n- Definitely cluster 6 defines those who shop often and with high amount.\n- Clusters 1 and 5 show good spending and good frequency, only deferring in how recent were their last purchases, where 5 is older, which suggests an active action to sell to group 5 as soon as possible and another to 1 seeking to raise its frequency.\n- Cluster 0 presents the fourth best purchase and a reasonable frequency, but this is a long time without buying. This group should be sensible to promotions and activations, so that they do not get lost and make their next purchase.\n- Cluster 5 is similar to 0, but has made its purchases more recently and has a slightly better periodicity. Then actions must be taken to raise their frequency and reduce the chances of them migrating to cluster 0 by staying longer without purchasing products.\n\n#### Drill Down Clusters:\n\nTo further drill down on this point and find out the quality of these difference, we can label our data with the corresponding cluster label and then visualize these differences. The following code will extract the clustering label and attach it with our customer summary dataset."
"Once we have the labels assigned to each of the customers, our task is simple. Now we want to find out how the summary of customer in each group is varying. If we can visualize that information we will able to find out the differences in the clusters of customers and we can modify our strategy on the basis of those differences.\n\nThe following code leverages plotly and will take the cluster labels we got for each configurations clusters and create boxplots. Plotly enables us to interact with the plots to see the central tendency values in each boxplot in the notebook. Note that we want to avoid the extremely high outlier values of each group, as they will interfere in making a good observation around the central tendencies of each cluster. Since we have only positive values, we will restrict the data such that only data points which are less than 0.95th percentile of the cluster is used. This will give us good information about the majority of the users in that cluster segment.\n\nI've used these charts to review my previously stated insights, but follow the same for you to explore:"
"The data has been split into two groups:\n\ntraining set (train.csv)\ntest set (test.csv)\nThe training set should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use feature engineering to create new features.\n\nThe test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.\n\n\n### Data Dictionary\nVariable	Definition	Key\n\nsurvival	Survival	0 = No, 1 = Yes\n\npclass	Ticket class	1 = 1st, 2 = 2nd, 3 = 3rd\n\nsex	Male or Female\n\nAge	Age in years	\n\nsibsp	# of siblings / spouses aboard the Titanic	\n\nparch	# of parents / children aboard the Titanic	\n\nticket	Ticket number	\n\nfare	Passenger fare	\n\ncabin	Cabin number	\n\nembarked	Port of Embarkation	C = Cherbourg, Q = Queenstown, S = Southampton\n\nVariable Notes\npclass: A proxy for socio-economic status (SES)\n1st = Upper\n2nd = Middle\n3rd = Lower\n\nage: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\nsibsp: The dataset defines family relations in this way...\n\nSibling = brother, sister, stepbrother, stepsister\n\nSpouse = husband, wife (mistresses and fianc√©s were ignored)\n\nparch: The dataset defines family relations in this way...\n\nParent = mother, father\n\nChild = daughter, son, stepdaughter, stepson\n\nSome children travelled only with a nanny, therefore parch=0 for them"
### Identifying Missing Value 
### Correlation Between The Features
"Interpreting The Heatmap\nThe first thing to note is that only the numeric features are compared as it is obvious that we cannot correlate between alphabets or strings. Before understanding the plot, let us see what exactly correlation is.\n\nPOSITIVE CORRELATION: If an increase in feature A leads to increase in feature B, then they are positively correlated. A value 1 means perfect positive correlation.\n\nNEGATIVE CORRELATION: If an increase in feature A leads to decrease in feature B, then they are negatively correlated. A value -1 means perfect negative correlation.\n\nNow lets say that two features are highly or perfectly correlated, so the increase in one leads to increase in the other. This means that both the features are containing highly similar information and there is very little or no variance in information. This is known as MultiColinearity as both of them contains almost the same information.\n\nSo do you think we should use both of them as one of them is redundant. While making or training models, we should try to eliminate redundant features as it reduces training time and many such advantages.\n\nNow from the above heatmap,we can see that the features are not much correlated. The highest correlation is between SibSp and Parch i.e 0.41. So we can carry on with all features."
### Pairplots\nFinally let us generate some pairplots to observe the distribution of data from one feature to the other. Once again we use Seaborn to help us.
"###  Model\nNow we are ready to train a model and predict the required solution. There are lot of predictive modelling algorithms to choose from. We must understand the type of problem and solution requirement to narrow down to a select few models which we can evaluate. Our problem is a classification and regression problem. We want to identify relationship between output (Survived or not) with other variables or features (Gender, Age, Port...). We are also perfoming a category of machine learning which is called supervised learning as we are training our model with a given dataset. With these two criteria - Supervised Learning plus Classification and Regression, we can narrow down our choice of models to a few. These include:\n\nLogistic Regression\n\nKNN\n\nSupport Vector Machines\n\nNaive Bayes classifier\n\nDecision Tree\n\nRandom Forrest\n\nLinear Discriminant Analysis\n\nAda Boost Classifier \n\nGradient Boosting Classifier\n\nAnd also compared above given classifiers and evaluate the mean accuracy of each of them by a stratified kfold cross validation procedure and plot accuracy based confusion matrix"
## LogisticRegression
## Random Forest Classifier 
## Random Forest Classifier 
## Support Vector Machines
## Support Vector Machines
## KNN Classifier
## KNN Classifier
## Gaussian Naive Bayes
## AdaBoost
## Linear Discriminant Analysis
## Linear Discriminant Analysis
## Gradient Boosting Classifier
## Gradient Boosting Classifier
## Model evaluation\nWe can now rank our evaluation of all the models to choose the best one for our problem.
### Target Variable Visualization (AveragePrice) : 
- Distribution of **AveragePrice** that is not resampled is pretty much a **normally distributed** curve. It highlights small double peaks but we will allow it in this case.\n- Distribution of **AveragePrice** of the resampled data displays a much better **normally distribution** curve.\n- We can clearly observe a positive trend in **AveragePrice** w.r.t **Date**. Repetitive 3 peaks at consistent intervals of time can be observed.\n- **AveragePrice** drops around the months of December / January and rises to it's highest value for the months September - November.
### Categorical Features vs Target Variable (AveragePrice) :
"- **AveragePrice** of Conventional(0) avocados is less than those of Organic(1).\n- **AveragePrice** of Avocados is near about the same for the **years** 2015, 2016 and 2018. A rise in **AveragePrice** can be clearly seen for the year 2017."
### Numerical Features :\n\n#### Distribution of Numerical Features :
"- Distributions of the non-resampled data were understandable, hence we visulize the distributions of resampled data.\n- **Total Volume**, **4046** & **4225** kind of display a **normally distributed** curve. Remaining numerical features display either a **Double Peak Distribution** or **Right / Positive Skewed Distribution**."
"- Initially, we will look for the bigger picture of the features : **AveragePrice**, **Total Volume**, **Total Bags**. Then we will proceed to visualize the features derived from the above features."
"- From the above graphs, we can say that **AveragePrice** vs **Total Volume** & **Total Bags** is negatively correlated or complement each other. \n- **AveragePrice** is inversely proportional to **Total Volume** & **Total Bags** whereas **Total Volume** displays a directly proportional relationship with **Total Bags**.\n- Assuming the graphs are superimposed, the crests & troughs of **AveragePrice** would overlap the troughs & crests of **Total Volume** & **Total Bags** respectively.\n- This relationship between **AveragePrice** vs **Total Volume** & **Total Bags** w.r.t **Date** can be associated with the **Law of Supply and Demand**.\n    - If the supply increases and demand stays the same, the price will go down. \n    - If the supply decreases and demand stays the same, the price will go up. \n    \n- **AveragePrice**, **Total Volume** and **Total Bags** displays an uptrend with **Date**."
"- From the above graphs, we can say that **AveragePrice** vs **Total Volume** & **Total Bags** is negatively correlated or complement each other. \n- **AveragePrice** is inversely proportional to **Total Volume** & **Total Bags** whereas **Total Volume** displays a directly proportional relationship with **Total Bags**.\n- Assuming the graphs are superimposed, the crests & troughs of **AveragePrice** would overlap the troughs & crests of **Total Volume** & **Total Bags** respectively.\n- This relationship between **AveragePrice** vs **Total Volume** & **Total Bags** w.r.t **Date** can be associated with the **Law of Supply and Demand**.\n    - If the supply increases and demand stays the same, the price will go down. \n    - If the supply decreases and demand stays the same, the price will go up. \n    \n- **AveragePrice**, **Total Volume** and **Total Bags** displays an uptrend with **Date**."
"- Avocados with PLU (Product Lookup Code) **4046** and **4225** kind of display the same patterns w.r.t **Date**. Crests and troughs are very similar to each other. \n- **4770** type of avocado has a seen a decrease in it's demand as time progresses. It has encountered a sharp drop in demand during the later months of 2016 and has not recovered since then.  \n- Use of **bags** w.r.t avocados has definitely been on the up! All the **bags** display a rising graph w.r.t **Date**. From the values present on y-axis, customers prefer to use the **Small Bags**."
#### Total Volume vs region w.r.t year :
"- This graph is unable to provide significant insights about year on year difference in avocado **Total Volume** consumed in different cities due to overlapping of data points.\n- **region** has mixed elements with the names of the cities and division of the country based on cardinal direction & non-cardinal directions like : **[Midsouth, Northeast, SouthCentral, Southeast, West, TotalUS]**.\n- All the elements based on cardinal directions & non-cardinal directions display a huge spike in demand for avocado. This is probably because of the data collected for these elements is a combination of mulitple city data.\n- **California**, **Great Lakes**, **Los Angeles** and **Plains** are some of the cities that highlight heavy consumption of avocado through **Total Volume**. "
#### Total Volume vs region w.r.t type :
"- This graph is very similar to the graph above in terms of the positions of the spike. \n- Elements based on cardinal directions & non-cardinal directions display the same spikes with a clear cut preference towards **Conventional(0)** avocados that supports the information gained from other visualizations.\n- Cities can like **DallasFtWorth**, **Denver**, **Houston**, **New York**, **PhoenixTucson** and **San Francisco** can be added to the previous of **California**, **Great Lakes**, **Los Angeles** and **Plains** that showcase their preference towards **Conventional(0)** avocado."
#### Total Bags vs region w.r.t year :
"- Use of **bags** is highly correlated with **Total Volume**.\n- From the data point **TotalUS** on the y-axis of the graph, we can say that there is crystal clear rise in use of bags."
#### Total Bags vs region w.r.t type :
- This graph is also very similar to the graphs above. No new information was gained.
"### Components of Time Series :\n- A Time Series consists of the following components :\n\n    - **Trend** : Long term direction of the data.\n    \n    E.g : Year on year rising temperature of the Earth due to Global Warming.\n    \n    - **Seasonality** : Short term repetitve patterns of the data due to the weather seasons.\n    \n    E.g : Sale of sweaters specifically in the winter season.\n    \n    - **Cyclic Variations** : Short term repetitive patterns of the data over a period of 1 year.\n    \n    E.g : It usually consists of the Business Quarters i.e Q1, Q2, Q3 & Q4.\n    \n    - **Irregularities** : Random and unforseen fluctuations in the data.\n    \n    E.g : Occurrences of Earthquakes or Floods, etc.\n    \n    \n- **In order to assess a Time Series, we need to consider the above components and make sure that our data is free from all these components in order to make a forecast.** \n\nLet's visualize the **AveragePrice** data for the above components!\n- For this purpose, we use a function **seasonal_decompose** from the **statsmodel** library.\n- This function has a parameter, **model**, that needs to be assigned the value **additive** or **multiplicative**.\n    - **Additive Model** : Data has same width and height of the seasonal patterns or peaks. Trend of the data is linear.\n    - **Multiplicative Model** : Data has increasing / decreasing width and height of the seasonal patterns or peaks. Trend of the data is non-linear.\n\nFrom the visualizations of **AveragePrice** executed in EDA section, we can say that the data represents a **Multiplicative Model**."
- Data clearly has a **non-linear uptrend**.\n- A clear cut **seasonal** pattern is present in the data.\n- The last plot is the **Residual** plot. It is the plot that describes the data if the **trend** and **seasonal** components of the data are completely eliminated.\n- We also need to check the statistical parameters w.r.t time. 
"- For the **AveragePrice** time series data, \n    - **Rolling Mean** is clearly variable with time. It is very close to the data. Thus, it can be a good descriptor of the data.\n    - **Rolling Standard Deviation** is pretty consistent in the initial stages however changes with time are observed in the later stages.\n    - **Test Statistic : (-2.36)** > **Critical Value (5%) : (-2.88)**\n    - **p-value (0.15)** > 0.05\n- Hence, **Null Hypothesis** cannot be rejected and we can conclude that the above **AveragePrice** time series is **not stationary**.\n- In order to eliminate trend, seasonality and make the time series stationary, we will use **differencing** i.e subtracting the previous value from it's next value. "
- We have taken the **log** of the data to deal with **stationarity** and **differencing** is done to handle **trend** and **seasonality**.\n- **Trend** and **Seasonality** of the data have near about died down & their values have been reduced as well.\n- We now check the **stationarity** of the time series.
"- Model fitting is a measure of how well a machine learning model generalizes to similar data to that on which it was trained.\n- After fitting the ARIMA model, we now check this fit and compare this series with the original values of **AveragePrice**.\n- In this process, we store the model fitted values in a series. This series then undergoes the process of cummulative summation i.e opposite of differencing.\n- Result of cummulative summation is the expected log values fitted by the model. These log values are then exponentiated that creates the fitted original series."
- RMSE value between **Fitted AveragePrice** and original **AveragePrice** is **21.83**.\n- This RMSE is slightly high and does creates the possibility of overfitting the model.
- We use the **np.exp()** as we fit the model on log values and hence we unlog the values using exponentiation function.
- Values generated by the **forecast_function** are constant.
"## SARIMA\n### Seasonal Auto Regressive Integrated Moving Average \n\n- **SARIMA** model is an extension of the ARIMA model that can handle the seasonal effects of the data.\n- It has kind of 2 orders **(p,d,q) x (P,D,Q,M)**. \n- **(p,d,q)** is the order that is similar to the order of the **ARIMA** model. \n- **(P,D,Q,M)** is known as the Seasonal Order where **(P,D,Q)** are similar to the **(p,d,q)** of the ARIMA model. \n- It's selection criteria is similar as well with an important condition i.e to handle the seasonality by differencing the data with the frequency of seasonal period or periodicity, **M**. "
"- For our data, it is in **weekly format** and the **seasonal period is of 1 year**.\n- Hence, we difference the already differenced data by a periodicity, **M**, value of 52.\n- We can observe that the seasonality of the data nearabout gone with y-axis values ranging from -0.1 to 0.1.\n- We will check this seasonal differenced data for stationarity."
\nDescriptive Statistics
üìå  It is clearly seems that there are ouliters in Quantity and UnitPrice that have to be handled \nüìå  There are negative values in UnitPrice and Quantity because of **cancelled orders**.  \nüìå  Missing values in Customer ID and Description. \nüìå  Quantity and Unit Price should be multiplied in order to create **Total Price**.               \n
\nSegmentation Map
\nModel Evaluation
"\nCohort Analysis\n\nA cohort is a group of people sharing something in common, such as the sign-up date to an app, the month of the first purchase, geographical location, acquisition channel (organic users, coming from performance marketing, etc.) and so on. In Cohort Analysis, we track these groups of users over time, to identify some common patterns or behaviors."
\nCustomer Lifetime Value\n\nCustomer lifetime value is how much money a customer will bring your brand throughout their entire time as a paying customer.
\nFrequency of Repeat Transactions
\nCredits\n    \n- https://www.businessnewsdaily.com/15918-how-to-use-crm-analytics.html\n    \n- https://www.analyticsvidhya.com/blog/2021/06/cohort-analysis-using-python-for-beginners-a-hands-on-tutorial/\n    \n- https://benalexkeen.com/bg-nbd-model-for-customer-base-analysis-in-python/
"**Understanding the Respondents:**\n\nLet us first try to understand the respondents who took the survey. We know that most of the respondents were found primarily through Kaggle channels, like our email list, discussion forums and social media channels.\n\nBut let us try to understand more about their demographics from the aspects of age, gender, locality, experience etc\n\n**Gender:**"
A high number of respondents are males compared to females\n\n**Employment Status:**
A high number of respondents are males compared to females\n\n**Employment Status:**
Employed full time forms the major chunk followed by people looking for work\n\n**Age:**\n\nLet us look at the histaogram of age.
Employed full time forms the major chunk followed by people looking for work\n\n**Age:**\n\nLet us look at the histaogram of age.
"Majority of the people have age between 18 to 60. Clearly we can see some increments is round values like 20, 25, 30 and so on. \n\nAlso looks like we have few respondents with age 0 and 100 ;) \n\n**Country:**\n\nLet us look at the top countries from where we get the responses."
"Highest number of respondents are from US followed by India. Others is a category where if a country or territory received less than 50 respondents, Kaggle grouped them into a group named ‚ÄúOther‚Äù for anonymity.\n\n**Formal Education:**\n\nLet us look at the formal education status of the respondents"
Looks like most of the respondents have Masters degree followed by Bachelors and then Doctoral. Let us now look at the education of the parents.\n\n**Parents Education:**
Looks like most of the respondents have Masters degree followed by Bachelors and then Doctoral. Let us now look at the education of the parents.\n\n**Parents Education:**
"In parents education, bachelors comes first followed by masters and then high school. There seems to be an improvement in the highest level of education over years.\n\n**Tenure:**\n\nTenure represents how long the respondents have been writing code to analyze the data. Let us look at it."
"In parents education, bachelors comes first followed by masters and then high school. There seems to be an improvement in the highest level of education over years.\n\n**Tenure:**\n\nTenure represents how long the respondents have been writing code to analyze the data. Let us look at it."
Majority of them have less than 5 years of experience.!\n\n**Students Analysis:**\n\n**Student Status:**\n\nThis field tells whether the respondent has currently enrolled as a student at a degree granting school or not. This question is asked only for the people who are not working.
Majority of them have less than 5 years of experience.!\n\n**Students Analysis:**\n\n**Student Status:**\n\nThis field tells whether the respondent has currently enrolled as a student at a degree granting school or not. This question is asked only for the people who are not working.
Majority of the students are from degree granting school. Now let us check whether the student respondents are learning data science skills formally or informally. Hopfully we will see a lot of yes since the survey mostly caters to Kaggle community.\n\n**Students Learning Datascience:**
Majority of the students are from degree granting school. Now let us check whether the student respondents are learning data science skills formally or informally. Hopfully we will see a lot of yes since the survey mostly caters to Kaggle community.\n\n**Students Learning Datascience:**
As expected most of the students are looking to learn DS.\n\n**Working Professionals Analysis:**\n\nNow let us look at the working professionals data. First let us start with how many people want to switch to DS career.\n\n**Looking to Switch Jobs:**
As expected most of the students are looking to learn DS.\n\n**Working Professionals Analysis:**\n\nNow let us look at the working professionals data. First let us start with how many people want to switch to DS career.\n\n**Looking to Switch Jobs:**
"About 70% of the repondents are looking for a career switch.\n\n**Current Job Title:**\n\nSince most of the people are looking for a career switch, let us look a the job title of working professionals."
"About 70% of the repondents are looking for a career switch.\n\n**Current Job Title:**\n\nSince most of the people are looking for a career switch, let us look a the job title of working professionals."
"Majority of the respondents have title as ""Data Scientist"" followed by ""Software Developer"".\n\nNow let us see how many people believe that the title fits what they have been doing."
"Majority of the respondents have title as ""Data Scientist"" followed by ""Software Developer"".\n\nNow let us see how many people believe that the title fits what they have been doing."
Most of the people feel that the title is fine. Around 1700 people feel that the title fits poorly with what they do.!\n\nLet us now look how many people write codes to analyze data of the respondents.
Most of the people feel that the title is fine. Around 1700 people feel that the title fits poorly with what they do.!\n\nLet us now look how many people write codes to analyze data of the respondents.
"Majority of the respondents who work, write codes to analyze the data. Let us now look at the top employer type to understand more about them."
"Most of the repondent's employers have a size of 10,000 or more employees. "
Most of the employers are from Technology industry followed by Academics.\n\nNow let us look at the the number of years the employer is using advanced analytics / datascience.
Most of the employers are from Technology industry followed by Academics.\n\nNow let us look at the the number of years the employer is using advanced analytics / datascience.
There seems to be sudden drop in 6 to 10 years.
"**Analysis on interesting questions asked to all participants:**\n\nIn this part, let us do some analysis on the interesting questions that has been asked to all the participants.\n\n**Do you currently consider yourself a Data Scientist?**\n\nWhat a question to begin with. Let us now see hoe people responded to that."
"Interestingly, higher number of people said ""no"" than ""yes"". I was expecting the other way around though.\n\nWe can now see how these numbers change based on Employment status."
"Interestingly, higher number of people said ""no"" than ""yes"". I was expecting the other way around though.\n\nWe can now see how these numbers change based on Employment status."
"Interestingly again, people who are employed answered ""No"" for this question than ""Not Employed but looking for work"" people.\n\n**How did you first start your DS / ML training?**\n\nLet us see what is the response for this question."
"Interestingly again, people who are employed answered ""No"" for this question than ""Not Employed but looking for work"" people.\n\n**How did you first start your DS / ML training?**\n\nLet us see what is the response for this question."
Most people responded as online courses followed by university and self-taught. **There are quite a few people who started  through Kaggle competitions as well.**\n\n**Which ML tool or technology are you most excited to learn next year?**\n\nThis is a very interesting question and I would love to know the answer as well.\n\n
Most people responded as online courses followed by university and self-taught. **There are quite a few people who started  through Kaggle competitions as well.**\n\n**Which ML tool or technology are you most excited to learn next year?**\n\nThis is a very interesting question and I would love to know the answer as well.\n\n
"Tensorflow seems to be a clear winner followed by Python and R. Big data technologies like Spark / MLlib, Hadoop are fast catching up as well.\n\n**Which ML method are you most excited to learn next year?:**\n\nHere is one more interesting question."
"Tensorflow seems to be a clear winner followed by Python and R. Big data technologies like Spark / MLlib, Hadoop are fast catching up as well.\n\n**Which ML method are you most excited to learn next year?:**\n\nHere is one more interesting question."
"In line with Tensorflow, Deep learning is the most popular method which people want to learn next year.!\n\n** Which language would you recommend to a new DS to learn first?:**\n\nOne more interesting question to start the language war."
"In line with Tensorflow, Deep learning is the most popular method which people want to learn next year.!\n\n** Which language would you recommend to a new DS to learn first?:**\n\nOne more interesting question to start the language war."
Wow. Python is so tall followed by R. This also goes well with the previous two questions since tensorflow primarily used python and also many deep learning methods.\n\n**Most important way to prove knowledge of ML / DS:**\n\nOne more nice question and let us see what people think.\n
Wow. Python is so tall followed by R. This also goes well with the previous two questions since tensorflow primarily used python and also many deep learning methods.\n\n**Most important way to prove knowledge of ML / DS:**\n\nOne more nice question and let us see what people think.\n
Experience from work turns out to be the most important one. Guess what came second? Kaggle competitions.!\n\n**Time spent on different aspects of DS projects during work:**\n\nIn this section let us see how people spend time on different aspects of Data science like\n\n* Gathering and cleaning data\n* Model building\n* Putting the work Production\n* Visualizing data\n* Finding insights in the data\n* Others\n\nSum of all the different aspects should sum up to 100 as per the instructions given during the survey.
Experience from work turns out to be the most important one. Guess what came second? Kaggle competitions.!\n\n**Time spent on different aspects of DS projects during work:**\n\nIn this section let us see how people spend time on different aspects of Data science like\n\n* Gathering and cleaning data\n* Model building\n* Putting the work Production\n* Visualizing data\n* Finding insights in the data\n* Others\n\nSum of all the different aspects should sum up to 100 as per the instructions given during the survey.
"From the graphs, most part of the time is spent on cleaning / pre-processing the data. It is atleast inline with my experience as well. Looks like productioning the models take lesser time compared to others.\n\n**Percentage of current ML training categories:**\n\nIn this section, let us analyze the different catefories that contributes to the ML / DS training of a person. The sum of all the actegories should be 100. So we could get the percentage share of each of the different categories."
"From the graphs, most part of the time is spent on cleaning / pre-processing the data. It is atleast inline with my experience as well. Looks like productioning the models take lesser time compared to others.\n\n**Percentage of current ML training categories:**\n\nIn this section, let us analyze the different catefories that contributes to the ML / DS training of a person. The sum of all the actegories should be 100. So we could get the percentage share of each of the different categories."
"""Self taught"" seems to be the most important contributor followed by ""Online courses"".\n\nAs a follow up of the previous plot, we can also look at how long people have been learning data science."
"""Self taught"" seems to be the most important contributor followed by ""Online courses"".\n\nAs a follow up of the previous plot, we can also look at how long people have been learning data science."
Almost 50% of the respondents mentioned that it is less than 1 year that they have started learning DS.\n\nNow let us also check how many hours people spend studying DS in a week.
Almost 50% of the respondents mentioned that it is less than 1 year that they have started learning DS.\n\nNow let us also check how many hours people spend studying DS in a week.
"**Work Details Analysis:**\n\nIn this section, let us analyze the different work related details that has been asked as part of the survey. Let us start with the dataset size which people used to work with as part of their job."
"**Work Details Analysis:**\n\nIn this section, let us analyze the different work related details that has been asked as part of the survey. Let us start with the dataset size which people used to work with as part of their job."
A right skewed normal distribution with a peak at 1 GB.\n\n**Computing Hardware used in work:**
A right skewed normal distribution with a peak at 1 GB.\n\n**Computing Hardware used in work:**
Basic laptop - Macbook is the most commonly used hardware.\n\n**Data type used in work:**\n\nNow let us check the different types of data people use in work.
Basic laptop - Macbook is the most commonly used hardware.\n\n**Data type used in work:**\n\nNow let us check the different types of data people use in work.
Relational data is the most commonly used data type followed by Text data.
Relational data is the most commonly used data type followed by Text data.
So 35% of the people responded that sometimes the ML models go to production. Only 7% of the respondents say that it always go to production.\n\n**Algorithms used in work:**
So 35% of the people responded that sometimes the ML models go to production. Only 7% of the respondents say that it always go to production.\n\n**Algorithms used in work:**
Logistic Regression is the most commonly used algorithm is work followed by Decision trees still. But it is worthy to note that neural networks occupy the 4th position and is fast catching up.\n\n**Tools used in work:**
Logistic Regression is the most commonly used algorithm is work followed by Decision trees still. But it is worthy to note that neural networks occupy the 4th position and is fast catching up.\n\n**Tools used in work:**
Python seems to be the most commonly used tool at work followed by R and SQL.\n\n**Methods used in work:**
Python seems to be the most commonly used tool at work followed by R and SQL.\n\n**Methods used in work:**
Data visualisation is the most common method used in work followed by logistic regressiona dn cross validation.\n\n**Challenges faced in work:**
Data visualisation is the most common method used in work followed by logistic regressiona dn cross validation.\n\n**Challenges faced in work:**
The top challenges are dirty data followed by lack of data. Company politics comes in third.\n\n**Data Visualizations in work:**
The top challenges are dirty data followed by lack of data. Company politics comes in third.\n\n**Data Visualizations in work:**
"""None"" category is the lowest of all. So most of the companies are using visualization in atleast some of the projects. \n\n**Usage of Internal and External tools at work:**"
"""None"" category is the lowest of all. So most of the companies are using visualization in atleast some of the projects. \n\n**Usage of Internal and External tools at work:**"
Mostly internal tools are prefereed over external tools.\n\n**Where does DS team sit in work:**\n
Mostly internal tools are prefereed over external tools.\n\n**Where does DS team sit in work:**\n
Standalone is the top answer followed by IT team.\n\n**Data Storage Models at work:**\n\nNow let us look at the data storage models people use at work.
Standalone is the top answer followed by IT team.\n\n**Data Storage Models at work:**\n\nNow let us look at the data storage models people use at work.
"Flat files are the most commonly used data storage methodology. \n\n**Data sharing at work:**\n\nAt work, how do people share source data with each other."
"Flat files are the most commonly used data storage methodology. \n\n**Data sharing at work:**\n\nAt work, how do people share source data with each other."
Most used method is shared drive / sharepoint followed by email. Let us now check how people share the codes at work.
Most used method is shared drive / sharepoint followed by email. Let us now check how people share the codes at work.
Github is the popular way of sharing the codes with each other at work.\n\n**Mathematical Understanding of the Algos used in Work:**
Github is the popular way of sharing the codes with each other at work.\n\n**Mathematical Understanding of the Algos used in Work:**
Most people feel that they are able to explian the algorithm to someone non technical.\n\n**Usage of different tools at work:**\n\nNow we can have a look at how often different tools are being used by the people at work. Please hoven over the bars to get the actual numbers.
**Job Satisfaction:**\n\nHere comes one more important questions. How satisfied are you with your job? The results are on a scale from 0 (Highly dissatisfied) to 10 (Highly Satisfied). I have removed the people who preferred not to share from the below plot.
The peak occurs at 7 and 8 and most people are fairly satisfied with the job. Now let us check how the job satisfation level varies based on the country. Please bear in mind that different countries have different number of respondents as seen from the previous map plot.
"\n\n**Top Data Science Blogs / Podcasts / Newsletters:**\n\nLet us see the responses for ""What are your top 3 favorite DS blogs / podcass / newsletters?"""
"Kdnuggets is the most popular blog followed by R bloggers.\n\n**Compensation Analysis:**\n\nIn this section, let us take a deeper look at the compensation details. \n\nBefore we delve deep, a quick look into the data revealed that there are 11492 null values out 16716 rows. *So in the following analysis, we are only using the rows where the compensation is provided. So please note that there might be some bias in the following analysis.*\n\nLet us first do a scatter plot to see if there are any outliers."
"Kdnuggets is the most popular blog followed by R bloggers.\n\n**Compensation Analysis:**\n\nIn this section, let us take a deeper look at the compensation details. \n\nBefore we delve deep, a quick look into the data revealed that there are 11492 null values out 16716 rows. *So in the following analysis, we are only using the rows where the compensation is provided. So please note that there might be some bias in the following analysis.*\n\nLet us first do a scatter plot to see if there are any outliers."
We could clearly see that there are couple of points which are almost 100B. We can impute them with the median (probably not a good strategy since the currencies are different but since there are only 2 points let us go ahead)  
We could clearly see that there are couple of points which are almost 100B. We can impute them with the median (probably not a good strategy since the currencies are different but since there are only 2 points let us go ahead)  
Looks better now. Also please note that there are multiple currencies in the denomination. So we can check how the distribution is for each currency.
Looks better now. Also please note that there are multiple currencies in the denomination. So we can check how the distribution is for each currency.
"Most of them are not really visible since some of the currencies like IRR, IDR have very high values.\n\nNow let us convert all of them to USD and then check the scatter plot."
"Most of them are not really visible since some of the currencies like IRR, IDR have very high values.\n\nNow let us convert all of them to USD and then check the scatter plot."
"Now the scale of the y-axis has come down from 1.2B to 10M. \n\nNow let us again look at the box plot by currency / country but this time let us also impute the compensations greater than 500K USD by median value of 53,812 USD for better visualization."
"Now the scale of the y-axis has come down from 1.2B to 10M. \n\nNow let us again look at the box plot by currency / country but this time let us also impute the compensations greater than 500K USD by median value of 53,812 USD for better visualization."
This plot might be affected by some currencies whose count is very less. So let us include only those currencies which occurs more than 30 times and plot this again.
This plot might be affected by some currencies whose count is very less. So let us include only those currencies which occurs more than 30 times and plot this again.
**More to come.Stay tuned.!**
## 4.1.3. Code Example:
# >> 4.2. DBSCAN clustering algorithm
## 4.2.3. Code Example:
# >> 4.3. Gaussian Mixture Model algorithm
## 4.3.3. Code Example :
# > 4.4. BIRCH algorithm\n\n**The Balance Iterative Reducing and Clustering using Hierarchies (BIRCH) algorithm works better on large data sets than the k-means algorithm.**\nIt breaks the data into little summaries that are clustered instead of the original data points. The summaries hold as much distribution information about the data points as possible.\n\n![](https://media.geeksforgeeks.org/wp-content/uploads/20200612004451/BIRCH.png)\n\nThis algorithm is commonly used with other clustering algorithm because the other clustering techniques can be used on the summaries generated by BIRCH.\nThe main downside of the BIRCH algorithm is that it only works on numeric data values. You can't use this for categorical values unless you do some data transformations.
## 4.3.4. Code Example:
# > 4.5. Affinity Propagation clustering algorithm
## 4.5.1. Code Examples
# > 4.6. Mean-Shift clustering algorithm
## 4.6.3. Code Example
"# > 4.7. OPTICS algorithm\n**OPTICS Clustering stands for Ordering Points To Identify Cluster Structure.** It draws inspiration from the DBSCAN clustering algorithm. It adds two more terms to the concepts of DBSCAN clustering.\n\nThey are:-\n- **Core Distance:** It is the minimum value of radius required to classify a given point as a core point. If the given point is not a Core point, then it‚Äôs Core Distance is undefined.\n- **Reachability Distance:** It is defined with respect to another data point q(Let). The Reachability distance between a point p and q is the maximum of the Core Distance of p and the Euclidean Distance(or some other distance metric) between p and q. Note that The Reachability Distance is not defined if q is not a Core point.\n\n![](https://media.geeksforgeeks.org/wp-content/uploads/20190711114717/reachability_distance1.png)"
## 4.7.2. Code Example: 
# > 4.8. Agglomerative Hierarchy clustering algorithm\nThe agglomerative clustering is the most common type of hierarchical clustering used to group objects in clusters based on their similarity. It's also known as AGNES (Agglomerative Nesting). The algorithm starts by treating each object as a singleton cluster.
## 4.8.3. Code Example: 
# > 4.9. DIANA or Divisive Analysis
## 4.10.3. Code Example 
"# 5. Comperative Analysis of Algorithms\nCredit: [*Sunit Prasad, Different Types of Clustering Methods and Applications*](https://www.analytixlabs.co.in/blog/types-of-clustering-algorithms/)"
\n\n\n\n0¬†¬†IMPORTS¬†¬†¬†¬†‚§í
\n\n\n\n1¬†¬†BACKGROUND INFORMATION¬†¬†¬†¬†‚§í\n\n---\n
\n\n\n\n\n\n\n    3¬†¬†HELPER FUNCTION & CLASSES¬†¬†¬†¬†‚§í\n\n\n---
\n\n\n\n\n\n\n    4¬†¬†DATASET CREATION AND EXPLORATION¬†¬†¬†¬†‚§í\n\n\n---
QUICK DOUBLECHECK ON RLE DECODE FUNCTION\n* https://www.kaggle.com/namgalielei/which-reshape-is-used-in-rle
4.2 INVESTIGATE THE TRAIN DATAFRAME\n\n---\n
4.2 INVESTIGATE THE TRAIN DATAFRAME\n\n---\n
4.3 VISUALIZE & INVESTIGATE THE LIVECell DATA\n\n---\n\nLet's review the LIVECell dataset\n
4.5 VISUALIZE & INVESTIGATE INDIVIDUAL CELLS\n\n---\n\nLet's review a single image and then look into the cells that make up that image\n
4.6 ADD BBOXES TO DATAFRAME\n\n---\n\nLet's iterate over the images and add the bounding box coordinates to the dataframe in order (matching the RLE)\n\n* We also add the 0-1 normalized version of the bboxes to the dataframe for later training purposes\n
4.7 STATS REGARDING BOUNDING BOXES\n\n---
"\n\n\n\n\n\n\n    5¬†¬†MODELLING¬†¬†¬†¬†‚§í\n\n\n---\n\nEfficientDET for Object Detection, Classification, and Segmentation"
5.3 INSTANIATE OUR DATALOADER\n\n---\n\nAugmentations are breaking the masks... so disablled for now
5.4 CREATE MODEL AND LOAD PRETRAINED WEIGHTS\n\n---\n\nCOCO weights
5.6 VALIDATE THE MODEL IS LEARNING\n\n---\n
5.7 IOU ON VALIDATION DATASET\n\n---\n
# Importing all the necessary libraries
# Reading the comma separated values file into the dataframe
"We now define our training loop and evaluation loop functions. Much of the code is inspired by @abhishek's [kernel](https://www.kaggle.com/abhishek/bert-multi-lingual-tpu-training-8-cores). As you will see later, these functions are run on each of the 8 cores.\n\nTo get the loss of a batch, since the data is spread across the 8 cores, we have to _reduce_ the loss. Now the `loss_reduced` will be the same on all the cores (average of the 8 losses). `xm.master_print` can be used to print this value once from only one core.\n\nPyTorch XLA requires that the optimizer be stepped using their own function `xm.optimizer_step(optimizer)`."
"We finally define our main function (which itself calls the above functions) that will be spawned by PyTorch XLA multiprocessing. This function will be run on each of the 8 cores. There are several steps for 8-core training:\n1. We need to use a `DistributedSampler` that will appropriately distribute the dataset across the 8 cores.\n2. We are using `num_workers=0` as that decreases memory usage (only master process loading data). On higher memory VMs, this could be increased to speed up the training.\n3. The learning rate is scaled by the number of TPU cores (`xm.xrt_world_size()`)\n4. We put the model onto the TPU\n5. We use `ParallelLoader` which is a PyTorch XLA-specific DataLoader for loading data onto the TPU.\n6. We save the model at the end of training with `xm.model_save`"
That's it! Now we can submit the results of our XLM-R model!\n\n### Post-processing:\n\nLet's try @christofhenkel's post-processing trick from [here](https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification/discussion/160980)
"# Acknowledgments:\n- Based on data from [Abhishek's code](https://www.kaggle.com/abhishek/bert-multi-lingual-tpu-training-8-cores-w-valid)\n- Model based on [xhlulu's code](https://www.kaggle.com/xhlulu/jigsaw-tpu-xlm-roberta)\n- Original attempt from [Aditya's code](https://www.kaggle.com/adityaecdrid/simple-xlmr-tpu-pytorch)\n- Discussion with Davide Libenzi and Daniel Sohn (PyTorch XLA team) - [code](https://www.kaggle.com/davidelibenzi/simple-xlmr-tpu-pytorch)\n- Fruitful discussions with Abhishek and Aditya\n\n\n# Fin\n\nIf you have any questions or suggestions, please drop a comment! :)"
"### Imports\n\n> We are using a typical data science stack: `numpy`, `pandas`, `sklearn`, `matplotlib`. \n"
"# 3. Read in Data\n Top\n\nFirst, we can list all the available data files. There are a total of 6 files: 1 main file for training (with target) 1 main file for testing (without the target), 1 example submission file, and 4 other files containing additional information about energy types based on historic usage rates and observed weather. . "
#### Count of Observation by Game/Video title
#### Count by World
#### Count by World
# 11. Simple Baseline \n Top\n\n
# Loading base packages\n\nNothing too surprising here. Collections deque data structure will help us keep track of past data.
# Load data\n\nLoading a pickle file. Check this notebook [pickling](https://www.kaggle.com/quillio/pickling) if you haven't pickled your data set yet. Check this notebook [one liner to halve your memory usage](https://www.kaggle.com/jorijnsmit/one-liner-to-halve-your-memory-usage) if you want to reduce memory usage before pickling.
There is no pattern visible in this plot and values are distributed equally around zero. So Linearity assumption is satisfied
Here the residuals are normally distributed. So normality assumption is satisfied
![](https://miro.medium.com/max/700/1*oB3S5yHHhvougJkPXuc8og.gif)
# Objective\n\nThe aim of this kernel is to provide all the tips and tricks required to train image classification model on any given image dataset in a single page.This kernel will hold almost all steps and steps required to implement image classification algorithm using SOTA such as ResNET on any given dataset.It could be a great time saver for you.Just utilize it anytime when you are working on Image Classification.\n\nI have learned them from [FastAI](https://docs.fast.ai/)
\n2.2 Library Import
\n3.1 Setting up path for training data
\n4.2 Finding LR\n
\n4.3 Train Model\n
## 3. Age
#### This graph shows the density of people who belong to the 3 class along with the age.
#### Here we will be taking the prefix values of the cabin number. The missing values will be replaced with 'X'.
#### These cabins will be mapped with a numeric value.
### This column has the ticket number of all the passengers. Here we will be taking the ticket prefix.
### Here is a tricky part. The training set and test set have a few tickets which are unique to themselves.
# Initial Overview
"# What are the happiest countries in the world?\n\n'Happiness' to me seems like an individual metric, something that is hard to generalise. However, some countries perform consistently well on the happiness index rankings. \n\nWe've also noted that 9 of the top 10 are in Europe, and that 7 of the bottom 10 are in Africa.\n\nLet's see which countries top the list currently, and those that are at the bottom."
Let's now view the top 10 and bottom 10 ...
Let's bring the top 10 & bottom 10 side-by-side now for an alternative view
Let's bring the top 10 & bottom 10 side-by-side now for an alternative view
"At a glance, we see that many of the happiest countries in the world are indeed in Europe. \n\nAn additional observation is that the countries in Europe in the top 10 are Northern European."
"# Is this the case often?\n\nI will explore temporal change more in depth later, but for now, let's look at the **top 20 countries over the years**.\n\nThis plot shows all scores from 2005 through to the present for the top 20 countries, with their Mean score and their 2021 scores hihglighted specifically.\n\nIt's remarkable that many countries 2021 score is higher than their mean, despite the pandemic.\n\nAlthough the scores do vary, they still remain relatively high."
"# Why are there differences?\n\nWe now understand that Northern European nations top the list, and have done for some time.\n\nLet's explore these differences between Europe and the rest of the world a little more closely."
"# Why are there differences?\n\nWe now understand that Northern European nations top the list, and have done for some time.\n\nLet's explore these differences between Europe and the rest of the world a little more closely."
"Happier countries tend to be those with longer life expectancies, and a higher GDP. This is also most of Western Europe.\n\nLet's explicitly highlight Africa now..."
"Happier countries tend to be those with longer life expectancies, and a higher GDP. This is also most of Western Europe.\n\nLet's explicitly highlight Africa now..."
"By and large, African countries have lower life expectancy, a lower GDP, and ultimately, lower happiness index scores.\n\n# Other factors\n\nSo GDP & Life expactancy are factors. What else can be considered?"
"By and large, African countries have lower life expectancy, a lower GDP, and ultimately, lower happiness index scores.\n\n# Other factors\n\nSo GDP & Life expactancy are factors. What else can be considered?"
"As I have noted in the plot, Freedom & Corruption are inversely related: higher corruption tends to be accompanied by lower freedom.\n\nHowever, it is interesting to note that several European nations have high percieved levels of corruption too.\n\n\n# A Continental view\n\nLet's wrap the countries up in to their respective continents to see if we can learn more.\n\nOf course we expect Western Europe to be high, but are there any other continents that perform particularly well or poorly in the happiness rankings?"
"As I have noted in the plot, Freedom & Corruption are inversely related: higher corruption tends to be accompanied by lower freedom.\n\nHowever, it is interesting to note that several European nations have high percieved levels of corruption too.\n\n\n# A Continental view\n\nLet's wrap the countries up in to their respective continents to see if we can learn more.\n\nOf course we expect Western Europe to be high, but are there any other continents that perform particularly well or poorly in the happiness rankings?"
There are **three clusters** of continents that are clear to see. **More on this later...**\n\nSub-Saharan Africa & South Asia have the lowest scores. Whilst Western Europe and North America & ANZ are far ahead at the top.
"# Differences between those above & below the mean happiness level\n\nLet's plot all many features at once, split by the mean happiness level. The happiest countries are, as always, shown in green."
"The plots above confirm what we saw earlier, with some notable features such as social support.\n\nThat genorisity is percieved as higher in unhappier countries is very interesting."
# A global view\n\nWe've now seen clear differences between countries based on severla factors. \n\nLet's see this globally
"This plot confirms what we discovered above, with South Asia and Africa being in the red.\n\nBut it also highlights areas we can investigate. For exmaple, China & India, both in the red, have **populations over 1 billion.** Can we investigate population and happiness levels?"
"We see clearly that happier countries tend to be **older, and less populous**. \n\nI've included Europe for reference.\n\nWhat about Fertility rate?"
"As I suspected, happier countries tend also to have **fewer children**. Likely this is due to access to contraception."
"As I suspected, happier countries tend also to have **fewer children**. Likely this is due to access to contraception."
I am surprised that **population density doesn't effect happiness** - though that is due to personal preferences!
# Have there been changes over time?\n\nDo the unhappy get happier?\n\nIs this just a snapshot of a moment in time? Or are the trends more long-standing?
"Of concern here is that the **unhappy stay unhappy, and worse still, they appear to be getting unhappier.**\n\nIs this trend consistent? Or do some countries scores improve over time?"
"Let's explore changes over time a little more.\n\nAbove, I took a sample of a few countries. Let's plot their changes in a **slopegraph from 2007 to 2020** to see if we can learn anything"
Clearly there has indeed been a lot of movement through the years.\n\nWhich countries have seen the biggest changes?
Let's compare the biggest gainer & the biggest loser in terms of the happiness index scores: Bulgaria & Jordan.\n\nWe'll compare how they both got on over the years
"Whilst I explore this idea of temporal change, I want to look at a continental perspective.\n\nFor example, are all countries in Western Europe 'happy'?"
"Whilst I explore this idea of temporal change, I want to look at a continental perspective.\n\nFor example, are all countries in Western Europe 'happy'?"
Europe though seems almost exclusively without worry. Is this true? Doubtful.\n\nLet's explore Europe a little more...
Europe though seems almost exclusively without worry. Is this true? Doubtful.\n\nLet's explore Europe a little more...
"so within Europe there are indeed countries that might be considered 'unhappy', such as North Cyprus and Greece.\n\nAs I noted above, the North-South split is a well known issue within Europe, and even within countries such as Italy.\n\nLet's explore the variance in happiness index scores across all continents now..."
"so within Europe there are indeed countries that might be considered 'unhappy', such as North Cyprus and Greece.\n\nAs I noted above, the North-South split is a well known issue within Europe, and even within countries such as Italy.\n\nLet's explore the variance in happiness index scores across all continents now..."
"As I suspected, there is of course variance within every continent/region.\n\nHowever, highlighted above is the Middle East & North Africa because it has such a large spread in happiness scores.\n\nLet's repeat the plot from earlier on to see how each country in that region has scored over the years..."
"As I suspected, there is of course variance within every continent/region.\n\nHowever, highlighted above is the Middle East & North Africa because it has such a large spread in happiness scores.\n\nLet's repeat the plot from earlier on to see how each country in that region has scored over the years..."
"This region contains many countries that are in vastly different political realities - from war to peace, which of course will have a large effect on happiness scores.\n\nDespite the intercontinental variance, it is actually the case that there are only a few countries to have ever topped the leaderboard. \n\nIt does not seem like this will be changing any time soon either.\n"
"This region contains many countries that are in vastly different political realities - from war to peace, which of course will have a large effect on happiness scores.\n\nDespite the intercontinental variance, it is actually the case that there are only a few countries to have ever topped the leaderboard. \n\nIt does not seem like this will be changing any time soon either.\n"
"# Last steps: Clustering\n\nNow we have a clear understanding of our data.\n\nEarlier, I mentioned that there appeared to be **three clusters** in our dataset between regions. \n\nLet's formally cluster our data and see if the intuitions, **uncovered purely by exploratory data analysis**, prove correct."
"# Last steps: Clustering\n\nNow we have a clear understanding of our data.\n\nEarlier, I mentioned that there appeared to be **three clusters** in our dataset between regions. \n\nLet's formally cluster our data and see if the intuitions, **uncovered purely by exploratory data analysis**, prove correct."
"I'll use **K-Means** and  **the Elbow Method** to select the number of clusters.\n\nIn the Elbow method, we are actually varying the number of clusters (K) from 1 ‚Äì 6 in this caase. \n\nFor each value of K, we are calculating WCSS (Within-Cluster Sum of Square). WCSS is the sum of squared distance between each point and the centroid in a cluster. When we plot the WCSS with the K value, the plot looks like an Elbow. As the number of clusters increases, the WCSS value will start to decrease. WCSS value is largest when K = 1. When we analyze the graph we can see that the graph will rapidly change at a point and thus creating an elbow shape. From this point, the graph starts to move almost parallel to the X-axis. The K value corresponding to this point is the optimal K value or an optimal number of clusters.\n\nA good read and source for above description:\n(https://www.analyticsvidhya.com/blog/2021/01/in-depth-intuition-of-k-means-clustering-algorithm-in-machine-learning/)"
"I'll use **K-Means** and  **the Elbow Method** to select the number of clusters.\n\nIn the Elbow method, we are actually varying the number of clusters (K) from 1 ‚Äì 6 in this caase. \n\nFor each value of K, we are calculating WCSS (Within-Cluster Sum of Square). WCSS is the sum of squared distance between each point and the centroid in a cluster. When we plot the WCSS with the K value, the plot looks like an Elbow. As the number of clusters increases, the WCSS value will start to decrease. WCSS value is largest when K = 1. When we analyze the graph we can see that the graph will rapidly change at a point and thus creating an elbow shape. From this point, the graph starts to move almost parallel to the X-axis. The K value corresponding to this point is the optimal K value or an optimal number of clusters.\n\nA good read and source for above description:\n(https://www.analyticsvidhya.com/blog/2021/01/in-depth-intuition-of-k-means-clustering-algorithm-in-machine-learning/)"
"We can now **create 3 clusters** within our dataset.\n\nThis can help for further analysis, and aid in **building our understanding even further**"
We can also view how our clusters are distributed geographically...
"# Conclusions\n\nWe see that there are many clear distinctions between happy and unhappy countries - at least at the extremes. \n\nGenerally, happier countries tend to:\n\n- be wealthier\n\n- be less populous\n\n- have fewer children\n\n- be older\n\n- be less corrupt\n\n- be more free\n\n- have a lot of social support\n\nOf course, a lot of these findings may come **as a result of** being wealthy, rather than be a cause of happiness in and of themselves.\n\nWe also know that, generally, the happy stay happy, and the unhappy are getting unhappier. \n\nHowever, as we have seen with Bulgaria - this does not have to be the case. Bulgaria improved by over 1.5 points between 2007 and 2020 - the most of any country. So there is hope that unhappy countries can break the historic trends.\n\nLastly, we clustered the data, using K-means and the elbow method. This enables us again to easily visualise and inderstand the differences between nations, and what features lead to high or low happiness index scores.\n"
"## 3. Practice with Facebook Prophet\n\n### 3.1 Installation in Python\n\nFirst, you need to install the library. Prophet is available for Python and R. The choice will depend on your personal preferences and project requirements. Further in this article we will use Python.\n\nIn Python you can install Prophet using PyPI:\n```\n$ pip install fbprophet\n```\n\nIn R you can find the corresponing CRAN package. Refer to the [documentation](https://facebookincubator.github.io/prophet/docs/installation.html) for details.\n\nLet's import the modules that we will need, and initialize our environment:"
"### 3.2 Dataset\n\nWe will predict the daily number of posts published on [Medium](https://medium.com/).\n\nFirst, we load our dataset (download it from [here](https://drive.google.com/file/d/1G3YjM6mR32iPnQ6O3f6rE9BVbhiTiLyU/view?usp=sharing) and place in the '../../data' folder if you'd like to reproduce the following code):"
"### 3.3 Exploratory visual analysis\n\nAs always, it may be helpful and instructive to look at a graphical representation of your data.\n\nWe will create a time series plot for the whole time range. Displaying data over such a long period of time can give clues about seasonality and conspicuous abnormal deviations.\n\nFirst, we import and initialize the `Plotly` library, which allows creating beautiful interactive plots:"
"We also define a helper function, which will plot our dataframes throughout the article:"
"We also define a helper function, which will plot our dataframes throughout the article:"
Let's try and plot our dataset *as is*:
Let's try and plot our dataset *as is*:
"High-frequency data can be rather difficult to analyze. Even with the ability to zoom in provided by `Plotly`, it is hard to infer anything meaningful from this chart apart from the prominent upward and accelerating trend.\n\nTo reduce the noise, we will resample the post counts down to weekly bins. Besides *binning*, other possible techniques of noise reduction include [Moving-Average Smoothing](https://en.wikipedia.org/wiki/Moving_average) and [Exponential Smoothing](https://en.wikipedia.org/wiki/Exponential_smoothing), among others.\n\nWe save our downsampled dataframe in a separate variable because further in this practice we will work only with daily series:"
"Finally, we plot the result:"
"This downsampled chart proves to be somewhat better for an analyst's perception.\n\nOne of the most useful functions that `Plotly` provides is the ability to quickly dive into different periods of timeline in order to better understand the data and find visual clues about possbile trends, periodic and irregular effects. \n\nFor example, zooming-in on a couple of consecutive years shows us time points corresponding to Christmas holidays, which greatly influence human behaviors.\n\nNow, we're going to omit the first few years of observations, up to 2015. First, they won't contribute much into the forecast quality in 2017. Second, these first years, having very low number of posts per day, are likely to increase noise in our predictions, as the model would be forced to fit this abnormal historical data along with more relevant and indicative data from the recent years."
"In the resulting dataframe you can see many columns characterizing the prediction, including trend and seasonality components as well as their confidence intervals. The forecast itself is stored in the `yhat` column.\n\nThe Prophet library has its own built-in tools for visualization that enable us to quickly evaluate the result.\n\nFirst, there is a method called `Prophet.plot` that plots all the points from the forecast:"
"This chart doesn't look very informative. The only definitive conclusion that we can draw here is that the model treated many of the data points as outliers.\n\nThe second function `Prophet.plot_components` might be much more useful in our case. It allows us to observe different components of the model separately: trend, yearly and weekly seasonality. In addition, if you supply information about holidays and events to your model, they will also be shown in this plot.\n\nLet's try it out:"
"### 3.6 Visualization\n\nLet's create our own visualization of the model built by Prophet. It will comprise the actual values, forecast and confidence intervals.\n\nFirst, we will plot the data for a shorter period of time to make the data points easier to distinguish. Second, we will show the model performance only for the period that we predicted, that is the last 30 days. It seems that these two measures should give us a more legible plot.\n\nThird, we will use `Plotly` to make our chart interactive, which is great for exploring.\n\nWe will define a custom helper function `show_forecast` and call it (for more on how it works please refer to the comments in the code and the [documentation](https://plot.ly/python/)):"
"At first glance, the prediction of the mean values by our model seems to be sensible. The high value of MAPE that we got above may be explained by the fact that the model failed to catch on to increasing peak-to-peak amplitude of weakly seasonality.\n\nAlso, we can conclude from the graph above that many of the actual values lie outside the confidence interval. Prophet may not be suitable for time series with unstable variance, at least when the default settings are used. We will try to fix this by applying a transform to our data."
### Example of a picture in CelebA dataset\n178 x 218 px
"### Distribution of the Attribute\n\nAs specified before, this Notebook is an imagine recognition project of the Gender. There are more Female gender than Male gender in the data set. This give us some insight about the need to balance the data in next steps."
"### Distribution of the Attribute\n\nAs specified before, this Notebook is an imagine recognition project of the Gender. There are more Female gender than Male gender in the data set. This give us some insight about the need to balance the data in next steps."
"## Step 2: Split Dataset into Training, Validation and Test\n\nThe recommended partitioning of images into training, validation, testing of the data set is: \n* 1-162770 are training\n* 162771-182637 are validation\n* 182638-202599 are testing\n\nThe partition is in file list_eval_partition.csv\n\nDue time execution, by now we will be using a reduced number of images:\n\n* Training 20000 images\n* Validation 5000 images\n* Test 5000 Images\n"
### 3.1. Let's start with an example: Data Augmentation\n\nThis is how an image will look like after data augmentation (based in the giving parameters below).
"The result is a new set of images with modifications from the original one, that allows to the model to learn from these variations in order to take this kind of images during the learning process and predict better never seen images."
\n#  6. Data Visualization \n
* 26.6 % of customers switched to another firm.\n* Customers are 49.5 % female and 50.5 % male.
* There is negligible difference in customer percentage/ count who chnaged the service provider. Both genders behaved in similar fashion when it comes to migrating to another service provider/firm.
* About 75% of customer with Month-to-Month Contract opted to move out as compared to 13% of customrs with One Year Contract and 3% with Two Year Contract
"* A lot of customers choose the Fiber optic service and it's also evident that the customers who use Fiber optic have high churn rate, this might suggest a dissatisfaction with this type of internet service.\n* Customers having DSL service are majority in number and have less churn rate compared to Fibre optic service."
* Customers without dependents are more likely to churn
* Customers without dependents are more likely to churn
* Customers that doesn't have partners are more likely to churn
* Customers that doesn't have partners are more likely to churn
* It can be observed that the fraction of senior citizen is very less.\n* Most of the senior citizens churn.
* It can be observed that the fraction of senior citizen is very less.\n* Most of the senior citizens churn.
"* Most customers churn in the absence of online security, "
"* Most customers churn in the absence of online security, "
* Customers with Paperless Billing are most likely to churn.
* Customers with Paperless Billing are most likely to churn.
* Customers with no TechSupport are most likely to migrate to another service provider.
* Customers with no TechSupport are most likely to migrate to another service provider.
"* Very small fraction of customers don't have a phone service and out of that, 1/3rd Customers are more likely to churn."
"* Very small fraction of customers don't have a phone service and out of that, 1/3rd Customers are more likely to churn."
* Customers with higher Monthly Charges are also more likely to churn
* New customers are more likely to churn
___
"**For absoulte beginners, do check the notebook**\n\n# [Beginners Notebook with EDA](https://www.kaggle.com/harshkothari21/beginners-notebook-90-accuracy-with-eda)"
# Table of content \n\n- EDA\n- Handle Missing Values\n- Feature Engineering\n- linear Regression\n- Logistic Regression\n- Scalling\n- KNN Classifier\n- Support Vector Machine(SVM)\n- Kernelize SVM\n- Decision Tree\n- Random Forest
**Let's look at target feature first**
"**So the plot says we have more number of non-survived people and females are more likely to survived than male!. so, 'Sex' looks like a very strong explanatory variable, and it can be good choice for our model!**"
**Let's first vizualize null values on our training set on graph**
**We will be dealling with null values later on.**
**Let's analysize Pclass**
looking at some satistical data!
**It's Time to look at the Age column!**
**Most Important thing when plotting histograms : Arrange Number of Bins**
**Most Important thing when plotting histograms : Arrange Number of Bins**
**Age column has non-uniform data and many outliers**\n\n**Outlier** : An outlier is an observation that lies an abnormal distance from other values in a random sample from a population.
**Age column has non-uniform data and many outliers**\n\n**Outlier** : An outlier is an observation that lies an abnormal distance from other values in a random sample from a population.
"**Age by surviving status**\n\nDid age had a big influence on chances to survive?\nTo visualize two age distributions, grouped by surviving status I am using boxlot and stripplot showed together:"
"**Age by surviving status**\n\nDid age had a big influence on chances to survive?\nTo visualize two age distributions, grouped by surviving status I am using boxlot and stripplot showed together:"
**Let's look at Number of siblings/spouses aboard**
**Let's look at Number of siblings/spouses aboard**
"**Looks like single person Non-survived count is almost double than survived, while others have 50-50 % ratio**"
**Now Looking at Port of embarkation**
**Can't say much!**
**Look in to relationships among dataset**
**Configure the heatmap**
**Configure the heatmap**
**Fare vs Embarked**
**Fare vs Embarked**
"- The wider fare distribution among passengers who embarked in Cherbourg. It makes scence - many first-class passengers boarded the ship here, but the share of third-class passengers is quite significant.\n- The smallest variation in the price of passengers who boarded in q. Also, the average price of these passengers is the smallest, I think this is due to the fact that the path was supposed to be the shortest + almost all third-class passengers."
**Fare vs Pclass**
"We can observe that the distribution of prices for the second and third class is very similar. The distribution of first-class prices is very different, has a larger spread, and on average prices are higher.\n\nLet's add colours to our points to indicate surviving status of passenger (there will be only data from training part of the dataset):"
"We can observe that the distribution of prices for the second and third class is very similar. The distribution of first-class prices is very different, has a larger spread, and on average prices are higher.\n\nLet's add colours to our points to indicate surviving status of passenger (there will be only data from training part of the dataset):"
Let's look at some maximum and minimum values of features!
one of the effectitve way to fill the null values is by finding correlation
"> **Pclass and age, as they had max relation in the entire set we are going to replace missing age values with median age calculated per class**"
\n## 3. Import Libraries üìö 
\n## 4. Read and Explain Dataset üßæ  
 Experience level
üéà Observations: Most of the staff have senior working experience
 Employment type
üéà Observations: Almost all employees have Full-Time job
 Remote ratio
"üéà Observations: For each year(2020,2021,2022), the count for work type-remote is high.The number of remote working is huge. The cause is most likely due to the impact of the COVID-19 epidemic, changing the working trend."
Employee residence
"üéà Observations: Most of the empolyees(TOP 5) are from US, UK, India, Canada and Germany."
Company location
"üéà Observations: Most of the countries(TOP 5) are located in US, UK, Canada, Germany and India."
Work year
üéà Observations: This shows a positive trend and shows that data science jobs are becoming more valuable as the years pass.
 Salary in USD based on company location
"Experience Level, Salary, and Company Size"
"Experience Level, Salary, and Company Size"
"Remote Ratio, Salary, and Employment Type"
"Remote Ratio, Salary, and Employment Type"
"Experience level, Salary, Job title"
"Experience level, Salary, Job title"
Word Cloud of Subject Titles
Word Cloud of Subject Titles
\n## 7. Ask a question and solve it üí° 
Q3. How has the average salary of data science jobs change over time?\n
üéà Observations: The average salaries increased from approximately USD96000 in 2020 to USD100000 in 2021 and finally USD125000 in 2022. This shows a positive trend and shows that data science jobs are becoming more valuable as the years pass.
"Q4. If you work full-time, is the salary higher than other types?"
"üéà Observations: Though Data science jobs that are full time based have a higher average salary than the ones that are part time or freelance, the data science jobs that are contract based have the highest average salary.\n"
 Q5. What are the salaries of Data Science jobs based on experience level?
üéà Observations: The entry level data science jobs have much lower average salaries than the Expert levels job.
"Q6: Working in large companies, the salary is higher than other types of companies?"
"üéà Observations: Data science jobs in larger sized company have a higher average salary than medium sized and small companies. This does not necessarily imply that larger companies will always guarantee a higher salary on average. For example, within the company there will be varying levels of expertise and hence differing salary."
# Loading packages and data \n\nBefore we can start we need to import some packages and the data of course:
**Peek at the data**
# Helper methods \n\nWe will use some helper methods that we collect in this chapter to reduce amount of code during analysis:\n\n* get_outliers\n* make_word_cloud\n* split_data_by_nan\n* scale_and_log
# The Gaussian Mixture Model 
"# Feature scaling and transformation \n\nAfter these steps our dataset is cleaned and optimized for our analysis, but because we want to use the Gaussian Mixture Model we still have to scale and transform the data a bit. Looking closer on how our model learns, we can see two problems in the M-Step:\n\n$$\mu_{k} = \frac{1}{N_{k}} \sum_{n=1}^{N} \gamma_{nk} x_{n} $$\n\n$$\Sigma_{k} =  \frac{1}{N_{k}}  \sum_{n=1}^{N}  \gamma_{nk} (x_{n} - \mu_{k}) (x_{n} - \mu_{k})^{T}$$ \n\n### Why should we transform features?\n\nThe cluster center $\mu_{k}$ and the covariance matrices $\Sigma_{k}$ are influenced by the feature values $x_{n}$ themselves. As we have skewed feature distributions with high outliers the cluster center are shifted towards higher values during each update step. This is caused by the weighted mean. Even though the responsibilities as weights are able to soften the effect of outliers, they can still contribute very much. In this case our model builds clusters during learning that try to match the structure of outliers. The cluster center would not explain the majority of its cluster members and could be even located in regions with no data spots at all. To solve this problem we transform our features using boxcox transformation. Doing so we like to expand the majority of data points to uncover hidden substructures and compress the outliers.\n\nEven this sounds easy, this is the most toughest part of all! \n\n### Why should we scale them?\nEven with transformed features there is still a problem left: So far all our features have positive values and the responsibilities are positive as well as they are probabilities between 0 and 1.  This will still cause a shift in the center and covariance updates as the assignments will be lead by the higher values. To correct this we want to shift our transformed feature distributions such that it is evenly distributed around 0. For this purpose we subtract the mean and scale to unit variance. \n\n### How does to boxcox knead our features?\nUsing the boxcox transformation we are able to knead the feature distributions as we like them to be. But with this this great power comes also great responsibility ;-) . Consequently we should try to understand what boxcox can do with our values!"
"Looking at the power parameter $\lambda$ of the boxcox transformation you can see that we are able to compress outliers more and more by chosing lower values of $\lambda$. In addition we obtain a ""negative"" stretching of low original values lower than one. If we fix $\lambda =0.5$ and vary the constant $c$ we can observe only slight differences of the high values compression. On the other hand we can see that the stretching of low values is even stronger of constants c close to zero. \n\n#### Take-away (not that simple but still good)\n\n* If you like to compress outliers - chose a low $\lambda$\n* If you like to stretch low values - chose a low $c$ close to zero.\n\n\n### Transform! \n\nOk, let's go! If you like, fork and chose other values. You will see that the resulting clusters highly depend on these hyperparameters!"
### Visual comparison\n\nLet's have a look at our transformed features:
Many of them look far better than before but the problem of setting optimal hyperparameters is still very difficult.
## How much data is covered per cluster?
### Take-away\n\n* We can see that most of our clusters cover around  5 % of our data. \n* Nonetheless there exist some clusters with verly low or very high coverage.\n\nWe should try to find out which kind of products and feature ranges belong to these clusters. This way we should try to find out if our clusters are inhomogenous or if we should use more components for our mixture model.
## Can we reveal the hidden product type?
"### Take-away\n\n* First of all: YES! We can find hidden product categories. That's amazing! No one told Gaussian Mixture something about product names or types. Only dealing with nutrition table information it was able to group similar products. We can find various interesting clusters of chocolate, pasta, ice cream, cheese, yoghurts, juice, grains, sauces, meat, water and nuts.\n* On the goarse-grained view our clustering was very successful. But if you take a closer look you may wonder why some clusters hold similar products or seem to be of a mixed type like cluster 6.\n\nFor this reason, some more question arise:\n1. How sure is our model about our cluster assignments?\n2. How valid is the data in context of app user errors given a cluster?"
To obtain a first impression of the similarity of our clusters we can look at the correlation of the cluster center:
"### Take-away\n\n* Many clusters do not correlate or show anti-correlation. This is what we like to have as we want them do be dissimilar in the feature space. This way we can make sure that they can cover different type of products. \n* Unfortunately we also find some high correlating clusters that have nearby cluster centers like the pasta cluster 9 and the whole grain cluster 16. In this case it seems to make sense. Even though these kind of products are very similar, pasta seems to be different from whole grain. That's cool! But we should try to understand in the feature space what makes this difference."
## How dense are our clusters?
"### Take-Away\n* The clusters differ a lot in their densities\n* Cluster 5 has the highest desity, which makes sense when we look at the produkt-types - beverages.  Products of this group all have in common that they contain almost no fat, proteins and salt. Only the sugar component differs due to sweet drinks like soda and unsweetened drinks like water. For this reason the sugar component has the lowest density.\n* Cluster 6 hast the lowest density. That makes sense as cluster 6 contains the most outlier products. For this reason it's the most mixed cluster of all - the product-types differ a lot and so do their nutritions.\n* There are also some other clusters with a quite high or low density:\n    * Cluster 12 has low density in carbohydrates, sugars, non-sugar-carbohydrates, salt and the g_sum. It contains candy, fruits and sauces and is of mixed type as well. As cluster 6 it contains many outliers that causes the low density.\n    * Cluster 8 and 19 have high densities in almost every feature. If we compare with the word-clouds we can see that they are very homogenous: milk & joghurts cluster and juices cluster."
"### Take-Away\n* The clusters differ a lot in their densities\n* Cluster 5 has the highest desity, which makes sense when we look at the produkt-types - beverages.  Products of this group all have in common that they contain almost no fat, proteins and salt. Only the sugar component differs due to sweet drinks like soda and unsweetened drinks like water. For this reason the sugar component has the lowest density.\n* Cluster 6 hast the lowest density. That makes sense as cluster 6 contains the most outlier products. For this reason it's the most mixed cluster of all - the product-types differ a lot and so do their nutritions.\n* There are also some other clusters with a quite high or low density:\n    * Cluster 12 has low density in carbohydrates, sugars, non-sugar-carbohydrates, salt and the g_sum. It contains candy, fruits and sauces and is of mixed type as well. As cluster 6 it contains many outliers that causes the low density.\n    * Cluster 8 and 19 have high densities in almost every feature. If we compare with the word-clouds we can see that they are very homogenous: milk & joghurts cluster and juices cluster."
"* In the energy-densities you can see a lot of different densities as well.\n* Cluster 5 and 9 have the highest densities. Cluster 5 (beverage) also has the highest density in its other features, but cluster 9 (pasta) also has a very high density - only sugars are a bit less dense.\n* Cluster 12, 6 and 17 have the lowest densities. Cluster 12 and 6 are outlier clusters. Consequently it makes sense that they are of low density. Interestingly cluster 17 (sauces & dressings) is of low density es well. It seems to hold salt outliers, as this feature is its most less dense of all features. "
## Are there some clusters with high uncertainty?\n\nPerhaps we have some cluster with more uncertain assigments than others. And perhaps similar clusters are both more uncertain. Let's find it out!
We can't find an obvious pattern!
### Certainty 3D Scatterplot\n\nJust to obtain an impression how uncertainty of cluster assignments are looking in the feature space:
"# Anomaly detection \n\nThe last and perhaps the most important part of the app is anomaly detection. But first of all, what is meant by an anomaly in the view of a gaussian mixture model? Before we start, we should be certain about the answer. Let's recap the formulation of the Gaussian Mixture Model:\n\n$$ p(x) = \sum_{k=1}^{K} \pi_{k} \cdot N(x|\mu_{k}, \Sigma_{k})$$\n\nThis sum yields a so called probability density function. If you are not very familiar with measure theory you are in good company. If we assume that all data spots are drawn randomly from that density distribution and finally take the natural log, we would end up with the following formula:\n\n$$ \ln p(x) = \sum_{n=1}^{N} ln \sum_{k=1}^{K} \pi_{k} \cdot N(x_{n}|\mu_{k}, \Sigma_{k}) $$\n\nThe stuff we need to score each data spot is now given by:\n\n$$ \ln p_{n}(x) = \ln \sum_{k=1}^{K} \pi_{k} \cdot N(x_{n}|\mu_{k}, \Sigma_{k}) $$\n\nThis is what we obtain by calling score samples of scikit-learns gaussian mixture model. But what does it tell us? We think that it is a measure how dense the region is where data spot $x_{n}$ is located. Consequently in low dense regions of our probability density function this would yield a low number. Let's go further - what can be detected as anomaly in low dense regions?\n\n* outliers that could be errors\n* seldom products\n\nWe should keep in mind that there is one kind of error we will not detect as anomaly: Imagine a user typed in nutrition table information that contains flips between features or a wrong product name. This kind of data spot can be located in dense regions and perfectly match with some cluster. Consequently we would not detect it as an anomaly and we can't find it as an error! "
## Is the percentage of anomalies dependent on the cluster?
"We found that some clusters have a very high percentage of anomalies or are completely occupied by them. We have already seen that cluster 6 has the tendency to hold all outliers. Consequently it's not strange that it mainly consists of data spots in low dense regions. Let's try to find out, how certain the cluster assignment of anomalies is and how they look like in some example clusters."
## How certain is our model about its anomalies in cluster assignment ?
"### Take-Away\n\n* We can find some very interesting patterns: In cluster 5 (water), 6 (mixed outlier cluster), 12 (sweets & sauces) and 17 (oils and cream dressings) the model is very certain in its cluster assignments in most of the anomaly cases! \n* In contrast cluster 3 (chocolate & cookies) and 1 (potatoes and beans) the model is very uncertain (below 0.6) about the cluster assignments of the majority of its anomalies. \n* Looking at all clusters we can see that most of them spread widely in their certainties."
## How do anomalies of normal clusters look like?
"### Take-Away\n\n* By looking at the most common product names of anomalies per cluster we can see different kinds:\n    *  We can find **outlier products like blackeye peas, pumpernickel, puffed grain, products without gluten**\n    * But we can also find **seldom products** that do not fit to their clusters like **sugarfree products** in pasta cluster for example\n* Besides these outliers there are probably user errors of the data type-in input process as well. We don't know! "
## Preparing environment and uploading data\n### Import Packages
### Load Datasets
"### Total Rooms above Ground and Living Area\nFrom a previews experience with Boston data set, you probably main expect to much from the total rooms above ground, as its 'RM' feature (the average number of rooms per dwelling), but here is not the same scenario. Our common sense make to think that live area maybe has some correlation to it and probably we can combine this two features to produce a better predictor. Let's see.\n![image](https://www.housing.iastate.edu/sites/default/files/imported//images/floorplans/Frederiksen-4BR.gif)"
"As we can see, the interaction between the two features did not present a better correlation than that already seen in the living area, include it improves to 0.74 with the cut of the outliers.\n\nOn the other hand, the ***multiplication*** not only demonstrated the living area **outliers** already identified, but it still **emphasized another**. If the strategy is to ***drop the TotRmsAbvGrd***, we should also ***exclude this additional outlier***."
"### Total Basement Area Vs 1st Flor Area\nIn our country it is not common to have Basement, I think we thought it was a little spooky. So I looked a bit more ""carefully"" at this variable...\n![image](https://lparchive.org/Scooby-Doo-Mystery/Update%2002/46-Fusion_2012-08-28_01-59-20-18.png)\nI noticed that in Ames has a lot of variation, but the predictive effect is very small, so I decided to study its composition with the first floor."
"[![image](https://www.wcibasementrepair.com/wp-content/themes/wci/images/home-popupdots.png)](https://www.manta.com/cost-basement-waterproofing-ames-ia)\nSimilar to what we saw in the garage analysis, we again have a better correlation by multiplying the variables, but now we don't have a significant gain with outliers exclusion. So let's continue with the multiplication strategy and remove only the two original metrics that have high correlation with each other."
"### Year Built Vs Garage Year Built\nOf course when we buy a property the date of its construction makes a lot of difference as it can be a source of great headaches. Depending on the age and conditions there will be need for renovations and very old houses there may be cases where the garage has been built or refit after the house itself.\n![image](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcROzLQY3lcdYJimrBS7fHjLE0vhecqf1HTCfBANuDX5_5ZGBv0b)\nWell, I'd be more worried about the plumbing, the electricity, ... the garage is only for car and trunk, or is it not? Is that so? it will be?\n\nSo, let's see the graphs below, and confirm that this two features are highly correlated, but as expect is not easy to find a good substitute by iteration."
"However, by making the year of construction of the garage an indicator of whether it is newer, it becomes easiest to identify a pattern of separation. \n\nAnd more, note that we have a rising price due to the lower age. Maybe the old cars had the garage would only be for themselves...\n![image](https://www.corsia.us/wp-content/uploads/2016/05/cars-period-1909-taft-white-steam-car-1-800x533-538x218.jpg)\n..., or put it in the barn. Today we must have other more usable uses for garage, right...?\n![image](http://bonjourmini.com/wp-content/uploads/2018/05/garage-man-cave-how-to-create-a-man-cave-garage-more-best-flooring-for-garage-man-cave.jpg)"
"However, by making the year of construction of the garage an indicator of whether it is newer, it becomes easiest to identify a pattern of separation. \n\nAnd more, note that we have a rising price due to the lower age. Maybe the old cars had the garage would only be for themselves...\n![image](https://www.corsia.us/wp-content/uploads/2016/05/cars-period-1909-taft-white-steam-car-1-800x533-538x218.jpg)\n..., or put it in the barn. Today we must have other more usable uses for garage, right...?\n![image](http://bonjourmini.com/wp-content/uploads/2018/05/garage-man-cave-how-to-create-a-man-cave-garage-more-best-flooring-for-garage-man-cave.jpg)"
"But note that although we have a rising price the newer the house, the growth rate is very smooth, even with the rate gain with a newer garage. This makes sense, given that the prices of these regressors are meeting with the mean price of each year."
"### Bathrooms Features\nIt's time to take a break and go to the toilet, to our luck there are 4 bathroom variables in our data set. FullBath has the largest correlation with SalePrice between than. The others individually, these features are not very important. \n"
"![image](http://www.danlanephotography.com/wp-content/uploads/20-funny-toilet-paper-holders-funny-toilet-paper-holders.jpg)\nHowever, I assume that I if I add them up into one predictor, this predictor is likely to become a strong one. A half-bath, also known as a powder room or guest bath, has only two of the four main bathroom components-typically a toilet and sink. Consequently, I will also count the half bathrooms as half."
"As we have seen, porch features have low correlation with price, and by the graphics we see all most has low bas and high variance, being a high risk to end complex models and fall into ouverfit.\n\n### Slope of property and Lot area\nEveryone knows that the size of the lot matters, but has anyone seen any ad talking about the slope?\n![image](https://www.abedward.com/wp-content/uploads/2015/10/upside-down-house1-1024x730.jpg)"
"It is interesting to note that the slope has a low correlation, but as an expected negative. On the other hand, the lot size does not present such a significant correlation, contrary to the interaction between these two characteristics, which is better and also allow us to identify some outliers. Let's take a look at the effect of removing the outliers."
"It is interesting to note that the slope has a low correlation, but as an expected negative. On the other hand, the lot size does not present such a significant correlation, contrary to the interaction between these two characteristics, which is better and also allow us to identify some outliers. Let's take a look at the effect of removing the outliers."
### Neighborhood\nLet's watch how much the neighborhood may be influencing the price.\n![image](https://files.sharenator.com/634178883988989120_NeighborhoodWatch_FailsWins_and_Motis-s800x600-87267-1020.jpg)
### Neighborhood\nLet's watch how much the neighborhood may be influencing the price.\n![image](https://files.sharenator.com/634178883988989120_NeighborhoodWatch_FailsWins_and_Motis-s800x600-87267-1020.jpg)
"As we can see prices are affected by the neighborhood, yes, if more similar more they attract. But we will delve a little and see how the year and month of the sale also has great influence on the price variation and confirm the seasonality.\n![image](http://blogs.tallahassee.com/community/wp-content/uploads/2015/10/real-estate-seasonality-impact.gif)"
"### Test hypothesis of better feature: Construction Area\nLet's call a specialist to help us create a new feature that sum all area features, the construct area, and evaluates if is better than their parcels. \n![image](https://im.ziffdavisinternational.com/ign_fr/screenshot/default/simpsonblackboard_h9ga.jpg)"
"As we can see, our built metric performs better than its parcels, even more than the living area. Besides better correlation, it presents less bias and variance.\n\nThis may lead us to think of a model option that uses only the constructed area, without including any of the parcels, that would be replaced by an indication variable of existence or not if there is no categorical variable associated with it.\n\nWe can also use them to compose other variables and finally remove them.\n\nAnyway the **living area** seems ***useless*** now, to prove it let's go see how a single linear regressor perform with this options:"
"### Check for any correlations between features\n![image](http://flowingdata.com/wp-content/uploads/2011/07/Cancer-causes-cell-phones-625x203.png)\nTo quantify the linear relationship between the features, I will now create a correlation matrix. \n \nThe correlation matrix is identical to a covariance matrix computed from standardized data. The correlation matrix is a square matrix that contains the Pearson product-moment correlation coefficients (often abbreviated as [Pearson's r](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)), which measure the linear dependence between pairs of features:\n![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/602e9087d7a3c4de443b86c734d7434ae12890bc)\nPearson's correlation coefficient can simply be calculated as the covariance between two features x and y (numerator) divided by the product of their standard deviations (denominator):\n![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/f76ccfa7c2ed7f5b085115086107bbe25d329cec)\nThe covariance between standardized features is in fact equal to their linear correlation coefficient.\nUse NumPy's corrcoef and seaborn's heatmap functions to plot the correlation matrix array as a heat map.\n\nTo fit a linear regression model, we are interested in those features that have a high correlation with our target variable. So, I will make a zoom in these features in order of their correlation with SalePrice."
"We can see that our target variable SalePrice shows the largest correlation with the OverallQual variable (0.79), followed by.  GrLivArea (0.71). This seems to make sense, since in fact we expect the overall quality and size of the living area to have a greater influence on our value judgments about a property.\n\nFrom the graph above, it also becomes clear the multicollinearity is an issue. \n - The correlation between GarageCars and GarageArea is very high (0.88), and has very close correlation with the SalePrice. \n - From total square feet of basement area (TotalBsmtSF) and first Floor square feet (1stFlrSF), we found 0.81 of correlation and same correlation with sale price (0.61).\n - Original construction date (YearBuilt) has a little more correlation with price (0.52) than GarageYrBlt (0.49), and a high correlation between them (0.83)\n - 0.83 is the correlation between total rooms above grade not include bathrooms (TotRmsAbvGrd) and GrLivArea, but TotRmsAbvGrd has only 0.51 of correlation with sale price.\n \nLet's see their distributions and type of relation curve between the 10th features with largest correlation with sales price,"
"As you can see, we were able at first to bring most the numerical values closer to normal. Maybe you're not satisfied with the results of MiscVal and Kitchener and want to understand if we really need to continue to transform some discrete data. So, let's take a look at the QQ test of these features."
"As you have seen, really MiscVal and Kitchener really do not seem to be good results, especially MiscVal, but it is a fact that both variables do not look good indifferent to their distribution. \n\nAs for the other discrete variables, in addition to having presented significant improvements, they also pass the QQ test and present interesting distributions as we can observe in their respective graphs.\n\nSo, we can continue to apply the BoxCox on this features and leave to feature selection algorithms to decide if we continue with some of then or not."
### Evaluate Apply Polynomials by Region Plots on the more Correlated Features 
"#### Evaluating Polynomials Options Performance\nOne way to account for the violation of linearity assumption is to use a polynomial regression model by adding polynomial terms.\n![image](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQQQIQ1HTrA1PzE7sw5CwOiV3XWhKXz-rGLj7FMmxYZO_CsU1Iz)\nAlthough we can use polynomial regression to model a nonlinear relationship, it is still considered a multiple linear regression model because of the linear regression coefficients w.\n\nMoreover, as we have seen, some of our features are better when interacting with each other than with just observed ones, but some have a negative effect.\n\nSo, let's check it more carefully."
![image](http://vignette1.wikia.nocookie.net/disney/images/e/e5/Asf.gif/revision/latest?cb=20160317185039)\n\nLet's take a look at the graphs of some of the interactions of the selected features:
"As expected, we can see that prices grow with the growth of the built area, although the reform does not seem to contribute higher prices, in fact we have to remember that if a house went through renovation it is indeed old enough to have needed, and New homes tend to be more expensive. If you wish, switch from Remod to IsNew and see for yourself.\n\nSomething similar can be seen in relation to the total points segmented by the external condition, while we see that prices grow with the growth of the points, we see that although the external condition presents a small positive coefficient, the graph may be suggesting something different, but note that level 3 stands out at the beginning and around the mean, which would explain a small positive coefficient.\n\nAlso see that more important than basement conditions is its purpose in itself. Basements with living conditions present higher prices, curiously unfinished ones too, perhaps because they get the new owners to make them what they want.\n\nAs for the lot multiplied by the slope, as we already know we see the trend of price increase with lot size, but much variation, since other aspects influence the price as well as the slope itself.\n\nFinally, the total of extra points, there is nothing new when we see that the bigger the better, segmented by the format of the lot, we see that the more regular the better it is, but if the terrain is unregulated the extra high score will not work.\n\nThis is the beauty of linear models, even with many features it is possible to understand them when evaluating their coefficients, significance and graphs , as we can see how certain variables present noise and its influence on variance and bias."
"#### Sequential feature selection\n\n**Sequential feature selection algorithms** are a family of **greedy search algorithms** that are used to reduce an initial d-dimensional feature space to a k-dimensional feature subspace where k < d. The motivation behind feature selection algorithms is to automatically select a subset of features that are most relevant to the problem to improve computational efficiency or reduce the generalization error of the model by removing irrelevant features or noise, ***which can be useful for algorithms that don't support regularization***.\n\nGreedy algorithms make locally optimal choices at each stage of a combinatorial search problem and generally yield a suboptimal solution to the problem in contrast to exhaustive search algorithms, which evaluate all possible combinations and are guaranteed to find the optimal solution. However, in practice, an exhaustive search is often computationally not feasible, whereas greedy algorithms allow for a less complex, computationally more efficient solution.\n\nAs you saw in the previous topic, RFE is computationally less complex using the feature weight coefficients (e.g., linear models) or feature importance (tree-based algorithms) to eliminate features recursively, whereas SFSs eliminate (or add) features based on a user-defined classifier/regression performance metric.\n\nThe SBS aims to reduce the dimensionality of the initial feature subspace with a minimum decay in performance of the regressor or classifier to improve upon computational efficiency. In certain cases, SBS can even improve the predictive power of the model if a model suffers from overfitting.\n\nSBS sequentially removes features from the full feature subset until the new feature subspace contains the desired number of features. In order to determine which feature is to be removed at each stage, we need to define criterion function J that we want to minimize. The criterion calculated by the criterion function can simply be the difference in performance of the classifier after and before the removal of a particular feature. Then the feature to be removed at each stage can simply be defined as the feature that maximizes this criterion.\n\nSo, let's see a example of SBS in our data, "
"As you saw, the SBS is straightforward code to understand, but is computationally expensive. In a nutshell, SFAs remove or add one feature at the time based on the classifier or regressior performance until a feature subset of the desired size k is reached. There are 4 different flavors of SFAs available via the SequentialFeatureSelector from [mlxtend](https://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/):\n- Sequential Forward Selection (SFS)\n- Sequential Backward Selection (SBS)\n- Sequential Forward Floating Selection (SFFS)\n- Sequential Backward Floating Selection (SBFS)\n\nThe next code use the SBS from the mlxten. It has interest features to explore, but is more computationally expensive than previous code, so, take care if you try running it."
"As you saw, the SBS is straightforward code to understand, but is computationally expensive. In a nutshell, SFAs remove or add one feature at the time based on the classifier or regressior performance until a feature subset of the desired size k is reached. There are 4 different flavors of SFAs available via the SequentialFeatureSelector from [mlxtend](https://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/):\n- Sequential Forward Selection (SFS)\n- Sequential Backward Selection (SBS)\n- Sequential Forward Floating Selection (SFFS)\n- Sequential Backward Floating Selection (SBFS)\n\nThe next code use the SBS from the mlxten. It has interest features to explore, but is more computationally expensive than previous code, so, take care if you try running it."
"SBS is actually computationally expensive, but also generated models with better performance when we go through the hyper parameterization phase."
"#### Univariate feature selection\nOn scikit-learn we find variety of implementation oriented to regression tasks to select features according to the [k highest scores](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html), see below some of that:\n- [f_regression](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html#sklearn.feature_selection.f_classif) The Pearson's Correlation are covert to F score then to a p-value. So, the selection is based on the F-value between label/feature for regression tasks.\n- [mutual_info_regression](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html#sklearn.feature_selection.mutual_info_regression) estimate mutual information for a continuous target variable. The MI between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency. The function relies on nonparametric methods based on entropy estimation from k-nearest neighbors distances.\n![image](https://blogradiusagent.files.wordpress.com/2018/07/tenor.gif?w=770)\n\nThe methods based on F-test estimate the degree of linear dependency between two random variables. On the other hand, mutual information methods can capture any kind of statistical dependency, but being nonparametric, they require more samples for accurate estimation.\n\nOther important point is if you use sparse data, for example if we continue consider hot-encode of some categorical data with largest number of distinct values, mutual_info_regression will deal with the data without making it dense.\n\nLet's see the SelectKBest of f_regression and mutual_info_regression for our data:"
"### Select Features by Embedded Methods\nIn addition to the return of the performance itself, some models has in their internal process some step to features select that best fit their proposal, and returns the features importance too. Thus, they provide two straightforward methods for feature selection and combine the qualities' of filter and wrapper methods. \n![image](https://paulbromford.files.wordpress.com/2018/02/c88e5e569aa7b412bff3f848ec9f7c53.gif)\nSome of the most popular examples of these methods are LASSO, RIDGE, SVM, Regularized trees, Memetic algorithm, and Random multinomial logit.\n\nIn the case of Random Forest, some other models base on trees, we have two basic approaches implemented in the packages:\n1. Gini/Entropy Importance or Mean Decrease in Impurity (MDI)\n2. Permutation Importance or Mean Decrease in Accuracy \n3. Permutation with Shadow Features\n4. Gradient Boosting\n\nOthers models has concerns om **multicollinearity** problem and adding additional **constraints** or **penalty** to **regularize**. When there are multiple correlated features, as is the case with very many real life datasets, the model becomes unstable, meaning that small changes in the data can cause large changes in the model, making model interpretation very difficult on the regularization terms. \n\nThis applies to regression models like LASSO and RIDGE. In classifier cases, you can use 'SGDClassifier' where you can set the loss parameter to 'log' for Logistic Regression or 'hinge' for 'SVM'. In 'SGDClassifier' you can set the penalty to either of 'l1', 'l2' or 'elasticnet' which is a combination of both.\n\nLet's start with more details and examples:"
"### Residuals Plots\nThe plot of differences or vertical distances between the actual and predicted values. Commonly used graphical analysis for diagnosing regression models to detect nonlinearity and outliers, and to check if the errors are randomly distributed.\n![image.png](https://i1.wp.com/condor.depaul.edu/sjost/it223/documents/resid-plots.gif)\nSome points for help you in your analysis:\n- Since `Residual = Observed ‚Äì Predicted` ***positive values*** for the residual (on the y-axis) mean the ***prediction was too low***, and ***negative values*** mean the ***prediction was too high***; 0 means the guess was exactly correct.\n\n- ***They're pretty symmetrically distributed, tending to cluster towards the middle of the plot.***\n\n    For a good regression model, we would expect that the errors are randomly distributed and the residuals should be randomly scattered around the centerline. \n    \n- ***Detect outliers, which are represented by the points with a large deviation from the centerline.***\n\n    Now, you might be wondering how large a residual has to be before a data point should be flagged as being an outlier. The answer is not straightforward, since the magnitude of the residuals depends on the units of the response variable. That is, if your measurements are made in pounds, then the units of the residuals are in pounds. And, if your measurements are made in inches, then the units of the residuals are in inches. Therefore, there is no one ""rule of thumb"" that we can define to flag a residual as being exceptionally unusual.\n\n    There's a solution to this problem. We can make the residuals **unitless**by dividing them by their standard deviation. In this way we create what are called **standardized residuals**. They tell us how many standard deviations above ‚Äî if positive ‚Äî or below ‚Äî if negative ‚Äî a data point is from the estimated regression line. \n    \n- ***They're clustered around the lower single digits of the y-axis (e.g., 0.5 or 1.5, not 30 or 150).***\n\n    Again, doesn't exist a unique rule for all cases. But, recall that the empirical rule tells us that, for data that are normally distributed, 95% of the measurements fall within 2 standard deviations of the mean. Therefore, any observations with a standardized residual greater than 2 or smaller than -2 might be flagged for further investigation. It is important to note that by using this ""greater than 2, smaller than -2 rule,"" approximately 5% of the measurements in a data set will be flagged even though they are perfectly fine. It is in your best interest not to treat this rule of thumb as a cut-and-dried, believe-it-to-the-bone, hard-and fast rule! So, in most cases it may be more practical to investigate further any observations with a standardized residual greater than 3 or smaller than -3. Using the empirical rule we would expect only 0.2% of observations to fall into this category.\n    \n- ***If we see patterns in a residual plot, it means that our model is unable to capture some explanatory information.***\n\n    A special case is  any systematic (non-random) pattern. It is sufficient to suggest that the regression function is not linear. For example, if the residuals depart from 0 in some systematic manner, such as being positive for small x values, negative for medium x values, and positive again for large x values. \n    \n- ***Non-constant error variance shows up on a residuals vs. fits (or predictor) plot in any of the following ways:***\n    - The plot has a ""fanning"" effect. That is, the residuals are close to 0 for small x values and are more spread out for large x values.\n    - The plot has a ""funneling"" effect. That is, the residuals are spread out for small x values and close to 0 for large x values.\n    - Or, the spread of the residuals in the residuals vs. fits plot varies in some complex fashion."
"### Model Hiperparametrization\n#### Lasso (Least Absolute Shrinkage and Selection Operator)\n[Lasso](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) was introduced in order to improve the prediction accuracy and interpretability of regression models by include a with L1 prior as regularizer and altering the model fitting process to select only a subset of the provided covariates for use in the final model rather than using all of them. [Lasso](https://en.wikipedia.org/wiki/Lasso_(statistics)) was originally formulated for least squares models and this simple case reveals a substantial amount about the behavior of the estimator, including its relationship to ridge regression and best subset selection and the connections between Lasso coefficient estimates and so-called soft thresholding. It also reveals that the coefficient estimates need not be unique if covariates are collinear.\n\nPrior to lasso, the most widely used method for choosing which covariates to include was stepwise selection, which only improves prediction accuracy in certain cases, such as when only a few covariates have a strong relationship with the outcome. However, in other cases, it can make prediction error worse. Also, at the time, ridge regression was the most popular technique for improving prediction accuracy. Ridge regression improves prediction error by shrinking large regression coefficients in order to reduce overfitting, but it does not perform covariate selection and therefore does not help to make the model more interpretable.\n\nLasso is able to achieve both of these goals by forcing the sum of the absolute value of the regression coefficients to be less than a fixed value, which depending on the regularization strength, certain weights can become zero, which makes the Lasso also useful as a supervised feature selection technique, by effectively choosing a simpler model that does not include those coefficients. However, a limitation of the Lasso is that it selects at most n variables if m > n.\n\nThis idea is similar to ridge regression, in which the sum of the squares of the coefficients is forced to be less than a fixed value, though in the case of ridge regression, this only shrinks the size of the coefficients, it does not set any of them to zero.\n\nThe optimization objective for Lasso is: `(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1`\n\nTechnically the Lasso model is optimizing the same objective function as the Elastic Net with `l1_ratio=1.0`,  no L2 penalty.\n\nThis characteristics turn Lasso a interesting alternative approach that can lead to sparse models.\n\nFrom sklearn its most important parameters are: \n- alpha: Constant that multiplies the L1 term. Defaults to 1.0. alpha = 0 is equivalent to an ordinary least square, solved by the LinearRegression object. For numerical reasons, using alpha = 0 with the Lasso object is not advised. Given this, you should use the LinearRegression object.\n- max_iter: The maximum number of iterations\n- selection: If set to 'random', a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to 'random') often leads to significantly faster convergence especially when tol is higher than 1e-4.\n- tol: The tolerance for the optimization: if the updates are smaller than tol, the optimization code checks the dual gap for optimality and continues until it is smaller than tol.\n\nSo, I run many times my model, with different parameters, selection features, reduction or not and with and without log1P transformation of Sales Price. Below I preserve the code with the best options and with few possibilities for you can see the grid search cv in action, but I encourage you to make changes and see for yourself. "
Let's analyze the gaps in the data.
"There are no gaps in the data, great!"
There are no categorical features. Let us analyze the distribution of features.
"Normal distribution, everyone who deals with statistics loves it :)"
Let's look at the mutual relationship between features.
"Conclusions from the presented graphs: with the growth of the store area, the number of items sold increases. The obvious conclusion is that no one will use the area of ‚Äã‚Äãthe store to accommodate fewer goods)"
"Let's analyze the presence of outliers in the data. According to the unspoken rule, no more than 2% of the data should be deleted (no more than 20 lines of the dataset in this case)."
There are minor outliers in the data. We'll remove them later.
\nCLUSTER ANALYSIS
The optimal number of clusters is 4.
# 2. Importing the necessary librariesüìó 
# 3. Reading the train.csv üìö
### Network Graph
## Exploring the Patient Number Columns
### Network Graph
## Exploring the Ethinicity Column
## HeatMap for hubmap
# 6. Visualising Images : Tiff üó∫Ô∏è 
Credit goes to https://www.kaggle.com/iafoss/256x256-images/data
# 8. Plot polygon from JSON Files
## Necessary Imports
![Pytorch](https://miro.medium.com/max/1200/1*4br4WmxNo0jkcsY796jGDQ.jpeg)
"# Tables of Content:\n\n**1. [Introduction](#Introduction)** \n**2. [Univariate Distribution](#Univariate)** \n**3. [Multivariate Distribution](#Multivariate)** \n	- 3.1 Categorical Variable by Categorical Variable\n	- 3.2 Continuous Variable by Categorical Variable\n	- 3.3 Continuous Variables  on Continuous Variables\n	- 3.4 Percentage Standardize Distribution Plots\n    \n**4. [Multivariate Analysis](#Multianalysis)** \n**5. [Working with Text](#Text)** \n	- 5.1 Text Pre-Processing\n	- 5.2 Sentiment Analysis\n**5. [Sentiment Analysis](#Sentiment Analysis)** \n**6. [Word Distribution and Word Cloud](#Word Distribution and Word Cloud)** \n**7. [N Grams by Recommended Feature](#NGRAM)** \n**8. [Supervised Learning](#Supervised Learning)** \n	- 8.1 Naive Bayes\n\n# **1. Introduction:**  \nThis notebook is concerned with using the Python programming language and Natural Language Processing technology to explore trends in the customer reviews from an anonymized women‚Äôs clothing E-commerce platform, and extract actionable plans to improve its online e-commerce. The data is a collection of 22641 Rows and 10 column variables. Each row includes a written comment as well as additional customer information. This analysis will focus on using Natural Language techniques to find broad trends in the written thoughts of the customers. The total number of unique words in the dataset is 9810. \n\nMy goal is to get to understand what it is the customers appreciate and dislike about their purchases. To reach this goal, I conduct an observational study of this sizable dataset, first by understanding the characteristics of individual features, and ramping the complexity of the analysis once a proper target is envisioned. \n\n[Notebook Counterpart: In-Depth Simple Linear Regression](https://www.kaggle.com/nicapotato/in-depth-simple-linear-regression)"
"**Code Explanation and Reasoning:** \nThese packages are separated in four categories: *General, Visualization, Pre-Processing, and Modeling*.\n\nThe General category includes the basic data manipulation tools for scientific computation (`numpy`), dataframes (`pandas`), Natural Language Processing (`NLTK`), path directory manipulation (`os`), and image saving (`PIL`).\n\nThe Visualization section enables the creation of simple graphics (`matplotlib`, `seaborn`), as well as `wordcloud`'s text frequency visualization.\n\nThe Pre-Processing section extracts more specialized modules from the NLTK package such as tokenizers and stemmers to enable the preparation of text data for mathematical analysis.\n\nThe Modeling section includes `nltk`‚Äôs sentiment analysis module, which can determine the mood of text, NLTK‚Äôs N-grams, and `gensim.models`‚Äôs word2vec. It also includes `statsmodels.api` which offers an array of linear models."
Just an overview. I want to explore these numbers using visualizations.\n\n***\n\n**Age and Positive Feedback Count Distributions:**
"**Code Explanation:** \nUsing seaborn, a simple variable frequency bar/density plot is created. In the log positive feedback count plot, I had to add 0.0001 to all values so that the logarithm of previously zero values can be taken. Matplotlib's subplots function is employed through assign each plot the **AX** argument.\n\n**Distribution of Age:** \nMy a priori expectation was that the biggest group of reviewing customers would be young, tech savvy women between the age of 18 and 34. However, this plot would say otherwise, since it appears that not only is the 34 to 50 year old age most engage in reviewing products, they also appear to be the most positive reviewers, since they proportionately give higher more reviews of 5. Before making insight about these point, it would be wise to gather further data on the age distribution of shoppers. Nevertheless, this trend suggest that the core market segment for this clothing brand is women between 34 and 50. With its single peak and slight right tail, the distribution of age is more or less normal.\n\n**Distribution of Positive Feedback Count:** \nThis kind of distribution is common for network effect phenomenon, where popularity has an exponential effect on response, and most individuals receive no attention. This phenomenon is also known as the *Cumulative-Advantage Effect / Matthew Effect* or the Pareto Principle.\n\n**Cumulative-Advantage Effect / Matthew Effect:** \nCoined by Robert K. Merton in 1968, this [states that once a social agent gains a small advantage over other agents, that advantage will compound over time into an increasingly larger advantage.](http://www.thwink.org/sustain/glossary/CumulativeAdvantagePrinciple.htm) Here is the passage from the New Testament:\n\n>""For to everyone who has will more be given, and he will have abundance; but from him who has not, even what he has will be taken away.""\n> Matthew 25:29\n\nThis tendency effects any system with a positive feedback loop, which compounds. This effect turns out to be quite common among competing agents, and what we end up with, is a the Pareto Distribution.\n\n**Pareto Distribution:** \nAlso known as the 80/20 rule. Often used to describe the distribution of wealth, 20% of the population hold 80% of the wealth. I wonder how accurate this rule of thumb applies to the distribution of Positive Feedback."
"**Code Explanation:** \nUsing seaborn, a simple variable frequency bar/density plot is created. In the log positive feedback count plot, I had to add 0.0001 to all values so that the logarithm of previously zero values can be taken. Matplotlib's subplots function is employed through assign each plot the **AX** argument.\n\n**Distribution of Age:** \nMy a priori expectation was that the biggest group of reviewing customers would be young, tech savvy women between the age of 18 and 34. However, this plot would say otherwise, since it appears that not only is the 34 to 50 year old age most engage in reviewing products, they also appear to be the most positive reviewers, since they proportionately give higher more reviews of 5. Before making insight about these point, it would be wise to gather further data on the age distribution of shoppers. Nevertheless, this trend suggest that the core market segment for this clothing brand is women between 34 and 50. With its single peak and slight right tail, the distribution of age is more or less normal.\n\n**Distribution of Positive Feedback Count:** \nThis kind of distribution is common for network effect phenomenon, where popularity has an exponential effect on response, and most individuals receive no attention. This phenomenon is also known as the *Cumulative-Advantage Effect / Matthew Effect* or the Pareto Principle.\n\n**Cumulative-Advantage Effect / Matthew Effect:** \nCoined by Robert K. Merton in 1968, this [states that once a social agent gains a small advantage over other agents, that advantage will compound over time into an increasingly larger advantage.](http://www.thwink.org/sustain/glossary/CumulativeAdvantagePrinciple.htm) Here is the passage from the New Testament:\n\n>""For to everyone who has will more be given, and he will have abundance; but from him who has not, even what he has will be taken away.""\n> Matthew 25:29\n\nThis tendency effects any system with a positive feedback loop, which compounds. This effect turns out to be quite common among competing agents, and what we end up with, is a the Pareto Distribution.\n\n**Pareto Distribution:** \nAlso known as the 80/20 rule. Often used to describe the distribution of wealth, 20% of the population hold 80% of the wealth. I wonder how accurate this rule of thumb applies to the distribution of Positive Feedback."
"**Interpretation:** \nIn this case, the 80/20 rule applies pretty closely. Nevertheless do not take this rule as granted, since sometimes the proportion of inequality may be much higher! Since the Pareto Principle is a power law, it is fundamentally embedded in itself. However, notice the green vertical line, where 47% of reviews received *no* feedback at all.\n\nSince I am on the topic of inequaliy, I want to quickly touch the Gini Coefficient.\n\nNext, lets see what happens when we look at the top 20% of the top 20%..."
"**Interpretation:** \nIn this case, the 80/20 rule applies pretty closely. Nevertheless do not take this rule as granted, since sometimes the proportion of inequality may be much higher! Since the Pareto Principle is a power law, it is fundamentally embedded in itself. However, notice the green vertical line, where 47% of reviews received *no* feedback at all.\n\nSince I am on the topic of inequaliy, I want to quickly touch the Gini Coefficient.\n\nNext, lets see what happens when we look at the top 20% of the top 20%..."
"**Interpretation:** \nOh look it didn't hold up. Well think about, the compounding influence of the Network Effect as as whole accounts for the sizable portion of the population that receive nothing.\n\n***\n\n**Division Name and Department Name Distribution:**"
"**Interpretation:** \nOh look it didn't hold up. Well think about, the compounding influence of the Network Effect as as whole accounts for the sizable portion of the population that receive nothing.\n\n***\n\n**Division Name and Department Name Distribution:**"
"**Code Explanation:** \nEnumerating the loop enables the loop iteration to coincide with the matplotlib subplot ax.\n\n**Distribution of Division Name** \nThis high level feature describes had three categories: General, Petite, and Intimates. This offers some insight into the clothing sizes of the customers leaving reviews.\n\n**Distribution of Department Name** \nIt is notable to observe that *Tops and Dresses* are the most commonly reviewed products. It would be interesting to investigate the motivation of leaving a review in the first place.\n\n***\n**Distribution of Clothing ID to Understand Product Popularity**"
"**Code Explanation** \nSince they are around one thousand unique *Clothing IDs*, I used boolean operators to only select the top 60 most popular cloth items, then optimizing notebook real estate by splitting them in two plot columns.\n\n**Interpretation** \nIt appears like there are around three products that receive a small magnitude more reviews than others. I follow up on these findings by observing the descriptive statistics of the top three items. These items received an average rating of ~4.2, and an average recommendation rate of 81%. Furthermore, it appears that these products are predominately normal sized dresses.\n\nThese observations make me wonder about the nature of review popularity and rating performance. A question that could shed light on the customer's motivation to leave a review.\n\n***\n**Distribution of Class Name**"
"**Interpretation:** \nExploring the class variable suggests that the most popular clothing types are: Petite and Anthro, Dresses, Blouses, and Cut and Sew Knits. The distribution of reviews is fairly constant, suggesting that there are not negative nor positive outliers. This statement has been further verified by taking the mean of the label by class group. The results show that no class falls above .80, and the majority rest at .90. Casual bottoms and Chemises scored the highest in this criteria with a 100% positive review rate, however upon investigation this is because only 4 reviews were made in these categories.\n\n***\n**Distribution of Rating, Recommended IND, and Label:**"
"**Interpretation:** \nExploring the class variable suggests that the most popular clothing types are: Petite and Anthro, Dresses, Blouses, and Cut and Sew Knits. The distribution of reviews is fairly constant, suggesting that there are not negative nor positive outliers. This statement has been further verified by taking the mean of the label by class group. The results show that no class falls above .80, and the majority rest at .90. Casual bottoms and Chemises scored the highest in this criteria with a 100% positive review rate, however upon investigation this is because only 4 reviews were made in these categories.\n\n***\n**Distribution of Rating, Recommended IND, and Label:**"
"**Code Explanation:**\nYet another way to iterate plots, where I both loop over the index position of cat_dtypes and subplot ax at the same time with range of the length of cat_dtypes.\n\n**Distribution of Rating:** \nThe vast majority of reviews were highly positive, with a score of five out of five. This suggests that this retail store is performing fairly well, but then again, I am not familiar with the industry benchmark. Competitor reviews may be scraped and analyzed. It is important to note that these reviews are subjective, and some negative reviews may a outcome of a bad day, instead of constructive feedback. In the plot below, the Label plot is the binary classification of 1 = good, and 0= bad.\n\n**Distribution of Recommended IND:** \nThis variable mirrors the positivity of the Rating distribution, but as mentioned earlier, I believe that it provides variation of positivity which is social, rather than personal.\n\n**Distribution of Label:** \nI am surprised to see that products are rated 3 and over, than are recommended by the customer. I am eager to see the multivariate interaction between Rating and Recommended.\n\nI find these three variables especially promising in the quest of finding how customers express dislike. In the multivariate section, I shall explore the interplay between these variables.\n\n***\n**Word and Length:**"
"**Code Explanation:**\nYet another way to iterate plots, where I both loop over the index position of cat_dtypes and subplot ax at the same time with range of the length of cat_dtypes.\n\n**Distribution of Rating:** \nThe vast majority of reviews were highly positive, with a score of five out of five. This suggests that this retail store is performing fairly well, but then again, I am not familiar with the industry benchmark. Competitor reviews may be scraped and analyzed. It is important to note that these reviews are subjective, and some negative reviews may a outcome of a bad day, instead of constructive feedback. In the plot below, the Label plot is the binary classification of 1 = good, and 0= bad.\n\n**Distribution of Recommended IND:** \nThis variable mirrors the positivity of the Rating distribution, but as mentioned earlier, I believe that it provides variation of positivity which is social, rather than personal.\n\n**Distribution of Label:** \nI am surprised to see that products are rated 3 and over, than are recommended by the customer. I am eager to see the multivariate interaction between Rating and Recommended.\n\nI find these three variables especially promising in the quest of finding how customers express dislike. In the multivariate section, I shall explore the interplay between these variables.\n\n***\n**Word and Length:**"
"**Interpretation:** \n- Review Character and Word Count are highly correlated.\n- I suspect that the retailer has a maximum word limit of 500, which caused the long tail to receive compression and spike. \n\n***\n\n## 3. Multivariate Distribution \n### 3.1 Categorical Variable by Categorical Variable\nIn this section, I utilize heatmaps to visualize the percentage occurrence pivot table. Note that I heavily utilized the technique of normalizing the proportion between variables classes by converting frequency into percentages. This technique is very fruitful because the relation upon which the percentage can be explored by aggregate, by index, and by column, each of which providing its own unique insight.\n\n**Division Name by Department Name:**"
"**Interpretation:** \n- Review Character and Word Count are highly correlated.\n- I suspect that the retailer has a maximum word limit of 500, which caused the long tail to receive compression and spike. \n\n***\n\n## 3. Multivariate Distribution \n### 3.1 Categorical Variable by Categorical Variable\nIn this section, I utilize heatmaps to visualize the percentage occurrence pivot table. Note that I heavily utilized the technique of normalizing the proportion between variables classes by converting frequency into percentages. This technique is very fruitful because the relation upon which the percentage can be explored by aggregate, by index, and by column, each of which providing its own unique insight.\n\n**Division Name by Department Name:**"
"**How to Interpret:** \nFor the second heatmap on the right, the percentages occurrence is in relation to the whole.\n\n**Interpretation:** \nEvidently, the most common product is a normal sized top."
"**How to Interpret:** \nFor the second heatmap on the right, the percentages occurrence is in relation to the whole.\n\n**Interpretation:** \nEvidently, the most common product is a normal sized top."
"**How to Interpret:** \nAlthough these two heatmaps use the same features, they different in the relation in which the percentage is taken. For the first plot on the left, the percentages add up to 100% by **column**, while the plot on the right has is **normalized into percentages by row**.\n\n**Interpretation:** \nThe dominance of the *General* size is consistent across the various categories within **Department Name**. There a notable overall between *General Petite* and *Department Name*.\n\n***\n\n**Class Name by Department Name:**"
"**How to Interpret:** \nAlthough these two heatmaps use the same features, they different in the relation in which the percentage is taken. For the first plot on the left, the percentages add up to 100% by **column**, while the plot on the right has is **normalized into percentages by row**.\n\n**Interpretation:** \nThe dominance of the *General* size is consistent across the various categories within **Department Name**. There a notable overall between *General Petite* and *Department Name*.\n\n***\n\n**Class Name by Department Name:**"
"**Interpretation:** \nHere we get a closer glimpse at the breakdown of specific clothing types. Up to now, the dominance of dress popularity has been evident, but not that of ""Knits"". This is a kind of thickly thread and colorful top item which I must confess I have not seen much of out in the real world."
"**Interpretation:** \nHere we get a closer glimpse at the breakdown of specific clothing types. Up to now, the dominance of dress popularity has been evident, but not that of ""Knits"". This is a kind of thickly thread and colorful top item which I must confess I have not seen much of out in the real world."
"**Interpretation:**\nThis normalization of percentage by column and index explains how clothing types are distributed across departments. This provides a clear way to see which products are dominant within each category. Following up on knits, it appears that the runner up in the ""Tops"" category, ""Blouses"", is not that far behind.\n\n***\n**Division Name by Department Name:**"
"**Interpretation:**\nThis normalization of percentage by column and index explains how clothing types are distributed across departments. This provides a clear way to see which products are dominant within each category. Following up on knits, it appears that the runner up in the ""Tops"" category, ""Blouses"", is not that far behind.\n\n***\n**Division Name by Department Name:**"
"**Interpretation:** \nI think this plot wraps up the interplay between Blouses, Dresses, and Knits by showing that most reviews revolve around the normal sized version of the products. It is interesting to note that Dresses attract higher proportion of ""Petite"" sized customers.\n\n***\n### 3.2  Continuous Variable by Categorical Variable\n\nHere I want to look at the behavior of the continuous variables when sliced by various categorical variables. The general theme of this section is that there is no clear slicing of continuous on categorical variables that provide a clear, distinct pattern.\n\n**Positive Feedback Count Distribution by Rating, Department Name, Recommended IND, and Class Name**"
"**Interpretation:** \nI think this plot wraps up the interplay between Blouses, Dresses, and Knits by showing that most reviews revolve around the normal sized version of the products. It is interesting to note that Dresses attract higher proportion of ""Petite"" sized customers.\n\n***\n### 3.2  Continuous Variable by Categorical Variable\n\nHere I want to look at the behavior of the continuous variables when sliced by various categorical variables. The general theme of this section is that there is no clear slicing of continuous on categorical variables that provide a clear, distinct pattern.\n\n**Positive Feedback Count Distribution by Rating, Department Name, Recommended IND, and Class Name**"
"**Interpretation:** \nSince Positive Feedback Count is in log form, the higher frequency of non-recommended [0] has a bigger effect than visually suggested. The more popular reviews are not recommended, which suggest that the content is in the form of constructive criticism."
"**Interpretation:** \nThe difference is not huge, but nevertheless, a higher gini coefficient signigies higher inequality. This means that there is a bigger divergence between recommended reviews than there is between non-recommended reviews.\n\n***\n\n**Positive Feedback Count by Class Name:** "
Not much to say here. There are too many classes to include a legend.. A statistical test method would operate better at this dimensionality.\n\n***\n**Age Distribution by the Usual Suspects.. round them up**
Not much to say here. There are too many classes to include a legend.. A statistical test method would operate better at this dimensionality.\n\n***\n**Age Distribution by the Usual Suspects.. round them up**
"**Interpretation:** \nUnlike Positive Feedback Count, Age has not been transformed into a logarithm. For these reasons, slight noise between the age distribution by these features are nothing to worry about. Age doesn't seem to receive influence on these dimensions.\n\n***\n\n### 3.3 Continuous Variables  on Continuous Variables\n\nTime for some scatter plots. with [Seaborn Joint Plot](https://tryolabs.com/blog/2017/03/16/pandas-seaborn-a-guide-to-handle-visualize-data-elegantly/)."
"**Interpretation:** \nUnlike Positive Feedback Count, Age has not been transformed into a logarithm. For these reasons, slight noise between the age distribution by these features are nothing to worry about. Age doesn't seem to receive influence on these dimensions.\n\n***\n\n### 3.3 Continuous Variables  on Continuous Variables\n\nTime for some scatter plots. with [Seaborn Joint Plot](https://tryolabs.com/blog/2017/03/16/pandas-seaborn-a-guide-to-handle-visualize-data-elegantly/)."
"**How to Interpret:** \nDon't be deceived by the seemingly numerous amount of points over the Positive Feedback Count value of 0! The distribution plot up top clearly shows that most points reside at ZERO!\n\n**Interpretation:** \nThere appears to be a slight correlation between age and positive feedback count received. It would be interesting to focus on the textual anatomy of high positive feedback reviews.\n***\n\n### 3.4 Percentage Standardize Distribution Plots\n\nSince many variables are severely unbalanced, I employ standardization by percentage to see if the proportion is consistent between categorical classes. This is the same idea used previously on heatmaps now applied to barcharts!\n\n[Percentage Standardize in Seaborn - Stackoverflow](https://stackoverflow.com/questions/34615854/seaborn-countplot-with-normalized-y-axis-per-group)"
"**How to Interpret:** \nDon't be deceived by the seemingly numerous amount of points over the Positive Feedback Count value of 0! The distribution plot up top clearly shows that most points reside at ZERO!\n\n**Interpretation:** \nThere appears to be a slight correlation between age and positive feedback count received. It would be interesting to focus on the textual anatomy of high positive feedback reviews.\n***\n\n### 3.4 Percentage Standardize Distribution Plots\n\nSince many variables are severely unbalanced, I employ standardization by percentage to see if the proportion is consistent between categorical classes. This is the same idea used previously on heatmaps now applied to barcharts!\n\n[Percentage Standardize in Seaborn - Stackoverflow](https://stackoverflow.com/questions/34615854/seaborn-countplot-with-normalized-y-axis-per-group)"
"**Code Explanation:** \nMany transformation are conducted here.\n- Groupby([x])[hue]: Groups the data by the x variable, what will become the X axis of the barplot.\n- Value_counts(normalized=True): Then the hue variable, which is rowed by the x variable, is ordered by most frequent to least, and that value is converted to decimal percentage.\n- rename().mull(100): Then this is renamed to ""Percentage"", and the decimal value is multiplied by 100 to be in proper percentage units.\n\n***\n**Recommended IND by Department and Division**"
"**Code Explanation:** \nMany transformation are conducted here.\n- Groupby([x])[hue]: Groups the data by the x variable, what will become the X axis of the barplot.\n- Value_counts(normalized=True): Then the hue variable, which is rowed by the x variable, is ordered by most frequent to least, and that value is converted to decimal percentage.\n- rename().mull(100): Then this is renamed to ""Percentage"", and the decimal value is multiplied by 100 to be in proper percentage units.\n\n***\n**Recommended IND by Department and Division**"
**Interpretation:** \nThe finding here is the same as the earlier heatmap. Nothing tremendous.\n\n***\n**Rating by Department and Divison Name**
**Interpretation:** \nThe finding here is the same as the earlier heatmap. Nothing tremendous.\n\n***\n**Rating by Department and Divison Name**
**Interpretation:** \nDepartment and Divison are consistent with the overall distribution of Rating.\n\n***\n**Positive Feedback Count over 40 by Recomended IND and Rating**
**Interpretation:** \nDepartment and Divison are consistent with the overall distribution of Rating.\n\n***\n**Positive Feedback Count over 40 by Recomended IND and Rating**
"**Code Explanation:** \nWhile I have mostly built my multi-plot visualization configuration from scratch, here is an facetplot example which is less complex, but nevertheless, requires careful planning of new variables/dimensions, such as my ""Cutoff"" variable.\n\nThe red vertical line corresponds to the cutoff rule. Note that KDE likes to smooth out its tails, even though the hard cutoff would contradict this. More realistic representation would appear with a barplot, but the clutter would be too in-intelligible.\n\n**Interpretation:** \nAs a follow-up on the preview analysis on the dominant high positive feedback count rate of reviews recommended by the customer, this plot offers even more nuance.\n\nFirst finding is the bump in on the bottom left: Cutoff = True | Recommended IND = 0. Now, this plot explores un-hopeful criticism about certain products, which is why the light blue's (rating = 1) second bump dominates the ~110 positive feedback count range.\n\nThe second finding is the the bottom right plot. Here, these are popular reviews which are recommended. It it interesting to see the high spread of the yellow distribution, rating = 3. This indicates that hopeful reviews which offer constructive criticism are the most socially appreciated.\n\n***\n**Rating by Recommended IND**\n"
"**Code Explanation:** \nWhile I have mostly built my multi-plot visualization configuration from scratch, here is an facetplot example which is less complex, but nevertheless, requires careful planning of new variables/dimensions, such as my ""Cutoff"" variable.\n\nThe red vertical line corresponds to the cutoff rule. Note that KDE likes to smooth out its tails, even though the hard cutoff would contradict this. More realistic representation would appear with a barplot, but the clutter would be too in-intelligible.\n\n**Interpretation:** \nAs a follow-up on the preview analysis on the dominant high positive feedback count rate of reviews recommended by the customer, this plot offers even more nuance.\n\nFirst finding is the bump in on the bottom left: Cutoff = True | Recommended IND = 0. Now, this plot explores un-hopeful criticism about certain products, which is why the light blue's (rating = 1) second bump dominates the ~110 positive feedback count range.\n\nThe second finding is the the bottom right plot. Here, these are popular reviews which are recommended. It it interesting to see the high spread of the yellow distribution, rating = 3. This indicates that hopeful reviews which offer constructive criticism are the most socially appreciated.\n\n***\n**Rating by Recommended IND**\n"
"**Interpretation:** \nThis is a big one, which returns to my question: ""How do customers express their dislike for a Product"". There is a conflicting interest between the customers personal interaction with the product, such as the personal size fit, experience, and other personal synergies, and what the customer would invision for other customers.\n\nMy theory is that when customers give product a low rating, but nevertheless recommend the item, the customer is protesting about a personal complaint they have, such as a fit issue or customer service and product handling problem all the while still expressing admiration for the product, an approval of style worthy for the body of another.\n\nLooking at the data, it appears like five star ratings are void of non-recommendations, but low rated products are recommended a small amount of the time.\n\nThe more even occurrence between recommended and non-recommended on products with three rating is a phenomenon worth getting to the bottom of. Especially the recommended portion of the reviews, which might shed light on the biggest limitations of the retailers personal servicing, and the customers personal clothing experience.\n\n***\n\n## 4. Multivariate Analysis and Descriptive Statistics\n\n\nIn this section, I will no longer look at merely observation count by feature, but also look at how averages and other descriptive statistics behave when cut up.\n\n**Rating by Recommended IND**"
"**Interpretation:** \nThis is a big one, which returns to my question: ""How do customers express their dislike for a Product"". There is a conflicting interest between the customers personal interaction with the product, such as the personal size fit, experience, and other personal synergies, and what the customer would invision for other customers.\n\nMy theory is that when customers give product a low rating, but nevertheless recommend the item, the customer is protesting about a personal complaint they have, such as a fit issue or customer service and product handling problem all the while still expressing admiration for the product, an approval of style worthy for the body of another.\n\nLooking at the data, it appears like five star ratings are void of non-recommendations, but low rated products are recommended a small amount of the time.\n\nThe more even occurrence between recommended and non-recommended on products with three rating is a phenomenon worth getting to the bottom of. Especially the recommended portion of the reviews, which might shed light on the biggest limitations of the retailers personal servicing, and the customers personal clothing experience.\n\n***\n\n## 4. Multivariate Analysis and Descriptive Statistics\n\n\nIn this section, I will no longer look at merely observation count by feature, but also look at how averages and other descriptive statistics behave when cut up.\n\n**Rating by Recommended IND**"
"**Interpretation:** \nRating is just under max rating when recommended, and halfed when not recommended. Trend is consistent across Division and Department.\n\n***\n**Correlating Average Rating and Recommended IND by Clothing ID** \nAnalysis of data grouped by Clothing ID."
"**Interpretation:** \nRating is just under max rating when recommended, and halfed when not recommended. Trend is consistent across Division and Department.\n\n***\n**Correlating Average Rating and Recommended IND by Clothing ID** \nAnalysis of data grouped by Clothing ID."
"**How to Interpret:** \nI must stress the *Grouped By Clothing ID* aspect of this analysis. This aggregation investigates if there is trend between average rating and number of reviews by product. This is a different lense of analysis than merely running a correlation on *all customers reviews*.\n\n**Interpretation:** \nThis correlation heatmap suggest that there is in fact no correlation between count and average value, which means that the popularity of the item does not lead to differential treatment when it comes to average scoring. The age variable behaves in this same as well.\n\nHowever, There is a strong positive correlation of .80 between rating and recommended IND mean."
"**How to Interpret:** \nI must stress the *Grouped By Clothing ID* aspect of this analysis. This aggregation investigates if there is trend between average rating and number of reviews by product. This is a different lense of analysis than merely running a correlation on *all customers reviews*.\n\n**Interpretation:** \nThis correlation heatmap suggest that there is in fact no correlation between count and average value, which means that the popularity of the item does not lead to differential treatment when it comes to average scoring. The age variable behaves in this same as well.\n\nHowever, There is a strong positive correlation of .80 between rating and recommended IND mean."
"**Interpretation:** \nHere is a closer look at this correlation of interest. And look at that p-value! Someone call a publisher.\n\nJokes aside, perhaps the dots are the bottom left could be the products that unarguably need attention from the retailer, in the hope of preserving brand image."
"**Interpretation:** \nHere is a closer look at this correlation of interest. And look at that p-value! Someone call a publisher.\n\nJokes aside, perhaps the dots are the bottom left could be the products that unarguably need attention from the retailer, in the hope of preserving brand image."
"Follow-up on the previous correlation plot. This plot displays that these outliers are not very strongly represented. Indeed, the average count for the **LOW QUADRANT**, as labeled at the bottom left of the plot, is only 2.3. For these reasons, hyper negative reviews may be unrepresentative outliers, and not taken as the public's general opinion. \n\nA practise I could envision tackling this problem is to include the average rating of the product class, such as ""Dress"", in order to relieve customers who may be worried about product with low, hyper negative reviews.\n\n***\n**Correlating Average Rating and Recommended IND by Class Name** \n- [Stackoverflow Annotating Outliers](https://stackoverflow.com/questions/43010462/annotate-outliers-on-seaborn-jointplot)"
"Follow-up on the previous correlation plot. This plot displays that these outliers are not very strongly represented. Indeed, the average count for the **LOW QUADRANT**, as labeled at the bottom left of the plot, is only 2.3. For these reasons, hyper negative reviews may be unrepresentative outliers, and not taken as the public's general opinion. \n\nA practise I could envision tackling this problem is to include the average rating of the product class, such as ""Dress"", in order to relieve customers who may be worried about product with low, hyper negative reviews.\n\n***\n**Correlating Average Rating and Recommended IND by Class Name** \n- [Stackoverflow Annotating Outliers](https://stackoverflow.com/questions/43010462/annotate-outliers-on-seaborn-jointplot)"
**Interpretation:** \nFor the various Class cateogries there a notable correlation between average age and recommendation likelihood. I shall investigate.
**Interpretation:** \nFor the various Class cateogries there a notable correlation between average age and recommendation likelihood. I shall investigate.
"**Interpretation:** \nCheck out my [**Other Kernel: In-Depth Simple Linear Regression\n**](https://www.kaggle.com/nicapotato/in-depth-simple-linear-regression) for a deep dive into this regression.\n\n***\n\n## 5.  Working with Text \n\nNow that a general understanding of the variables have been laid out, I will begin to analysis the customer reviews.\n\n### 5.1 Text Pe-Processing"
"Evidently, the text data requires further processing ."
"**Code Explanation:** \nThis chunk of code creates a function that takes each review and combines them into one seamless text. It then applies lowercase, tokenizer, removes stopwords and punctuation, and finally uses the PorterStemmer.\n\n***\n\n**Interpretation:** \nIn order to process the data set's centerpiece, the review body, I utilized the NLTK package to lowercase, tokenize, and remove stopwords and punctuation. Tokenizing treats each word as its own value, while the other steps gets rid of the noise and irrelevant symbols in the data, standardizing the reviews for analysis. Upon reviewing the performance of text analysis, I decided to implement the Porter Stemmer on the tokens in order to combine words with tense and plurality deviance. I contemplated exploring the use of sequential models, such as Long Short-term memory, which would benefit from stop words, but unfortunately I could only find predictive applications of it, no insight extracting aspects. \n\nThe last piece of data transformation conducted was to bin the continuous variable age into a categorical variable: age category.\n\n***\n\n### 5.2 Sentiment Analysis \n\nMy first attempt at understanding the customer reviews is to see how the textual sentiment relates to the rating scores. With this method, it will be possible to distinguish outright positive and negative comments from the constructive variant.\n\nI will also explore the interaction between sentiment score:\n- Raiting\n- Recommended\n- Positive Feedback Count"
"#### **Code Explanation:** \n*Pre-processing* chunk loads the NLTK Sentiment Intensity Analyzer module, selects desired variables, and finally applies lowercasing to the column of reviews in the dataframe. The second paragraph of code *Applying Model and Variable Creation* classifies each review in the dataset on three dimensions: Positive, Neutral, and Negative. These results are stored in three respective columns. The overall sentiment is then determined and stored in the Sentiment column.\n\n- **Neutral/Negative/Positive Score:** Indicates the potency of these classes between 0 and 1. Onl\n- **Polarity Score:** Measures the difference between the Positive/Neutral/Negative values, where a positive numbers closer to 1 indicates overwhelming positivity, and a negative number closer to -1 indicates overwhelming negativity.\n\n***\n\n**Normalize Plots for Sentiment Distribution**"
**Interpretation:** \nRecommended is a variable that clearly indicates positive sentiment in the review.
**Interpretation:** \nRecommended is a variable that clearly indicates positive sentiment in the review.
"**Code Interpretation:** \nThe last chunk, Visualization, plots the frequency of sentiments in a bar plot using matplotlib.\n\n**Interpretation:** \nLike the distribution of rating, most reviews have a positive sentiment. Unlike the distribution of rating, there is a lower occurrence of neutral rating is lower in proportion to the occurrence of medium ranged ratings.\n\nThe plot on the bottom right tells and interesting story. The rating of positive sentiment reviews have an increasing occurrence as the rating gets higher. But, but negative and neutral sentiment reviews, the highest occurrence rating has 3 rating, further emphasizing that people's motivation of assigning a review score of three are multiple.\n\n***"
"**Code Interpretation:** \nThe last chunk, Visualization, plots the frequency of sentiments in a bar plot using matplotlib.\n\n**Interpretation:** \nLike the distribution of rating, most reviews have a positive sentiment. Unlike the distribution of rating, there is a lower occurrence of neutral rating is lower in proportion to the occurrence of medium ranged ratings.\n\nThe plot on the bottom right tells and interesting story. The rating of positive sentiment reviews have an increasing occurrence as the rating gets higher. But, but negative and neutral sentiment reviews, the highest occurrence rating has 3 rating, further emphasizing that people's motivation of assigning a review score of three are multiple.\n\n***"
"**How to Interpret:** \nIn this plot, the upper and lower rows use the same variables, but the upper row is for non-recommended reviews, while the bottom row is for recommended reviews. This enables use to explore the nature of recommended reviews in terms of the mood of the writing, as well as the rating assigned by the customer.\n\n**Interpretation:** \nWhile the distribution of departments does not seem to change depending on status of recommendation, rating is almost entirely inverted. My previous theory that recommended reviews hold more criticizing  weight does not hold up in this case since recommended reviews have a highly positive sentiment occurrence."
"**How to Interpret:** \nIn this plot, the upper and lower rows use the same variables, but the upper row is for non-recommended reviews, while the bottom row is for recommended reviews. This enables use to explore the nature of recommended reviews in terms of the mood of the writing, as well as the rating assigned by the customer.\n\n**Interpretation:** \nWhile the distribution of departments does not seem to change depending on status of recommendation, rating is almost entirely inverted. My previous theory that recommended reviews hold more criticizing  weight does not hold up in this case since recommended reviews have a highly positive sentiment occurrence."
"**Interpretation:** \nInterestingly, there appears to be a substantial negative correlation between Positive Feedback Count and Positive Score, which suggests that the most acclaimed reviews on the platform are probably in the form on constructive criticism, rather than outright positivity.\n***\n\n## 6. Word Distribution and Word Cloud  \n\n** For this section, I deviated from the book and heavily relied upon the following online resources:** \n- [Kaggle Longdoa: Word Cloud in Python](https://www.kaggle.com/longdoan/word-cloud-with-python)\n- [Word Cloud Package Forum](https://github.com/amueller/word_cloud/issues/134)\n- [Amueller Github](https://amueller.github.io/word_cloud/auto_examples/masked.html)"
"**Interpretation:** \nInterestingly, there appears to be a substantial negative correlation between Positive Feedback Count and Positive Score, which suggests that the most acclaimed reviews on the platform are probably in the form on constructive criticism, rather than outright positivity.\n***\n\n## 6. Word Distribution and Word Cloud  \n\n** For this section, I deviated from the book and heavily relied upon the following online resources:** \n- [Kaggle Longdoa: Word Cloud in Python](https://www.kaggle.com/longdoan/word-cloud-with-python)\n- [Word Cloud Package Forum](https://github.com/amueller/word_cloud/issues/134)\n- [Amueller Github](https://amueller.github.io/word_cloud/auto_examples/masked.html)"
"#### **Code Explanation:** \nThis code creates the word cloud visualization function. This function‚Äôs mathematical processes are hidden, since it does not explicitly state that it determines the frequency occurrence of each word in relation to the entire dictionary of words. Within the function, the Setting Function Parameter section creates the graphic structure using matplotlib. Then the text is formatted, and the word frequency is determined. Finally, the matplotlib structure is filled with words, where the larger the word size, the higher the word occurrence. "
### Visualize Reviews
"#### **Code Interpretation:** \nThe central flaw of these word clouds is that they only show the distribution of individual words. This removes the context of the word, as well as disregard negative prefixes. In order to solve this problem I will utilize n-grams, which increases the size of observed values from one word to multiple words, enabling frequency counts to be conducted to word sequences. Although I would have prefered to visualize these findings through the use of Word Clouds, I was unable to program this in, thus leaving me with a simple table."
**Taking a Different Lense: WordClouds by Department Name** 
"***\n## 7. N Grams by Recommended Feature\n\n\nAt this point, fit and product inconsistency strongly emerge as major topics in the reviews. From this information, I can infer that the dataset belongs to a online retailer, since brick and mortar stores have changing rooms to prevent this problem. The central themes in the product reviews brought to light by the n-grams are:\n- **Fit:** Whether the product‚Äôs advertised size actually corresponds to customer size and height.\n- **Love or Hate:** The customer's personal feelings towards the product.\n- **Complements:** The customer's social experience wearing the product.\n- **Product consistency:** Whether the product appears as advertised, lives up to quality expectations."
"**Interpretation:** \nIn the negative reviews, customers express their disappointment in the product, stating that they ‚Äúreally wanted to love‚Äù the item. This signifies that the product did not live up to the customers expectations. This occurred for multiple reasons. ‚ÄúOrder wear size‚Äù and ‚ÄúUsual wear size‚Äù suggest that the fit did not suit their typical universal body size. Perhaps if better product dimension information could be provided, then the likelihood of this negative response could decrease. Furthermore, perhaps the product platform could track the user‚Äôs size through previous purchase in order to warn customer for potential size conflict.\nAnother form of negative review is in the disappointment in the product turnout. ‚ÄúToo much fabric‚Äù and ‚ÄúLooks nothing like‚Äù suggest inconsistency with online retail presentation and actual product. These reviews are especially destructive, since they damage the reputation of the store product quality, which is a online platforms biggest asset.\nOn the other hand, positive reviews are void of criticism, and are preoccupied with confirming fit and sharing social experience with the clothing. ‚ÄúTrue Size‚Äù, ‚ÄúFit Perfectly‚Äù, ‚ÄúFit like a glove‚Äù, on top of the multiple 2-grams with customer‚Äôs height suggest that a large part of positive reviews are employed to confirm product fit according to certain size. The high occurrence of this review suggest that height and size is usually a big issue, which this retail managed to consistently satisfy.\n‚ÄúReceived many compliments‚Äù, ‚ÄúLook forward to wearing‚Äù, ‚ÄúEverytime I wear‚Äù, ‚ÄúLooks great with jeans‚Äù are all comments which reflect the customer's experience wearing the product out in public. This not only express the relevance of trendy, jaw dropping fashion for customers in a social context, but also suggests that the product review are a highly social space, in which customers not only talk with the retailer, but with the other customers as well.\n\n***\n\n## 8. Intelligible Supervised Learning\n\nSupervised learning requires features (independent variable) and a label (dependent variable).  The Formatting section does just this by creating a tuple with the comment and customer rating label. Currently the independent variable is the entire comment. However, in order to the Na√Øve Bayes Algorithm to work, each word must be treated as a variable. Instead of utilizing sequential words, the model notes which words are present out of the entire dictionary of words available in the comments corpus. In order to reduce computational intensity, only the top 5000 most common words will be considered, instead of the 9000 unique words in the corpus. The find_features function does just this by checking the presence of words for a piece of text against word_features, a variable created earlier which includes the top 5000 most common words used by customers in this dataset. The Apply Function to Data section applies the find_features function to each individual customer review using a loop, while also retaining each review‚Äôs label."
\n**Converting Text to a Model-able format: One Hot Encoding**
Image: parade.com
"Jingle Bell Rock - Let's sing along - Lyrics: \nJingle bell, jingle bell, jingle bell rock/\nJingle bells swing and jingle bells ring/\nSnowing and blowing up bushels of fun/\nNow the jingle hop has begun/\nJingle bell, jingle bell, jingle bell rock/\nJingle bells chime in jingle bell time/\nDancing and prancing in Jingle Bell Square/\nIn the frosty air/\nWhat a bright time, it's the right time/\nTo rock the night away/\nJingle bell time is a swell time/\nTo go gliding in a one-horse sleigh/\nGiddy-up jingle horse, pick up your feet/\nJingle around the clock/\nMix and a-mingle in the jingling feet/\nThat's the jingle bell rock/\nJingle bell, jingle bell, jingle bell rock/\nJingle bells chime in jingle bell time/\nDancing and prancing in Jingle Bell Square/\nIn the frosty air/\nJingle bell, jingle bell, jingle bell rock/\nJingle bells chime in jingle bell time/\nSnowing and blowing up bushels of fun/\nNow the jingle hop has begun/\nJingle bell, jingle bell, jingle bell rock/\nJingle bells chime in jingle bell time/\nDancing and prancing in Jingle Bell Square/\nIn the frosty air/\nWhat a bright time, it's the right time/\nTo rock the night away (rock the night away)/\nJingle bell time is a swell time/\nTo go gliding in a one-horse sleigh/\nGiddy-up jingle horse, pick up your feet/\nJingle around the clock/\nMix and a-mingle in the jingling feet/\nThat's the jingle bell/\nThat's the jingle bell/\nThat's the jingle bell (rock)/    https://www.youtube.com/watch?v=VfLf7A_-1Vw  113,514,564 views"
Let's do some visualization.
We can see from training set that almost all people with Age higher than 63 years didn't survive. Can use these information in modeling post processing.
"The game points are 3 for win, 1 for draw and 0 for lose and are different than the FIFA rank points that are already in the database. Also, it's supposed that FIFA Rank points and FIFA Ranking of the same team are negative correlated, and we should use only one of them to create new features. This supposition is checked below:"
"Now, we create columns that will help in the creation of the features: ranking difference, points won at the game vs. team faced rank, and goals difference in the game. All features that are not differences should be created for the two teams (away and home)."
"Due to the low values, the violin plot was not a good choice to analyze if features are really separating the data in this case. We will see then the boxplot:"
"Difference of points (full and last 5 games), difference of points by ranking faced (full and last 5 games) and difference of rank faced (full and last 5 games) are good features. Also, some of the generated features have very similar distributions which will be analyzed using scatterplots."
"Difference of points (full and last 5 games), difference of points by ranking faced (full and last 5 games) and difference of rank faced (full and last 5 games) are good features. Also, some of the generated features have very similar distributions which will be analyzed using scatterplots."
"Goals difference by ranking faced and its last 5 games version has very similar distributions. So, we will use only the full version (goals_per_ranking_dif)."
\n# **1. Importing Libraries and Packages**\nWe will use these packages to help us manipulate the data and visualize the features/labels as well as measure how well our model performed. Numpy and Pandas are helpful for manipulating the dataframe and its columns and cells. We will use matplotlib along with Seaborn to visualize our data.
"\n# **2. Loading and Viewing Data Set**\nWith Pandas, we can load both the training and testing set that we wil later use to train and test our model. Before we begin, we should take a look at our data table to see the values that we'll be working with. We can use the head and describe function to look at some sample data and statistics. We can also look at its keys and column names."
We take a look at the distribution of the Age column to see if it's skewed or symmetrical. This will help us determine what value to replace the NaN values.
"Looks like the distribution of ages is slightly skewed right. Because of this, we can fill in the null values with the median for the most accuracy. \n> **Note:** We do not want to fill with the mean because the skewed distribution means that very large values on one end will greatly impact the mean, as opposed to the median, which will only be slightly impacted."
**Gender**
> **Note:** The numbers printed above are the proportion of male/female survivors of all the surviviors ONLY. The graph shows the propotion of male/females out of ALL the passengers including those that didn't survive.
Here is one final cumulative graph of a pair plot that shows the relations between all of the different features
"\n# **5. Feature Engineering**\nBecause values in the Sex and Embarked columns are categorical values, we have to represent these strings as numerical values in order to perform our classification with our model. We can also do this process through **One-Hot-Encoding**."
The goal of this section is to gain an understanding of our data in order to inform what we do in the feature engineering section.  \n\nWe begin our exploratory data analysis by loading our standard modules.
"We then load the data, which we have downloaded from the Kaggle website ([here](https://www.kaggle.com/c/titanic/data) is a link to the data if you need it)."
\n##  üõ≥  Description of the Dataset \n\n\nThe dataset contains ten variables and 891 passenger details. Survived is the response variable for the study. \n\n\n\n\n  \n    Variable Name \n    Description\n    Type\n  \n  \n    survival\n    Did Survive the incident?\n    Categoricol\n  \n  \n    pclass \n    Class of the ticket\n    Categoricol\n  \n  \n    sex \n    Gender \n    Categoricol\n  \n  \n    Age \n    Age of the passenger\n    Numeric\n  \n  \n    sibsp \n    no of siblings / spouses aboard the Titanic \n    Numeric\n  \n  \n    parch\n    no of parents / children aboard the Titanic\n    Numeric\n  \n  \n    ticket\n    Unique ticket number\n    Categoricol\n  \n  \n    fare\n    Passenger fare \n    Numeric\n  \n  \n    cabin\n    cabin number \n    Categoricol\n  \n  \n    Embarked\n    Port of Embarkation\n    Categoricol\n  \n\n
#### üö§ Loading the ... dataset 
#### checking missing values in the training set.
 There are some missing values in the dataset. So we need to remove or impute them.
##   üìä Descriptive Analysis 
#### Univariate Analysis
#### Univariate Analysis
 The majority of the passenegers from southampton and least number of passengers from Queenstown.
 The majority of the passenegers from southampton and least number of passengers from Queenstown.
 There were 64.8% male passengers on the ship which is significantly greater than female passengers. 
 There were 64.8% male passengers on the ship which is significantly greater than female passengers. 
 Age of the passengers are positively skewed and also there were some missing values in this variable.
 The distribution of fare is also skewed. we can use log transformation if we want to noramlize these positively skewed distributions.
 The majority of the passengers had bought class 3 tickets and class 2 tickets are the less. 
 The majority of the passengers had bought class 3 tickets and class 2 tickets are the less. 
####  Bivariate analysis 
 Out of all the survivers there were many females. this may due to the fact that higher priority given to save the children and women on the ship. and also there would be a high priority to the class 1 passengers. So class of the ticket you are buying will have a significant effect to the survival of tragic incidents.
 This shows the above mentioned fact of given higher priority to the class 1 passengers. hence there are many survivars from class 1 while there are many who could not survive in class 3.
 This shows the above mentioned fact of given higher priority to the class 1 passengers. hence there are many survivars from class 1 while there are many who could not survive in class 3.
 Although there is a high percentage of survivers in who's port of embarkation is churbog the no of passengers in churbog is low compared to southampton and also this may happen due to the fact that many in churbog are class 1 passengers.
 Although there is a high percentage of survivers in who's port of embarkation is churbog the no of passengers in churbog is low compared to southampton and also this may happen due to the fact that many in churbog are class 1 passengers.
 This shows the above mentioned fact that churbog has higher number of class 1 passengers compared to other classes.
#### Numerical vs Numerical correlation
 Correlations among some variables are high but not very significant apart from the dummy variable embarked_c. Since we have done one hot encoding there is a additional variable for embarked I will remove embarked_c since those 3 levels can be described using two variables.
####  Ridge Classifier
####  logistic regression
####  logistic regression
####  linear discriminant analysis
####  linear discriminant analysis
#### decision tree classifier 
#### decision tree classifier 
#### random forest classifier 
#### random forest classifier 
#### gradient boost classifier  
#### gradient boost classifier  
#### adaboost classifier  
#### adaboost classifier  
#### extreme gradient boost classifier 
#### extreme gradient boost classifier 
#### extra tree classifier
#### extra tree classifier
#### voting classifier
#### voting classifier
#### stacking classifier
#### stacking classifier
#### catboost classifier
#### catboost classifier
##  üåå resampling with SMOTE\n\n The response variable is imbalanced hence I will use SMOTE to resample the dataset. 
#### response variable after oversampling
#### logistic regression with resampling
#### logistic regression with resampling
#### linear discriminant analysis with resampling
#### linear discriminant analysis with resampling
#### decision tree classifier with resampling
#### decision tree classifier with resampling
#### RandomForest Classifier with resampling
#### RandomForest Classifier with resampling
#### Bagging Classifier with resampling
#### Bagging Classifier with resampling
#### Gradient boosting classifier with resampling
#### Gradient boosting classifier with resampling
#### Ada boosting classifier with resampling
#### Ada boosting classifier with resampling
#### Extreme gradient boosting classifier with resampling
#### Extreme gradient boosting classifier with resampling
#### Extra tree classifier with resampling
#### Extra tree classifier with resampling
#### Voting classifier with resampling
#### Voting classifier with resampling
#### Stacking classifier with resampling
#### Stacking classifier with resampling
From the above fitted models stacking classifier has given the best classification accuracy. The performance was not increased by using SMOTE resampling. So the best model is stacking classifier without resampling with 87.15% accuracy.
### Part 3. Explaining the model
Confusion matrix is quite balanced.
Here's a [cheat sheet I made in a google sheet](https://docs.google.com/spreadsheets/d/1woVi7wq13628HJ-tN6ApaRGVZ85OdmHsDBKLAf5ylaQ/edit?usp=sharing) to help folks keep the options straight. \n\nLet's set things up and start making some distributions!
# Original Distributions
"If we put this on the same plot as the original distributions, you can't even see the earlier columns."
"The new, high-value distribution is way to the right. And here's a plot of the values."
MinMaxScaler subtracts the column mean from each value and then divides by the range.
"Notice how the shape of each distribution remains the same, but now the values are between 0 and 1."
RobustScaler subtracts the column median and divides by the interquartile range.
Let's check the minimums and maximums for each column after RobustScaler.
StandardScaler is scales each column to have 0 mean and unit variance.
"Qutie a nice chart, don't you think? You can see that all features now have 0 mean."
"# Normalizer\n\nNote that normalizer operates on the rows, not the columns. It applies l2 normalization by default."
Let's check the minimums and maximums for each column after scaling.
# Combined Plot
You can see that after any transformation the distributions are on a similar scale. Also notice that MinMaxScaler doesn't distort the distances between the values in each feature.
"## 1. Dataset\n\nFirst, we will set up our environment by importing all necessary libraries. We will also change the display settings to better show plots."
"Now, let's load the dataset that we will be using into a `DataFrame`. I have picked a dataset on video game sales and ratings from [Kaggle Datasets](https://www.kaggle.com/rush4ratio/video-game-sales-with-ratings).\nSome of the games in this dataset lack ratings; so, let's filter for only those examples that have all of their values present."
"## 2. DataFrame.plot\n\nBefore we turn to Seaborn and Plotly,  discuss the simplest and often most convenient way to visualize data from a `DataFrame`: using its own `plot()` method.\n\nAs an example, we will create a plot of video game sales by country and year. First,  keep only the columns we need. Then, we will calculate the total sales by year and call the `plot()` method on the resulting `DataFrame`."
Note that the implementation of the `plot()` method in `pandas` is based on `matplotlib`.
"Using the `kind` parameter, you can change the type of the plot to, for example, a *bar chart*. `matplotlib` is generally quite flexible for customizing plots. You can change almost everything in the chart, but you may need to dig into the [documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.plot.html) to find the corresponding parameters. For example, the parameter `rot` is responsible for the rotation angle of ticks on the x-axis (for vertical plots):"
"## 3. Seaborn\n\nNow, let's move on to the `Seaborn` library. `seaborn` is essentially a higher-level API based on the `matplotlib` library. Among other things, it differs from the latter in that it contains more adequate default settings for plotting. By adding `import seaborn as sns; sns.set()` in your code, the images of your plots will become much nicer. Also, this library contains a set of complex tools for visualization that would otherwise (i.e. when using bare `matplotlib`) require quite a large amount of code.\n\n#### pairplot()\n\nLet's take a look at the first of such complex plots, a *pairwise relationships plot*, which creates a matrix of scatter plots by default. This kind of plot helps us visualize the relationship between different variables in a single output."
"## 3. Seaborn\n\nNow, let's move on to the `Seaborn` library. `seaborn` is essentially a higher-level API based on the `matplotlib` library. Among other things, it differs from the latter in that it contains more adequate default settings for plotting. By adding `import seaborn as sns; sns.set()` in your code, the images of your plots will become much nicer. Also, this library contains a set of complex tools for visualization that would otherwise (i.e. when using bare `matplotlib`) require quite a large amount of code.\n\n#### pairplot()\n\nLet's take a look at the first of such complex plots, a *pairwise relationships plot*, which creates a matrix of scatter plots by default. This kind of plot helps us visualize the relationship between different variables in a single output."
"As you can see, the distribution histograms lie on the diagonal of the matrix. The remaining charts are scatter plots for the corresponding pairs of features.\n\n#### distplot()\n\nIt is also possible to plot a distribution of observations with `seaborn`'s `distplot()`. For example, let's look at the distribution of critics' ratings: `Critic_Score`. By default, the plot displays a histogram and the [kernel density estimate](https://en.wikipedia.org/wiki/Kernel_density_estimation)."
"As you can see, the distribution histograms lie on the diagonal of the matrix. The remaining charts are scatter plots for the corresponding pairs of features.\n\n#### distplot()\n\nIt is also possible to plot a distribution of observations with `seaborn`'s `distplot()`. For example, let's look at the distribution of critics' ratings: `Critic_Score`. By default, the plot displays a histogram and the [kernel density estimate](https://en.wikipedia.org/wiki/Kernel_density_estimation)."
"#### jointplot()\n\nTo look more closely at the relationship between two numerical variables, you can use *joint plot*, which is a cross between a scatter plot and histogram. Let's see how the `Critic_Score` and `User_Score` features are related."
"#### jointplot()\n\nTo look more closely at the relationship between two numerical variables, you can use *joint plot*, which is a cross between a scatter plot and histogram. Let's see how the `Critic_Score` and `User_Score` features are related."
#### boxplot()\n\nAnother useful type of plot is a *box plot*. Let's compare critics' ratings for the top 5 biggest gaming platforms.
#### boxplot()\n\nAnother useful type of plot is a *box plot*. Let's compare critics' ratings for the top 5 biggest gaming platforms.
"It is worth spending a bit more time to discuss how to interpret a box plot. Its components are a *box* (obviously, this is why it is called a *box plot*), the so-called *whiskers*, and a number of individual points (*outliers*).\n\nThe box by itself illustrates the interquartile spread of the distribution; its length determined by the $25\% \, (\text{Q1})$ and $75\% \, (\text{Q3})$ percentiles. The vertical line inside the box marks the median ($50\%$) of the distribution. \n\nThe whiskers are the lines extending from the box. They represent the entire scatter of data points, specifically the points that fall within the interval $(\text{Q1} - 1.5 \cdot \text{IQR}, \text{Q3} + 1.5 \cdot \text{IQR})$, where $\text{IQR} = \text{Q3} - \text{Q1}$ is the [interquartile range](https://en.wikipedia.org/wiki/Interquartile_range).\n\nOutliers that fall out of the range bounded by the whiskers are plotted individually.\n\n#### heatmap()\n\nThe last type of plot that we will cover here is a *heat map*. A heat map allows you to view the distribution of a numerical variable over two categorical ones. Let's visualize the total sales of games by genre and gaming platform."
"It is worth spending a bit more time to discuss how to interpret a box plot. Its components are a *box* (obviously, this is why it is called a *box plot*), the so-called *whiskers*, and a number of individual points (*outliers*).\n\nThe box by itself illustrates the interquartile spread of the distribution; its length determined by the $25\% \, (\text{Q1})$ and $75\% \, (\text{Q3})$ percentiles. The vertical line inside the box marks the median ($50\%$) of the distribution. \n\nThe whiskers are the lines extending from the box. They represent the entire scatter of data points, specifically the points that fall within the interval $(\text{Q1} - 1.5 \cdot \text{IQR}, \text{Q3} + 1.5 \cdot \text{IQR})$, where $\text{IQR} = \text{Q3} - \text{Q1}$ is the [interquartile range](https://en.wikipedia.org/wiki/Interquartile_range).\n\nOutliers that fall out of the range bounded by the whiskers are plotted individually.\n\n#### heatmap()\n\nThe last type of plot that we will cover here is a *heat map*. A heat map allows you to view the distribution of a numerical variable over two categorical ones. Let's visualize the total sales of games by genre and gaming platform."
"## 4. Plotly\n\nWe have examined some visualization tools based on the `matplotlib` library. However, this is not the only option for plotting in `Python`. Let's take a look at the `plotly` library. Plotly is an open-source library that allows creation of interactive plots within a Jupyter notebook without having to use Javascript.\n\nThe real beauty of interactive plots is that they provide a user interface for detailed data exploration. For example, you can see exact numerical values by mousing over points, hide uninteresting series from the visualization, zoom in onto a specific part of the plot, etc.\n\nBefore we start,  import all the necessary modules and initialize `plotly` by calling the `init_notebook_mode()` function."
"## 4. Plotly\n\nWe have examined some visualization tools based on the `matplotlib` library. However, this is not the only option for plotting in `Python`. Let's take a look at the `plotly` library. Plotly is an open-source library that allows creation of interactive plots within a Jupyter notebook without having to use Javascript.\n\nThe real beauty of interactive plots is that they provide a user interface for detailed data exploration. For example, you can see exact numerical values by mousing over points, hide uninteresting series from the visualization, zoom in onto a specific part of the plot, etc.\n\nBefore we start,  import all the necessary modules and initialize `plotly` by calling the `init_notebook_mode()` function."
"#### Line plot\n\nFirst of all,  build a *line plot* showing the number of games released and their sales by year."
"`Figure` is the main class and a work horse of visualization in `plotly`. It consists of the data (an array of lines called `traces` in this library) and the style (represented by the `layout` object). In the simplest case, you may call the `iplot` function to return only `traces`.\n\nThe `show_link` parameter toggles the visibility of the links leading to the online platform `plot.ly` in your charts. Most of the time, this functionality is not needed, so you may want to turn it off by passing `show_link=False` to prevent accidental clicks on those links."
"As an option, you can save the plot in an html file:"
"As an option, you can save the plot in an html file:"
#### Bar chart\n\nLet's use a *bar chart* to compare the market share of different gaming platforms broken down by the number of new releases and by total revenue.
#### Box plot\n\n`plotly` also supports *box plots*. Let's consider the distribution of critics' ratings by the genre of the game.
"Using `plotly`, you can also create other types of visualization. Even with default settings, the plots look quite nice. Additionally, the library makes it easy to modify various parameters: colors, fonts, captions, annotations, and so on."
From the above plot we see that gender has no direct relation to segmenting customers. That's why we can drop it and move on with other features which is why we will X parameter from now on.
"Elbow method tells us to select the cluster when there is a significant change in inertia. As we can see from the graph, we can say this may be either 3 or 5. Let's see both results in graph and decide.\n\n###  Creating the Visual Plots"
By judging from the plots we could say that 5 cluster seems better than the 3 ones. As this is a unsupervised problem we can't really know for sure which one is the best in real life but by looking at the data it's safe to say that 5 would be our choice. \n\nWe can analyze our 5 clusters in detail now:\n\n- `Label 0` is low income and low spending\n- `Label 1` is high income and high spending\n- `Label 2` is mid income and mid spending\n- `Label 3` is high income and low spending\n- `Label 4` is low income and high spending\n\nAlso let's see them more clearly with swarmplot:
"We can clearly see our clusters as we indicated before.\n\n## Hierarchical Clustering\n\n## Agglomerative\n\nWe will be looking at a clustering technique, which is Agglomerative Hierarchical Clustering. Agglomerative is the bottom up approach which is more popular than Divisive clustering.  \nWe will also be using Complete Linkage as the Linkage Criteria. \n\nThe  Agglomerative Clustering  class will require two inputs:\n\n     n_clusters: The number of clusters to form as well as the number of centroids to generate. \n     linkage: Which linkage criterion to use. The linkage criterion determines which distance to use between sets of observation. The algorithm will merge the pairs of cluster that minimize this criterion. \n     \n         Value will be: 'complete'  \n         Note: It is recommended that try everything with 'average' as well \n    \n"
"We can clearly see our clusters as we indicated before.\n\n## Hierarchical Clustering\n\n## Agglomerative\n\nWe will be looking at a clustering technique, which is Agglomerative Hierarchical Clustering. Agglomerative is the bottom up approach which is more popular than Divisive clustering.  \nWe will also be using Complete Linkage as the Linkage Criteria. \n\nThe  Agglomerative Clustering  class will require two inputs:\n\n     n_clusters: The number of clusters to form as well as the number of centroids to generate. \n     linkage: Which linkage criterion to use. The linkage criterion determines which distance to use between sets of observation. The algorithm will merge the pairs of cluster that minimize this criterion. \n     \n         Value will be: 'complete'  \n         Note: It is recommended that try everything with 'average' as well \n    \n"
"\n### Dendrogram Associated for the Agglomerative Hierarchical Clustering\nRemember that a distance matrix contains the  distance from each point to every other point of a dataset . \nWe can use the function  distance_matrix,  which requires two inputs. \nRemember that the distance values are symmetric, with a diagonal of 0's. This is one way of making sure your matrix is correct. "
"\n### Dendrogram Associated for the Agglomerative Hierarchical Clustering\nRemember that a distance matrix contains the  distance from each point to every other point of a dataset . \nWe can use the function  distance_matrix,  which requires two inputs. \nRemember that the distance values are symmetric, with a diagonal of 0's. This is one way of making sure your matrix is correct. "
"Using the  linkage  class from hierarchy, pass in the parameters:\n\n     The distance matrix \n     'complete' for complete linkage \n"
"A Hierarchical clustering is typically visualized as a dendrogram as shown in the following cell. Each merge is represented by a horizontal line. The y-coordinate of the horizontal line is the similarity of the two clusters that were merged, where cities are viewed as singleton clusters. \nBy moving up from the bottom layer to the top node, a dendrogram allows us to reconstruct the history of merges that resulted in the depicted clustering. "
"We used __complete__ linkage for our case, let's change it to __average__ linkage to see how the dendogram changes."
"We used __complete__ linkage for our case, let's change it to __average__ linkage to see how the dendogram changes."
"## Density Based Clustering (DBSCAN)\n\nMost of the traditional clustering techniques, such as k-means, hierarchical and fuzzy clustering, can be used to group data without supervision. \n\nHowever, when applied to tasks with arbitrary shape clusters, or clusters within cluster, the traditional techniques might be unable to achieve good results. That is, elements in the same cluster might not share enough similarity or the performance may be poor.\nAdditionally, Density-based Clustering locates regions of high density that are separated from one another by regions of low density. Density, in this context, is defined as the number of points within a specified radius.\n\nIn this part, the main focus will be manipulating the data and properties of DBSCAN and observing the resulting clustering.\n\n### Modeling\nDBSCAN stands for Density-Based Spatial Clustering of Applications with Noise. This technique is one of the most common clustering algorithms  which works based on density of object.\nThe whole idea is that if a particular point belongs to a cluster, it should be near to lots of other points in that cluster.\n\nIt works based on two parameters: Epsilon and Minimum Points  \n__Epsilon__ determine a specified radius that if includes enough number of points within, we call it dense area  \n__minimumSamples__ determine the minimum number of data points we want in a neighborhood to define a cluster."
"## Density Based Clustering (DBSCAN)\n\nMost of the traditional clustering techniques, such as k-means, hierarchical and fuzzy clustering, can be used to group data without supervision. \n\nHowever, when applied to tasks with arbitrary shape clusters, or clusters within cluster, the traditional techniques might be unable to achieve good results. That is, elements in the same cluster might not share enough similarity or the performance may be poor.\nAdditionally, Density-based Clustering locates regions of high density that are separated from one another by regions of low density. Density, in this context, is defined as the number of points within a specified radius.\n\nIn this part, the main focus will be manipulating the data and properties of DBSCAN and observing the resulting clustering.\n\n### Modeling\nDBSCAN stands for Density-Based Spatial Clustering of Applications with Noise. This technique is one of the most common clustering algorithms  which works based on density of object.\nThe whole idea is that if a particular point belongs to a cluster, it should be near to lots of other points in that cluster.\n\nIt works based on two parameters: Epsilon and Minimum Points  \n__Epsilon__ determine a specified radius that if includes enough number of points within, we call it dense area  \n__minimumSamples__ determine the minimum number of data points we want in a neighborhood to define a cluster."
"As we can see DBSCAN doesn't perform very well because the density in our data is not that strong. Label -1 means outliers so it will appear most as outliers. We may have performed better if we had had a bigger data.\n\n## Mean Shift Algorithm\n\nMeanShift clustering aims to discover blobs in a smooth density of samples. It is a centroid based algorithm, which works by updating candidates for centroids to be the mean of the points within a given region. These candidates are then filtered in a post-processing stage to eliminate near-duplicates to form the final set of centroids.\n\nThe algorithm automatically sets the number of clusters, instead of relying on a parameter bandwidth, which dictates the size of the region to search through. This parameter can be set manually, but can be estimated using the provided estimate_bandwidth function, which is called if the bandwidth is not set."
"As we can see DBSCAN doesn't perform very well because the density in our data is not that strong. Label -1 means outliers so it will appear most as outliers. We may have performed better if we had had a bigger data.\n\n## Mean Shift Algorithm\n\nMeanShift clustering aims to discover blobs in a smooth density of samples. It is a centroid based algorithm, which works by updating candidates for centroids to be the mean of the points within a given region. These candidates are then filtered in a post-processing stage to eliminate near-duplicates to form the final set of centroids.\n\nThe algorithm automatically sets the number of clusters, instead of relying on a parameter bandwidth, which dictates the size of the region to search through. This parameter can be set manually, but can be estimated using the provided estimate_bandwidth function, which is called if the bandwidth is not set."
## Wrap Up All in One Place\n\nLet's visualize all the algorithms we used so far and see their clustering distributions.
## Wrap Up All in One Place\n\nLet's visualize all the algorithms we used so far and see their clustering distributions.
## Conclusions\n\nThe Scikit learn official website provides the math behind the algorithms and if you're not sure which algorithm you want to use check their usage [here](https://scikit-learn.org/stable/modules/clustering.html)\n\nDon't forget to upvote if you like my kernel :)
"\n# 1. Importing the necessary libraries\n\nIncase you fork the notebook, make sure to keep the Internet in `ON` mode."
# 2. Reading the Image datasets
## Location of imaged site w.r.t gender
## Age Distribution of patients
"## Visualising Age KDEs\nSummarizing the data with Density plots to see where the mass of the data is located. [A kernel density estimate plot](https://chemicalstatistician.wordpress.com/2013/06/09/exploratory-data-analysis-kernel-density-estimation-in-r-on-ozone-pollution-data-in-new-york-and-ozonopolis/) shows the distribution of a single variable and can be thought of as a smoothed histogram (it is created by computing a kernel, usually a Gaussian, at each data point and then averaging all the individual kernels to develop a single smooth curve). We will use the seaborn kdeplot for this graph.\n\n### Distribution of Ages w.r.t Target"
\n### Distribution of Ages w.r.t gender
\n### Distribution of Ages w.r.t gender
## Distribution of Diagnosis
# 4. Visualising Images : JPEG\n\n## Visualizing a random selection of images
We do see that the JPEG format images vary in sizes
## Visualizing Images with Malignant lesions
"## Histograms\n\nHistograms are a graphical representation showing how frequently various color values occur in the image i.e frequency of pixels intensity values. In a RGB color space, pixel values range from 0 to 255 where 0 stands for black and 255 stands for white. Analysis of a histogram can help us understand thee brightness, contrast and intensity distribution of an image. Now let's look at the histogram of a random selected sample from each category.\n\n### Benign category"
"## Histograms\n\nHistograms are a graphical representation showing how frequently various color values occur in the image i.e frequency of pixels intensity values. In a RGB color space, pixel values range from 0 to 255 where 0 stands for black and 255 stands for white. Analysis of a histogram can help us understand thee brightness, contrast and intensity distribution of an image. Now let's look at the histogram of a random selected sample from each category.\n\n### Benign category"
### Malignant category
### Malignant category
"# 5 Preprocessing DIOCOM files \n[Digital Imaging and Communications in Medicine (DICOM)](https://en.wikipedia.org/wiki/DICOM) is the standard for the communication and management of medical imaging information and related data.DICOM is most commonly used for storing and transmitting medical images enabling the integration of medical imaging devices such as scanners, servers, workstations, printers, network hardware, and picture archiving and communication systems (PACS) from multiple manufacturers\n\nDICOM images have the extension dcm. A DICOM file has two parts: the header and the dataset. The header contains information on the encapsulated dataset. It consists of a File Preamble, a DICOM prefix, and the File Meta Elements.\nFortunately we have a library in Python called Pydicom which can be used to read the DIOCOM files.pydicom makes it easy to read these complex files into natural pythonic structures for easy manipulation. Modified datasets can be written again to DICOM format files.\n\nThere is very nice [kernel](https://www.kaggle.com/schlerp/getting-to-know-dicom-and-the-data) from a competition couple of years ago which serves as a great introduction to DIOCOM image files.I have borrowed the below mentioned code from there.\nKernel: https://www.kaggle.com/schlerp/getting-to-know-dicom-and-the-data\n"
# 2. Importing Libraries üìö\nüëâ Importing libraries that will be used in this notebook.
"# 3. Reading Data Set üëì\nüëâ After importing libraries, we will also import the dataset that will be used."
#### 4.2.2.1 Histogram Distribution üìâ
#### 4.2.2.2 Violin Plot üéª
#### 4.2.2.2 Violin Plot üéª
"*   The distribution of **Applicant income, Co Applicant Income, and Loan Amount** are **positively skewed** and **it has outliers** (can be seen from both histogram and violin plot).\n*   The distribution of **Loan Amount Term** is **negativly skewed** and **it has outliers.**\n\n"
### 4.3.1 Heatmap üî•
üëâ There is positive correlation between Loan Amount and Applicant Income
### 4.3.2 Categorical üìä - Categorical üìä
"üëâ Most male applicants are already married compared to female applicants. Also, the number of not married male applicants are higher compare to female applicants that had not married."
"üëâ Most male applicants are already married compared to female applicants. Also, the number of not married male applicants are higher compare to female applicants that had not married."
üëâ Most not self employed applicants have good credit compared to self employed applicants.
üëâ Most not self employed applicants have good credit compared to self employed applicants.
üëâ Most of loan that got accepted has property in Semiurban compared to Urban and Rural.
### 4.3.3 Categorical üìä- Numerical üìà
"üëâ It can be seen that there are lots of outliers in Applicant Income, and the distribution also positively skewed"
"üëâ It can be seen that there are lots of outliers in Applicant Income, and the distribution also positively skewed"
"üëâ It's clear that Co Applicant Income has a number of outliers, and the distribution is also positively skewed."
"üëâ It's clear that Co Applicant Income has a number of outliers, and the distribution is also positively skewed."
"üëâ As can be seen, Co Applicant Income has a high number of outliers, and the distribution is also positively skewed."
### 4.3.4 Numerical üìà - Numerical üìà
"*   There is **negative correlation** between Applicant income and Co Applicant Income.\n*   The correlation coefficient is **significant** at the 95 per cent confidence interval, as it has a **p-value of 1.46**\n"
## 6.2 K-Nearest Neighbour (KNN)
## 6.3 Support Vector Machine (SVM)
## 6.5 Decision Tree
## 6.6 Random Forest
## 6.6 Random Forest
## 6.7 Gradient Boosting
A visual inspection of the number of rows:
"1. Why small datasets lead to overfitting?\n\nThe goal of a machine learning model is to **generalize** patterns in training data so that you can correctly predict new data that has never been presented to the model. Overfitting occurs when a model adjusts excessively to the training data, seeing patterns that do not exist, and consequently performing poorly in predicting new data:\n\n![](https://raw.githubusercontent.com/rafjaa/curso-mineracao-de-dados-aplicada/master/img/kernel_overfitting/under_over.png)\nSource: https://medium.com/@shubhapatnim86/generalisation-training-validation-test-data-machine-learning-part-6-1de9dbb7d3d5\n\n\n\nThe fewer samples for training, the more models can fit our data. In an extreme example (a), for just one training point, any model will be able to ""explain"" it, however simple or complex the model may be. As we get to have more samples (b, c), fewer models are able to explain them:\n\n![](https://raw.githubusercontent.com/rafjaa/curso-mineracao-de-dados-aplicada/master/img/kernel_overfitting/few_samples.png)\nSource: https://towardsdatascience.com/breaking-the-curse-of-small-datasets-in-machine-learning-part-1-36f28b0c044d\n\n\n\nThat way, for a dataset with only 250 samples, we need to be very careful not to be fooled by overfitting. In this kernel we will see some tips that can help.\n\n![](https://raw.githubusercontent.com/rafjaa/curso-mineracao-de-dados-aplicada/master/img/kernel_overfitting/meme1.jpeg)"
We can plot a chart of the importances:
"In the next example, we will try to identify the most important features by successively training a model and recursively eliminating those that do not contribute to a good final solution (according to the selected model).\n\nWe will use the recursive feature elimination RFE from the scikit-learn library and the XGBClassifier model:"
5. Balance the dataset with synthetic samples (SMOTE)\n\n\n\nLet's look at the distribution of target values:
"In addition to being extremely small, our training dataset has the unbalanced target binary variable, which can undermine some models' predictability. We will perform an oversampling, which consists of creating new samples to increase the 0 minority class. For this we will use the SMOTE technique.\n\nSMOTE (Synthetic Minority Oversampling TEchnique) consists of synthesizing elements for the minority class, based on those that already exist. It works randomly picingk a point from the minority class and computing the k-nearest neighbors for this point. The synthetic points are added between the chosen point and its neighbors.\n\n![](https://raw.githubusercontent.com/rafjaa/machine_learning_fecib/master/src/static/img/smote.png)\nSource: https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets\n\n\n\nWe'll use the SMOTE implementation from the library imbalanced-learn, with the parameter ratio='minority' to resample the minority class:"
"In addition to being extremely small, our training dataset has the unbalanced target binary variable, which can undermine some models' predictability. We will perform an oversampling, which consists of creating new samples to increase the 0 minority class. For this we will use the SMOTE technique.\n\nSMOTE (Synthetic Minority Oversampling TEchnique) consists of synthesizing elements for the minority class, based on those that already exist. It works randomly picingk a point from the minority class and computing the k-nearest neighbors for this point. The synthetic points are added between the chosen point and its neighbors.\n\n![](https://raw.githubusercontent.com/rafjaa/machine_learning_fecib/master/src/static/img/smote.png)\nSource: https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets\n\n\n\nWe'll use the SMOTE implementation from the library imbalanced-learn, with the parameter ratio='minority' to resample the minority class:"
"6. Combine models for the final submission\n\nCombine the prediction of several models or the same model with different values of hyperparameters reduces variance and enhances generalization.\n\n![](https://raw.githubusercontent.com/rafjaa/curso-mineracao-de-dados-aplicada/master/img/kernel_overfitting/combine.jpg)\nSource: https://medium.com/rants-on-machine-learning/what-to-do-with-small-data-d253254d1a89\n\n\n\nOften, combining weak models that are poorly correlated with each other can lead to superior results than a strong individual model. There are several ways to do this. The simplest is to perform a weighted average of the various predictions:"
### ‚¨áÔ∏èüòâ Custom Functions Below 
# 2. The Data\n\n> Let's observe the structure of the data first:\n\n
"So now, our distribution would look like this:"
"Ok, now let's look at the `standard_error` in terms of segmentation.\n\nYou can observe that indeed the error decreases a little for medium complexity. However, we can state that usually **we'll encounter a `standard_error` of 0.5 on a normal rating**."
"**We can also look at feature importance**, to see which features (out of the ones we've already created) are the most important. This way, we can choose afterwards which one to choose when creating the more complex models (more details are coming in my second notebook).\n\n\nüìå Note:We can see that freq_sum, freq_min and freq_mean are the most important features, although we have more than 11,700 columns for the words in our texts. It means that the word_frequencies dataset is actually helpful!\n"
"\nüìå Yay! This is a big improvement! The RMSE dropped from a value of 2.31 to around 0.82! I would call this a win, especially because we didn't really do much to our dataset.\n\n\n\n\n\n\n# ‚å®Ô∏èüé® My Specs\n\n* **Z8 G4** Workstation üñ•\n* 2 CPUs & 96GB Memory üíæ\n* NVIDIA **Quadro RTX 8000** üéÆ\n* **RAPIDS** version 0.17 üèÉüèæ‚Äç‚ôÄÔ∏è\n\n\n> üìå **Leaderboard**: And the leaderboard score for the XGBRF Model using Repeated Folds is **0.93** (if you have any questions on how to submit, don't hesitate to ask - don't forget to name your submission `submission.csv` so you won't get an error!)\n"
You can see how important rank features are in this competition.\n\n## 3. Feature importances of Tree models\n\nTree models can output feature importances.
"There are several options for measuring importance like ""split"" (How many times the feature is used to split), ""gain"" (The average training loss reduction gained when using a feature for splitting).\n\nHowever, sometimes it doesn't represent the actual contribution.\n\n\n\n> To our dismay we see that the feature importance orderings are very different for each of the three options provided by XGBoost! For the cover method it seems like the capital gain feature is most predictive of income, while for the gain method the relationship status feature dominates all the others. This should make us very uncomfortable about relying on these measures for reporting feature importance without knowing which method is best.\n\n\n    Interpretable Machine Learning with XGBoost\n\n\nThis is the background of Interpretable machine learning which is a field receiving a lot of attention recently. You can find papers and libraries here: [lopusz/awesome-interpretable-machine-learning](https://github.com/lopusz/awesome-interpretable-machine-learning)"
"# **W & B Artifacts**\n\nAn artifact as a versioned folder of data.Entire datasets can be directly stored as artifacts .\n\nW&B Artifacts are used for dataset versioning, model versioning . They are also used for tracking dependencies and results across machine learning pipelines.Artifact references can be used to point to data in other systems like S3, GCP, or your own system.\n\nYou can learn more about W&B artifacts [here](https://docs.wandb.ai/guides/artifacts)\n\n![](https://drive.google.com/uc?id=1JYSaIMXuEVBheP15xxuaex-32yzxgglV)"
Snapshot of the artifacts created  \n\n![](https://drive.google.com/uc?id=16ROHOYdW3ewGESfCwewUWW8X3mvNbFKT)
Logging plots to W&B dashboard
# **Frequency Distribution of Categorical Features**\n\n
# **Distribution of Target Variable - Pressure**
# **Numerical Variables Vs Target**
# **Numerical Variables Vs Target**
# **Categorical Variables Vs Target**
# **Categorical Variables Vs Target**
# **Analysis for single breath_id**
"On closer inspection, we can see that the `Rainfall`, `Evaporation`, `WindSpeed9am` and `WindSpeed3pm` columns may contain outliers.\n\n\nI will draw boxplots to visualise outliers in the above variables. "
The above boxplots confirm that there are lot of outliers in these variables.
"### Check the distribution of variables\n\n\nNow, I will plot the histograms to check distributions to find out if they are normal or skewed. If the variable follows normal distribution, then I will do `Extreme Value Analysis` otherwise if they are skewed, I will find IQR (Interquantile range)."
"We can see that all the four variables are skewed. So, I will use interquantile range to find outliers."
"The confusion matrix shows `20892 + 3285 = 24177 correct predictions` and `3087 + 1175 = 4262 incorrect predictions`.\n\n\nIn this case, we have\n\n\n- `True Positives` (Actual Positive:1 and Predict Positive:1) - 20892\n\n\n- `True Negatives` (Actual Negative:0 and Predict Negative:0) - 3285\n\n\n- `False Positives` (Actual Negative:0 but Predict Positive:1) - 1175 `(Type I error)`\n\n\n- `False Negatives` (Actual Positive:1 but Predict Negative:0) - 3087 `(Type II error)`"
# **16. Classification metrices** \n\n\n[Table of Contents](#0.1)
"# **18. ROC - AUC** \n\n\n[Table of Contents](#0.1)\n\n\n\n## ROC Curve\n\n\nAnother tool to measure the classification model performance visually is **ROC Curve**. ROC Curve stands for **Receiver Operating Characteristic Curve**. An **ROC Curve** is a plot which shows the performance of a classification model at various \nclassification threshold levels. \n\n\n\nThe **ROC Curve** plots the **True Positive Rate (TPR)** against the **False Positive Rate (FPR)** at various threshold levels.\n\n\n\n**True Positive Rate (TPR)** is also called **Recall**. It is defined as the ratio of `TP to (TP + FN).`\n\n\n\n**False Positive Rate (FPR)** is defined as the ratio of `FP to (FP + TN).`\n\n\n\n\nIn the ROC Curve, we will focus on the TPR (True Positive Rate) and FPR (False Positive Rate) of a single point. This will give us the general performance of the ROC curve which consists of the TPR and FPR at various threshold levels. So, an ROC Curve plots TPR vs FPR at different classification threshold levels. If we lower the threshold levels, it may result in more items being classified as positve. It will increase both True Positives (TP) and False Positives (FP).\n\n"
ROC curve help us to choose a threshold level that balances sensitivity and specificity for a particular context.
Look at a random face image:
## The dataset and data loaders
"To plot the image, we need to unnormalize it and also permute it from (3, 224, 224) to (224, 224, 3). "
"To use the PyTorch data loader, we need to create a `Dataset` object.\n\nBecause of the class imbalance (many more fakes than real videos), we're using a dataset that samples a given number of REAL faces and the same number of FAKE faces, so it's always 50-50."
"Simple training loop. I prefer to write those myself from scratch each time, because then you can tweak it to do whatever you like."
## The model
We see that in the training data only around 38.4% of the passengers managed to survive the disaster: this is an important value that we have to keep in mind.\n# Feature analysis and creation\nThe goal of this section is to gain a general understanding of our data to perform a more precise feature selection in the modeling part.  \nWe will thus explore one feature at a time in order to determine its importance in predicting if a passenger survived or not.\n## Sex\nWe see that around 65% of the passengers were male while the remaining 35% were female.  \nThe important thing to notice here is that the survival rate for women was four times the survival rate for men and this makes `Sex` one of the most informative features.  \nIt is not a case that the gender submission on its own scores 0.76555!
"## Pclass\nThere were three classes on the ship and from the plot we see that the number of passengers in the third class was higher than the number of passengers in the first and second classes combined.  \nHowever, the survival rate by class is not the same: more than 60% of first-class passengers and around half of the second class passengers were rescued, whereas 75% of third class passengers were not able to survive the disaster.  \nFor this reason, this is definitely an important aspect to consider."
"## Age\nDespite this column contains a lot of missing values, we see that in the training data the average age was just under 30 years.  \nHere is the plot of the age distribution in general compared to the one for the survivors and the deads."
"At a first look, the relationship between `Age` and `Survived` appears not to be very clear: we notice for sure that there is a peak corresponding to young passengers for those who survived, but apart from that the rest is not very informative.  \nWe can appreciate this feature more if we consider `Sex` too: now it is clearer that a good number of male survivors had less than 12 years, while the female group has no particular properties."
"At a first look, the relationship between `Age` and `Survived` appears not to be very clear: we notice for sure that there is a peak corresponding to young passengers for those who survived, but apart from that the rest is not very informative.  \nWe can appreciate this feature more if we consider `Sex` too: now it is clearer that a good number of male survivors had less than 12 years, while the female group has no particular properties."
"Another interesting thing to look at is the relation between `Age`, `Pclass` and `Survived`.  \nWe see the influence of `Pclass` is the important one as there are no super clear horizontal patterns.  \nAlso, we note that there were not many children in the first class."
"Another interesting thing to look at is the relation between `Age`, `Pclass` and `Survived`.  \nWe see the influence of `Pclass` is the important one as there are no super clear horizontal patterns.  \nAlso, we note that there were not many children in the first class."
"After all these plots I am not sure about the importance of `Age` in a model: I guess we will see later, even though I am thinking of not using it.\n## Fare\nFrom the description, we see that the `Fare` distribution is positively skewed, with 75% of data under 31 and a maximum of 512.  \nJust to understand better this feature, the simplest idea here could be creating fare ranges using quartiles.  \nAt a first look, we notice that the higher the fare, the higher the possibility of surviving."
"However, when it came down to modeling, these fare categories did not help at all as they underfit quite substantially.  \nLooking at the more detailed plot below, we also see for example that all males with fare between 200 and 300 died: that is not what we would expect.  \nFor this reason, I left the `Fare` feature as it is in order to prevent losing too much information: at deeper levels of a tree, a more discriminant relationship might open up and it could become a good group detector."
"After seeing Erik's kernel [here](https://www.kaggle.com/erikbruin/titanic-2nd-degree-families-and-majority-voting), it reminded me I had not analyzed this feature deep enough.  \nWhen I printed the description, I should have also noticed that the minimum value for `Fare` is zero and that is a bit strange.  \nIs this information correct? Let's see who these passengers are."
"## Embarked \n`Embarked` tells us where a passenger boarded from.  \nThere are three possible values for it: Southampton, Cherbourg and Queenstown.  \nIn the training data, more than 70% of the people boarded from Southampton, slightly under 20% from Cherbourg and the rest from Queenstown.  \nCounting survivors by boarding point, we see that more people who embarked from Cherbourg survived than those who died."
"Since we don't expect that a passenger's boarding point could change the chance of surviving, we guess this is probably due to the higher proportion of first and second class passengers for those who came from Cherbourg rather than Queenstown and Southampton.  \nTo check this, we see the class distribution for the different embarking points."
"Since we don't expect that a passenger's boarding point could change the chance of surviving, we guess this is probably due to the higher proportion of first and second class passengers for those who came from Cherbourg rather than Queenstown and Southampton.  \nTo check this, we see the class distribution for the different embarking points."
"The claim is correct and hopefully justifies why that survival rate is so high.  \nAgain this feature might be useful in detecting groups at a deeper level of a tree and this is the only reason why I keep it.\n## Name\nThe `Name` column contains useful information as for example we could identify family groups using surnames.  \nIn this notebook, however, I extracted only the passengers' title from it, creating a new feature for both train and test data."
"This should help our model a little, so I think we are fine here.\n## SibSp\n`SibSp` is the number of siblings or spouses of a person aboard the Titanic.  \nWe see that more than 90% of people traveled alone or with one sibling or spouse.  \nThe survival rate between the different categories is a bit confusing but we see that the chances of surviving are lower for those who traveled alone or with more than 2 siblings.  \nFurthermore, we notice that no one from a big family with 5 or 8 siblings was able to survive."
"## Parch\nSimilar to the `SibSp` column, this feature contains the number of parents or children each passenger was traveling with.  \nHere we draw the same conclusions as `SibSp`: we see again that small families had more chances to survive than bigger ones and passengers who traveled alone."
"## Parch\nSimilar to the `SibSp` column, this feature contains the number of parents or children each passenger was traveling with.  \nHere we draw the same conclusions as `SibSp`: we see again that small families had more chances to survive than bigger ones and passengers who traveled alone."
"## Family type\nSince we have two seemingly weak predictors, one thing we can do is combine them to get a stronger one.  \nIn the case of `SibSp` and `Parch`, we can join the two variables to get a family size feature, which is the sum of `SibSp`, `Parch` and 1 (who is the passenger himself). "
"Plotting the survival rate by family size it is clear that people who were alone had a lower chance of surviving than families up to 4 components, while the survival rate drops for bigger families and ultimately becomes zero for very large ones."
"To further summarize the previous trend, as my final feature I created four groups for family size."
Here is the final result: I think we discovered a nice pattern.
"After all these considerations it is finally time to put everything together in a simple and quite efficient model.\n# Modeling\nWe start by selecting the features we will use and isolating the target.  \nAs I said, I will not consider `Cabin` and in the end, I also excluded `Age` as the relevant information which is being a young man is encoded in the Master title.  \nI also did not use `Sex` as it is not useful given the `Title` column: adult males and young children have the same sex but are really different categories as we saw before, so we don't want to confuse our algorithm.  \nIf you don't extract the `Title` column, remember to put `Sex` in your models as it is pretty important!"
Plot statistics for number of kernels:
### 2. How does number of views and number of comments affect number of votes?
"Find out correlation between number of views, number of comments and number of votes:"
"We can see that votes, comments and views are __highly correlated__. So my first assumption would be that __we should get as many views of the kernels as possible to gain votes__.\n Let's try to visualize dependency between views and votes:"
"At first let's look at correlations between TotalKernelVotes, TotalDatasetVotes and	TotalDatasetDownloads."
As we can see there is no correlation between number of kernel votes and number of votes or downloads for a dataset.\n Let's also make scatter plots:
Let's check the correlation coefficient:
Find out the correlation between the medal rank and the number of mentions of the kernel:
Find out the correlation between the medal rank and the number of mentions of the kernel:
"Finally, some numbers considering the number of mentions of the kernels on Kaggle forum:"
Let's see the feature importances:
"We can explain some of the results:\n* __Kernel version number__: this feature may be related to the kernel quality, the kernel with lot's of versions may be updated and improved,\n* __Forum topic__: the importance of this feature indicates that a forum topic helps to promote the kernel,\n* __Related competition__: we can see that the fact that there is a competition related to a kernel plays a vital role in the number of votes. Probably related competition helps to promote the kernel among the users.\n* __Author performance tier__: this feature is essential because more experienced authors create better kernels and have more followers."
The survival probability visualization for each pclass using the above method is as follows.
## Awesome Heatmap\n\n- A **heat map (or heatmap)** is a data visualization technique that shows magnitude of a phenomenon as color in two dimensions.\n\n---\n\n### Simple Explanation\n\n- (tip) `mask` (remove symetric)\n- (tip) `square` (to make x-y scale same)\n- (tip) `colormap (diverging colormap)\n- (tip) text as watermark\n
# 8. Use interactive plots   [‚Üë](#top)\n\nInteractive plots are way better than static plots. [Plotly](https://plotly.com/) provides a huge set of interactive plots that can make your presentation much more appealing.
You could also use [matplotlib](https://matplotlib.org/) but the `%matplotlib notebook` magic is not working in Kaggle.
You could also use [matplotlib](https://matplotlib.org/) but the `%matplotlib notebook` magic is not working in Kaggle.
# 9. Use CSS styling inside HTML  [‚Üë](#top)\n\nEvery markdown supports HTML and thus you can customize your cells using CSS. Look around my `style` inside the next markdown to get a reference. You can research a little more about HTML and CSS to improve your markdowns.
"Let's get to know our data by performing a preliminary data analysis.\n\n#  Part 1. Preliminary data analysis\n\nFirst, we will initialize the environment:"
"You will use the `seaborn` library for visual analysis, so let's set that up too:"
"You will use the `seaborn` library for visual analysis, so let's set that up too:"
"To make it simple, we will work only with the training part of the dataset:"
It would be instructive to peek into the values of our variables.\n \nLet's convert the data into *long* format and depict the value counts of the categorical features using [`factorplot()`](https://seaborn.pydata.org/generated/seaborn.factorplot.html).
We can see that the target classes are balanced. That's great!\n\nLet's split the dataset by target values. Can you already spot the most significant feature by just looking at the plot?
We can see that the target classes are balanced. That's great!\n\nLet's split the dataset by target values. Can you already spot the most significant feature by just looking at the plot?
"You can see that the distribution of cholesterol and glucose levels great differs by the value of the target variable. Is this a coincidence?\n\nNow, let's calculate some statistics for the feature unique values:"
"For EDA on image datasets I think one should at least examine the label distribution, the images before preprocessing and the images after preprocessing. Through examining these three aspects we can get a good sense of the problem. Note that the distribution on the test set can still vary wildly from the training data."
We will visualize a random image from every label to get a general sense of the distinctive features that seperate the classes. We will take this into account and try to enhance these features in our preprocessing. For these images there some to be increasingly more spots and stains on the retina as diabetic retinopathy worsens.
We will visualize a random image from every label to get a general sense of the distinctive features that seperate the classes. We will take this into account and try to enhance these features in our preprocessing. For these images there some to be increasingly more spots and stains on the retina as diabetic retinopathy worsens.
## Preprocessing 
"# Introduction\n\nAutocorrelation analysis is an important step in the Exploratory Data Analysis (EDA) of time series. **The autocorrelation analysis helps in detecting hidden patterns and seasonality and in checking for randomness.**\nIt is especially important when you intend to use an ARIMA model for forecasting because the autocorrelation analysis helps to identify the AR and MA parameters for the ARIMA model.\n\n**Overview**\n* [Fundamentals](#Fundamentals)\n    * [Auto-Regressive and Moving Average Models](#[Auto-Regressive-and-Moving-Average-Models])\n    * [Stationarity](#Stationarity)\n    * [Autocorrelation Function and Partial Autocorrelation Function](#Autocorrelation-Function-and-Partial-Autocorrelation-Function)\n    * [Order of AR, MA, and ARMA Model](#Order-of-AR-MA-and-ARMA-Model)\n* [Examples](#Examples)\n    * [AR(1) Process](#AR(1%29-Process)\n    * [AR(2) Process](#AR(2%29-Process)\n    * [MA(1) Process](#MA(1%29-Process)\n    * [MA(2) Process](#MA(2%29-Process)\n    * [Periodical](#Periodical)\n    * [Trend](#Trend)\n    * [White Noise](#White-Noise)\n    * [Random-Walk](#Random-Walk)\n    * [Constant](#Constant)\n* [üöÄ Cheat Sheet](#üöÄ-Cheat-Sheet)\n* [Case Study](#Case-Study)\n    * [Bitcoin](#Bitcoin)  \n    * [Ethereum](#Ethereum) \n    * [Discussion on Random-Walk](#Discussion-on-Random-Walk) \n    \nIf you need some introduction to or a refresher on the ACF and PACF, I recommend the following video:\n"
"# Fundamentals\n\n## Auto-Regressive and Moving Average Models\n\n### Auto-Regressive (AR) Model\n\n$\hat{y}_t = \alpha_1 y_{t-1} + \dots + {\alpha_p}y_{t-p}$\n\nThe AR model assumes that the current value ($y_t$) is **dependent on previous values** ($y_{t-1}, y_{t-2}, y_{t-3},...$). Because of this assumption, we can build a **linear** regression model.\n\nTo figure out the order of an AR model, you would use the **PACF**.\n\n### Moving Average (MA) Model\n\n$\hat{y}_t = \epsilon_t + \beta_1 \epsilon_{t-1} + \dots + \beta_{q} \epsilon_{t-q}$\n\nThe MA model assumes that the current value ($y_t$) is **dependent on the error terms** including the current error ($\epsilon_{t}, \epsilon_{t-1}, \epsilon_{t-2}, \epsilon_{t-3},...$). Because error terms are random, there is **no linear** relationship between the current value and the error terms.\n\nTo figure out the order of an MA model, you would use the **ACF**.\n\n## Stationarity\n\nACF and PACF assume stationarity of the underlying time series.\nStaionarity can be checked by performing an **Augmented Dickey-Fuller (ADF) test**:\n\n> - p-value > 0.05: Fail to reject the null hypothesis (H0), the data has a unit root and is non-stationary.\n> - p-value <= 0.05: Reject the null hypothesis (H0), the data does not have a unit root and is stationary.\n>\n> [...] We can see that our [ADF] statistic value [...] is less than the value [...] at 1%.\nThis suggests that we can reject the null hypothesis with a significance level of less than 1% (i.e. a low probability that the result is a statistical fluke).\nRejecting the null hypothesis means that the process has no unit root, and in turn that the time series is stationary or does not have time-dependent structure. - [Machine Learning Mastery: How to Check if Time Series Data is Stationary with Python](https://machinelearningmastery.com/time-series-data-stationary-python/)\n\nIf the time series is stationary, continue to the next steps.\n**If the time series is not stationary, try differencing the time series** and check its stationarity again.\n"
"### 2. Check ACF and PACF\n\nWe can make the following observations:\n- There are several autocorrelations that are significantly non-zero. Therefore, the time series is non-random.\n- High degree of autocorrelation between adjacent (lag = 1)\n\n| | AR($p$) | MA($q$) | ARMA($p$, $q$) |\n|-|-|-|-|\n|ACF|Tails off (Geometric decay) |Significant at lag $q$ / Cuts off after lag $q$  |Tails off (Geometric decay) |\n|PACF|  Significant at each lag $p$ / Cuts off after lag $p$ |Tails off (Geometric decay) |Tails off (Geometric decay) |\n\n-> We can use an **AR(1) model** to model this process.\n\nSo that for AR(1), we would model the AR(p) formula\n$\hat{y}_t = \alpha_1 y_{t-1} + \dots + {\alpha_p}y_{t-p}$\nto the following:\n\n$\hat{y}_t = \alpha_1 y_{t-1}$\n"
### 3. Modelling
### 3. Modelling
"As you can see, the AR(1) model fits an $\alpha_1 = 0.4710$, which is quite close to the `alpha_1 = 0.5` which we have set. However, the predicted values seem to be quite off in this case."
"### 2. Check ACF and PACF\n\nWe can make the following observations:\n- There are several autocorrelations that are significantly non-zero. Therefore, the time series is non-random.\n- High degree of autocorrelation between adjacent (lag = 1) and near-adjacent (lag = 2) observations\n\n| | AR($p$) | MA($q$) | ARMA($p$, $q$) |\n|-|-|-|-|\n|ACF|Tails off (Geometric decay) |Significant at lag $q$ / Cuts off after lag $q$  |Tails off (Geometric decay) |\n|PACF|  Significant at each lag $p$ / Cuts off after lag $p$ |Tails off (Geometric decay) |Tails off (Geometric decay) |\n\n-> We can use an **AR(2) model** to model this process.\n\nSo that for AR(2), we would model the AR(p) formula\n$\hat{y}_t = \alpha_1 y_{t-1} + \dots + {\alpha_p}y_{t-p}$\nto the following:\n\n$\hat{y}_t = \alpha_1 y_{t-1} + \alpha_2 y_{t-2} $\n"
### 3. Modelling
### 3. Modelling
"As you can see, the AR(2) model fits $\alpha_1 = 0.5191$ and $\alpha_2 = -0.5855$, which is quite close to the `alpha_1 = 0.5` and `alpha_2 = -0.5` which we have set. However, the predicted values seem to be quite off as well in this case - similarly to the AR(1) case."
"### 2. Check ACF and PACF\n\nWe can make the following observations:\n- There are several autocorrelations that are significantly non-zero. Therefore, the time series is non-random.\n- High degree of autocorrelation between adjacent (lag = 1)\n\n| | AR($p$) | MA($q$) | ARMA($p$, $q$) |\n|-|-|-|-|\n|ACF|Tails off (Geometric decay) |  Significant at lag $q$ / Cuts off after lag $q$   |Tails off (Geometric decay) |\n|PACF| Significant at each lag $p$ / Cuts off after lag $p$ |Tails off (Geometric decay) |Tails off (Geometric decay) |\n\n-> We can use an **MA(1) model** to model this process.\n\nSo that for MA(1), we would model the MA(q) formula\n$\hat{y}_t = \epsilon_t + \beta_1 \epsilon_{t-1} + \dots + \beta_{q} \epsilon_{t-q}$\nto the following:\n\n$\hat{y}_t = \epsilon_t + \beta_1 \epsilon_{t-1}$\n"
### 3. Modelling
### 3. Modelling
"As you can see, the MA(1) model fits $\beta_1 = 0.5172$, which is quite close to the `beta_1 = 0.5`. However, the predicted values seem to be quite off as well in this case - similarly to the AR(p) cases."
"### 2. Check ACF and PACF\n\nWe can make the following observations:\n- There are several autocorrelations that are significantly non-zero. Therefore, the time series is non-random.\n- High degree of autocorrelation between adjacent (lag = 1) and near-adjacent (lag = 2) observations\n\n| | AR($p$) | MA($q$) | ARMA($p$, $q$) |\n|-|-|-|-|\n|ACF|Tails off (Geometric decay) |  Significant at lag $q$ / Cuts off after lag $q$   |Tails off (Geometric decay) |\n|PACF| Significant at each lag $p$ / Cuts off after lag $p$ |Tails off (Geometric decay) |Tails off (Geometric decay) |\n\n-> We can use an **MA(2) model** to model this process.\n\nSo that for MA(2), we would model the MA(q) formula\n$\hat{y}_t = \epsilon_t + \beta_1 \epsilon_{t-1} + \dots + \beta_{q} \epsilon_{t-q}$\nto the following:\n\n$\hat{y}_t = \epsilon_t + \beta_1 \epsilon_{t-1} + \beta_2 \epsilon_{t-2}$"
### 3. Modelling
### 3. Modelling
"As you can see, the MA(2) model fits $\beta_1 = 0.5226$ and $\beta_2 = -0.5843$, which is quite close to the `beta_1 = 0.5` and `beta_2 = 0.5` which we have set. However, the predicted values seem to be quite off as well in this case - similarly to the MA(1) case."
## Periodical\nThe following time series is periodical with T=12. It consists of 48 timesteps.
"### 1. Check Stationarity\nThe sample data is stationary. Therefore, we do not need to difference the time series."
"### 2. Check ACF and PACF\n\nWe can make the following observations:\n- There are several autocorrelations that are significantly non-zero. Therefore, the time series is non-random.\n- High degree of autocorrelation between adjacent (lag = 1) and near-adjacent observations\n- From both the ACF and PACF plot, we can see a strong correlation with the adjacent observation (lag = 1) and also at a lag of 12, which is the amount of T.\n\n\n| | AR($p$) | MA($q$) | ARMA($p$, $q$) |\n|-|-|-|-|\n|ACF|Tails off (Geometric decay) |Significant at lag $q$ / Cuts off after lag $q$  |Tails off (Geometric decay) |\n|PACF|  Significant at each lag $p$ / Cuts off after lag $p=12$ |Tails off (Geometric decay) |Tails off (Geometric decay) |\n\n-> We can use an AR(12) model to model this process."
### 3. Modelling
## Trend\n\nThe following time series is the same as [Periodical](#Periodical) (periodical with T=12) with added trend. It consists of 48 timesteps.
### 1. Check Stationarity\nThe sample data is non-stationary. 
"### 2. Check ACF and PACF\n\nWe can make the following observations:\n- There are several autocorrelations that are significantly non-zero. Therefore, the time series is non-random.\n- High degree of autocorrelation between adjacent (lag = 1) and near-adjacent observations"
### 3. Modelling
## White Noise\n\nThe following time series is random. It consists of 48 timesteps.
"### 1. Check Stationarity\nThe sample data is stationary. Therefore, we do not need to difference the time series."
"### 2. Check ACF and PACF\n\nWe can make the following observations:\n- There is only one autocorrelation that is significantly non-zero at a lag of 0. Therefore, the time series is random."
### 3. Modelling\n\nModelling white noise is difficult because we cannot retrieve any parameters from the ACF and PACF plots.
"## Random-Walk\n\nThe following time series is random like [White Noise](#White-Noise). However, the current value depends on the previous one. It consists of 48 timesteps."
"### 1. Check Stationarity\nThe sample data is non-stationary. Therefore, we need to difference the time series."
## Constant\n\nThe following time series is constant. It consists of 48 timesteps.
"### 1. Check Stationarity\nThe sample data is non-stationary. Therefore, we need to difference the time series."
### 2. Check ACF and PACF\n\n- ACF/PACF was applied to non-stationary time series
"### 3. Modelling\n\nModelling a constant as an AR or MA process is diffucult because we cannot retrieve any parameters from the ACF and PACF plots. But on the other hand, if you can retrieve that a time series is constant, it should not be too difficult to forecast it, right?"
## Bitcoin
"### 1. Check Stationarity\nThe sample data is non-stationary. Therefore, we need to difference the time series."
### 2. Check ACF and PACF
## Ethereum
## Ethereum
"### 1. Check Stationarity\nThe sample data is non-stationary. Therefore, we need to difference the time series."
### 2. Check ACF and PACF
"## Discussion on Random-Walk\nFor both Bitcoin and Ethereum, we can observe a **random-walk** behavior (see [üöÄ Cheat Sheet](#üöÄ-Cheat-Sheet)). This is fairly common in stock prices (see [Random Walk Theory](https://www.investopedia.com/terms/r/randomwalktheory.asp))\n\n> A **random walk is unpredictable**; it cannot reasonably be predicted.\n>\n> Given the way that the random walk is constructed, we can expect that the best prediction we could make would be to use the observation at the previous time step as what will happen in the next time step.\n>\n>Simply because we know that the next time step will be a function of the prior time step.\n>\n>This is often called the naive forecast, or a persistence model. - [A Gentle Introduction to the Random Walk for Times Series Forecasting with Python](https://machinelearningmastery.com/gentle-introduction-random-walk-times-series-forecasting-python/)\n\nWell, that's an unsatisfying finding. So, where do we go from here?"
"Overview\n\nThe High Definition-Advanced Imaging Technology (HD-AIT) system files supplied in this contest range in size from 10MB to over 2GB per subject.  With just under 1200 examples, we'll need to figure out how to make or find more and almost any approach will have to substantially reduce the size of the 512x660x16 image data to be fed into a machine learning model.  In the instructions, the organizers suggest that one may even be able to win the contest with one of the smaller image suites. So in this notebook, I take a whack at preprocessing the lowest res images we have and providing some basic building blocks for the preprocessing pipeline.\n\nA quick disclaimer, I'm not an expert on these systems or the related scans.  If you see something I've misunderstood or you think I've made an error, let me know and I'll correct it.  This contest is aimed at a critical problem.  I'm in this to help us get better at threat detection.  The community can definitely improve the predictive veracity of these scans.  Anyway, I'm excited to get going so let's jump in.  \n\nTo begin I collect all of the imports used in the notebook at the top.  It makes it easier when you're converting to a preprocessing script."
Next I collect the constants.  You'll need to replace the various file name reference constants with a path to your corresponding folder structure.
"Analyzing Threat Zones\n\nThe scans to be analyzed in this contest all segment the body into numbered ""threat zones"".  Your modeling and results will have to give probability of contraband within a given threat zone.  The contest sponsors included a  visualization of the threat zones and the corresponding numbering scheme.  (Note that you will need to uncomment the code in the next block for it to run in your own environment)"
*Output removed by request of DHS.  Run in your own environment to review threat zones.*
"Viewing and Selecting Images\n\nI always like to actually see the data before I start messing with it.  So this next function prints a nice, but small 4x4 matrix of the 16 images.  Since I did this originally in Google Datalab, there is an output size constraint that this function hits.  So I used a cv2.resize to get it under the wire.  Since there is no data on Kaggle it doesn't matter for this notebook.  But watch out for this if you run it yourself.  If your environment does not have the same size constraint, show the images full size.  Its good grounding.  After you've seen them once, you can comment out the unit test.  Note also that this only applies the unit test and visualization.\n"
*Output removed by request of DHS.  Uncomment the unit test and run in your own environment.*
"Digging into the Images\n\nWhen we get to running a pipeline, we will want to pull the scans out one at a time so that they can be processed.  So here's a function to return the nth image.  In the unit test, I added a histogram of the values in the image."
Here's the histogram:\n![Raw Histogram](https://storage.googleapis.com/kaggle-datasets-jbf/tsa_datasets/raw_hist.png)\n\n*Image output removed at the request of DHS.  Uncomment the unit test and run in your own environment.*
Rescaling the Image\n\nMost image preprocessing functions want the image as grayscale.  So here's a function that rescales to the normal grayscale range.
Here's the histogram:\n![Grayscale Histogram](https://storage.googleapis.com/kaggle-datasets-jbf/tsa_datasets/grayscale_hist.png)\n\n*Image output removed at the request of DHS.  Uncomment the unit test and run in your own environment.*
"Spreading the Spectrum\n\nFrom the histogram, you can see that most pixels are found between a value of 0 and and about 25.  The entire range of grayscale values in the scan is less than ~125.  You can also see a fair amount of ghosting or noise around the core image.  Maybe the millimeter wave technology scatters some noise?  Not sure... Anyway, if someone knows what this is caused by, drop a note in the comments.  That said, let's see what we can do to clean the image up.\n\nIn the following function, I first threshold the background.  I've played quite a bit with the threshmin setting (12 has worked best so far), but this is obviously a parameter to play with.\n\nNext we equalize the distribution of the grayscale spectrum in this image.  See this tutorial if you want to learn more about this technique.  But in the main, it redistributes pixel values to a the full grayscale spectrum in order to increase contrast.\n\n"
Here's the histogram:\n![Raw Histogram](https://storage.googleapis.com/kaggle-datasets-jbf/tsa_datasets/spread_spectrum_hist.png)\n\n*Image output removed at the request of DHS.  Uncomment the unit test and run in your own environment.*
"Masking the Region of Interest\n\nUsing the slice lists from above, getting a set of masked images for a given threat zone is straight forward.  The same note applies here as in the 4x4 visualization above, I used a cv2.resize to get around a size constraint (see above), therefore the images are quite blurry at this resolution.  Note that the blurriness only applies to the unit test and visualization.  The data returned by this function is at full resolution.\n"
*Image output removed at the request of DHS.  Uncomment the unit test and run in your own environment.*
"Cropping the Images\n\nUsing the crop lists from above, getting a set of cropped images for a given threat zone is also straight forward.  The same note applies here as in the 4x4 visualization above, I used a cv2.resize to get around a size constraint (see above), therefore the images are quite blurry at this resolution.  If you do not face this size constraint, drop the resize.  Note that the blurriness only applies the unit test and visualization.  The data returned by this function is at full resolution."
*Image output removed at the request of DHS.  Uncomment the unit test and run in your own environment.*
# Import The Necessary Libraries & Define Data Access Variables
# Create Train Image Batches & Output Feature Variable
"**Bitcoin Time Series Forecasting**\n\nBitcoin is the longest running and most well known cryptocurrency, first released as open source in 2009 by the anonymous Satoshi Nakamoto. Bitcoin serves as a decentralized medium of digital exchange, with transactions verified and recorded in a public distributed ledger (the blockchain) without the need for a trusted record keeping authority or central intermediary. Transaction blocks contain a SHA-256 cryptographic hash of previous transaction blocks, and are thus ""chained"" together, serving as an immutable record of all transactions that have ever occurred. As with any currency/commodity on the market, bitcoin trading and financial instruments soon followed public adoption of bitcoin and continue to grow.  If you don't know what Bitcoin is , then get some knowledge about Bitcoin [here](https://www.coindesk.com/information/what-is-bitcoin) .\n\nThis Kernel is divided into two parts:-\n\n* Data Exploration\n* Time Series Analysis\n\nAnd further for the **Time Series Forecasting:**-\n\n*  Time Series forecasting with **LSTM**\n* Time Series forecasting with **XGBoost**\n* Time Series forecasting with Facebook **Prophet**\n* Time Series forecasting with **ARIMA**\n\nThis kernel takes inspiration from the following kernels,\n* [Time Series forecasting with Prophet by Rob Mulla](https://www.kaggle.com/robikscube/tutorial-time-series-forecasting-with-prophet)\n* [Time Series forecasting with XGBoost by Rob Mulla ](https://www.kaggle.com/robikscube/tutorial-time-series-forecasting-with-xgboost)\n* [Bitcoin Price. Prediction by ARIMA by –ê—Ä—Ç—ë–º](https://www.kaggle.com/myonin/bitcoin-price-prediction-by-arima)"
**Data Exploration**\n\nIn this section we just explore the Data i.e the Historic Bitcoin Prices and try to find some insights. We will be using the Coinbase dataset as it is one of the mostly used Bitcoin Exchange/Wallet in the world.
Lets visualize Historical Bitcoin Prices (2015-2018)
Lets visualize Historical Bitcoin Market Volume (2015-2018)
"**LSTM , XGBoost and Prophet - How good are they ?**\n\nHere lets visualize and compare the predictive results of LSTM, XGBoost and Prophet in a single plot,"
"**Time Series forecasting using ARIMA**\n\nARIMA is an acronym that stands for AutoRegressive Integrated Moving Average. It is a class of model that captures a suite of different standard temporal structures in time series data.\nThis acronym is descriptive, capturing the key aspects of the model itself. Briefly, they are:\n* AR: Autoregression. A model that uses the dependent relationship between an observation and some number of lagged observations.\n* I: Integrated. The use of differencing of raw observations (e.g. subtracting an observation from an observation at the previous time step) in order to make the time series stationary.\n* MA: Moving Average. A model that uses the dependency between an observation and a residual error from a moving average model applied to lagged observations.\n\nARIMA is one of the mostly used techniques for Time Series analysis. In Python,  ARIMA based forecasting models can be created either using AutoARIMA[(Pyramid ARIMA)](https://pypi.org/project/pyramid-arima/) or [StatsModel ](https://www.statsmodels.org/dev/generated/statsmodels.tsa.arima_model.ARIMA.html). Here we will be using StatsModel as Kaggle do not support Pyramid ARIMA till now."
## Examples\n### Fisher's iris dataset\n\nLet's start by uploading all of the essential modules and try out the iris example from the `scikit-learn` documentation. 
Now let's see how PCA will improve the results of a simple model that is not able to correctly fit all of the training data:
"Let's start by visualizing our data. Fetch the first 10 numbers. The numbers are represented by 8 x 8 matrixes with the color intensity for each pixel. Every matrix is flattened into a vector of 64 numbers, so we get the feature version of the data."
"Our data has 64 dimensions, but we are going to reduce it to only 2 and see that, even with just 2 dimensions, we can clearly see that digits separate into clusters."
"Our data has 64 dimensions, but we are going to reduce it to only 2 and see that, even with just 2 dimensions, we can clearly see that digits separate into clusters."
"Indeed, with t-SNE, the picture looks better since PCA has a linear constraint while t-SNE does not. However, even with such a small dataset, the t-SNE algorithm takes significantly more time to complete than PCA."
"Indeed, with t-SNE, the picture looks better since PCA has a linear constraint while t-SNE does not. However, even with such a small dataset, the t-SNE algorithm takes significantly more time to complete than PCA."
"In practice, we would choose the number of principal components such that we can explain 90% of the initial data dispersion (via the `explained_variance_ratio`). Here, that means retaining 21 principal components; therefore, we reduce the dimensionality from 64 features to 21."
"In practice, we would choose the number of principal components such that we can explain 90% of the initial data dispersion (via the `explained_variance_ratio`). Here, that means retaining 21 principal components; therefore, we reduce the dimensionality from 64 features to 21."
"## 2. Clustering\n\nThe main idea behind clustering is pretty straightforward. Basically, we say to ourselves, ""I have these points here, and I can see that they organize into groups. It would be nice to describe these things more concretely, and, when a new point comes in, assign it to the correct group."" This general idea encourages exploration and opens up a variety of algorithms for clustering.\n\n*The examples of the outcomes from different algorithms from scikit-learn*\n\nThe algorithms listed below do not cover all the clustering methods out there, but they are the most commonly used ones.\n\n### K-means\n\nK-means algorithm is the most popular and yet simplest of all the clustering algorithms. Here is how it works:\n1. Select the number of clusters $k$ that you think is the optimal number.\n2. Initialize $k$ points as ""centroids"" randomly within the space of our data.\n3. Attribute each observation to its closest centroid.\n4. Update the centroids to the center of all the attributed set of observations. \n5. Repeat steps 3 and 4 a fixed number of times or until all of the centroids are stable (i.e. no longer change in step 4).\n\nThis algorithm is easy to describe and visualize. Let's take a look."
"The following algorithm is the simplest and easiest to understand among all the the clustering algorithms without a fixed number of clusters.\n\n\nThe algorithm is fairly simple:\n1. We start by assigning each observation to its own cluster\n2. Then sort the pairwise distances between the centers of clusters in descending order\n3. Take the nearest two neigbor clusters and merge them together, and recompute the centers\n4. Repeat steps 2 and 3 until all the data is merged into one cluster\n\nThe process of searching for the nearest cluster can be conducted with different methods of bounding the observations:\n1. Single linkage \n$d(C_i, C_j) = min_{x_i \in C_i, x_j \in C_j} ||x_i - x_j||$\n2. Complete linkage \n$d(C_i, C_j) = max_{x_i \in C_i, x_j \in C_j} ||x_i - x_j||$\n3. Average linkage \n$d(C_i, C_j) = \frac{1}{n_i n_j} \sum_{x_i \in C_i} \sum_{x_j \in C_j} ||x_i - x_j||$\n4. Centroid linkage \n$d(C_i, C_j) = ||\mu_i - \mu_j||$\n\nThe 3rd one is the most effective in computation time since it does not require recomputing the distances every time the clusters are merged.\n\nThe results can be visualized as a beautiful cluster tree (dendogram) to help recognize the moment the algorithm should be stopped to get optimal results. There are plenty of Python tools to build these dendograms for agglomerative clustering.\n\nLet's consider an example with the clusters we got from K-means:"
"## Accuracy metrics\n\nAs opposed to classfication, it is difficult to assess the quality of results from clustering. Here, a metric cannot depend on the labels but only on the goodness of split. Secondly, we do not usually have true labels of the observations when we use clustering.\n\nThere are *internal* and *external* goodness metrics. External metrics use the information about the known true split while internal metrics do not use any external information and assess the goodness of clusters based only on the initial data. The optimal number of clusters is usually defined with respect to some internal metrics. \n\nAll the metrics described below are implemented in `sklearn.metrics`.\n\n**Adjusted Rand Index (ARI)**\n\nHere, we assume that the true labels of objects are known. This metric does not depend on the labels' values but on the data cluster split. Let $N$ be the number of observations in a sample. Let $a$ to be the number of observation pairs with the same labels and located in the same cluster, and let $b$ to be the number of observations with different labels and located in different clusters. The Rand Index can be calculated using the following formula: \n\n$$\Large \text{RI} = \frac{2(a + b)}{n(n-1)}.$$ \n\nIn other words, it evaluates a share of observations for which these splits (initial and clustering result) are consistent. The Rand Index (RI) evaluates the similarity of the two splits of the same sample. In order for this index to be close to zero for any clustering outcomes with any $n$ and number of clusters, it is essential to scale it, hence the Adjusted Rand Index: \n\n$$\Large \text{ARI} = \frac{\text{RI} - E[\text{RI}]}{\max(\text{RI}) - E[\text{RI}]}.$$\n\nThis metric is symmetric and does not depend in the label permutation. Therefore, this index is a measure of distances between different sample splits. $\text{ARI}$ takes on values in the $[-1, 1]$ range. Negative values indicate the independence of splits, and positive values indicate that these splits are consistent (they match $\text{ARI} = 1$).\n\n**Adjusted Mutual Information (AMI)**\n\nThis metric is similar to $\text{ARI}$. It is also symmetric and does not depend on the labels' values and permutation. It is defined by the [entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory) function and interprets a sample split as a discrete distribution (likelihood of assigning to a cluster is equal to the percent of objects in it). The $MI$ index is defined as the [mutual information](https://en.wikipedia.org/wiki/Mutual_information) for two distributions, corresponding to the sample split into clusters. Intuitively, the mutual information measures the share of information common for both clustering splits i.e. how information about one of them decreases the uncertainty of the other one.\n\nSimilarly to the $\text{ARI}$, the $\text{AMI}$ is defined. This allows us to get rid of the $MI$ index's increase with the number of clusters. The $\text{AMI}$ lies in the $[0, 1]$ range. Values close to zero mean the splits are independent, and those close to 1 mean they are similar (with complete match at $\text{AMI} = 1$).\n\n**Homogeneity, completeness, V-measure**\n\nFormally, these metrics are also defined based on the entropy function and the conditional entropy function, interpreting the sample splits as discrete distributions: \n\n$$\Large h = 1 - \frac{H(C\mid K)}{H(C)}, c = 1 - \frac{H(K\mid C)}{H(K)},$$\n\nwhere $K$ is a clustering result and $C$ is the initial split. Therefore, $h$ evaluates whether each cluster is composed of same class objects, and $c$ measures how well the same class objects fit the clusters. These metrics are not symmetric. Both lie in the $[0, 1]$ range, and values closer to 1 indicate more accurate clustering results. These metrics' values are not scaled as the $\text{ARI}$ or $\text{AMI}$ metrics are and thus depend on the number of clusters. A random clustering result will not have metrics' values closer to zero when the number of clusters is big enough and the number of objects is small. In such a case, it would be more reasonable to use $\text{ARI}$. However, with a large number of observations (more than 100) and the number of clusters less than 10, this issue is less critical and can be ignored.\n\n$V$-measure is a combination of $h$, and $c$ and is their harmonic mean:\n$$\Large v = 2\frac{hc}{h+c}.$$\nIt is symmetric and measures how consistent two clustering results are.\n\n**Silhouette**\n\nIn contrast to the metrics described above, this coefficient does not imply the knowledge about the true labels of the objects. It lets us estimate the quality of the clustering using only the initial, unlabeled sample and the clustering result. To start with, for each observation, the silhouette coefficient is computed. Let $a$ be the mean of the distance between an object and other objects within one cluster and $b$ be the mean distance from an object to an object from the nearest cluster (different from the one the object belongs to). Then the silhouette measure for this object is \n\n$$\Large s = \frac{b - a}{\max(a, b)}.$$\n\nThe silhouette of a sample is a mean value of silhouette values from this sample. Therefore, the silhouette distance shows to which extent the distance between the objects of the same class differ from the mean distance between the objects from different clusters. This coefficient takes values in the $[-1, 1]$ range. Values close to -1 correspond to bad clustering results while values closer to 1 correspond to dense, well-defined clusters. Therefore, the higher the silhouette value is, the better the results from clustering.\n\nWith the help of silhouette, we can identify the optimal number of clusters $k$ (if we don't know it already from the data) by taking the number of clusters that maximizes the silhouette coefficient."
"To conclude, let's take a look at how these metrics perform with the MNIST handwritten numbers dataset:"
"## 4. Demo assignment\n\nTo practice with PCA and clustering, you can complete [this assignment](https://www.kaggle.com/kashnitsky/a7-demo-unsupervised-learning) where you'll be analyzing data from accelerometers and gyros of Samsung Galaxy S3 mobile phones. The assignment is just for you to practice, and goes with [solution](https://www.kaggle.com/kashnitsky/a7-demo-unsupervised-learning-solution)."
"You can see that Keras will keep you updated on the loss as the model trains.\n\nOften, a better way to view the loss though is to plot it. The `fit` method in fact keeps a record of the loss produced during training in a `History` object. We'll convert the data to a Pandas dataframe, which makes the plotting easy."
"Notice how the loss levels off as the epochs go by. When the loss curve becomes horizontal like that, it means the model has learned all it can and there would be no reason continue for additional epochs."
"# **12. Implement a Movie Recommender System in Python** \n\n\n[Table of Contents](#0.1)\n\n\n- In this section, we will develop a very simple movie recommender system in Python that uses the correlation between the ratings assigned to different movies. Thus, we will find the similarity between the movies.\n\n\n- The dataset that we are going to use for this problem is the [MovieLens Dataset](https://www.kaggle.com/ayushimishra2809/movielens-dataset).\n\n\n- Let's import the basic libraries and import the data."
- We can see that there are 2 files in the dataset - `ratings` and `movies`. Let's explore them.
"- Now, let's plot a histogram for the number of ratings represented by the `rating_counts` column in the above dataframe. "
"- From the above plot, we can see that most of the movies have received less than 50 ratings and there are no movies having more than 100 ratings."
"- Now, we will plot a histogram for average ratings."
"- We can see that the integer values have taller bars than the floating values since most of the users assign rating as integer value i.e. 1, 2, 3, 4 or 5. \n\n- Furthermore, it is evident that the data has a weak normal distribution with the mean of around 3.5. There are a few outliers in the data as well."
"- Movies with a higher number of ratings usually have a high average rating as well since a good movie is normally well-known and a well-known movie is watched by a large number of people, and thus usually has a higher rating. \n\n- Let's see if this is also the case with the movies in our dataset. We will plot average ratings against the number of ratings."
"- The graph shows that, in general, movies with higher average ratings actually have more number of ratings, compared with movies that have lower average ratings."
\n        \n             3 ) Exploratory Data Analysis (EDA):\n        \n\n\n
"We saw before that only 338 (38%) of the passengers survived, We need to dig down more to get better insights from the data and see which categories of the passengers did survive and who didn't.\n\n"
\n        \n            Discovering the features correlation with Survived:\n        \n\n
#### Sex Vs Survived:
#### Age:
- Infants (age<=5) and childrens (between 10 and 15 years old) are most likely to survive.\n- elder passengers (>75) survived.\n- most passengers are between 15 and 40 years old.\n\n**insights:** It's good to convert the age feature to age band groups of length 5.
\n        \n            Discovering the correlation between the features:\n        \n\n
- Passenger Id has no correlation with any feature.\n- PClass has strong negative correlation with age and Fare.\n- Age has negative correlation with parch and sibsp.
#### Pclass - Age - Survived:
"- Pclass=3 had most passengers, Most if them did not survive.\n- Infant passengers in Pclass=2 and Pclass=3 mostly survived.\n- Most passengers in Pclass=1 survived."
#### Sex - Age - Survived:
"- as we saw before, Females are most likely to survive.\n- Elder passengers (>=70) are all males.\n"
 \n# **Loading Library and Dataset**\n
"By Changing the data type of each column, I reduced memory usages by 75%. By taking the minimum and the maximum of each column, the function assigns which numeric data type is optimal for the column and change the data type. If you want to know more about how it works, I suggest you to read [Eryk's article](https://towardsdatascience.com/make-working-with-large-dataframes-easier-at-least-for-your-memory-6f52b5f4b5c4)! "
**Data Visualization**
**Normalization**
# 4. Evaluate the model\n## 4.1 Training and validation curves
## 4.2 Confusion matrix
Confusion matrix can be very helpfull to see your model drawbacks.\n\nI plot the confusion matrix of the validation results.
# 5. Prediction and submition
## 5.1 Prediction validation results
## 5.2 Submition
 LogisticRegression
 Support Vector Machines
 Support Vector Machines
 KNeighborsClassifier
 KNeighborsClassifier
 GaussianNB
 GaussianNB
 Perceptron
 Perceptron
 LinearSVC 
 LinearSVC 
 SGDClassifier
 SGDClassifier
 DecisionTreeClassifier
 DecisionTreeClassifier
 RandomForestClassifier
 RandomForestClassifier
 MLPClassifier
 MLPClassifier
 XGBClassifier
 XGBClassifier
 ExtraTreesClassifier
 ExtraTreesClassifier
 AdaBoostClassifier
 AdaBoostClassifier
 lgb Classifier
 lgb Classifier
 NuSVC 
 NuSVC 
 HistGradientBoostingClassifier 
 HistGradientBoostingClassifier 
 GaussianProcessClassifier 
 GaussianProcessClassifier 
 RidgeClassifier 
 RidgeClassifier 
 CalibratedClassifierCV 
 CalibratedClassifierCV 
 PassiveAggressiveClassifier 
 PassiveAggressiveClassifier 
 Comparing Different Models
Plotting the duplicate Images
"We have other methods to find duplicates that will help us in identifying more soft duplicates if any in the dataset , that will come in later versions of this kernel"
It isn't surprising that kagglers usually have (or plan to get) higher education degree. Master degree is the most common one (though in India Bachelor degree is more wide-spread).\n\nIt is quite interesting that the rate of having a higher degree (master and doctoral) is higher for women than for men.
### Major
### Industry and profession
"I know that many people has shown this graph, but still let's look at it again. I think that most of these titles can be joined into several groups:\n- Students can be a separate group,\n- Let's leave DS also by themselves;\n- People in research;\n- Next we have analysts - DA, BA and others, who need a different set of skills, but could be considered a level before DS;\n- DE and SE who build production systems;\n- Managers to lead the products;\n- And others;\n\nThis grouping is arbitrate and could be wrong, but let's see what will be the result."
"I know that many people has shown this graph, but still let's look at it again. I think that most of these titles can be joined into several groups:\n- Students can be a separate group,\n- Let's leave DS also by themselves;\n- People in research;\n- Next we have analysts - DA, BA and others, who need a different set of skills, but could be considered a level before DS;\n- DE and SE who build production systems;\n- Managers to lead the products;\n- And others;\n\nThis grouping is arbitrate and could be wrong, but let's see what will be the result."
Now we see that the number of DE and DS is almost equal and the number of DA isn't far behind.\nIt is worth noticing that different companies could have very different titles. For example in Facebook DS could work as DA; in some companies situation could be opposite.
### Years of learning ML vs self-confidence
"Quite interesting. Usually people with 2 or more years of ML-experience are confident that they are DS, but in Russia people with 1-2 or even less that 1 year of experience consider themselves to be DS."
"After defining the callback, add it as an argument in `fit` (you can have several, so put it in a list). Choose a large number of epochs when using early stopping, more than you'll need."
"And sure enough, Keras stopped the training well before the full 500 epochs!\n\n# Your Turn #\n\nNow [**predict how popular a song is**](https://www.kaggle.com/kernels/fork/11906770) with the *Spotify* dataset."
Let's also look at one image in more detail:
"## Load data into DataBunch\nNow that we have the right folder structure and images inside of the folders we can continue. Before training a model in fast.ai, we have to load the data into a [DataBunch](https://docs.fast.ai/basic_data.html#DataBunch), in this case, we use a ImageDataBunch, a special version of the DataBunch. Fast.ai offers different functions to create a DataBunch. We will use the from_folder method of the ImageDataBunch class to create the dataset.\nThere are different hyperparameters we can tweak to make the model perform better:\n\n- [valid_pct](#What-are-Train,-Test-and-Validation-datasets?)\n- [size](#What-image-size-should-I-choose?)\n- [num_workers](#What-is-multiprocessing?)\n- [ds_tfms](#What-are-transforms-and-which-transforms-should-I-use?)\n- [bs (batch size)](#What-is-the-batch-size?)"
## Datasets distribution\n\n### Competition data
### 2019 competition data
### 2019 competition data
### Dataset oversampled
### Dataset oversampled
"### Learning rate schedule\n\nWe are going to use a `cosine learning rate schedule with a warm-up phase`, this may be a good idea since we are using a pre-trained model, the warm-up phase will be useful to avoid the pre-trained weights degradation resulting in catastrophic forgetting, during the schedule the learning rate will slowly decrease to very low values, this helps the model to land on more stable weights."
"### Learning rate schedule\n\nWe are going to use a `cosine learning rate schedule with a warm-up phase`, this may be a good idea since we are using a pre-trained model, the warm-up phase will be useful to avoid the pre-trained weights degradation resulting in catastrophic forgetting, during the schedule the learning rate will slowly decrease to very low values, this helps the model to land on more stable weights."
# Model
"# Confusion matrix\n\nLet's also take a look at the confusion matrix, this will give us an idea about what classes the model is mixing or having a hard time."
"# Visualize predictions\n\nFinally, it is a good practice to always inspect some of the model's prediction by looking at the data, this can give an idea if the model is getting some predictions wrong because the data is really hard, of if it is because the model is actually bad.\n\n\n### Class map\n```\n0: Cassava Bacterial Blight (CBB)\n1: Cassava Brown Streak Disease (CBSD)\n2: Cassava Green Mottle (CGM)\n3: Cassava Mosaic Disease (CMD)\n4: Healthy\n```\n\n\n## Train set"
"We're going to try to create a model to enter the Dogs vs Cats competition at Kaggle. There are 25,000 labelled dog and cat photos available for training, and 12,500 in the test set that we have to try to label for this competition. According to the Kaggle web-site, when this competition was launched (end of 2013): ""State of the art: The current literature suggests machine classifiers can score above 80% accuracy on this task"". So if we can beat 80%, then we will be at the cutting edge as of 2013!"
Here we import the libraries we need. We'll learn about what each does during the course.
Note that in the previous plot *iteration* is one iteration (or *minibatch*) of SGD. In one epoch there are \n(num_train_samples/num_iterations) of SGD.\n\nWe can see the plot of loss versus learning rate to see where our loss stops decreasing:
"The loss is still clearly improving at lr=1e-2 (0.01), so that's what we use. Note that the optimal learning rate can change as we train the model, so you may want to re-run this function from time to time."
"## 9.2. Visualizing Distributions of X,y"
"## 9.3. Train Test, Holdout sets"
## 9.3. Plot the training set data
# 10. Plotting with Simple Regression
## 10.2. Plotting simple liner regression
- It clearly underfits.
## 12.3. Ploting
- It still underfits.
## 13.3. Ploting
- A fair fit now.
## 14.3. Ploting
- A proper fit now.
## 16.1. Plotting MSEs
## 16.2. Potting Bias
## 16.2. Potting Bias
## 16.2. Potting Variance
## 16.2. Potting Variance
## 16.2. Potting Bias - Variance
# **Target Variable Distribution**
# **Target Class Balance**
# **Target Class Balance**
# **Distribution of features Vs Target**
"# **W & B Artifacts**\n\nAn artifact as a versioned folder of data.Entire datasets can be directly stored as artifacts .\n\nW&B Artifacts are used for dataset versioning, model versioning . They are also used for tracking dependencies and results across machine learning pipelines.Artifact references can be used to point to data in other systems like S3, GCP, or your own system.\n\nYou can learn more about W&B artifacts [here](https://docs.wandb.ai/guides/artifacts)\n\n![](https://drive.google.com/uc?id=1JYSaIMXuEVBheP15xxuaex-32yzxgglV)"
Snapshot of the artifacts created  \n\n![](https://drive.google.com/uc?id=1w8g5VUO34Wy6Mi3y2M6-Yu6yOdz7qXqA)
\n# **Logging to W & B environment**
"# **Tensorflow Decision Forests**\n\nSource : https://blog.tensorflow.org/2021/05/introducing-tensorflow-decision-forests.html\n\n![](https://drive.google.com/uc?id=1u8C0iutX50ajnYPdvnoTyCrtNNECvI_R)\n\nDecision forests are a family of machine learning algorithms with quality and speed competitive with (and often favorable to) neural networks, especially when you‚Äôre working with tabular data. They‚Äôre built from many decision trees, which makes them easy to use and understand - and you can take advantage of a plethora of interpretability tools and techniques that already exist today.\n\nTF-DF brings this class of models along with a suite of tailored tools to TensorFlow users:\n\nBeginners will find it easier to develop and explain decision forest models. There is no need to explicitly list or pre-process input features (as decision forests can naturally handle numeric and categorical attributes), specify an architecture (for example, by trying different combinations of layers like you would in a neural network), or worry about models diverging. Once your model is trained, you can plot it directly or analyse it with easy to interpret statistics.\nAdvanced users will benefit from models with very fast inference time (sub-microseconds per example in many cases). And, this library offers a great deal of composability for model experimentation and research. In particular, it is easy to combine neural networks and decision forests.\nIf you‚Äôre already using decision forests outside of TensorFlow, here‚Äôs a little of what TF-DF offers:\n\nIt provides a slew of state-of-the-art Decision Forest training and serving algorithms such as random forests, gradient-boosted trees, CART, (Lambda)MART, DART, Extra Trees, greedy global growth, oblique trees, one-side-sampling, categorical-set learning, random categorical learning, out-of-bag evaluation and feature importance, and structural feature importance.\nThis library can serve as a bridge to the rich TensorFlow ecosystem by making it easier for you to integrate tree-based models with various TensorFlow tools, libraries, and platforms such as TFX.\nAnd for users new to neural networks, you can use decision forests as an easy way to get started with TensorFlow, and continue to explore neural networks from there.\n"
# **Visualize the Output**
# **References**\n\nhttps://blog.tensorflow.org/2021/05/introducing-tensorflow-decision-forests.html\n\nhttps://www.kaggle.com/subinium/tps-oct-simple-eda\n\nhttps://www.kaggle.com/craigmthomas/tps-oct-2021-eda\n\n
"## Drivers\n\n\nDrivers are abstraction for the process of executing a policy in an environment for a specified number of steps  during data collection, evaluation and generating a video of the agent.The data encountered by the driver at each step like observation , action , reward , current and next step is saved in Trajectory and broadcast to a set of observers such as replay buffers and metrics. \n\n\nImplementations for drivers are available both in Python and TensorFlow\n\n**Python Drivers :**\n\nThe PyDriver class takes a python environment, a python policy and a list of observers to update at each step. "
"The code below runs a random policy on the CartPole environment, saving the results to a **replay buffer**.\n"
# Imports
# Getting the data
"The word **Love** is the most commonly used word in movie titles. **Girl**, **Day** and **Man** are also among the most commonly occuring words. I think this encapsulates the idea of the ubiquitious presence of romance in movies pretty well."
"**Life** is the most commonly used word in Movie titles. **One** and **Find** are also popular in Movie Blurbs. Together with **Love**, **Man** and **Girl**, these wordclouds give us a pretty good idea of the most popular themes present in movies. "
"There are over 93 languages represented in our dataset. As we had expected, English language films form the overwhelmingly majority. French and Italian movies come at a very distant second and third respectively. Let us represent the most popular languages (apart from English) in the form of a bar plot."
"As mentioned earlier, **French** and **Italian** are the most commonly occurring languages after English. **Japanese** and **Hindi** form the majority as far as Asian Languages are concerned."
"Do popularity and vote average share a tangible relationship? In other words, is there a strong positive correlation between these two quanitties? Let us visualise their relationship in the form of a scatterplot."
"Surprisingly, the Pearson Coefficient of the two aforementioned quantities is a measly **0.097** which suggests that **there is no tangible correlation**. In other words, popularity and vote average and independent quantities. It would be interesting to discover how TMDB assigns numerical popularity scores to its movies."
"Surprisingly, the Pearson Coefficient of the two aforementioned quantities is a measly **0.097** which suggests that **there is no tangible correlation**. In other words, popularity and vote average and independent quantities. It would be interesting to discover how TMDB assigns numerical popularity scores to its movies."
There is a very small correlation between Vote Count and Vote Average. A large number of votes on a particular movie does not necessarily imply that the movie is good.
"With these features in hand, let us now check the most popular and most successful months and days."
"It appears that **January** is the most popular month when it comes to movie releases. In Hollywood circles, this is also known as the *the dump month* when sub par movies are released by the dozen. \n\nIn which months do bockbuster movies tend to release? To answer this question, we will consider all movies that have made in excess of 100 million dollars and calculate the average gross for each month."
"It appears that **January** is the most popular month when it comes to movie releases. In Hollywood circles, this is also known as the *the dump month* when sub par movies are released by the dozen. \n\nIn which months do bockbuster movies tend to release? To answer this question, we will consider all movies that have made in excess of 100 million dollars and calculate the average gross for each month."
"We see that the months of **April**, **May** and **June** have the highest average gross among high grossing movies. This can be attributed to the fact that blockbuster movies are usually released in the summer when the kids are out of school and the parents are on vacation and therefore, the audience is more likely to spend their disposable income on entertainment.\n\nDo some months tend to be more successful than others? Let us visualise the boxplot between the return and the months."
"We see that the months of **April**, **May** and **June** have the highest average gross among high grossing movies. This can be attributed to the fact that blockbuster movies are usually released in the summer when the kids are out of school and the parents are on vacation and therefore, the audience is more likely to spend their disposable income on entertainment.\n\nDo some months tend to be more successful than others? Let us visualise the boxplot between the return and the months."
"The months of **June** and **July** tend to yield the highest median returns. **September** is the least successful months on the aforementioned metrics. Again, the success of June and July movies can be attributed to them being summer months and times of vacation. September usually denotes the beginning of the school/college semester and hence a slight reduction in the consumption of movies.\n\nLet us now have a look at the most popular days as we did for months."
"The months of **June** and **July** tend to yield the highest median returns. **September** is the least successful months on the aforementioned metrics. Again, the success of June and July movies can be attributed to them being summer months and times of vacation. September usually denotes the beginning of the school/college semester and hence a slight reduction in the consumption of movies.\n\nLet us now have a look at the most popular days as we did for months."
**Friday** is clearly the most popular day for movie releases. This is understandable considering the fact that it usually denotes the beginning of the weekend. **Sunday** and **Monday** are the least popular days and this can be attributed to the same aforementioned reason.
"#### Number of Movies by the year\n\nThe Dataset of 45,000 movies available to us does not represent the entire corpus of movies released since the inception of cinema. However, it is reasomnable to assume that it does include almost every major film released in Hollywood as well as other major film industries across the world (such as Bollywood in India). With this assumption in mind, let us take a look at the number of movies produced by the year."
"We notice that there is a sharp rise in the number of movies **starting the 1990s decade.** However, we will not look too much into this as it is entirely possible that recent movies were oversampled for the purposes of this dataset.\n\nNext, let us take a look at the earliest movies represented in the dataset.\n\n#### Earliest Movies Represented"
"The movie with the most number of languages, **Visions of Europe** is actually a collection of 25 short films by 25 different European directors. This explains the sheer diversity of the movie in terms of language."
The **Spearman Coefficient** is 0.018 indicating no correlation between the two quantities.
We are aware that most movies are less than 5 hours (or 300 minutes) long. Let us plot a distribution of these mainstream movies.
Is there any meaningful relationship between runtime and return? Let us find out!
Is there any meaningful relationship between runtime and return? Let us find out!
"There seems to be relationship between the two quantities. **The duration of a movie is independent of its success.** However, I have a feeling this might not be the case with duration and budget. A longer movie should entail a higher budget. Let us find out if this is really the case."
"There seems to be relationship between the two quantities. **The duration of a movie is independent of its success.** However, I have a feeling this might not be the case with duration and budget. A longer movie should entail a higher budget. Let us find out if this is really the case."
"The two quantities have a much weaker correlation than I had expected. In retrospect, the genre of the movie tends to have a much greater impact on budget. A 3 hour art film will cost significantly lesser than a 90 minute Sci-Fi movie. \n\nNext, I'd like to see the average lengths of movies through time, right from the 1890s to the 2017s. It would be interesting to see the trends in what filmmakers adjudged would be the appropriate length of a movie at that time."
"The two quantities have a much weaker correlation than I had expected. In retrospect, the genre of the movie tends to have a much greater impact on budget. A 3 hour art film will cost significantly lesser than a 90 minute Sci-Fi movie. \n\nNext, I'd like to see the average lengths of movies through time, right from the 1890s to the 2017s. It would be interesting to see the trends in what filmmakers adjudged would be the appropriate length of a movie at that time."
"We notice that films started hitting the **60 minute mark as early as 1914**. Starting **1924**, films started having the traiditonal 90 minute duration and has remained more or less constant ever since.\n\nFinally in this section, let us see the longest and the shortest movies of all time (with respect to the movies in the dataset). "
"Two **Pirates of the Carribean** films occupy the top spots in this list with a staggering budget of over **300 million dollars**. All the top 10 most expensive films made a profit on their investment except for **The Lone Ranger** which managed to recoup less than 35% of its investment, taking in a paltry 90 million dollars on a **255 million dollar** budget.\n\nHow strong a correlation does the budget hold with the revenue? A stronger correlation would directly imply more accurate forecasts."
The pearson r value of **0.73** between the two quantities indicates a very strong correlation. 
"The mean gross of a movie is **68.7 million dollars** whereas the median gross is much lower at **16.8 million dollars**, suggesting the skewed nature of revenue. The lowest revenue generated by a movie is **just 1 dollar** whereas the highest grossing movie of all time has raked in an astonishing **2.78 billion dollars.*"
The distribution of revenue undergoes exponential decay just like budget. We also found that the two quantities were strongly correlated. Let us now take a look at the highest and least grossing movies of all time.
"These figures have not been adjusted for inflation. Therefore, we see a disproportionate number of movies from very recent times in the top 10 list. To get an understanding of the revenue garnered by movies, let us plot the maximum revenue through the years."
"As can be seen from the figure, the maximum gross has steadily risen over the years. The world of movies broke the 1 billion dollar mark in 1997 with the release of **Titanic**. It took another 12 years to break the 2 billion dollar mark with **Avatar**. Both these movies were directed by James Cameron."
**Animation** movies has the largest 25-75 range as well as the median revenue among all the genres plotted. **Fantasy** and **Science Fiction** have the second and third highest median revenue respectively. 
"From the boxplot, it seems like **Animation** Movies tend to yield the highest returns on average. **Horror** Movies also tend to be a good bet. This is partially due to the nature of Horror movies being low budget compared to Fantasy Movies but being capable of generating very high revenues relative to its budget."
Let us now take a look at the actors and the directors who have raked in the most amount of money with their movies.\n\n#### Actors with the Highest Total Revenue
#### Directors with the Highest Total Revenue
#### Directors with the Highest Total Revenue
"For average revenues, we will consider only actors and directors who have acted and directed in at least 5 movies respectively."
#### Actors with Highest Average Revenue
#### Directors with Highest Average Revenue
#### Directors with Highest Average Revenue
"Which actors and directors are the safest bet? For this, we will consider the average return brought in by a particular director or actor. We will only consider those movies that have raked in at least 10 million dollars. Also, we will only consider actors and directors that have worked in at least 5 films.\n\n#### Most Successful Actors"
"We see that our model performs far more superiorly than the Dummy Regressor. Finally, let us plot the feature importances in the form of a bar plot to deduce which features were the most significant in our making predictions."
"We notice that **vote_count**, a feature we *cheated* with, is the most important feature to our Gradient Boosting Model. This goes on to show the improtance of popularity metrics in determining the revenue of a movie. **Budget** was the second most important feature followed by **Popularity** (Literally, a popularity metric) and **Crew Size**."
It seems that movies that belong to a franchise have a higher probability of being a success. 
"We see that with homepages, there is not a very huge difference in probability. To avoid the curse of dimensionality, we will eliminate this feature as it is not very useful."
"### EDA & VISUALIZATION\n\nBefore working with any kind of data it is important to understand them. A crucial step to this aim is the ***Exploratory data analysis (EDA)***: a combination of visualizations and statistical analysis (uni, bi, and multivariate) that helps us to better understand the data we are working with and to gain insight into their relationships. So, let's explore our target variable and how the other features influence it."
"In literature, acceptable values for skewness are between -0.5 and 0.5 while -2 and 2 for Kurtosis. Looking at the plot, we can clearly see how the distribution does not seem to be normal, but highly right-skewed. The non-normality of our distribution is also supported by the Shapiro test for normality (p-value really small that allows us to reject the hypotesis of normality). Despite that, let's leave it like that for now, we'll deal with that later in the notebook. "
"In literature, acceptable values for skewness are between -0.5 and 0.5 while -2 and 2 for Kurtosis. Looking at the plot, we can clearly see how the distribution does not seem to be normal, but highly right-skewed. The non-normality of our distribution is also supported by the Shapiro test for normality (p-value really small that allows us to reject the hypotesis of normality). Despite that, let's leave it like that for now, we'll deal with that later in the notebook. "
The correlation matrix is the best way to see all the numerical correlation between features. Let's see which are the feature that correlate most with our target variable.
The correlation matrix is the best way to see all the numerical correlation between features. Let's see which are the feature that correlate most with our target variable.
Now that we know which features correlates most with our target variable we can investigate them more in depth.
"there is only one value for `R`, one value for `C` for the `breath_id`. \n\nLet us visualize `u_in`, `u_out` and `pressure` with respect to the `time_step`:"
"# All breaths\nWhat values do we have for `R`, which represents how restricted the airway is (in cmH2O/L/S)."
"This value is safely below the point where pressure relief valve opens (at 70 cmH20) in order to prevent excessive pressures in the lung, thus reducing any barotrauma risk.\n\nThe pressures in the training data have the following distribution"
with a median value of 
"Note however that in this competition the expiratory phase is not scored, so for practical purposes we are only really interested in the pressure for `u_out=0`, *i.e.* the first second of the experiments:"
with a median value of 
"We have nine combinations of experiments; `C` can be 10, 20 or 50, and `R` can be 5, 20 or 50. Lets take a quick look at an example of each"
# Positive end-expiratory pressure (PEEP)\nIt is worth noting that even before the experiments start (*i.e.* the `time_step=0` and `u_in=0`) there is a positive pressure in the airway. The system is maintained above atmospheric pressure to promote gas exchange to the lungs.
# Positive end-expiratory pressure (PEEP)\nIt is worth noting that even before the experiments start (*i.e.* the `time_step=0` and `u_in=0`) there is a positive pressure in the airway. The system is maintained above atmospheric pressure to promote gas exchange to the lungs.
The average value of PEEP at the beginning of each cycle is
Both of these breaths have a somewhat unusual aspect
"Note that all of the instances of negative pressure occur only in the `R=50` (high restriction) with `C=10` (thick latex) systems.\n# Simple feature engineering\nWe shall add a new feature, which is the [cumulative sum](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.cumsum.html) of the `u_in` feature:"
"The thinking behind this feature is that it is reasonable to assume the pressure in the lungs is approximately proportional to how much air has actually been pumped into them. It goes almost without saying that this feature is not useful when breathing out, but given that the expiratory phase is not scored in this competition this should not be too much of a problem.\n\n### Shifting `u_in`\nLet us take a look at the first second of `breath_id=928`, which is an excellent example of an oscillatory experiment"
"It can be observed that there is a lag between `u_in` and the resulting `pressure` of around 0.1 seconds. I am sure it is with this in mind that [Chun Fu](https://www.kaggle.com/patrick0302) wrote his excellent notebook [""*Add lag u_in as new feat*""](https://www.kaggle.com/patrick0302/add-lag-u-in-as-new-feat/notebook), which introduces a new *shifted* `u_in` feature. Here we shall use a shift of 2 rather than his original shift of 1, which is now more in line with the delay seen:"
"\n# **1. Load Your Data, Load your Libraries**\n\nWell, lets get started loading a bunch of libraries that we will be showing off with the data\n\n"
\n# **2. Navigating your data**\nThe following section is a primer for loading data into **dataframes** which are essentially very powerful data grids that have a lot of great functionality for manipulating data. These can also be thought of as super powerful excel sheets. Below we will show some basic syntax usage of getting around your dataframe. \n\n\n#### Dataset size\n\n- **Train**: 1460\n- **Test**: 1458\n\n### **reading in CSV's from a file path**
"### **Generating a Confusion Matrix**\n\nNote, this code is taken straight from the SKLEARN website, an nice way of viewing confusion matrix. This is useful in classification problems. Consider this issue:\n- Your underlying data only has **10 / 90** virus to non-virus samples to detect\n- if you get an accuracy of 90%, what does that mean? Is that better than guessing? Nope. It's the same\n- **Practical sense** if you are a hospital, you want to be very careful with **False Positives: telling people they have a disease when they don't**. That's much worse than **False Negatives**, not detecting a disease. As a result, people will design around the priority of these two different metrics. \n\n#### A sample confusion matrix\n"
"\n## ** 7. Grid Search/Randomized Search: the quest for hyperparameters ** \n> \n\n#### **Look at how many options are in  logistic regression:**\n>        LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n>                  intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n>                 penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n>                  verbose=0, warm_start=False)\n\n\nMany of the advanced machine learning functions have a large number of model options that can be entered. these are often called **hyper parameters**. These address questions such as: \n- ""how long should the model run"", or \n- ""how many times should my computer re-look at the data"" or \n- ""how slow should the computer work through the problem?"" \n\nTo assist answering some of these questions, `sklearn` has `GridSearch` and `RandomizedSearch` which will try various combinations with a provided model, compare scores and return the optimal model that should be tried."
# Importing Libraries
---
# Color
---
"It is incredible how we can apply data science to different areas of our life. Even in sports, data science can help to make the game safer for the athletes.\n\nThe main goal of this analysis is to find out factors, which lead to injury. In particular, find out if the effects that synthetic turf versus natural turf can have on player movements and the factors that may contribute to lower extremity injuries."
## Load Data
`2` Now let's visualize the correlations:
"Unfortunately, there is __no correlation between our features and the injury.__"
"It looks like the heatmap of injuries is somewhat different from the general heatmap of the field! Probably, there are more dangerous areas on the field, where players are more likely to get injured!\n\nWe can use this insight for the feature engineering for the injury prediction model!"
This is the KDE plot for the injury play locations on the field:
"So is the speed of the player, his position on the field and the number of the game (`PlayerGame`) correlated with the injury?"
The diagram above shows that there is no correlation between the injury and the number of games/plays per game played.
Explore the correlations:
We can see that the distances and speed per play have a negative correlation with PlayerGamePlay. Does it mean that players get tired and move slower?
Let's look at the distribution of motion features for plays with injury:
It seems that speed for plays with injuries may come from a different distribution. Let's try [Kolmogorov‚ÄìSmirnov test](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test) to check if those samples come from one distribution:
"The p-value is very small, so we can reject the null hypothesis that both samples are drawn from one distribution. It means that most certainly, we do have some difference between speed for normal plays and plays with the injury."
Apply Kolmogorov-Smirnov test:
Explore the speed:
We can see that the speed distribution for the natural and synthetic field type are almost identical!
We can see that the speed distribution for the natural and synthetic field type are almost identical!
The same thing is for the distance.
The same thing is for the distance.
We get identical distributions for the angle too.\n\nIt means that the type of turf does not affect the main motion features.
### Target Exploration
### Corralation between features (variables)
### Corralation between features (variables)
### Numeric Features Exploration
### Numeric Features Exploration
### Categorical (Ordinal) Features Exploration
### Categorical (Ordinal) Features Exploration
### Categorical (Nominal) Features Exploration
### Categorical (Nominal) Features Exploration
### Please upvote if my notebook helped you in any way :)
"# Missing Values\n\nLooks like our external data has some missing values, which is expected. Body part missing numbers increased around three times, missing ages increased about four times and gender missing rates didn't change much but small increase... [2020 values here](https://www.kaggle.com/datafan07/analysis-of-melanoma-metadata-and-effnet-ensemble#Missing-Values)."
"# Distribution Differences\n\nMeanwhile age and gender distributions seems pretty similar with external data anatom site part has some differences: palms/soles, oral/genital and head/neck parts increased a lot in train data we might have to check these further..."
"# Distribution Differences\n\nMeanwhile age and gender distributions seems pretty similar with external data anatom site part has some differences: palms/soles, oral/genital and head/neck parts increased a lot in train data we might have to check these further..."
"# Imputing Missing Values\n\nThis time I decided to replace missing values with fixed values instead of replacing them with most frequent ones, since we have higher miss ratio..."
# Body Part Ratio by Gender and Target\n\nWe have very different malignant ratio for external data it seems. You can check [2020 values here](https://www.kaggle.com/datafan07/analysis-of-melanoma-metadata-and-effnet-ensemble#Body-Part-Ratio-by-Gender-and-Target). This is big difference but might be useful for unseen test data...
"# Sunburst Chart\n\n\n- We almost doubled our malignant examples with upsampling and external data,\n- Malignant images became more balanced 56% male to 44% female. It was 62% vs 38% in 2020 data,\n- Gender wise benign images has same ratio as 2020,\n- Malignant image scan locations differs based on the patients gender:\n    - Torso still most common location in males even with 3% decrease; meanwhile female malignant torso scans decreased like 6% with external data,\n    - Lower extremity still more common with female scans than males 12% males vs 26% females (it was 18% vs 26% in 2020 data)\n    - Again upper extremity malignant scans are more common with females than males with 15% males vs 20% females (used to be 17% - 23%)\n    - Head/Neck malignant scans increased with external data on both genders."
"# Sunburst Chart\n\n\n- We almost doubled our malignant examples with upsampling and external data,\n- Malignant images became more balanced 56% male to 44% female. It was 62% vs 38% in 2020 data,\n- Gender wise benign images has same ratio as 2020,\n- Malignant image scan locations differs based on the patients gender:\n    - Torso still most common location in males even with 3% decrease; meanwhile female malignant torso scans decreased like 6% with external data,\n    - Lower extremity still more common with female scans than males 12% males vs 26% females (it was 18% vs 26% in 2020 data)\n    - Again upper extremity malignant scans are more common with females than males with 15% males vs 20% females (used to be 17% - 23%)\n    - Head/Neck malignant scans increased with external data on both genders."
"# Age and Scan Result Relations\n\nAge distribution seems little bit different with external data added. In general we have older patients, benign scan age distributions seems more closer to each other than 2020 data, meanwhile malignant difference stay at same levels between genders. We can say generally examples which missing gender values are also missing age values too. Lastly we still have age effect on malignant scans with the difference visible on age below 0 because of the imputing method we choose."
"# Age and Scan Result Relations\n\nAge distribution seems little bit different with external data added. In general we have older patients, benign scan age distributions seems more closer to each other than 2020 data, meanwhile malignant difference stay at same levels between genders. We can say generally examples which missing gender values are also missing age values too. Lastly we still have age effect on malignant scans with the difference visible on age below 0 because of the imputing method we choose."
"# Age \n\nAge distribution seems similar with external data, you can [you can compare them here](https://www.kaggle.com/datafan07/analysis-of-melanoma-metadata-and-effnet-ensemble#Age-Round-Two)."
"# Age \n\nAge distribution seems similar with external data, you can [you can compare them here](https://www.kaggle.com/datafan07/analysis-of-melanoma-metadata-and-effnet-ensemble#Age-Round-Two)."
"# Image Resolutions \n\nWe had decent observations in the previous notebook about image sizes so wanted to check them here with external data again. With the new examples we increased variety of the image sizes (with high number of 1024x1024 images coming in) but we can see 1920x1080 images in test set still seperated, so we should be really careful about image sizes in our models..."
### 1. Library
### 2.Data generate
### 3.plot data
### 4.Define data in place holder
### 6.Session Start for Model training\n\n\n![](https://camo.githubusercontent.com/7491264fba17ff7eb3ec5cce2e0f8db3e58e1c7b/68747470733a2f2f6d6f7276616e7a686f752e6769746875622e696f2f7374617469632f726573756c74732f746f7263682f312d312d322e676966)
"### Other Related Notebook for learning\n- **Linear Regression** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/2_BasicModels/linear_regression.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/2_BasicModels/linear_regression.py)). Implement a Linear Regression with TensorFlow.\n- **Linear Regression (eager api)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/2_BasicModels/linear_regression_eager_api.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/2_BasicModels/linear_regression_eager_api.py)). Implement a Linear Regression using TensorFlow's Eager API.\n- **Logistic Regression** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/2_BasicModels/logistic_regression.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/2_BasicModels/logistic_regression.py)). Implement a Logistic Regression with TensorFlow.\n- **Logistic Regression (eager api)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/2_BasicModels/logistic_regression_eager_api.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/2_BasicModels/logistic_regression_eager_api.py)). Implement a Logistic Regression using TensorFlow's Eager API.\n- **Nearest Neighbor** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/2_BasicModels/nearest_neighbor.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/2_BasicModels/nearest_neighbor.py)). Implement Nearest Neighbor algorithm with TensorFlow.\n- **K-Means** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/2_BasicModels/kmeans.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/2_BasicModels/kmeans.py)). Build a K-Means classifier with TensorFlow.\n- **Random Forest** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/2_BasicModels/random_forest.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/2_BasicModels/random_forest.py)). Build a Random Forest classifier with TensorFlow.\n- **Gradient Boosted Decision Tree (GBDT)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/2_BasicModels/gradient_boosted_decision_tree.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/2_BasicModels/gradient_boosted_decision_tree.py)). Build a Gradient Boosted Decision Tree (GBDT) with TensorFlow.\n- **Word2Vec (Word Embedding)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/2_BasicModels/word2vec.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/2_BasicModels/word2vec.py)). Build a Word Embedding Model (Word2Vec) from Wikipedia data, with TensorFlow."
### 1. Library
### 2.Data Generation
### 3.Plot data
### 4. Load data into placeholder
### 6. Model Result
## Save and reload\n---\n[**Go to Top**](#Tensorflow-Tutorial)
### 1. Load Library and Generate Data
### 2.Model Save
### 2.Model Save
### 3. Model Reload
### 3. Model Reload
### 4.Model Save Loss and Reload Model Loss
### 1.Load Library
### 2.Define Parameter and Generate Data
### 3.Plot Data
### 4.Default Network
### 6.Model Training and Prediction
## Tensorboard\n---\n[**Go to Top**](#Tensorflow-Tutorial)\n\n![](https://www.tensorflow.org/images/graph_vis_animation.gif)\n\n### 1. Load dataset
### 3. Model Training with Placeholder of data
### 4.Generate TensorBoard and see the results
### 1.Load Library
### 2. Read dataset and set parameter
### 3.Plot one example
### 4.  Define Placeholder to store data
### 6. Model Run from this session and plot model with t-sne
### 7. Top 10 Prediction Results
###  1.Load data
### 2.Hyper parameter and data Load
### 3.Plot data
### 4.Placeholder data loader
### RNN Regression\n---\n[**Go to Top**](#Tensorflow-Tutorial)\n\n### RNN\n\n* This time we will use **RNN for regression training (Regression).** We will continue **to predict a cos curve using the sin curve we created. Next we will determine the various parameters of the RNN (super-parameters):)**\n \n ### 1. Load Packages
### 2. Data plot
### 2. Data plot
### 3. Define Placeholder
### 5. Final Model Training
"### AutoEncoder\n---\n[**Go to Top**](#Tensorflow-Tutorial)\n\n### AutoEncoder\n\n![](https://cdn-images-1.medium.com/max/2000/1*woWzbXU2bmshM1czEur72g.gif)\n* **Autoencoder** is an **unsupervised learning algorithm** that **uses a backpropagation algorithm** to **make the target value equal to the input value**. as the picture shows:\n\n![](https://cdn-images-1.medium.com/max/1600/1*wr9QeopG3BK4Lz6DGhlqbA.png)\n\n* A **autoencoder is a neural network that has three layers:** an ***input layer, a hidden (encoding) layer, and a decoding layer.*** \n* *The network is trained to reconstruct its inputs, which forces the hidden layer to try to learn good representations of the inputs.*\n* ***An autoencoder neural network is an unsupervised Machine learning algorithm that applies backpropagation, setting the target values to be equal to the inputs. An autoencoder is trained to attempt to copy its input to its output. Internally, it has a hidden layer that describes a code used to represent the input.***\n\n* The autoencoder tries to learn a function hW,b(x)‚âàxhW,b(x)‚âàx. In other words, it is trying to learn an approximation to the identity function, so as to output xÃÇ x^ that is similar to xx.\n* Autoencoders belong to the neural network family, but they are also closely related to PCA (principal components analysis).\n\n#### Some Key Facts about the autoencoder:\n\n* It is an unsupervised ML algorithm similar to PCA\n* It minimizes the same objective function as PCA\n* It is a neural network\n* The neural network‚Äôs target output is its input\n\n* ***Autoencoders although is quite similar to PCA but its Autoencoders are much more flexible than PCA. Autoencoders can represent both liners and non-linar transformation in encoding but PCA can only perform linear transformation. Autoencoders can be layered to form deep learning network due to it‚Äôs Network representation.***\n\nFore More Read this article : [**Autoencoder**](https://www.jeremyjordan.me/autoencoders/)"
### 1.Load data
### 2.Generate dataset
### 3. Plot the data
### 4.Placeholder
### 6.AutoEncoder Training
### Visualozation in 3D Plot
### Visualozation in 3D Plot
"### Generative Adversarial Nets\n\n---\n[**Go to Top**](#Tensorflow-Tutorial)\n\n**1. The Story Behind GAN**\n\n* In the academic world, **GAN founder Ian Goodfellow** discussed academic issues with colleagues after the drunkenness of the bar. At that time, Emmanuel raised the initial **idea of GAN**, but at that time ***he did not get the approval of his colleagues. After returning from the bar, he found that his girlfriend had fallen asleep. Then, I wrote the code day and night, and found that it was really effective, so after some research, GAN was born,*** a mountain work. Attach a photo of the Great God.\n\n![](https://mmbiz.qpic.cn/mmbiz_png/iaTa8ut6HiawCUoIVNsXpWVcLibMiaesQkjxuV9uR0D6XeOpaRbic6AzvDbLloEYYIavMicMYMlLCsic6dIrr7hPicEWoQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1)\n\n**Architecture of GAN**\n\n![](https://mmbiz.qpic.cn/mmbiz_png/iaTa8ut6HiawCUoIVNsXpWVcLibMiaesQkjxuxMTNqrJJy7A9mNicyyGwqWmKJWUseJgBhlNOKBIOc9B3Gr64umFrJA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1)\n\n**2. The principle of GAN:**\n\n* **GAN's main inspiration comes from the idea of zero-sum game in game theory.** When applied to **deep learning neural network, it is through continuous generation of network G (Generator) and discriminant network D (Discriminator), so that G learns the distribution of data.** If the image generation is used, G can generate a realistic image from a random number after the training is completed. The main functions of G, D are:\n\n*  **G** is a ***generative network that receives a random noise z (random number) and generates an image from this noise.***\n* **D** is a ***discriminating network that discriminates whether a picture is ""real"". Its input parameter is x, x represents a picture, and the output D(x) represents the probability that x is a real picture. If it is 1, it means that 100% is the real picture, and the output is 0, it means that it is impossible to be true.\n* **During the training process,** the **goal of generating the network G is to generate a real picture** as much as possible to **deceive the discriminant network D. The goal of D is to identify the false images and real images generated by G as much as possible.** Thus, G and D constitute a dynamic ""gaming process"", and the final equilibrium point is the Nash equilibrium point.\n\n**3. Features of GAN:**\n\n* **Compared to the traditional model, he has two different networks instead of a single network, and the training method uses the confrontation training method.**\n* **The gradient update information of G in GAN comes from discriminator D, not from data sample.**\n\n**4. Advantages of GAN:**\n(The following section is taken from ian goodfellow's Q&A in Quora)\n* **GAN is a generative model that uses only backpropagation compared to other generation models (Boltzmann machines and GSNs) without the need for complex Markov chains.**\n* **GAN** can produce a **clearer, more realistic sample** than all other models\n* **GAN uses an unsupervised learning style training** that can be widely used in unsupervised and semi-supervised learning.\n* Compared to the **variational self-encoder**, **GANs** does not **introduce any deterministic bias, and the variational method introduces deterministic bias** because ***they optimize the lower bound of the log likelihood rather than the likelihood itself, which looks The examples that led to the generation of VAEs are more blurred than GANs***\n* **Compared to VAE, GANs has no variation lower bound.** If the **discriminator** is well trained, the **generator can perfectly learn the distribution of training samples.** In other words, GANs are gradual, but VAE is biased.\n* **GAN is applied to some scenes, such as picture style migration, super resolution, image completion, denoising, avoiding the difficulty of loss function design, regardless of the three seven twenty-one, as long as there is a benchmark, directly on the discriminator,** The rest is handed over to the confrontation training.\n\n**5. The disadvantages of GAN:**\n* **Training GAN needs to reach Nash equilibrium, sometimes it can be done by gradient descent method, sometimes it can't be done. We have not found a good way to reach Nash Equilibrium, so training GAN is unstable compared to VAE or PixelRNN. But I think in practice it is still more stable than training the Boltzmann machine.**\n* **GAN is not suitable for processing discrete forms of data, such as text**\n* **GAN has problems with unstable training, gradient disappearance, and mode collapse (currently resolved)**\n\nGenerally, when the GAN training is unstable, the result is very poor, but it cannot be improved even after the training time is lengthened.\n\n**6.Why is the optimizer in GAN not commonly used for SGD?**\n\n* 1. SGD is easy to oscillate, and it is easy to make GAN training unstable.\n* 2. The purpose of GAN is to find the Nash equilibrium point in the high-dimensional non-convex parameter space. The Nash equilibrium point of GAN is a saddle point, but SGD will only find the local minimum value, because SGD solves the problem of finding the minimum value, GAN It is a game problem.\n\n**7.Why GAN is not suitable for processing text data**\n\n1. Text data is discrete compared to image data, because for text, it is usually necessary to map a word to a high-dimensional vector, and the final predicted output is a one-hot vector, assuming that the output of softmax is ( 0.2, 0.3, 0.1, 0.2, 0.15, 0.05) then become onehot is (0,1,0,0,0,0), if the softmax output is (0.2, 0.25, 0.2, 0.1, 0.15, 0.1), one -hot is still (0, 1, 0, 0, 0, 0), so for the generator, G outputs different results but D gives the same result, and the gradient update information is not very good. Passed to G, so the judgment of the final output of D is meaningless.\n2. In addition, the loss function of GAN is JS divergence, and JS divergence is not suitable for measuring the distance between the distributions that do not want to intersect.\n\n(WGAN uses the wassertein distance instead of the JS divergence, but the ability to generate text is still limited. GAN uses seq-GAN in the generated text, and the product of reinforcement learning)\n\n**8.Some tips for training GAN**\n1. The input is normalized to (-1,1), and the activation function of the last layer uses tanh (except BEGAN)\n2. Using the loss function of wassertein GAN,\n3. If there is tag data, try to use tags. It is also suggested that using reverse tags works well. In addition, tag smoothing, single-sided label smoothing or bilateral label smoothing is used.\n4. Use mini-batch norm, if you don't use batch norm you can use instance norm or weight norm\n5. Avoid using the RELU and pooling layers to reduce the possibility of sparse gradients. You can use the leanrelu activation function.\n6. The optimizer should choose ADAM as much as possible. The learning rate should not be set too large. The initial 1e-4 can be used for reference. In addition, the learning rate can be continuously reduced as the training progresses.\n7. Adding Gaussian noise to the network layer of D is equivalent to a regular"
### 1.Load data
### 2.Plot the data
### 2.Plot the data
### 3.Design Art work
### 5.Model Running
### Notebook is continue updated
"## Introduction\n\n* **Natural Language Processing (NLP):** The discipline of computer science, artificial intelligence and linguistics that is concerned with the creation of computational models that process and understand natural language. These include: making the computer understand the semantic grouping of words (e.g. cat and dog are semantically more similar than cat and spoon), text to speech, language translation and many more\n\n* **Sentiment Analysis:** It is the interpretation and classification of emotions (positive, negative and neutral) within text data using text analysis techniques. Sentiment analysis allows organizations to identify public sentiment towards certain words or topics.\n\nIn this notebook, we'll develop a **Sentiment Analysis model** to categorize a tweet as **Positive or Negative.**\n\n\n## Table of Contents\n1. [Importing dependencies](#p1)\n2. [Importing dataset](#p2)\n3. [Preprocessing Text](#p3)\n4. [Analysing data](#p4)\n5. [Splitting data](#p5)\n6. [TF-IDF Vectoriser](#p6)\n7. [Transforming Dataset](#p7)\n8. [Creating and Evaluating Models](#p8)\n    * [BernoulliNB Model](#p8-1)\n    * [LinearSVC Model](#p8-2)\n    * [Logistic Regression Model](#p8-3)\n9. [Saving the Models](#p9)\n10. [Using the Model](#p10)\n\n## Importing Dependencies"
"## Importing dataset\nThe dataset being used is the **sentiment140 dataset**. It contains 1,600,000 tweets extracted using the **Twitter API**. The tweets have been annotated **(0 = Negative, 4 = Positive)** and they can be used to detect sentiment.\n \n*[The training data isn't perfectly categorised as it has been created by tagging the text according to the emoji present. So, any model built using this dataset may have lower than expected accuracy, since the dataset isn't perfectly categorised.]*\n\n**It contains the following 6 fields:**\n1. **sentiment**: the polarity of the tweet *(0 = negative, 4 = positive)*\n2. **ids**: The id of the tweet *(2087)*\n3. **date**: the date of the tweet *(Sat May 16 23:58:44 UTC 2009)*\n4. **flag**: The query (lyx). If there is no query, then this value is NO_QUERY.\n5. **user**: the user that tweeted *(robotickilldozr)*\n6. **text**: the text of the tweet *(Lyx is cool)*\n\nWe require only the **sentiment** and **text** fields, so we discard the rest.\n\nFurthermore, we're changing the **sentiment** field so that it has new values to reflect the sentiment. **(0 = Negative, 1 = Positive)**"
"## Importing dataset\nThe dataset being used is the **sentiment140 dataset**. It contains 1,600,000 tweets extracted using the **Twitter API**. The tweets have been annotated **(0 = Negative, 4 = Positive)** and they can be used to detect sentiment.\n \n*[The training data isn't perfectly categorised as it has been created by tagging the text according to the emoji present. So, any model built using this dataset may have lower than expected accuracy, since the dataset isn't perfectly categorised.]*\n\n**It contains the following 6 fields:**\n1. **sentiment**: the polarity of the tweet *(0 = negative, 4 = positive)*\n2. **ids**: The id of the tweet *(2087)*\n3. **date**: the date of the tweet *(Sat May 16 23:58:44 UTC 2009)*\n4. **flag**: The query (lyx). If there is no query, then this value is NO_QUERY.\n5. **user**: the user that tweeted *(robotickilldozr)*\n6. **text**: the text of the tweet *(Lyx is cool)*\n\nWe require only the **sentiment** and **text** fields, so we discard the rest.\n\nFurthermore, we're changing the **sentiment** field so that it has new values to reflect the sentiment. **(0 = Negative, 1 = Positive)**"
"## Preprocess Text\n**Text Preprocessing** is traditionally an important step for **Natural Language Processing (NLP)** tasks. It transforms text into a more digestible form so that machine learning algorithms can perform better.\n\n**The Preprocessing steps taken are:**\n1. **Lower Casing:** Each text is converted to lowercase.\n2. **Replacing URLs:** Links starting with **""http"" or ""https"" or ""www""** are replaced by **""URL""**.\n3. **Replacing Emojis:** Replace emojis by using a pre-defined dictionary containing emojis along with their meaning. *(eg: "":)"" to ""EMOJIsmile"")*\n4. **Replacing Usernames:** Replace @Usernames with word **""USER""**. *(eg: ""@Kaggle"" to ""USER"")*\n5. **Removing Non-Alphabets:** Replacing characters except Digits and Alphabets with a space.\n6. **Removing Consecutive letters:** 3 or more consecutive letters are replaced by 2 letters. *(eg: ""Heyyyy"" to ""Heyy"")*\n7. **Removing Short Words:** Words with length less than 2 are removed.\n8. **Removing Stopwords:** Stopwords are the English words which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence. *(eg: ""the"", ""he"", ""have"")*\n9. **Lemmatizing:** Lemmatization is the process of converting a word to its base form. *(e.g: ‚ÄúGreat‚Äù to ‚ÄúGood‚Äù)*"
### Word-Cloud for Negative tweets.
### Word-Cloud for Positive tweets.
### Word-Cloud for Positive tweets.
## Splitting the Data\nThe Preprocessed Data is divided into 2 sets of data: \n* **Training Data:** The dataset upon which the model would be trained on. Contains 95% data.\n* **Test Data:** The dataset upon which the model would be tested against. Contains 5% data.
### Evaluate Model Function
### BernoulliNB Model
"So we've **gained some understanding** on the distributiona of our numeric variables, **but we can add more information** to this plot. \n\nLet's see how the distribution of our numeric variables is different for **those that have strokes, and those that do not.**\n\nThis could be important for modelling later on"
"# Insight\n\nBased on the above plots, it seems clear that **Age is a big factor** in stroke patients - the older you get the more at risk you are.\n\nThough less obvious, there are also differences in Avg. Glucose Levels and BMI.\n\nLet's explore those variables further..."
"As we suspected, Age is a big factor, and also has slight relationships with BMI & Avg. Glucose levels.\n\nWe might understand intuitively that **as Age increases, the risk of having a stroke increases too, but can ve visualise this?**"
"This confirms what our intuitions told us. The older you get, the more at risk you get.\n\nHowever, you may have notices the low risk values on the y-axis. This is because the **dataset is highly imbalanced**.\n\nOnly 249 strokes are in our dataset which totals 5000 - around 1 in 20. "
"This confirms what our intuitions told us. The older you get, the more at risk you get.\n\nHowever, you may have notices the low risk values on the y-axis. This is because the **dataset is highly imbalanced**.\n\nOnly 249 strokes are in our dataset which totals 5000 - around 1 in 20. "
"This needs to be considered when modelling of course, but also when formulating risk.\n\nStrokes are still relatively rare, we are not saying anything is guaranteed, just that risk is increasing."
"# General Overview\n\nWe've assessed a few variables so far, and gained some powerful insights. \n\nI'll now plot several variables in one place, so we can spot interesting trends or features.\n\nI will split the data in to 'Stroke' and 'No-Stroke' so we can see if these two populations differ in any meaningful way."
"# Insights\n\nThe plots above are quite enlightening.\n\nAs discussed earlier, we again note the importance of Age, amongst other things."
"One can see how, by manipulating the threshold, we can catch more strokes. \n\nHowever, one needs to be careful with this approach. We could just change the threshold such that every patient is predicted to have a stroke so as not to miss any - but this helps no one.\n\nThe art is in finding the balance between 'hits' and 'misses'. \n\nF1 score is a decent starting point for this as it is the weighted average of several metrics.\n\nHere's a chart showing what I mean"
"# For completeness, I will try to optimize SVM also"
"# Model by Model Confusion Matrix\n\nNow we have selected our models, we can view how they performed in each prediction.\n\nA great way to visualise where your data performs well, and where it performs poorly.\n\n\n\n"
"# Model Success\n\nSo all of our models have quite a high accuracy, the highest being 95% (Tuned Random Forest).\n\nBut the recall of Strokes is quite poor across the board.\n\nResults always need to be considered carefully - ask yourself: 'Why do I need to predict this value?'\nIn our case, I would assume it would be to offer medical advice / preventative treatment to those we predict will have a stroke, therefore, in the real-world, I would probably select the model with the highest recall.\n\n\nThe model's can be considered a success - that is, healthcare professionals would be better equipped with this model than without it.\n\n\nSeeing as Random Forest did have the highest accuracy, I will delve deeper in to the model and how it works - woth feature importance & LIME.\n\nHowever, the actual selection of model would be up for debate due to the recall variance. \n\n\n\n # Which model would you choose?\n \n I am curious to hear your opinions."
"# Selection\n\nI would opt for Logistic Regression. \n\nIt Has a decent accuracy, and the best recall. I feel that on balance it provides the best overall results."
"# Model Interpretation\n\nI'll use some valuable tools which help to uncover the supposed ""Black Box"" of machine learning algorithms.\n\nAs I always say, the models we create need to be sold to business stakeholders. And if business stakeholders don't understand what we're creating, they may be less likely to back the project."
Let's plot this with the top 3 features highlighted
# We can also use SHAP\n\nSHAP Values (SHapley Additive exPlanations) break down a prediction to show the impact of each feature. \n\nIt interprets the impact of having a certain value for a given feature in comparison to the prediction we'd make if that feature took some baseline value (e.g. zero)\n\n\nIn this case I will use it for the Random Forest Model. It can be used for any type of model but it is by far the quickest with tree based models.\n\nIt is possible to change the colour values in SHAP plots too\n
"# SHAP explained\n\nThe plot above shows the effext of each data point on our predictions. \n\nFor example, for age, the top left point reduced the prediction by 0.6.\n\nThe color shows whether that feature was high or low for that row of the dataset\nHorizontal location shows whether the effect of that value caused a higher or lower prediction.\n\n\nWe can also see how our Random Forest Model is heavily skewed in favour of predicting no-strokes.\n\n# There's more? SHAP dependence plots\n\nWe can also focus on how the impact of each variable changes, as the variable itself changes.\n\nFor instance, Age. When this variable increases, the SHAP value also increases - pushing the patient closer to  our 1 condition (stroke). \n\nThis is also shown with colour -  pink/red representing those who suffered a stroke."
"The same plot, but with a more interesting varibale. \n\nHere we see a clear cutoff point for when strokes become far more common - after a BMI of around 30 or so.\n\nSuch is the power of SHAP visualization."
"The same plot, but with a more interesting varibale. \n\nHere we see a clear cutoff point for when strokes become far more common - after a BMI of around 30 or so.\n\nSuch is the power of SHAP visualization."
"# One-Step Further: Logistic Regression with LIME\n\nWhen it comes to model interpretation, sometimes it is useful to unpack and focus on one example at a time.\n\nThe LIME package enables just that.\n\nLime stands for Local Interpretable Model-agnostic Explanations - here's an example:"
"### Visualizing masks (using matplotlib)\n\nGiven that the masks are just integer matrices, you can also use other packages to display the masks. For example, using matplotlib and a custom color map we can quickly visualize the different cancer regions:"
"### Overlaying masks on the slides\n\nAs the masks have the same dimension as the slides, we can overlay the masks on the tissue to directly see which areas are cancerous. This overlay can help you identifying the different growth patterns. To do this, we load both the mask and the biopsy and merge them using PIL.\n\n**Tip:** Want to view the slides in a more interactive way? Using a WSI viewer you can interactively view the slides. Examples of open source viewers that can open the PANDA dataset are [ASAP](https://github.com/computationalpathologygroup/ASAP) and [QuPath](https://qupath.github.io/). ASAP can also overlay the masks on top of the images using the ""Overlay"" functionality. If you use Qupath, and the images do not load, try changing the file extension to `.vtif`."
"## Loading image regions\n\nSimilar to OpenSlide, we can extract regions from the whole image. Because the image is already in memory, this boils down to a slice on the numpy array. To illustrate we use the same coordinates as in the OpenSlide example:"
"To load the same region from level 1, we have to devide the coordinates with the downsample factor (4 per level). This is different from Openslide that always works with coordinates from level 0."
\n\n\n\n0¬†¬†IMPORTS¬†¬†¬†¬†‚§í
\n\n\n\n1¬†¬†BACKGROUND INFORMATION¬†¬†¬†¬†‚§í\n\n---\n
"\n\n4.2 EXPLORE THE `PROTEIN_SEQUENCE` COLUMN\n\n---\n\n\n\nHost Description\n* Amino acid sequence of each protein variant. \n* The stability (as measured by $t_m$) of protein is determined by its protein sequence. \n\n\n\n---\n\n\n\nObservations\n* The frequency of some amino acids respective to the length of the AA sequence has a low, but non-zero correlation with the target $t_m$ (melting point)\n    * The fractional frequency appears slightly more informative\n* Some amino acid sequences are REALLY long... especially considering all of our test AA sequences are relatively short ~240 vs more than 32,000\n* While some amino acids are more common than others, all are relatively common (Sequences on average contain 1.5% to 10% of each amino acid respectively).\n\n\n\n---\n\n"
\n\n4.3 EXPLORE THE `PH` COLUMN\n\n---\n\n\n\nHost Description\n* The scale used to specify the acidity of an aqueous solution under which the stability of protein was measured.\n* Stability of the same protein can change at different pH levels.\n\n\n\n---\n\n\n\nObservations\n* All training pH values fall within the range from 0.0-11.0 \n\n\n\n---\n\n
\n\n4.3 EXPLORE THE `PH` COLUMN\n\n---\n\n\n\nHost Description\n* The scale used to specify the acidity of an aqueous solution under which the stability of protein was measured.\n* Stability of the same protein can change at different pH levels.\n\n\n\n---\n\n\n\nObservations\n* All training pH values fall within the range from 0.0-11.0 \n\n\n\n---\n\n
\n\n4.4 EXPLORE THE `DATA_SOURCE` COLUMN\n\n---\n\n\n\nHost Description\n* Source where the data was published\n\n\n\n---\n\n\n\nObservations\n* High cardinality\n* All data source is Novozymes in test dataset... so maybe useless?\n\n\n\n---\n\n
"\n\n4.5 EXPLORE THE `TM` COLUMN\n\n---\n\n\n\nHost Description\n* The target column. \n* Since only the Spearman Correlation Coefficient will be used for the evaluation, the correct prediction of the relative order is more important than the absolute tm values.\n    * Higher tm means the protein variant is more stable.\n\n\n\n\n---\n\n\n\nObservations\n* -1 to 130\n* Float\n\n\n\n---\n\n"
\n\n\n\n\n\n\n    5¬†¬†TEST DATASET EXPLORATION¬†¬†¬†¬†‚§í\n\n\n---\n\n\n\n**GENERAL OBSERVATIONS**\n* fadgda
"So, for 23 series let's create a 6 by 4 grid which will be resulted in 24 slots and fill it with the plot of our series."
It seems like there are pretty much similar time series such as ```MRTSSM44000USS``` and ```RETAILMSA``` or ```MRTSSM7221USN``` and ```MRTSSM44611USN```.
It seems like there are pretty much similar time series such as ```MRTSSM44000USS``` and ```RETAILMSA``` or ```MRTSSM7221USN``` and ```MRTSSM44611USN```.
## 2. 2. Preprocessing\n\nBefore we start analyzing let's check if our data is uniform in length.
"After handling missing values, the other issue is the scale of the series. Without, normalizing data the series that looks like each other will be seen so different from each other and will affect the accuracy of the clustering process. We can see the effect of the normalizing in the following images.\n\n"
"Note that we normalized each time series by their own values, not the values of other time series."
"## 2. 3. Clustering\n\nI will be using 2 different methods for clustering these series. The first of the methods is Self Organizing Maps(SOM) and the other method is K-Means.\n\n### 2. 3. 1. SOM\n \nSelf-organizing maps are a type of neural network that is trained using unsupervised learning to produce a low-dimensional representation of the input space of the training samples, called a map.\n\n![SOM](https://raw.githubusercontent.com/izzettunc/Kohonen-SOM/master/data/screenshots/landing.png)\nSource : Github Repo: landing.png\n    \nAlso, self-organizing maps  differ from other artificial neural networks as they apply competitive(or cooperative) learning as opposed to error-correction learning (such as backpropagation with gradient descent), and in the sense that they use a neighborhood function to preserve the topological properties of the input space.\n\n![Learning process of som](https://upload.wikimedia.org/wikipedia/commons/3/35/TrainSOM.gif)\nSource : Wiki Commons: TrainSOM.gif\n\nBecause of the ability to produce a map, som deemed as a method to do dimensionality reduction. But in our case, when each node of the som is accepted as medoids of the cluster, we can use it for clustering. To do so, we should remove our time indices from our time series, and instead of measured values of each date, we should accept them as different features and dimensions of a single data point.\n\nFor more info about some, you can check [this medium post](https://medium.com/@abhinavr8/self-organizing-maps-ff5853a118d4)."
"For the implementation of the som algorithm I used [miniSom](https://github.com/JustGlowing/minisom) and set my parameters as follows:\n- sigma: 0.3\n- learning_rate: 0.5\n- random weight initialization\n- 50.000 iteration\n- Map size: square root of the number of series\n\nAs a side note, I didn't optimize these parameters due to the simplicity of the dataset."
#### 2. 3. 1. 2. Cluster Distribution\nWe can see the distribution of the time series in clusters in the following chart.
"#### 2. 3. 1. 3. Cluster Mapping\n\n(Thank you for this wonderful question Stephen Tseng)\nWell, we did cluster our series but how de we know which series belonging to which cluster? Ain't that the whole purpose of clustering? \n\nAs we can see in [these illustrations](#2.-3.-1.-SOM) each node (or multiple of nodes in some cases) represents a cluster. Therefore we can find out which series is belonging to which cluster by checking the winner node of each series. "
"#### 2. 3. 2. 1. Results\n\nAfter the training, I plotted the results as I did with the som. For each cluster, I plotted every series, a little bit transparent and in gray, and in order to see the movement or the shape of the cluster, I took the average of the cluster and plotted that averaged series in red."
"As you can see from the plot below, k-means clustered the 23 different series into 5 clusters. 2 of the clusters contains only 1 time series which may be deemed as an outlier."
"As you can see from the plot below, k-means clustered the 23 different series into 5 clusters. 2 of the clusters contains only 1 time series which may be deemed as an outlier."
"As I did before, I used [DBA](https://github.com/fpetitjean/DBA) to see much more time dilated series."
"As I did before, I used [DBA](https://github.com/fpetitjean/DBA) to see much more time dilated series."
"#### 2. 3. 2. 2. Cluster Distribution\n\nWe can see the distribution of the time series in clusters in the following chart. And it seems like k-means clustered 15 of the time series as cluster 1, which is a bit skewed. The reason why this happens is the most probably ```The Curse of Dimentionality``` You can check it out from the links that I provided at section 5 (See Also)\n"
"#### 2. 3. 2. 2. Cluster Distribution\n\nWe can see the distribution of the time series in clusters in the following chart. And it seems like k-means clustered 15 of the time series as cluster 1, which is a bit skewed. The reason why this happens is the most probably ```The Curse of Dimentionality``` You can check it out from the links that I provided at section 5 (See Also)\n"
"#### 2. 3. 2. 3. Cluster Mapping\n\nAs we did before, in this part we will be finding which series belonging to which cluster. Thanks to awesome scikit-learn library we actually already have that information. Order of the labels is the same order with our series.\n"
"Now with less dimension than before, we can see how our series distributed in 2 dimensions."
"The result of PCA is basically, representation of a 333-dimensional data point as a 2-dimensional data point. As a result of that instead of a time series, we have just 2 value for each series."
"And this is the result of the basic KMeans, pretty logical and straight forward."
"And again thanks to the clever implementation of ```KMeans``` algorithm by ```sklearn``` team, labels are returned in the same order. Thus, we can use the same code to visualize our cluster in series."
"And again thanks to the clever implementation of ```KMeans``` algorithm by ```sklearn``` team, labels are returned in the same order. Thus, we can use the same code to visualize our cluster in series."
"And we can see that now with the ```PCA``` algorithm, our series are much more equally distributed to clusters than before."
\n\n\n\n4¬†¬†IMPORTS¬†¬†¬†¬†‚§í
\n\n\n\n5¬†¬†SETUP AND HELPER FUNCTIONS¬†¬†¬†¬†‚§í
"\n\n5.3 SHOW OFF THE SENSOR ARRAY AS A 3D SCATTER PLOT\n\n---\n\nWe can easily plot the sensor array as we are provided with the coordinates for each sensor as (x,y,z) points. Note that, the **azimuth and zenith** angles are determined with respect to a spherical coordinate system centered within the sensor array."
\n\n5.4 CHECK DATA ACCESS BY EXAMINING THE PROVIDED ILLUSTRATION\n\n---\n\nLet's find and plot this particular example:\n\n\n\n
### 4.2 Summary Statistics:\n\nSpark DataFrames include some built-in functions for statistical processing. The describe() function performs summary statistics calculations on all numeric columns and returns them as a DataFrame.
Look at the minimum and maximum values of all the (numerical) attributes. We see that multiple attributes have a wide range of values: we will need to normalize your dataset.
\nFare Column 
We can see that lot of zero values are there in Fare column so we will replace zero values with mean value of Fare column later. 
We can see that lot of zero values are there in Fare column so we will replace zero values with mean value of Fare column later. 
\nSibSp Column 
That's where we are wrong. You can't trust feature importances from XGBoost because they are inconsistent across different calculations. Watch how feature importances change with the calculation type:
"In contrast, feature importances obtained from Shapley values are consistent and trustworthy.\n\nWe won't also stop here. In the above plots, we only looked at absolute values of importance. We don't know which feature positively or negatively influences the model. Let's do that with SHAP `summary_plot`:"
"Data Science in 2021 : Adaptation or Adoption?\n\nUnderstanding the Current State of AI / Data Science Adoption\n\nThe *COVID-19* pandemic transformed the way organizations work, collaborate, and operate. In just a few months, the crisis brought about years of change in the ways different companies, sectors, and regions do business. In many industries, the changes were damaging, while in others, they were promising and beneficial. The healthcare/pharma sector, for example, became aggressive, supply chains witnessed major disruptions, and the e-commerce sector boomed up. On the other hand, the local business, travel, and tourism industries were critically affected.\n\nFrom a technology perspective, organizations felt a need for more stabilization and more innovation in which the *adoption of* / *adapting towards* Data Science and Artificial Intelligence played an important role. To stay competitive, ambitious, and operate just like normal times, organizations were required to make strategic shifts, alter their long-term views and develop a data-first / technology-first mindset. Usually, when organizations need to adjust their technology landscape, they follow one of the following two types of approaches:\n\n""The Adapt Approach"" or ""The Adopt Approach""\n\n- The Adapt approach is a conventional idea where companies need to continuously modify their core technology (including the data and the models) to meet the business objectives. This approach only offers a short-term solution which is also inefficient, expensive, and innovation-killing.\n\n- The Adopt Approach enables the organizations to incorporate long-term solutions with best practices and design principles. Companies need to invest in building specialized teams and start adopting a data-first / technology-first culture. \n\nThe more incorporation of the ""adoption approach"" can assist the organizations to accelerate their innovation. Several studies conducted in the pandemic year suggest that digital adoption has taken a quantum leap and the Covid-19 crisis has accelerated the adoption of analytics and AI, This momentum is likely to continue in the 2020s. \n\nBut a vital question that needs to be answered is: Are the organizations and governments ready for a data-first culture and AI Adoption? \n\nAs part of our regular job as customer-facing data scientists, we interact with several organizations - across different countries and multiple industries. This involves companies of varying sizes, different revenues, and different data maturity levels. Some are at a very early stage, some are highly advanced. A common observation is that ""many organizations are still at very early stages in their AI adoption"".  If we have to categorize them in different groups, it can be analogous to  stages of plant growth : \n\n\n\n\nThis scale ranks the entities (organizations, industries, or countries) at different levels of data science and artificial intelligence adoption. \n\n\n \n     Seedling \n     Vegetative \n     Budding \n     Flowering \n     Ripening \n \n\n    Stages\n    Roots Development\n    Stems Growth\n    Plants Formation\n    Flowers/Fruits\n    More Nutrition\n\n\n    Sector / Country\n    Lack of goodof Data\n    Building a data culuture\n    Supporting the Startups with Talent, Fundings\n    Major Investmentsin AI Initiatives\n    Qualified ProfessionalsBetter JobsEducation Degrees\n\n\n    Organizations\n    Establishing the needof Data / AI\n    Focus on data procurement\n    Minimal analysis,and basic modelling\n    Use of data, predictive modelling,and machine learning\n    Models in Production  with Greater ROI\n\n\n\n\n2021 Kaggle Machine Learning & Data Science Survey is the most comprehensive dataset available on the state of ML and data science. By blending this data with several external datasets, we have analyzed the state of data science and artificial intelligence adoption in 2021. We utilized some of the questions directly related to this topic such as: \n\n- Q23: Does your current employer incorporate machine learning methods into their business?\n- Q22: How many individuals are responsible for data science workloads at your place of business?\n- Q26: How much money have you or your team spent on machine learning/data science?  \n\nand many indirect questions that reflect the state of AI/data science: \n\n- Q16-19: Do you use ML, NLP, Computer Vision, etc.?  \n- Q33: Which big data products (relational database, data warehouse, data lake, or similar) do you use?\n- Q38: Do you use tools to help manage machine learning experiments?\n\n\nMethodology\n\nWe aggregated the survey dataset by regions and industries. We then measured the mean, count, diversity among the following: education qualifications, job roles, data science techniques, cloud usage, machine learning platforms usage, data team size, incorporation of machine learning by employers, coding and machine learning experiences, big data tools, auto ml tools, etc. Using the aggregated information, we defined a unified data science adoption index that reflects the current state of maturity levels. We used the simple mean aggregation to derive the adoption index. We then visualized the adoption index, current machine learning incorporation levels by various internal and external data attributes such as - Global innovation index, competitive data science rankings, Industry revenues, job postings by countries, etc. This analysis helped us to identify which groups are leading while which ones are lagging. \n\n**Validation:** To make sure that our methodology, insights, and findings are aligned with industry trends, we manually cross-checked a number of online references. This includes online reports, articles, and surveys published by organizations such as IBM, Mckinsey, Dataiku, Stanford, World Intellectual Property Organization, Element AI, etc. The references are added in the last section.  \n\n**Visual Theme:** We developed a common visualization theme of the plots - HotSpot charts (bubble charts with hotness as the color theme), where something higher in quantity is colored as hot (red), and something lesser is colored as cold (blue). We selected the seed-plant growth stages analogy to reflect different stages of adoption levels. \n\n**External Datasets Used:**\n- **[Kaggle Analytics 2021 Survey Data](https://www.kaggle.com/c/kaggle-survey-2021):** We used the entire data of kaggle survey respondents but removed the 'Students', 'Currently Not Employed', from industries - 'Other' and 'Academics/Education', from countries - 'Other', 'I do not wish to disclose my location. For some parts of the analysis, we also removed responses 'I do not know' where respondents were asked about their current employer AI / ML maturity. \n\n- **[Kaggle Analytics 2019 Survey](https://www.kaggle.com/c/kaggle-survey-2019):** We selected the Kaggle dataset from the year 2019 to compare a few metrics from pre-pandemic year to post-pandemic year. Most of the questions from this data were similar to the 2021 dataset so it was easier to compare. \n\n- **[Data Science Jobs, Revenues by Sectors, Countries](https://www.kaggle.com/shivamb/glassdoor-jobs-data):** This data contains data science/machine learning / AI-related jobs counts from Glassdoor and Linkedin. The Glassdoor data contains information about the companies such as - Revenues, Sectors, and Linkedin Data contains the count of jobs by country. We developed a scraper to obtain Glassdoor jobs data, while we manually obtained the counts of job counts by country from Linkedin. \n\n- **[Tech Fundings Since 2020](https://www.kaggle.com/shivamb/tech-company-fundings-2020-onwards):** We used the data of fundings in the technology sector by countries, by sectors, and time. The data is from 2020 and was obtained from GrowthList. The data also contains a specific subset of artificial intelligence fundings.  \n\n- **[Competitive Data Science by Countries](https://www.kaggle.com/hdsk38/comp-top-1000-data):** This is the dataset of top 1000 users in the four types of Kaggle rankings (i.e. Competition, Datasets, Notebooks, and Discussions) as of October 2021. The dataset was prepared by [tropicbird](https://www.kaggle.com/hdsk38) obtaining the kaggle rankings and identifying the locations of the users. \n\n- **[Global Innovation Index 2021](https://www.wipo.int/edocs/pubdocs/en/wipo_pub_gii_2021.pdf):** The Global innovation index (GII) is computed by World Intellectual Property Organization. The GII's overall formula for measuring an economy's innovative capacity and output provides clarity for decision-makers in government, business, and elsewhere as they look forward to creating policies that enable their people to invent and create more efficiently.\n\n\n1. How does AI Adoption looks like in 2021 ?"
"‚óè ¬†¬† Israel with 21% has the highest proportion of models in production for more than 2 years. \n\nThe data shows that about 24% have well-established machine learning methods. Some characteristics of people from this group are:  \n- more in technology and engineering focussed job roles (36% Data Scientists, 12% ML Engineers) than the managerial jobs (4% Managers)\n- more coding experiences (5-10 Years: 23%, 3-5 Years: 20%)\n- larger data science teams (20+ team size: 55%)\n\n**Israel** (with 21% of organizations having models in production for 2 years and more), is no stranger to innovation and is widely acknowledged as a startup nation. Studies suggest that Israel has exceptional levels of time and effort on building a ripe environment in which new tech companies can flourish. Israel Government has played an important role, they even stated in 2019, ""We are now making a national plan to make Israel, within five years, one of the top five leading countries in the world in artificial intelligence technology,"", It seems that they are right on the track in 2021 and going ahead. \n\n‚óè ¬†¬† 45% Organizations have not started exploring data science, machine learning or are in the very early stages of exploring\n\n45% is a big number given that it is 2021. Among this group, 38% are from middle-east countries: 'Saudi Arabia', and 'Iraq'. Saudi Arabia Government have started making major efforts in improving their Data Science adoption, they recently launched the development of [data science consortium in next decade](https://www.c4isrnet.com/artificial-intelligence/2021/01/15/saudi-arabia-makes-artificial-intelligence-a-cornerstone-of-its-2030-vision/). So we should expect to see this number improve in the coming years. 25% from this group are from Belarus which is a surprise addition in this group, as most of the other European countries are relatively mature in this field. In general, this 45% has: \n\n- respondents with either **no formal education past high school (about 14%)** or they choose not to answer this question (10%)\n- 1 in every 5 individuals (20%) never written code\n\nThe lower adoption of AI and data science can be due to multiple factors. An AI startup called DLabs.AI from Europe compiled the biggest challenges cited during AI / ML adoption. These included: \n\n1. Skillsets, Data, Use-Cases\n2. Understand the Need for AI \n3. Failing to Explain how AI solutions work \n4. Lack of Collaboration between AI teams \n5. Regulations, Management Issues, Complexity\n\n‚óè ¬†¬† 35% from Retail, Online Services, and Insurance sectors have well established data science \n\n- These three sectors especially 'Insurance' have shown a significant improvement in AI adoption over the last couple of years. \n\n2. The Industry Outlook of Data Science Adoption"
"‚óè ¬†¬† Israel with 21% has the highest proportion of models in production for more than 2 years. \n\nThe data shows that about 24% have well-established machine learning methods. Some characteristics of people from this group are:  \n- more in technology and engineering focussed job roles (36% Data Scientists, 12% ML Engineers) than the managerial jobs (4% Managers)\n- more coding experiences (5-10 Years: 23%, 3-5 Years: 20%)\n- larger data science teams (20+ team size: 55%)\n\n**Israel** (with 21% of organizations having models in production for 2 years and more), is no stranger to innovation and is widely acknowledged as a startup nation. Studies suggest that Israel has exceptional levels of time and effort on building a ripe environment in which new tech companies can flourish. Israel Government has played an important role, they even stated in 2019, ""We are now making a national plan to make Israel, within five years, one of the top five leading countries in the world in artificial intelligence technology,"", It seems that they are right on the track in 2021 and going ahead. \n\n‚óè ¬†¬† 45% Organizations have not started exploring data science, machine learning or are in the very early stages of exploring\n\n45% is a big number given that it is 2021. Among this group, 38% are from middle-east countries: 'Saudi Arabia', and 'Iraq'. Saudi Arabia Government have started making major efforts in improving their Data Science adoption, they recently launched the development of [data science consortium in next decade](https://www.c4isrnet.com/artificial-intelligence/2021/01/15/saudi-arabia-makes-artificial-intelligence-a-cornerstone-of-its-2030-vision/). So we should expect to see this number improve in the coming years. 25% from this group are from Belarus which is a surprise addition in this group, as most of the other European countries are relatively mature in this field. In general, this 45% has: \n\n- respondents with either **no formal education past high school (about 14%)** or they choose not to answer this question (10%)\n- 1 in every 5 individuals (20%) never written code\n\nThe lower adoption of AI and data science can be due to multiple factors. An AI startup called DLabs.AI from Europe compiled the biggest challenges cited during AI / ML adoption. These included: \n\n1. Skillsets, Data, Use-Cases\n2. Understand the Need for AI \n3. Failing to Explain how AI solutions work \n4. Lack of Collaboration between AI teams \n5. Regulations, Management Issues, Complexity\n\n‚óè ¬†¬† 35% from Retail, Online Services, and Insurance sectors have well established data science \n\n- These three sectors especially 'Insurance' have shown a significant improvement in AI adoption over the last couple of years. \n\n2. The Industry Outlook of Data Science Adoption"
"\n\n‚óè ¬†¬† 2 out of 5 respondents from Governments or Public Services shared that they have not even started any machine learning or data science \n\n\n\nA key reason for the lower AI adoption among governments is the established set of practices and processes. In governments, there can be less encouragement for employees to take risks and innovate. In private sectors, employers tend to put a strong focus on experimentation, innovation, and growth. These numbers clearly show a big scope of improvements in the AI and data science vertical. Since all the governments have access to huge amounts of data, they have a very high potential of using data science in a meaningful way. Among this group: \n\n- Approx 50% of respondents had less than or equal to 3 years of coding experience  \n- 40% shared that there is no data science team, and the other 40% shared max 1 or 2 people are handling the data science workloads\n- About 85% has less than 2 years of machine learning experience; among then 27% had never used machine learning \n\n\nGartner has identified that the most common use case of data science and AI among Governments is the chatbots to assist customer communications.   \n\n\n\n\n\nBut are these so-called productized/deployed chatbots Mature? Are they good enough to serve the common public? **Maybe not!** \n\nA funny incident occurred last month (October 2021) in Singapore, where a chatbot named Jamie of Singapore's Ministry of Health responded with the highly misaligned reply to a Covid-19 Question. A person asked about covid-19 precautions but the generated response was about safe sex practices. It became a comedy meme on social media and the government was criticized, Ultimately they had to shut down Jamie. [Source - Channel News Asia - Singapore MOH ChatBot Funny Response](https://www.channelnewsasia.com/singapore/moh-ask-jamie-covid-19-query-social-media-2222571). \n\nFollowing example shows that Governments not only need a strong focus on data science but also need strong governance and thought leadership. \n\n\n‚óè ¬†¬† Insurance Industry: 1/3rd participants suggested that their current ML landscape are advanced (more models in the production)\n\n\nAn interesting find in the analysis shows the better adoption of data science in the insurance industry. One possible intution is that this group contains more actuaries, statisticians, and data scientists. Typical productionable use cases in InsurTech includes - claims forecasting and prediction, underwriting risk assessment, improved customer service and experience, customer acquisition, retention, and churn prediction. Also interesting to note that 64% respondents from Insurance sector have higher education and 44% are in the Data Science roles. \n\n\n\n‚óè ¬†¬† Online Services / Internet Based Business have significantly higher proportion of more AI / ML adoption\n\n\n- More than **25%** respondents mentioned that they or their teams have spent more than **USD100,000** on ML / Cloud\n- Close to **50%** from this group have larger Data Science / AI teams with more than 20 Individuals part of it. \n- **One-Third** are from larger Multi-National Corporations (with 10K+ employees)\n- Usage of **Transformer Networks (BERT, GPT-3, etc)** is the highest for Online Services / Internet Based Business: About **15.73%**\n\n3. AI Adoption Index - A Unified Index using Kaggle Survey Data\n\nSo far we have explored only the question about the extent of machine learning incorporated by the organizations (Q23). In the Kaggle Survey Data 2021, there is a bunch of other useful and valuable information that reflects the state of data science, machine learning, and artificial intelligence adoption in different groups. There are questions regarding the skills of the people, the technology they use, the job roles they have, and their educational qualifications, etc. All these aspects are the essential elements that define how mature is the adoption of AI in a country/sector. A standard hypothesis regarding a country having more AI data science adoption and maturity can be due to a combination of:  \n- multiple job openings in a country related to data science\n- many highly qualified individuals in a country (with masters, PHDs)\n- several people/organizations using cloud architectures, machine learning management platforms, machine learning tools\n- more fundings, investments in the technology sector \n- more competitive data science professionals (example - kagglers in top 1000)\n- more organizations with models in production, the larger data science team size, and spend considerate amounts on data related workflows\n- strong focus on innovation, research, and development  \n\nUsing the Kaggle Analytics survey data of 2021, we tried to map the responses of individuals into different categories to derive a standard adoption index. \n\n| | Component | Question | What it means | \n| --- | ---- | ---- | ---- | \n| 1 | Strategy | Q5: Job Roles, Q22: Team Size, Q26: Money Spent | Organizational Strategies - hiring more diverse job roles ie. a mix of engineers, analysts, scientists, managers, etc.,spending more in building data infrastructures and data science teams  |\n| 2 | Data | Q32, Q33 Data Tools Usage | Organizations using more data tools, databases, data lakes are already in better data / ai adoption stages  |\n| 3 | Technology | Q38, Q23: Current ML Adoption, Tools Used | How advanced is current technology stack  |\n| 4 | People | Q4, Q5: Roles, Qualification | Are there enough qualified people, enough people with relevant job titles  |\n| 5 | Governance | Q31, Q37: Platforms Enabling Governance | Use of Saas platforms to enable Governance - Explainability, Regulations, Controlled Access  |\n| 6 | Automation | Q36, Q37: AutoML Platforms, AutoFE  | Organizations focussing on productivity and automating mundane tasks   |"
"\n\n‚óè ¬†¬† 2 out of 5 respondents from Governments or Public Services shared that they have not even started any machine learning or data science \n\n\n\nA key reason for the lower AI adoption among governments is the established set of practices and processes. In governments, there can be less encouragement for employees to take risks and innovate. In private sectors, employers tend to put a strong focus on experimentation, innovation, and growth. These numbers clearly show a big scope of improvements in the AI and data science vertical. Since all the governments have access to huge amounts of data, they have a very high potential of using data science in a meaningful way. Among this group: \n\n- Approx 50% of respondents had less than or equal to 3 years of coding experience  \n- 40% shared that there is no data science team, and the other 40% shared max 1 or 2 people are handling the data science workloads\n- About 85% has less than 2 years of machine learning experience; among then 27% had never used machine learning \n\n\nGartner has identified that the most common use case of data science and AI among Governments is the chatbots to assist customer communications.   \n\n\n\n\n\nBut are these so-called productized/deployed chatbots Mature? Are they good enough to serve the common public? **Maybe not!** \n\nA funny incident occurred last month (October 2021) in Singapore, where a chatbot named Jamie of Singapore's Ministry of Health responded with the highly misaligned reply to a Covid-19 Question. A person asked about covid-19 precautions but the generated response was about safe sex practices. It became a comedy meme on social media and the government was criticized, Ultimately they had to shut down Jamie. [Source - Channel News Asia - Singapore MOH ChatBot Funny Response](https://www.channelnewsasia.com/singapore/moh-ask-jamie-covid-19-query-social-media-2222571). \n\nFollowing example shows that Governments not only need a strong focus on data science but also need strong governance and thought leadership. \n\n\n‚óè ¬†¬† Insurance Industry: 1/3rd participants suggested that their current ML landscape are advanced (more models in the production)\n\n\nAn interesting find in the analysis shows the better adoption of data science in the insurance industry. One possible intution is that this group contains more actuaries, statisticians, and data scientists. Typical productionable use cases in InsurTech includes - claims forecasting and prediction, underwriting risk assessment, improved customer service and experience, customer acquisition, retention, and churn prediction. Also interesting to note that 64% respondents from Insurance sector have higher education and 44% are in the Data Science roles. \n\n\n\n‚óè ¬†¬† Online Services / Internet Based Business have significantly higher proportion of more AI / ML adoption\n\n\n- More than **25%** respondents mentioned that they or their teams have spent more than **USD100,000** on ML / Cloud\n- Close to **50%** from this group have larger Data Science / AI teams with more than 20 Individuals part of it. \n- **One-Third** are from larger Multi-National Corporations (with 10K+ employees)\n- Usage of **Transformer Networks (BERT, GPT-3, etc)** is the highest for Online Services / Internet Based Business: About **15.73%**\n\n3. AI Adoption Index - A Unified Index using Kaggle Survey Data\n\nSo far we have explored only the question about the extent of machine learning incorporated by the organizations (Q23). In the Kaggle Survey Data 2021, there is a bunch of other useful and valuable information that reflects the state of data science, machine learning, and artificial intelligence adoption in different groups. There are questions regarding the skills of the people, the technology they use, the job roles they have, and their educational qualifications, etc. All these aspects are the essential elements that define how mature is the adoption of AI in a country/sector. A standard hypothesis regarding a country having more AI data science adoption and maturity can be due to a combination of:  \n- multiple job openings in a country related to data science\n- many highly qualified individuals in a country (with masters, PHDs)\n- several people/organizations using cloud architectures, machine learning management platforms, machine learning tools\n- more fundings, investments in the technology sector \n- more competitive data science professionals (example - kagglers in top 1000)\n- more organizations with models in production, the larger data science team size, and spend considerate amounts on data related workflows\n- strong focus on innovation, research, and development  \n\nUsing the Kaggle Analytics survey data of 2021, we tried to map the responses of individuals into different categories to derive a standard adoption index. \n\n| | Component | Question | What it means | \n| --- | ---- | ---- | ---- | \n| 1 | Strategy | Q5: Job Roles, Q22: Team Size, Q26: Money Spent | Organizational Strategies - hiring more diverse job roles ie. a mix of engineers, analysts, scientists, managers, etc.,spending more in building data infrastructures and data science teams  |\n| 2 | Data | Q32, Q33 Data Tools Usage | Organizations using more data tools, databases, data lakes are already in better data / ai adoption stages  |\n| 3 | Technology | Q38, Q23: Current ML Adoption, Tools Used | How advanced is current technology stack  |\n| 4 | People | Q4, Q5: Roles, Qualification | Are there enough qualified people, enough people with relevant job titles  |\n| 5 | Governance | Q31, Q37: Platforms Enabling Governance | Use of Saas platforms to enable Governance - Explainability, Regulations, Controlled Access  |\n| 6 | Automation | Q36, Q37: AutoML Platforms, AutoFE  | Organizations focussing on productivity and automating mundane tasks   |"
" Source : DataIku - Key Elements of Enterprise AI  \n\n These components align with several other interesting metrics and indexes developed by organizations. Using the kaggle data, we developed the index in the following manner: \n    \n\nFirst, aggregate the kaggle data by different groups (country/sector), Perform ordinal encoding of questions such as current levels of ML maturity, data science team sizes, current ML spendings. Then measure the counts and mean of tools/technologies used in a group, for example - ml tools used, data tools used, auto ml tools used, technologies used. Also measure the mean and standard deviation of groups such as job roles, education backgrounds, coding experiences, and ML experiences. Finally, the measures are combined and normalized to get a unified index, called the AI adoption index. This index is a mix of people, tools, skills, qualifications, maturity, data, and governance. \n\n \n\n4. Industry Rankings using AI Adoption Index\n\nLet's identify the industries which rank higher in the derived AI adoption index. And also how it is related to educational qualifications within the industry. "
" Source : DataIku - Key Elements of Enterprise AI  \n\n These components align with several other interesting metrics and indexes developed by organizations. Using the kaggle data, we developed the index in the following manner: \n    \n\nFirst, aggregate the kaggle data by different groups (country/sector), Perform ordinal encoding of questions such as current levels of ML maturity, data science team sizes, current ML spendings. Then measure the counts and mean of tools/technologies used in a group, for example - ml tools used, data tools used, auto ml tools used, technologies used. Also measure the mean and standard deviation of groups such as job roles, education backgrounds, coding experiences, and ML experiences. Finally, the measures are combined and normalized to get a unified index, called the AI adoption index. This index is a mix of people, tools, skills, qualifications, maturity, data, and governance. \n\n \n\n4. Industry Rankings using AI Adoption Index\n\nLet's identify the industries which rank higher in the derived AI adoption index. And also how it is related to educational qualifications within the industry. "
"\n\n‚óè ¬†¬† Manufacturing Sector shows a huge scope of AI adoption going forward while Insurance Sector Leads \n\n\n\nReports suggest that the global market for manufacturing is expected to grow from USD 86.7B in 2020 to USD 117.7B by 2025. These numbers are significant and data science can play a tremendous role in this. However, despite the big market size, the current adoption levels of data science are relatively low. A key reason for the slow adoption is the complexity of edge deployments which allows processing and inferring the data locally on the equipment or machines. Deployments at the edge are not an easy/straightforward task. It requires models to be capable of low latency and high-frequency scoring and can process large chunks of data in near-real-time. In this sector the respondents were: \n\n- Majority (18%) are from Japan, which is a hub for manufacturing giants \n- Majority shared that their day to day job role includes: Analyzing of Understanding data to influence product or business decisions\n- 12% have been the adopters of Google Cloud AutoML \n    \nBroadcasting/Communication sector ranked 4th among the 16 scored industries in this dataset. With an increased focus on hyper-personalization and an increasing number of data-centric use-cases like ""advanced search"", ""content monetization and moderation"", ""auto caption, and sub-title generation"" becoming common, the industry is growing well in AI adoption. Most numbers of the respondents from the broadcasting sector are from India (14%) where the AI companies struggle to serve a billion-plus population. 34% of respondents shared that at least 20 people are responsible for data science workloads in their teams.   \n\nMedical/Pharma sector and Military/Defense sector have the highest percentage of qualified professionals (PHDs, Doctorates, Master's). Military sector had 53% with master's degree while medical had 47%. Both of these sectors especially are doing much better than public services or governments. \n\nThe general insights of the AI adoption index align with the machine learning incorporation by organizations (as asked in Q23). All the computers, technology or internet related sectors leads while Governments, Non-Profits lags. The data shows that these sectors have the least spendings on data science workloads. The insurance sector shows the highest levels of AI adoption, they also have the highest usage of machine learning platforms such as - Sagemaker from Amazon, or DataBricks. This particular analysis is interesting since the market size and the average revenues of the Manufacturing Sector are very high, still, AI adoption remains low. In the next section, we focussed on the analysis of sector-wise revenues. \n\n5. Industries (with Data Science Jobs): Revenues and Adoption \n\n**External Dataset Used:** [Data Science Jobs by Sectors, Countries (2021)](https://www.kaggle.com/shivamb/glassdoor-jobs-data)\n\nWe analyzed an important metric - the average revenue of a sector especially those which are adopting data science or have data science departments. To obtain this data and information, we procured the data about data science-related job roles from Glassdoor and Linkedin along with meta information. The meta-information includes the average revenue of the company, sector, country, and company size. The initial data was very messy and contained text in different languages, we cleaned it manually, used text processing, and translated it all to English. After the preprocessing, we got about 10000 job listings with proper metadata.  "
"\n\n‚óè ¬†¬† Manufacturing Sector shows a huge scope of AI adoption going forward while Insurance Sector Leads \n\n\n\nReports suggest that the global market for manufacturing is expected to grow from USD 86.7B in 2020 to USD 117.7B by 2025. These numbers are significant and data science can play a tremendous role in this. However, despite the big market size, the current adoption levels of data science are relatively low. A key reason for the slow adoption is the complexity of edge deployments which allows processing and inferring the data locally on the equipment or machines. Deployments at the edge are not an easy/straightforward task. It requires models to be capable of low latency and high-frequency scoring and can process large chunks of data in near-real-time. In this sector the respondents were: \n\n- Majority (18%) are from Japan, which is a hub for manufacturing giants \n- Majority shared that their day to day job role includes: Analyzing of Understanding data to influence product or business decisions\n- 12% have been the adopters of Google Cloud AutoML \n    \nBroadcasting/Communication sector ranked 4th among the 16 scored industries in this dataset. With an increased focus on hyper-personalization and an increasing number of data-centric use-cases like ""advanced search"", ""content monetization and moderation"", ""auto caption, and sub-title generation"" becoming common, the industry is growing well in AI adoption. Most numbers of the respondents from the broadcasting sector are from India (14%) where the AI companies struggle to serve a billion-plus population. 34% of respondents shared that at least 20 people are responsible for data science workloads in their teams.   \n\nMedical/Pharma sector and Military/Defense sector have the highest percentage of qualified professionals (PHDs, Doctorates, Master's). Military sector had 53% with master's degree while medical had 47%. Both of these sectors especially are doing much better than public services or governments. \n\nThe general insights of the AI adoption index align with the machine learning incorporation by organizations (as asked in Q23). All the computers, technology or internet related sectors leads while Governments, Non-Profits lags. The data shows that these sectors have the least spendings on data science workloads. The insurance sector shows the highest levels of AI adoption, they also have the highest usage of machine learning platforms such as - Sagemaker from Amazon, or DataBricks. This particular analysis is interesting since the market size and the average revenues of the Manufacturing Sector are very high, still, AI adoption remains low. In the next section, we focussed on the analysis of sector-wise revenues. \n\n5. Industries (with Data Science Jobs): Revenues and Adoption \n\n**External Dataset Used:** [Data Science Jobs by Sectors, Countries (2021)](https://www.kaggle.com/shivamb/glassdoor-jobs-data)\n\nWe analyzed an important metric - the average revenue of a sector especially those which are adopting data science or have data science departments. To obtain this data and information, we procured the data about data science-related job roles from Glassdoor and Linkedin along with meta information. The meta-information includes the average revenue of the company, sector, country, and company size. The initial data was very messy and contained text in different languages, we cleaned it manually, used text processing, and translated it all to English. After the preprocessing, we got about 10000 job listings with proper metadata.  "
"\n\n‚óè ¬†¬† Shipping, Energy, Manufacturing and Retail sectors are ranked much higher in average revenues but AI adoption is relatively low\n\n\n\nAccounting/Finance or Military/Defence sectors have higher percentage of companies with more than USD 10B+ USD, but their current AI / data science maturity levels are lower than some of the other sectors such as Insurance. The bigger size of the bubbles shows that there are multiple data science job opportunities in the organizations with higher revenue. The analysis shows the industries/sectors which are leading, such as - Online related, Insurance, Computers and the ones which are lagging such as - Governments, NonProfits, Manufacturing. Hospitality/Entertainment/Sports have also higher revenue but lower percentage of people having machine learning experiences. About 48% shared their current ML experience is less than a year. \n\n6. A Geographical Perspective of AI Adoption \n\nRespondents were asked about their country/region in the Kaggle Survey. There were respondents from 66 different countries, including all parts of the world - Americas, Asia, Europe, Africa, the Middle East, and Australias. \n\nLet's take a look at how different ""Countries"" stand at different levels for machine learning incorporation. The following visualization lists down the top 20 countries with the most kaggle survey respondents. The y-axis is measured directly using the question (Q23) asked - Do your employers incorporate machine learning methods. The size and the color represent the percentage of respondents from that country who mentioned how much ML is incorporated in their organizations. "
"\n\n‚óè ¬†¬† Shipping, Energy, Manufacturing and Retail sectors are ranked much higher in average revenues but AI adoption is relatively low\n\n\n\nAccounting/Finance or Military/Defence sectors have higher percentage of companies with more than USD 10B+ USD, but their current AI / data science maturity levels are lower than some of the other sectors such as Insurance. The bigger size of the bubbles shows that there are multiple data science job opportunities in the organizations with higher revenue. The analysis shows the industries/sectors which are leading, such as - Online related, Insurance, Computers and the ones which are lagging such as - Governments, NonProfits, Manufacturing. Hospitality/Entertainment/Sports have also higher revenue but lower percentage of people having machine learning experiences. About 48% shared their current ML experience is less than a year. \n\n6. A Geographical Perspective of AI Adoption \n\nRespondents were asked about their country/region in the Kaggle Survey. There were respondents from 66 different countries, including all parts of the world - Americas, Asia, Europe, Africa, the Middle East, and Australias. \n\nLet's take a look at how different ""Countries"" stand at different levels for machine learning incorporation. The following visualization lists down the top 20 countries with the most kaggle survey respondents. The y-axis is measured directly using the question (Q23) asked - Do your employers incorporate machine learning methods. The size and the color represent the percentage of respondents from that country who mentioned how much ML is incorporated in their organizations. "
"\n\n‚óè ¬†¬† Asia is lagging, Europe is leading, Africa,Middle East are growing\n\n\n\n\n- The most number of countries from Asia which includes China, Bangladesh, India, Indonesia, Pakistan, South Korea, and Taiwan have comparatively lower levels of machine learning incorporation. On an average, close to 30% of respondents from these countries suggested that they are just exploring machine learning methods, the other 25% suggested they have not even started any machine learning in their organizations. In total, about 60% suggested that they are just venturing into the machine learning space. Europe on the other hand, shows several countries having well-established methods in the organizations. \n\n- African and Middle East countries are showing up more in recent years. Though about 45% from Nigera said they have not yet started any machine learning, The growth of AI and data-related startups in countries like Nigeria have increased and they are improving in the overall machine learning. Due to the latest developments (such as - technology adoption, government policies relaxation, improvements in education) in the continent, Many companies are looking at Africa as the next place for expansion and investment. A [report](https://ircai.org/the-growth-of-artificial-intelligence-in-africa-on-diversity-and-representation/), by Internation Research Center of AI (IRCAI), suggested that use AI and related technologies as an opportunity to create and reinforce diversity is the main focus area for companies operating in the African region. This also includes a focus on the skills development of diverse people and to make concerted efforts at leveling the playing field for women and other minorities in the industry.\n\nIn recent years, Government efforts to improve the AI landscape have improved across the globe and it is encouraging to see the countries realizing the true potential of AI and data science. However, The coronavirus pandemic affected all sectors of the economy in 2020, and the AI sector is no different. \n\n7. Pandemic Effect\n\nAs the organizations worldwide manage ongoing pandemic effects and hold a slowly evolving post-pandemic future, a key question became important: Just how much did COVID impact the AI industry? \n\nSeveral AI startups emerged during this period that assisted the impacted industries, including accelerating the Covid-19 Drug Discovery, facilitating the supply chains, and optimizing the logistics during lockdowns. Many organizations had to reprioritize their hiring and operational strategies. The virtual formats of meetings, conferences, and meetups lead to significant spikes in attendance too. There were a lot of layoffs too, companies had to cut down their budgets. The following chart shows the change in percentage change of machine learning incorporation by country before the pandemic and after."
"\n\n‚óè ¬†¬† Asia is lagging, Europe is leading, Africa,Middle East are growing\n\n\n\n\n- The most number of countries from Asia which includes China, Bangladesh, India, Indonesia, Pakistan, South Korea, and Taiwan have comparatively lower levels of machine learning incorporation. On an average, close to 30% of respondents from these countries suggested that they are just exploring machine learning methods, the other 25% suggested they have not even started any machine learning in their organizations. In total, about 60% suggested that they are just venturing into the machine learning space. Europe on the other hand, shows several countries having well-established methods in the organizations. \n\n- African and Middle East countries are showing up more in recent years. Though about 45% from Nigera said they have not yet started any machine learning, The growth of AI and data-related startups in countries like Nigeria have increased and they are improving in the overall machine learning. Due to the latest developments (such as - technology adoption, government policies relaxation, improvements in education) in the continent, Many companies are looking at Africa as the next place for expansion and investment. A [report](https://ircai.org/the-growth-of-artificial-intelligence-in-africa-on-diversity-and-representation/), by Internation Research Center of AI (IRCAI), suggested that use AI and related technologies as an opportunity to create and reinforce diversity is the main focus area for companies operating in the African region. This also includes a focus on the skills development of diverse people and to make concerted efforts at leveling the playing field for women and other minorities in the industry.\n\nIn recent years, Government efforts to improve the AI landscape have improved across the globe and it is encouraging to see the countries realizing the true potential of AI and data science. However, The coronavirus pandemic affected all sectors of the economy in 2020, and the AI sector is no different. \n\n7. Pandemic Effect\n\nAs the organizations worldwide manage ongoing pandemic effects and hold a slowly evolving post-pandemic future, a key question became important: Just how much did COVID impact the AI industry? \n\nSeveral AI startups emerged during this period that assisted the impacted industries, including accelerating the Covid-19 Drug Discovery, facilitating the supply chains, and optimizing the logistics during lockdowns. Many organizations had to reprioritize their hiring and operational strategies. The virtual formats of meetings, conferences, and meetups lead to significant spikes in attendance too. There were a lot of layoffs too, companies had to cut down their budgets. The following chart shows the change in percentage change of machine learning incorporation by country before the pandemic and after."
"Many countries saw an overall improvement machine learning adoption and incorporation in the organizations. The positive increase is encouraging to see even during the pandemic. Israel, Norway, UK, Australia, and Poland have the highest percentage of change since 2021. Though some countries observed a decline, this includes - Turkey, Greece, Romania, Ireland, and the Netherlands. Varying levels of AI adoption in different regions can be linked to several factors such as:\n\n 1. Investments \n 2. Jobs\n 3. Research\n 4. Skills\n\n- Investments: AI / Tech Company Fundings  \n- Jobs: AI Related Hiring / Jobs \n- Research: Global Innovation Index   \n- Competitive Data Science: More skilled data scientists \n\nThere could be more factors such as - Education or Government Regulations. In this notebook, we have only explored the above listed factors by using different external datasets and relate them with the AI adoption index using the kaggle survey data. We will use the same methodology to derive the AI adoption index as described in section 3. \n\n8. Investments/Tech Fundings and AI Adoption \n\n**External Dataset:** [Tech Fundings Since 2020](https://www.kaggle.com/shivamb/tech-company-fundings-2020-onwards)\n\nThis external dataset contains funding information in the technology sector by country, by sector, by date. Original source of the data is GrowthList and the cleaned data is posted on Kaggle. The data contains funding information since January 2020 and clearly shows that more dollars are flowing into the AI sector. "
"Many countries saw an overall improvement machine learning adoption and incorporation in the organizations. The positive increase is encouraging to see even during the pandemic. Israel, Norway, UK, Australia, and Poland have the highest percentage of change since 2021. Though some countries observed a decline, this includes - Turkey, Greece, Romania, Ireland, and the Netherlands. Varying levels of AI adoption in different regions can be linked to several factors such as:\n\n 1. Investments \n 2. Jobs\n 3. Research\n 4. Skills\n\n- Investments: AI / Tech Company Fundings  \n- Jobs: AI Related Hiring / Jobs \n- Research: Global Innovation Index   \n- Competitive Data Science: More skilled data scientists \n\nThere could be more factors such as - Education or Government Regulations. In this notebook, we have only explored the above listed factors by using different external datasets and relate them with the AI adoption index using the kaggle survey data. We will use the same methodology to derive the AI adoption index as described in section 3. \n\n8. Investments/Tech Fundings and AI Adoption \n\n**External Dataset:** [Tech Fundings Since 2020](https://www.kaggle.com/shivamb/tech-company-fundings-2020-onwards)\n\nThis external dataset contains funding information in the technology sector by country, by sector, by date. Original source of the data is GrowthList and the cleaned data is posted on Kaggle. The data contains funding information since January 2020 and clearly shows that more dollars are flowing into the AI sector. "
"Interesting to see that few countries with a low AI adoption index have had more technology fundings in recent years. \n\nThis includes countries such as - Indonesia where the data science adoption remains at the exploration stage. However, there a number of tech companies which are making big impacts, example - GoJek, a logistic compnay who recently bagged the funding of USD 1.2B to scale up their technology, AI, and data science operations. Another popular rising startup is the payment platform provider Xendit, which rased USD 150M to incoportate ML and AI driven systems in thier core processes. Other countries include - Belgium, Mexico, and Denmark with more recent fundings in the technology sector. \n\nNigeria is an important regional player in Africa, making about half of West Africa‚Äôs population with 202M people and one of the largest youth populations in the world. It has recently become a center of attraction for technology and AI investors. This year SoftBank Vision Fund 2 led a USD 400M funding round for **OPay**, valuing the Nigerian mobile-payments platform at USD 2B and marking the investment vehicle‚Äôs first bet in Africa. The other investments in Nigeria included - **FairMoney (USD 42M)** which offers digital banking and instant loans providing collateral-free personal loans. \n\n\n9. Jobs in AI vs Adoption \n\nWhich countries are hiring in the data-related / AI-related roles?\n\n**External Data Used:** [Linkedin Data Science Jobs in 2021 by Country](https://www.kaggle.com/shivamb/glassdoor-jobs-data)\n\nThe AI industry witnessed strong hiring growth during the pandemic. While health care, finance, and other service industries have been strong early adopters of AI, manufacturing, retail, and other sectors are expected to grow in hiring and AI-skill penetration as well. Let's see what data is talking about. \n"
"Interesting to see that few countries with a low AI adoption index have had more technology fundings in recent years. \n\nThis includes countries such as - Indonesia where the data science adoption remains at the exploration stage. However, there a number of tech companies which are making big impacts, example - GoJek, a logistic compnay who recently bagged the funding of USD 1.2B to scale up their technology, AI, and data science operations. Another popular rising startup is the payment platform provider Xendit, which rased USD 150M to incoportate ML and AI driven systems in thier core processes. Other countries include - Belgium, Mexico, and Denmark with more recent fundings in the technology sector. \n\nNigeria is an important regional player in Africa, making about half of West Africa‚Äôs population with 202M people and one of the largest youth populations in the world. It has recently become a center of attraction for technology and AI investors. This year SoftBank Vision Fund 2 led a USD 400M funding round for **OPay**, valuing the Nigerian mobile-payments platform at USD 2B and marking the investment vehicle‚Äôs first bet in Africa. The other investments in Nigeria included - **FairMoney (USD 42M)** which offers digital banking and instant loans providing collateral-free personal loans. \n\n\n9. Jobs in AI vs Adoption \n\nWhich countries are hiring in the data-related / AI-related roles?\n\n**External Data Used:** [Linkedin Data Science Jobs in 2021 by Country](https://www.kaggle.com/shivamb/glassdoor-jobs-data)\n\nThe AI industry witnessed strong hiring growth during the pandemic. While health care, finance, and other service industries have been strong early adopters of AI, manufacturing, retail, and other sectors are expected to grow in hiring and AI-skill penetration as well. Let's see what data is talking about. \n"
"### 10. Global Innovation Index\n\nThe Global Innovation Index (GII) is developed by World Intellectual Properrty Organization and takes the pulse of the most recent global innovation trends. It ranks the innovation ecosystem performance of economies around the globe each year while highlighting innovation strengths and weaknesses and particular gaps in innovation metrics. The different metrics that the GII offers can be used to monitor performance and benchmark developments against economies within the same region or income group classification. The Global Innovation Index 2021 was released on in September 2021. Switzerland, Sweden, the U.S., and the U.K. continued to lead the innovation ranking, and have all ranked in the top 5 in the past three years. Using this interesting index, we wanted to check if it correlates with our AI adoption index and the relationship between the two. \n\n**External Dataset Used:** **[Global Innovation Index 2021](https://www.wipo.int/edocs/pubdocs/en/wipo_pub_gii_2021.pdf)**"
"### 10. Global Innovation Index\n\nThe Global Innovation Index (GII) is developed by World Intellectual Properrty Organization and takes the pulse of the most recent global innovation trends. It ranks the innovation ecosystem performance of economies around the globe each year while highlighting innovation strengths and weaknesses and particular gaps in innovation metrics. The different metrics that the GII offers can be used to monitor performance and benchmark developments against economies within the same region or income group classification. The Global Innovation Index 2021 was released on in September 2021. Switzerland, Sweden, the U.S., and the U.K. continued to lead the innovation ranking, and have all ranked in the top 5 in the past three years. Using this interesting index, we wanted to check if it correlates with our AI adoption index and the relationship between the two. \n\n**External Dataset Used:** **[Global Innovation Index 2021](https://www.wipo.int/edocs/pubdocs/en/wipo_pub_gii_2021.pdf)**"
"\n\n‚óè ¬†¬† Switzerland - Most Innovative Country, Highest AI Adoption, Most models in Production !! \n\n\n\nIt is interesting to note that the two indexes show a significant positive correlation, and also the winners of both indexes are the same - Switzerland. The swiss nation is pushed to focus on research and development and has invested in the establishment and funding of a high number of world-renowned universities, which in turn attract multinationals seeking highly qualified employees. In terms of the AI adoption index, Switzerland has:  \n\n- Very high usage (75%) of Cloud Platforms (GCP, AWS)\n- Very large number of Master's Degree holders (46%)\n- Very large number of PHDs (26%)\n- Large number of individuals with 10+ years of coding experience (44%)\n- The highest percentage of Organizations with Models in production (49%) \n\nOther European countries had similar insights - Norway, Austria, France, Netherlands, Germany, along with UK and USA. \n\n- United Arab Emirates shows good progress and advancements in terms of both innovation and data science first. More than 50% mentioned they have about 0 - 2 years of the machine learning experience. Mostly mid-sized companies with employee size <250 have been prominent (about 46%). Nepal and Ethiopia are appealing as the innovation index is low but the AI adoption level is better. This is because of the startup culture, startup investments, and people building their data science skills through multiple channels. In these countries, more than 50% of respondents are from non-multinational companies. \n\n\n\n11. Competitive Data Science (Top 1000 Kagglers)\n\nWe looked at countries by the number of kagglers in the top 1000 competition ranks. The idea was to understand the presence of skilled people active in competitive data science (by taking kaggle rankings as the proxy). We understand that identifying the numbers by population can give a relative percentage, but we restricted the analysis to absolute counts, to understand which countries lead and is there any correlation between the two. "
"\n\n‚óè ¬†¬† Switzerland - Most Innovative Country, Highest AI Adoption, Most models in Production !! \n\n\n\nIt is interesting to note that the two indexes show a significant positive correlation, and also the winners of both indexes are the same - Switzerland. The swiss nation is pushed to focus on research and development and has invested in the establishment and funding of a high number of world-renowned universities, which in turn attract multinationals seeking highly qualified employees. In terms of the AI adoption index, Switzerland has:  \n\n- Very high usage (75%) of Cloud Platforms (GCP, AWS)\n- Very large number of Master's Degree holders (46%)\n- Very large number of PHDs (26%)\n- Large number of individuals with 10+ years of coding experience (44%)\n- The highest percentage of Organizations with Models in production (49%) \n\nOther European countries had similar insights - Norway, Austria, France, Netherlands, Germany, along with UK and USA. \n\n- United Arab Emirates shows good progress and advancements in terms of both innovation and data science first. More than 50% mentioned they have about 0 - 2 years of the machine learning experience. Mostly mid-sized companies with employee size <250 have been prominent (about 46%). Nepal and Ethiopia are appealing as the innovation index is low but the AI adoption level is better. This is because of the startup culture, startup investments, and people building their data science skills through multiple channels. In these countries, more than 50% of respondents are from non-multinational companies. \n\n\n\n11. Competitive Data Science (Top 1000 Kagglers)\n\nWe looked at countries by the number of kagglers in the top 1000 competition ranks. The idea was to understand the presence of skilled people active in competitive data science (by taking kaggle rankings as the proxy). We understand that identifying the numbers by population can give a relative percentage, but we restricted the analysis to absolute counts, to understand which countries lead and is there any correlation between the two. "
"Countries like Japan and Russia, despite lower populations than USA and China, have a lot of competitive data scientists or kagglers in the top 1000 in the competition category. It is in this environment that AI Dynamics is finding increased traction in these countries. The government has also played an important role in the improvement of AI adoption over the years. For example, Russia has adopted a new law (the ""AI Law"") establishing a special legal framework for the development and adoption of AI technologies in Moscow for the period from 1 July 2020 to 1 July 2025 (the ""AI Framework""). Enforcing this type of Law straight by the Government shows that many country leaders realize the potential of AI adoption and have to make more efforts to remain competitive. \n\nIt is interesting to note that all the key European countries where AI adoption levels are higher have fewer than 10 kagglers only in the top 1000. Yes! the Country population does matter, but again our aim here is to compare the regions in an absolute sense. This group of countries includes - Norway, Switzerland, Austria, Netherlands. \n\n\n\nConclusion\n\nOur intention for choosing the theme of AI adoption was to understand the current maturity levels of the data science field across industries, sectors, and regions. The chosen theme and the richness of datasets motivated us to create a unified measure (index) that quantifies the state of AI/data science maturity/adoption. This type of index is analogous to the growth stages in plants, where a plant matures through different stages in its lifetime. This can also be measured by the 'hotness' of something, which means something popular has more hotness and something not popular has less hotness. These reasons inspired us to create a HotSpot chart that measures ""hotness"" by color and shows different stages of plants to reflect the current stage. \n\nIn summary, Adopters of AI continue to have confidence in their capabilities to drive value and advantage, they are realizing the competitive advantage and expect AI-powered transformation to happen for both their organization and industry, ultimately for their country. Those following the adaption approach need to seriously consider shifting to adopting side. By following the right practices and aligning them with current trends, current and future AI adopters can place themselves not just to survive but to flourish in the emerging era of pervasive AI.\n\nFrom the overall analysis and our research during this analysis, one fact became evident; Beyond the pandemic, the adoption of data science is anticipated to grow both horizontally and vertically. It is likely to move from fast adopters to less tech-focused industries, functions, and geographies in 2022 and onwards. To keep up to date in this period of change and transformations, organizations and leaders need to think of the right strategies to incorporate a data-first culture. While organizations are aware of the importance of adopting this culture, they often fail to approach it from a strategic standpoint. It is important to understand that doing data science to be cool will be disappointing. One needs to have a proper adoption plan along with a purpose.\n\nTo overcome this, organizations definitely need ***K.A.G.G.L.E.*** \n\nK: Knowledgable and Skilled Workforce (Skills)   \nA: Adoption of a data-first, data-driven culture (Data)      \nG: Growth-Oriented technology (Strategy)     \nG: Governance and Compliance (Governance)   \nL: Leadership that values a change (Strategy)       \nE: Experienced and Educated Professionals (People)\n\n\nReferences\n\n- [1] [SaferBrand Plant Gorwth Stages](https://www.saferbrand.com/articles/plant-growth-stages)\n- [2] [Stanford Human Centered Intelligence](https://hai.stanford.edu/news/how-has-covid-affected-ai-economy)\n- [3] [ElementAI - AI Maturity Framework](https://s3.amazonaws.com/element-ai-website-bucket/AI-Maturity-Framework_White-Paper_EN.pdf)\n- [4] [Oxford Insights AI Readiness Report 2020](https://static1.squarespace.com/static/58b2e92c1e5b6c828058484e/t/5f7747f29ca3c20ecb598f7c/1601653137399/AI+Readiness+Report.pdf)\n- [5] [Sopra Banking Software - Which Approach is best for your business?](https://www.soprabanking.com/insights/adopt-vs-adapt-which-approach-is-best-for-your-business/)\n- [6] [World Global Intellectual Property Organization - Global Innvoation Index 2021](https://www.wipo.int/global_innovation_index/en/2021/)\n- [7] [Dataiku - 5 Elements of Enterprise AI, Insurance Industry](https://www.dataiku.com/stories/how-ai-empowers-the-insurance-industry/)\n- [8] [HBR - AI Adoption in last 18 Months](https://hbr.org/2021/09/ai-adoption-skyrocketed-over-the-last-18-months)\n- [9] [Deloitte AI Adoption Insights](https://www2.deloitte.com/us/en/insights/focus/cognitive-technologies/state-of-ai-and-intelligent-automation-in-business-survey.html)\n- [10] [Tech Fundings in Pandemic](https://theconversation.com/tax-pandemic-profiteering-by-tech-companies-to-help-fund-public-education-155705)\n"
# Import Libraries
# Import Data
Below we are plotting heatmap showing nullity correlation between various columns of dataset.\n\nThe nullity correlation ranges from -1 to 1.\n\n* -1 - Exact Negative correlation represents that if the value of one variable is present then the value of other variables is definitely absent.\n* 0 - No correlation represents that variables values present or absent do not have any effect on one another.\n* 1 - Exact Positive correlation represents that if the value of one variable is present then the value of the other is definitely present.
"#  k-Nearest Neighbour Imputation  \n\n\nA fancy way of filling in the missing values would be to use a **k-nearest neighbour** method. You can select a sample with missing values and find the nearest\nneighbours utilising some kind of distance metric, for example, Euclidean distance. Then you can take the mean of all nearest neighbours and fill up the missing value. You can use the KNN imputer implementation for filling missing values like this.\n\n![image1](https://static.javatpoint.com/tutorial/machine-learning/images/k-nearest-neighbor-algorithm-for-machine-learning2.png)\n\n\n[K-Nearest Neighbors (KNN) Algorithm for Machine Learning](https://medium.com/capital-one-tech/k-nearest-neighbors-knn-algorithm-for-machine-learning-e883219c8f26)\n\n\n### How it works?\n\nStep-1: Select the K number of the neighbors\n        \nlet say we select K = 5\n        \n![image2](https://static.javatpoint.com/tutorial/machine-learning/images/k-nearest-neighbor-algorithm-for-machine-learning3.png) \n\n\nStep-2: Calculate the Euclidean distance of K number of neighbors\n\nIn this step we search for those k = 5 neighbors having minimum Euclidean Distance from unknown data point\n\n\n![Image4](https://static.javatpoint.com/tutorial/machine-learning/images/k-nearest-neighbor-algorithm-for-machine-learning4.png)\n\n        \nStep-3: Among these k neighbors, count the number of the data points in each category.\n\n![image3](https://static.javatpoint.com/tutorial/machine-learning/images/k-nearest-neighbor-algorithm-for-machine-learning5.png)\n\n\nStep-4: Assign the new data points to that category for which the number of the neighbor is maximum.\n\nYou can also visit below given youtube video link to understand it bit nicely\n\n[Step-by-Step procedure of KNN Imputer for imputing missing values](https://www.youtube.com/watch?v=AHBHMQyD75U)"
"# Evaluation Metrics \n\nI think before selecting an optimal model for given data first we have to analayze target feature. Target Feature can be discrete in case of classification problem or continuous in case of Regression Problem\n\nIf we talk briefly about classification problems, the most common metrics used are:\n\n\n- **[Accuracy](https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/)** : It is one of the most straightforward metrics used in machine learning. It defines how accurate your model is. For example, if you build a model that classifies 90 samples accurately, your accuracy is 90% or 0.90. If only 83 samples are classified correctly, the accuracy of your model is 83% or 0.83. Simple.\n             \n     **[Scikit-learn user guide for accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)**\n     \n\n- **[Precision](https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/)** :  Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. The question that this metric answer is of all passengers that labeled as survived, how many actually survived? High precision relates to the low false positive rate. We have got 0.788 precision which is pretty good\n\n     **[True Positives (TP)](https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/)** - These are the correctly predicted positive values which means that the value of actual class is yes and the value of predicted  class is also yes. E.g. if actual class value indicates that this passenger survived and predicted class tells you the same thing.\n\n     **[True Negatives (TN)](https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/)** - These are the correctly predicted negative values which means that the value of actual class is no and value of predicted class is also no. E.g. if actual class says this passenger did not survive and predicted class tells you the same thing.\n\n     False positives and false negatives, these values occur when your actual class contradicts with the predicted class.\n\n     **[False Positives (FP)](https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/)** ‚Äì When actual class is no and predicted class is yes. E.g. if actual class says this passenger did not survive but predicted class tells you that this passenger will survive.\n\n     **[False Negatives (FN)](https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/)** ‚Äì When actual class is yes but predicted class in no. E.g. if actual class value indicates that this passenger survived and predicted class tells you that passenger will die.\n\n     **[Scikit-learn user guide for Precision ](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score)**\n     \n     ![](https://2.bp.blogspot.com/-EvSXDotTOwc/XMfeOGZ-CVI/AAAAAAAAEiE/oePFfvhfOQM11dgRn9FkPxlegCXbgOF4QCLcBGAs/s1600/confusionMatrxiUpdated.jpg)     \n     \n\n- **[Recall(Sensitivity)](https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/)** : Recall is the ratio of correctly predicted positive observations to the all observations in actual class - yes. The question recall answers is: Of all the passengers that truly survived, how many did we label? We have got recall of 0.631 which is good for this model as it‚Äôs above 0.5.\n\n     **[Scikit-learn user guide for Recall](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score)**\n\n\n- **[Confusion Matrix](https://www.geeksforgeeks.org/confusion-matrix-machine-learning/)** : A much better way to evaluate the performance of a classifier is to look at the confusion matrix. The general idea is to count the number of times instances of class A are classified as class B. For example, to know the number of times the classifier confused images of 5s with 3s, you would look in the 5th row and 3rd column of the confusion matrix.\n\n    **[Scikit-Learn user guide for Confusion Matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)**\n     \n     \n- **[F1 score (F1)](https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/)** : F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. Intuitively it is not as easy to understand as accuracy, but F1 is usually more useful than accuracy, especially if you have an uneven class distribution. Accuracy works best if false positives and false negatives have similar cost. If the cost of false positives and false negatives are very different, it‚Äôs better to look at both Precision and Recall. In our case, F1 score is 0.701. \n\n     **[Scikit-learn user guide for F1 score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)**\n\n\n- **[Area under the ROC (Receiver Operating Characteristic) curve](https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/)** : AUC - ROC curve is a performance measurement for the classification problems at various threshold settings. ROC is a probability curve and AUC represents the degree or measure of separability. It tells how much the model is capable of distinguishing between classes. Higher the AUC, the better the model is at predicting 0s as 0s and 1s as 1s. By analogy, the Higher the AUC, the better the model is at distinguishing between patients with the disease and no disease.\n\n     **[Scikit-learn user guide for AUC under the ROC curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)**\n\n\n\n\n\n\n**When it comes to regression, the most commonly used [evaluation metrics](https://towardsdatascience.com/evaluation-metrics-model-selection-in-linear-regression-73c7573208be) are:**\n\n\n- Mean absolute error (MAE)\n- Mean squared error (MSE)\n- Root mean squared error (RMSE)\n- Root mean squared logarithmic error (RMSLE)\n- Mean percentage error (MPE)\n- Mean absolute percentage error (MAPE)\n- R2\n\n **[Scikit-learn user guide for regression evaluation metrics](https://scikit-learn.org/stable/modules/model_evaluation.html)**"
"We see that the target is **skewed** and thus the best metric for this binary classification problem would be Area Under the ROC Curve (AUC). We can use precision and recall too, but AUC combines these two metrics. Thus, we will be using AUC to evaluate the model that we build on this dataset."
It's a high variance problem 
Let's try to increase data in balanced manner using Synthetic Minority Oversampling Technique (SMOTE) 
Let's explore these outliers\n
Edit: Look for fliers in other columns
Edit: Look for fliers in other columns
More filtering of data to try
"This chart does not look linear, or at least the line is not matching the data across the entire x axis. Looks like a drop off for High GrLivArea, seems home buyers are not willing to pay a corresponding amount extra for the large living area, looking for a ""volume discount"" maybe...\nLets look at this closer, first fit a line, next try a polynomial fit to compare"
Need to look at the y_log relationship since that is what we will be predicting in the model (convert back later)
Need to look at the y_log relationship since that is what we will be predicting in the model (convert back later)
Now we look at a polynomial fit
plot the poly fit data
Now lets use a log y scale
**SalePrice** is the variable we need to predict. So let's do some analysis on this variable first.
"The target variable is right skewed.  As (linear) models love normally distributed data , we need to transform this variable and make it more normally distributed.\n"
Cross Correlation chart to view collinearity within the features. Kendall's seems appropriate as the Output Variable is numerical and much of the input is categorical. Here is a chart to guide you to which method to use based on your dataset.\n\n![image.png](attachment:image.png)
Look at some correlation values in a list format
We use the scipy  function boxcox1p which computes the Box-Cox transformation of **\\(1 + x\\)**. \n\nNote that setting \\( \lambda = 0 \\) is equivalent to log1p used above for the target variable.  \n\nSee [this page][1] for more details on Box Cox Transformation as well as [the scipy function's page][2]\n[1]: http://onlinestatbook.com/2/transformations/box-cox.html\n[2]: https://docs.scipy.org/doc/scipy-0.19.0/reference/generated/scipy.special.boxcox1p.html
"I recommend plotting the cumulative sum of eigenvalues (assuming they are in descending order). If you divide each value by the total sum of eigenvalues prior to plotting, then your plot will show the fraction of total variance retained vs. number of eigenvalues. The plot will then provide a good indication of when you hit the point of diminishing returns (i.e., little variance is gained by retaining additional eigenvalues)."
"Do some PCA for the dataset, to remove some of the collinearity. Not sure if this will have any effect as collinearity is usually not detrimental to many or most algorithms"
"We choose number of eigenvalues to calculate based on previous chart, 20 looks like a good number, the chart starts to roll off around 15 and almost hits max a 20. We can also try 30,40 and 50 to squeeze a little more variance out..."
Compare different predictions to each other:\n\nhttps://scikit-learn.org/stable/auto_examples/ensemble/plot_voting_regressor.html
Example plot\n\n![image.png](attachment:image.png)
Show adjustments
"We have bumped up the predictions and they look correct so far, now to verify on the previous chart"
Show new predictions
correlation looks much better at the high end
Show both adjustments
Show new predictions
Final stacked model
distribution of residual errors looks normal except for ET() and SVR()
distribution of residual errors looks normal except for ET() and SVR()
**Create File for Submission to Kaggle**
# **Species Distribution - Whale vs Dolphin**
# **Most Frequently Occuring Individual IDs**
# **Most Frequently Occuring Individual IDs**
# **Dolphin Subspecies Distribution**
# **Dolphin Subspecies Distribution**
# **Whale Subspecies Distribution**
# **Whale Subspecies Distribution**
# **Species distribution by Individual IDs**
# **Species distribution by Individual IDs**
# **Visualizing Whales**
# **Visualizing Whales**
# **Visualizing Dolphins**
# **Visualizing Dolphins**
"\n# **Map individual_id to Images using W&B Tables**\n\n[Source](https://docs.wandb.ai/guides/data-vis/tables)\n\nW&B Tables are used to log and visualize data and model predictions. Interactively explore your data:\n\nüìå Compare changes precisely across models, epochs, or individual examples\n\nüìå Understand higher-level patterns in your data\n\nüìå Capture and communicate your insights with visual samples\n"
"\n# **Map individual_id to Images using W&B Tables**\n\n[Source](https://docs.wandb.ai/guides/data-vis/tables)\n\nW&B Tables are used to log and visualize data and model predictions. Interactively explore your data:\n\nüìå Compare changes precisely across models, epochs, or individual examples\n\nüìå Understand higher-level patterns in your data\n\nüìå Capture and communicate your insights with visual samples\n"
"# **TPU Intialization and Distribution Strategy **\n\n\n![](https://drive.google.com/uc?id=1q6AUi9XZRRWBjov49PSl3thB9idGsUKV)\n\nTensor Processing Units (TPUs) are Google's custom-developed application-specific integrated circuits (ASICs) used to accelerate machine learning workloads.It's easy to run replicated models on Cloud TPU using High-level Tensorflow APIs .\n\n### **Performance**\n\nTPU can achieve a high computational throughput on massive multiplications and additions for neural networks , at blazingly fast speeds with much less power consumption and smaller footprint.\n\nüìå **TPU Initialization:** TPUs  are usually on Cloud TPU workers and hence have to be connected to remote clusters and then initialized .\n\nüìå **Distribution strategies :** A distribution strategy is an abstraction that can be used to drive models on CPU, GPUs or TPUs."
\n\n\n\n0¬†¬†IMPORTS¬†¬†¬†¬†‚§í
\n\n\n\n1¬†¬†BACKGROUND INFORMATION¬†¬†¬†¬†‚§í\n\n---\n
"\n\n4.1 INVESTIGATE THE FREQUENCY OF VARIOUS ORGAN SEGMENTATION EXAMPLES\n\n---\n\nOBSERVATIONS\n\nWe can observe the following distribution:\n* **kidney**\n    * **99** images\n* **prostate**\n    * **93** images\n* **large intestine**\n    * **58** images\n* **spleen**\n    * **53** images\n* **lung**\n    * **48** images\n    \nAll of the organ types are represented fairly well within our data, with **kidney** and **prostate** being about twice as common as the other three organ types: **large intestine**, **spleen**, and **lung**"
"\n\n4.2 INVESTIGATE THE IMAGE SIZES\n\n---\n\nIt's observable that not all images have the same size... however, given that, there is not that much variation between image sizes:\n\nOBSERVATIONS\n* Globally, we can see that most of the images are dominated by a single size $(3000 \times 3000)$\n* All the other sizes (19) only have 1 or 2 occurences each.\n* All image sizes are square\n\nNOTE: THE HOSTS TELL US THAT ALL HPA IMAGES SHOULD BE 3000x3000... HOWEVER, WE CAN SEE THAT ISN'T THE CASE!\n\nOTHER INFORMATION\n\n* Expect roughly 550 images in the hidden test set. \n* All HPA images are 3000 x 3000 pixels with a tissue area within the image around 2500 x 2500 pixels. \n* The Hubmap images range in size from 4500x4500 down to 160x160 pixels.\n\n"
"\n\n4.2 INVESTIGATE THE IMAGE SIZES\n\n---\n\nIt's observable that not all images have the same size... however, given that, there is not that much variation between image sizes:\n\nOBSERVATIONS\n* Globally, we can see that most of the images are dominated by a single size $(3000 \times 3000)$\n* All the other sizes (19) only have 1 or 2 occurences each.\n* All image sizes are square\n\nNOTE: THE HOSTS TELL US THAT ALL HPA IMAGES SHOULD BE 3000x3000... HOWEVER, WE CAN SEE THAT ISN'T THE CASE!\n\nOTHER INFORMATION\n\n* Expect roughly 550 images in the hidden test set. \n* All HPA images are 3000 x 3000 pixels with a tissue area within the image around 2500 x 2500 pixels. \n* The Hubmap images range in size from 4500x4500 down to 160x160 pixels.\n\n"
\n\n4.3 INVESTIGATE AGE\n\n---\n\nOBSERVATIONS\n* The age distribution is skewed to be 50+\n* The distribution of examples with **large intestine** segmented is much higher among the oldest demographic\n* The general distribution of organ segmentation types across age groups appears relatively stratified (with the previously noted exception)\n\n
\n\n4.3 INVESTIGATE AGE\n\n---\n\nOBSERVATIONS\n* The age distribution is skewed to be 50+\n* The distribution of examples with **large intestine** segmented is much higher among the oldest demographic\n* The general distribution of organ segmentation types across age groups appears relatively stratified (with the previously noted exception)\n\n
\n\n4.4 INVESTIGATE SEX\n\n---\n\nOBSERVATIONS\n* Women do not have a prostate and as such there are no prostate examples where gender is Female.\n* Kidney's seem skewed towards the Male gender\n* All other organ's appear evenly distributed\n\n
\n\n4.4 INVESTIGATE SEX\n\n---\n\nOBSERVATIONS\n* Women do not have a prostate and as such there are no prostate examples where gender is Female.\n* Kidney's seem skewed towards the Male gender\n* All other organ's appear evenly distributed\n\n
\n\n4.5 INVESTIGATE FTUS ACROSS DIFFERENT ORGANS\n\n---\n\nHere is a zoom sequence from the human body to the single-cell level for the kidney. \n* **Notice that FTU is one level higher than cells.**\n\n\n\n\n\nFTUs in the organs presented in this competition are: \n* **glomeruli** in the **kidney** ‚Äì (a)\n* **crypt** in the **large intestine**  ‚Äì (b)\n* **alveolus** in the **lung**  ‚Äì (c)\n* **glandular acinus** in the **prostate**  ‚Äì (d)\n* **white pulp** in the **spleen**  ‚Äì (e)\n\n\n\n\n\nOBSERVATIONS\n* TBD
\n\n4.5 INVESTIGATE FTUS ACROSS DIFFERENT ORGANS\n\n---\n\nHere is a zoom sequence from the human body to the single-cell level for the kidney. \n* **Notice that FTU is one level higher than cells.**\n\n\n\n\n\nFTUs in the organs presented in this competition are: \n* **glomeruli** in the **kidney** ‚Äì (a)\n* **crypt** in the **large intestine**  ‚Äì (b)\n* **alveolus** in the **lung**  ‚Äì (c)\n* **glandular acinus** in the **prostate**  ‚Äì (d)\n* **white pulp** in the **spleen**  ‚Äì (e)\n\n\n\n\n\nOBSERVATIONS\n* TBD
\n\n4.6 AVERAGE FTU SIZE (BOUNDING-RECT) BY ORGAN\n\n---\n\nSince it would take too much processing to comb through all the available images and respective FTUs we will look at the previously obtained sampling of crops (bounding-rects) and the respective dimensions therewithin.\n\nAs we know all the HPA images have a pixel resolution (size) of 0.4¬µm we can therefore determine the real dimensions of the bounding rectangles around each FTU.\n\nNote dimensions are represented as (HEIGHTxWIDTH) unless otherwise communicated\n\nOBSERVATIONS\n* **Kidney**\n    * **Actual** FTU size is on-average SQUARE (most images aren't square but it averages as such) at **~157x155¬µm**\n    * **HPA** FTU size in pixels will be **~392x389px**\n    * **HuBMAP** FTU size in pixels will be **~314x310px**\n* **Large Intestine**\n    * **Actual** FTU size is on-average RECTANGULAR (1Hx2W) (images are mostly horizontal but occasionally vertical or square) at **~60x131¬µm**\n    * **HPA** FTU size in pixels will be **~150x328px**\n    * **HuBMAP** FTU size in pixels will be **~262x572px**\n* **Lung**\n    * FTUs are SMALL!\n    * **Actual** FTU size is on-average SQUARE (images aren't square but average to mostly square) at **~90x80¬µm**\n    * **HPA** FTU size in pixels will be **~225x200px**\n    * **HuBMAP** FTU size in pixels will be **~119x106px**\n* **Prostate**\n    * **Actual** FTU size is on-average SQUARE (images aren't square but average to mostly square) at **~68x77¬µm**\n    * **HPA** FTU size in pixels will be **~172x193px**\n    * **HuBMAP** FTU size in pixels will be **~11x13px**\n* **Spleen**\n    * **Actual** FTU size is on-average RECTANGULAR (2Hx1W) at **~407x197¬µm**\n    * **HPA** FTU size in pixels will be **~1018x493px**\n    * **HuBMAP** FTU size in pixels will be **~823x399px**\n\n
"\n\n4.8 WHY DO WE HAVE JSON FILES AND RLE?\n\n---\n\nOBSERVATIONS\n* The RLE is the entire binary mask as a simple compressed string\n* The JSON contains the description of the masks as a collection (list) of polygons representing the respective masks.\n    * A polygon is represented as a collection (list) of vertices.\n    * A vertex is a point represented by a pair of values ‚Äì‚Äì the x,y position\n    * If you took these polygons and plotted them alongside the RLE decoded mask, they SHOULD be the same. I think it's simply another representation for us to use.\n\nNOTE: The RLE is more accurate than the Polygon Mask and should be used.\n\n\n\n"
**VISUALIZE DAB STAIN AND HEMATOXYLIN COUNTERSTAIN**\n* DAB is brown and present at locations of enzymatic activity\n* HEMATOXYLIN is light blue when found within cells\n* HEMATOXYLIN is dark blue when found within nuclei
**VISUALIZE DAB STAIN AND HEMATOXYLIN COUNTERSTAIN**\n* DAB is brown and present at locations of enzymatic activity\n* HEMATOXYLIN is light blue when found within cells\n* HEMATOXYLIN is dark blue when found within nuclei
\n\n\n\n\n\n\n    5¬†¬†MODELLING¬†¬†¬†¬†‚§í\n\n\n---
\n\n5.1 ALL ONES WITH SOME INTELLIGENCE\n\n---\n\nWe simply submit a mask made up of ones in a circular shape in the centre of the respective image.\n\n
\n\n\n\n\n\n\n    9¬†¬†APPENDIX¬†¬†¬†¬†‚§í\n\n\n---\n\nThis section is mostly if not entirely copied from various sources listed below:\n* Yashvardhan Jain's Background Info Notebook
"# Titanic Data Science Solutions\n\n\n### This notebook is a extension to the notebook [titanic data science solutions](https://www.kaggle.com/startupsci/titanic-data-science-solutions). \n\nThe notebook walks us through a typical workflow for solving data science competitions at sites like Kaggle.\n\nThere are several excellent notebooks to study data science competition entries. However many will skip some of the explanation on how the solution is developed as these notebooks are developed by experts for experts. The objective of this notebook is to follow a step-by-step workflow, explaining each step and rationale for every decision we take during solution development.\n\n## Workflow stages\n\nThe competition solution workflow goes through seven stages described in the Data Science Solutions book.\n\n1. Question or problem definition.\n2. Acquire training and testing data.\n3. Wrangle, prepare, cleanse the data.\n4. Analyze, identify patterns, and explore the data.\n5. Model, predict and solve the problem.\n6. Visualize, report, and present the problem solving steps and final solution.\n7. Supply or submit the results.\n\nThe workflow indicates general sequence of how each stage may follow the other. However there are use cases with exceptions.\n\n- We may combine mulitple workflow stages. We may analyze by visualizing data.\n- Perform a stage earlier than indicated. We may analyze data before and after wrangling.\n- Perform a stage multiple times in our workflow. Visualize stage may be used multiple times.\n- Drop a stage altogether. We may not need supply stage to productize or service enable our dataset for a competition.\n\n\n## Question and problem definition\n\nCompetition sites like Kaggle define the problem to solve or questions to ask while providing the datasets for training your data science model and testing the model results against a test dataset. The question or problem definition for Titanic Survival competition is [described here at Kaggle](https://www.kaggle.com/c/titanic).\n\n> Knowing from a training set of samples listing passengers who survived or did not survive the Titanic disaster, can our model determine based on a given test dataset not containing the survival information, if these passengers in the test dataset survived or not.\n\nWe may also want to develop some early understanding about the domain of our problem. This is described on the [Kaggle competition description page here](https://www.kaggle.com/c/titanic). Here are the highlights to note.\n\n- On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. Translated 32% survival rate.\n- One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.\n- Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\n![](https://img.memecdn.com/titanic_fb_1023579.jpg)\n\n## Workflow goals\n\nThe data science solutions workflow solves for seven major goals.\n\n**Classifying.** We may want to classify or categorize our samples. We may also want to understand the implications or correlation of different classes with our solution goal.\n\n**Correlating.** One can approach the problem based on available features within the training dataset. Which features within the dataset contribute significantly to our solution goal? Statistically speaking is there a [correlation](https://en.wikiversity.org/wiki/Correlation) among a feature and solution goal? As the feature values change does the solution state change as well, and visa-versa? This can be tested both for numerical and categorical features in the given dataset. We may also want to determine correlation among features other than survival for subsequent goals and workflow stages. Correlating certain features may help in creating, completing, or correcting features.\n\n**Converting.** For modeling stage, one needs to prepare the data. Depending on the choice of model algorithm one may require all features to be converted to numerical equivalent values. So for instance converting text categorical values to numeric values.\n\n**Completing.** Data preparation may also require us to estimate any missing values within a feature. Model algorithms may work best when there are no missing values.\n\n**Correcting.** We may also analyze the given training dataset for errors or possibly innacurate values within features and try to corrent these values or exclude the samples containing the errors. One way to do this is to detect any outliers among our samples or features. We may also completely discard a feature if it is not contribting to the analysis or may significantly skew the results.\n\n**Creating.** Can we create new features based on an existing feature or a set of features, such that the new feature follows the correlation, conversion, completeness goals.\n\n**Charting.** How to select the right visualization plots and charts depending on nature of the data and the solution goals."
## Acquire data\n\nThe Python Pandas packages helps us work with our datasets. We start by acquiring the training and testing datasets into Pandas DataFrames. We also combine these datasets to run certain operations on both datasets together.
## Analyze by visualizing data\n\nNow we can continue confirming some of our assumptions using visualizations for analyzing the data.\n\n### Correlating numerical features\n\nLet us start by understanding correlations between numerical features and our solution goal (Survived).\n\nA histogram chart is useful for analyzing continous numerical variables like Age where banding or ranges will help identify useful patterns. The histogram can indicate distribution of samples using automatically defined bins or equally ranged bands. This helps us answer questions relating to specific bands (Did infants have better survival rate?)\n\nNote that x-axis in historgram visualizations represents the count of samples or passengers.\n\n**Observations.**\n\n- Infants (Age <=4) had high survival rate.\n- Oldest passengers (Age = 80) survived.\n- Large number of 15-25 year olds did not survive.\n- Most passengers are in 15-35 age range.\n\n**Decisions.**\n\nThis simple analysis confirms our assumptions as decisions for subsequent workflow stages.\n\n- We should consider Age (our assumption classifying #2) in our model training.\n- Complete the Age feature for null values (completing #1).\n- We should band age groups (creating #3).
"### Correlating numerical and ordinal features\n\nWe can combine multiple features for identifying correlations using a single plot. This can be done with numerical and categorical features which have numeric values.\n\n**Observations.**\n\n- Pclass=3 had most passengers, however most did not survive. Confirms our classifying assumption #2.\n- Infant passengers in Pclass=2 and Pclass=3 mostly survived. Further qualifies our classifying assumption #2.\n- Most passengers in Pclass=1 survived. Confirms our classifying assumption #3.\n- Pclass varies in terms of Age distribution of passengers.\n\n**Decisions.**\n\n- Consider Pclass for model training."
"### Correlating numerical and ordinal features\n\nWe can combine multiple features for identifying correlations using a single plot. This can be done with numerical and categorical features which have numeric values.\n\n**Observations.**\n\n- Pclass=3 had most passengers, however most did not survive. Confirms our classifying assumption #2.\n- Infant passengers in Pclass=2 and Pclass=3 mostly survived. Further qualifies our classifying assumption #2.\n- Most passengers in Pclass=1 survived. Confirms our classifying assumption #3.\n- Pclass varies in terms of Age distribution of passengers.\n\n**Decisions.**\n\n- Consider Pclass for model training."
"### Correlating categorical features\n\nNow we can correlate categorical features with our solution goal.\n\n**Observations.**\n\n- Female passengers had much better survival rate than males. Confirms classifying (#1).\n- Exception in Embarked=C where males had higher survival rate. This could be a correlation between Pclass and Embarked and in turn Pclass and Survived, not necessarily direct correlation between Embarked and Survived.\n- Males had better survival rate in Pclass=3 when compared with Pclass=2 for C and Q ports. Completing (#2).\n- Ports of embarkation have varying survival rates for Pclass=3 and among male passengers. Correlating (#1).\n\n**Decisions.**\n\n- Add Sex feature to model training.\n- Complete and add Embarked feature to model training."
"### Correlating categorical features\n\nNow we can correlate categorical features with our solution goal.\n\n**Observations.**\n\n- Female passengers had much better survival rate than males. Confirms classifying (#1).\n- Exception in Embarked=C where males had higher survival rate. This could be a correlation between Pclass and Embarked and in turn Pclass and Survived, not necessarily direct correlation between Embarked and Survived.\n- Males had better survival rate in Pclass=3 when compared with Pclass=2 for C and Q ports. Completing (#2).\n- Ports of embarkation have varying survival rates for Pclass=3 and among male passengers. Correlating (#1).\n\n**Decisions.**\n\n- Add Sex feature to model training.\n- Complete and add Embarked feature to model training."
"### Correlating categorical and numerical features\n\nWe may also want to correlate categorical features (with non-numeric values) and numeric features. We can consider correlating Embarked (Categorical non-numeric), Sex (Categorical non-numeric), Fare (Numeric continuous), with Survived (Categorical numeric).\n\n**Observations.**\n\n- Higher fare paying passengers had better survival. Confirms our assumption for creating (#4) fare ranges.\n- Port of embarkation correlates with survival rates. Confirms correlating (#1) and completing (#2).\n\n**Decisions.**\n\n- Consider banding Fare feature."
"### Correlating categorical and numerical features\n\nWe may also want to correlate categorical features (with non-numeric values) and numeric features. We can consider correlating Embarked (Categorical non-numeric), Sex (Categorical non-numeric), Fare (Numeric continuous), with Survived (Categorical numeric).\n\n**Observations.**\n\n- Higher fare paying passengers had better survival. Confirms our assumption for creating (#4) fare ranges.\n- Port of embarkation correlates with survival rates. Confirms correlating (#1) and completing (#2).\n\n**Decisions.**\n\n- Consider banding Fare feature."
![](https://pics.conservativememes.com/exit-titanic-uextremopolis-the-lceberg-heave-ho-the-ship-is-63789220.png)
"### Completing a numerical continuous feature\n\nNow we should start estimating and completing features with missing or null values. We will first do this for the Age feature.\n\nWe can consider three methods to complete a numerical continuous feature.\n\n1. A simple way is to generate random numbers between mean and [standard deviation](https://en.wikipedia.org/wiki/Standard_deviation).\n\n2. More accurate way of guessing missing values is to use other correlated features. In our case we note correlation among Age, Gender, and Pclass. Guess Age values using [median](https://en.wikipedia.org/wiki/Median) values for Age across sets of Pclass and Gender feature combinations. So, median Age for Pclass=1 and Gender=0, Pclass=1 and Gender=1, and so on...\n\n3. Combine methods 1 and 2. So instead of guessing age values based on median, use random numbers between mean and standard deviation, based on sets of Pclass and Gender combinations.\n\nMethod 1 and 3 will introduce random noise into our models. The results from multiple executions might vary. We will prefer method 2."
Let us start by preparing an empty array to contain guessed Age values based on Pclass x Gender combinations.
Plot to see distribution of 7 different classes of cell type
Its seems from the above plot that in this dataset cell type Melanecytic nevi has very large number of instances in comparison to other cell types
"Plotting of Technical Validation field (ground truth) which is dx_type to see the distribution of its 4 categories which are listed below :\n**1. Histopathology(Histo):**  Histopathologic diagnoses of excised lesions have been\nperformed by specialized dermatopathologists. \n**2. Confocal:** Reflectance confocal microscopy is an in-vivo imaging technique with a resolution at near-cellular level , and some facial benign with a grey-world assumption of all training-set images in Lab-color space before\nand after  manual histogram changes.\n**3. Follow-up:** If nevi monitored by digital dermatoscopy did not show any changes during 3 follow-up visits or 1.5 years biologists  accepted this as evidence of biologic benignity. Only nevi, but no other benign diagnoses were labeled with this type of ground-truth because dermatologists usually do not monitor dermatofibromas, seborrheic keratoses, or vascular lesions. \n**4. Consensus:** For typical benign cases without histopathology or followup biologists  provide an expert-consensus rating of authors PT and HK. They applied the consensus label only if both authors independently gave the same unequivocal benign diagnosis. Lesions with this type of groundtruth were usually photographed for educational reasons and did not need\nfurther follow-up or biopsy for confirmation.\n"
Plotting the distribution of localization field 
Plotting the distribution of localization field 
"It seems back , lower extremity,trunk and upper extremity are heavily compromised regions of skin cancer "
"Now, check the distribution of Age"
It seems that there are larger instances of patients having age from 30 to 60
Lets see the distribution of males and females
Now lets visualize agewise distribution of skin cancer types
Now lets visualize agewise distribution of skin cancer types
"It seems that skin cancer types 0,1, 3 and 5 which are Melanocytic nevi,dermatofibroma,Basal cell carcinoma and Vascular lesions are not much prevalant below the age of 20 years "
Sexwise distribution of skin cancer type
"# Step 6: Loading and resizing of images\nIn this step images will be loaded into the column named image from the image path from the image folder. We also resize the images as the original dimension of images are 450 x 600 x3 which TensorFlow can't handle, so that's why we resize it into 100 x 75. As this step resize all the 10015 images dimensions into 100x 75 so be patient it will take some time."
"Now, lets which category has much incorrect predictions"
"# Conclusion\nIt seems our model has maximum number of incorrect predictions for Basal cell carcinoma which has code 3, then second most missclassified type is Vascular lesions code 5 then Melanocytic nevi code  0 where as Actinic keratoses code 4 has least misclassified type.\n\nWe can also further tune our model to easily achieve the accuracy above 80% and I think still this model is efficient in comparison to detection with human eyes having 77.0344% accuracy \n\nI hope kagglers like my stepwise approach to classify cancer types. If like then kindly dont forget to hit the **like**\n"
"## Loading a bunch of stuff\nImports are from my Jupyter notebooks on my PC, in those notebooks I import 'em all so that later I don't have to bother with importing things."
## Loading datasets
\n# 1. Importing Libraries and Packages\nWe will use these packages to help us manipulate the data and visualize the features/labels as well as measure how well our model performed. Numpy and Pandas are helpful for manipulating the dataframe and its columns and cells. We will use matplotlib along with Seaborn to visualize our data.
"\n# 2. Loading and Viewing Data Set\nWith Pandas, we can load both the training and testing set that we wil later use to train and test our model. Before we begin, we should take a look at our data table to see the values that we'll be working with. We can use the head and describe function to look at some sample data and statistics. We can also look at its keys and column names."
We take a look at the distribution of the Age column to see if it's skewed or symmetrical. This will help us determine what value to replace the NaN values.
"* Looks like the distribution of ages is slightly skewed right. Because of this, we can fill in the null values with the median for the most accuracy."
**Gender**
> Note that the numbers printed above are the proportion of male/female survivors of all the surviviors ONLY. The graph shows the propotion of male/females out of ALL the passengers including those that didn't survive.
Here is one final cumulative graph of a pair plot that shows the relations between all of the different features
"\n# 5. Feature Engineering\nBecause values in the Sex and Embarked columns are categorical values, we have to represent these strings as numerical values in order to perform our classification with our model. We can also do this process through **One-Hot-Encoding**."
First Let's check the impact of feature **Sex** on **Survived**
We can say that Female passangers have higher probability of survival than Male passangers
We can say that Female passangers have higher probability of survival than Male passangers
Ratio of Survived and Not Survived passangers for S and Q Embarked are similar but Passengers from C embarked have higer chances of survival.
Ratio of Survived and Not Survived passangers for S and Q Embarked are similar but Passengers from C embarked have higer chances of survival.
Passengers from **Pclass 3** have lesser chances of Survival while passengers from **Pclass 1** have higher chances of survival
Passengers from **Pclass 3** have lesser chances of Survival while passengers from **Pclass 1** have higher chances of survival
Average Fare for passangers who survived is higher than not survived.
## Visualising the Time Series data\n
There has been a steady increase in Maruti's Stock prices except a slimp around 2019. We shall use Pandas to investigate it further in the coming sections.\n
"# Visualizing the volume weighted average price (VWAP)\n\nWhen working with time-series data, a lot can be revealed through visualizing it. \n\n\n## Visualizing using markers\nIt is possible to add markers in the plot to help emphasize the specific observations or specific events in the time series."
## Visualising using KDEs\n\nSummarizing the data with Density plots to see where the mass of the data is located
## Visualising using KDEs\n\nSummarizing the data with Density plots to see where the mass of the data is located
## Visualising using Lineplots
## Visualising using Lineplots
It appears that Maruti had a more or less steady increase in its stock price over the from 2004 to the mid of 2018 window.There appears to be some drop in 2019 though.let's further analyse the data for the year 2018. 
It appears that Maruti had a more or less steady increase in its stock price over the from 2004 to the mid of 2018 window.There appears to be some drop in 2019 though.let's further analyse the data for the year 2018. 
We see that there was a dip in the stock prices particularly around end of October and November. Let's zoom in on these dates
We see that there was a dip in the stock prices particularly around end of October and November. Let's zoom in on these dates
So there is a dip in stock prices around the last week of october and first week of November. One could investigate it further by finding out if there was some special event that occured on that day.\n
We can also use time sampling to plot charts for specific columns.
"The above bar plot corresponds to Maruti‚Äôs VWAP at year-end for each year in our data set. \n\nSimilarly, year start mean VWAP can be found below. "
"The above bar plot corresponds to Maruti‚Äôs VWAP at year-end for each year in our data set. \n\nSimilarly, year start mean VWAP can be found below. "
# Time Shifting
"\n# Rolling windows\n\nTime series data can be noisy due to high fluctuations in the market. As a result, it becomes difficult to gauge a trend or pattern in the data. Here is a visualization of the Amazon‚Äôs adjusted close price over the years where we can see such noise:  "
"As we‚Äôre looking at daily data, there‚Äôs quite a bit of noise present. It would be nice if we could average this out by a week, which is where a rolling mean comes in. A rolling mean, or moving average, is a transformation method which helps average out noise from data. It works by simply splitting and aggregating the data into windows according to function, such as `mean()`, `median()`, `count()`, etc. For this example, we‚Äôll use a rolling mean for 7 days."
"The first six values have all become blank as there wasn‚Äôt enough data to actually fill them when using a window of seven days.      \n\nSo, what are the key benefits of calculating a moving average or using this rolling mean method? Our data becomes a lot less noisy and more reflective of the trend than the data itself. Let‚Äôs actually plot this out. First, we‚Äôll plot the original data followed by the rolling data for 30 days.     "
"The **blue line** is the original open price data. The **red line represents the 30-day rolling window**, and has less noise than the orange line. Something to keep in mind is that once we run this code, the first 29 days aren‚Äôt going to have the blue line because there wasn‚Äôt enough data to actually calculate that rolling mean."
"### For visualization,before using visual library (matplotlib, seaborn, ..)we need to convert SparkDataframe to PandasDataFrame "
"## Checking null values in Pyspark\n\n* isnan() is a function of the pysparq.sql.function package, we have to set which column we want to use as an argument of the function. \n* isNull()"" belongs to pyspark.sql.Column package, to check the null status of a column\n\nTo check null in Pyspark, we use both function above"
### We using PysparkDataFrame.na.fill() to fill a value to specific column
"#### After handling missing values, we do some simple feature engineering\n#### in Feature engineering, we can use Pyspark multiple condition with syntax: ""When otherwise""\n#### To learn more about multiple condition in pyspark, you can visit at https://sparkbyexamples.com/spark/spark-case-when-otherwise-example/"
### Visualizing AUC metrics
## Random Forest
### Visualizing AUC metrics
"## Gradient Boosted Tree\n\nIf you want to run the below part, just uncomment it. I dit it in order to reduce the time of committing the kernel."
### Visualizing AUC metrics
I hope you find this notebook beneficial and enjoyable
"Data scientist respondents from United States and India are nearly equal as seen below. Both of them have been predominate over the survey. So i have analyzed other countries separately. Because most of the data scientist taken part in the survey from India and US, I have looked closer the two countries."
"# Age, Gender, Education and Job Title Distribution"
"The age group distribution of all respondents and data scientists are similar except for 18-21... I think most of 18-21 are students. Apparently, because of the popularity of data science, young ones are trying yo enter this field."
"Although young people are the majority, all age groups seem to be interested in data science. It's never too late to begin."
"Although young people are the majority, all age groups seem to be interested in data science. It's never too late to begin."
In the chart females and males show a similar distribution
In the chart females and males show a similar distribution
"It seems that the younger generations in the India are the peak unlike the United States. It's closely related to more students participation from India. (See ""Titles of Respondents (India - USA Comparison)"")"
It can be deduced from the chart that master's and doctoral degrees in data science gain importance.
"It seems, US is better than India in terms of education degree. That's because the Indian Kagglers is younger than the American ones. (see above ""Age Group (India - U.S. Comparison)"")"
The students have participated in the survey as much as data scientist.\nI think it's so interesting that more Software Engineer participate in the survey than Data Engineer and Statistician.
The difference between the US and India is the interchange of students and data scientists. The rest have similar distribution.
The 25 countries with the highest average data scientist salary in the map chart can be seen as a bar chart below.
"I compared the salary status of countries in the chart below. This countries are the top 6 countries with more data scientist participants in the survey (see above ""Country Distribution of Data Scientist by Gender"") There are only two countries where data scientists earn more than other professions. These are United States and Russia. It seems that the two rival have understood the power of the artificial intelligence."
"I compared the salary status of countries in the chart below. This countries are the top 6 countries with more data scientist participants in the survey (see above ""Country Distribution of Data Scientist by Gender"") There are only two countries where data scientists earn more than other professions. These are United States and Russia. It seems that the two rival have understood the power of the artificial intelligence."
"In the following chart i showed the correlation between salary and spent on machine learning and/or cloud computing products. If you choose 0 and 100.000 $ (the first and the last) on spent legend, you can see clearly that the more money data scientists earn, the more spend to ML products."
"In the following chart i showed the correlation between salary and spent on machine learning and/or cloud computing products. If you choose 0 and 100.000 $ (the first and the last) on spent legend, you can see clearly that the more money data scientists earn, the more spend to ML products."
"In the United States, most data scientist salaries range between 100k and 200k meanwhile in India between 5k and 60k."
"In the United States, most data scientist salaries range between 100k and 200k meanwhile in India between 5k and 60k."
"In the following chart, I have showed each data scientist salary group with the percentage distribution of education level in the United States."
### 2D Representation: Sound Waves
"### Fourier Transform\n\n* Function that gets a signal in the time domain as input, and outputs its decomposition into frequencies\n* Transform both the y-axis (frequency) to log scale, and the ‚Äúcolor‚Äù axis (amplitude) to Decibels, which is approx. the log scale of amplitudes."
"### The Spectrogram\n\n* What is a spectrogram? A spectrogram is a visual representation of the spectrum of frequencies of a signal as it varies with time. When applied to an audio signal, spectrograms are sometimes called sonographs, voiceprints, or voicegrams ([wiki](https://en.wikipedia.org/wiki/Spectrogram)).\n* Here we convert the frequency axis to a logarithmic one."
"### Mel Spectrogram\n\n* The Mel Scale, mathematically speaking, is the result of some non-linear transformation of the frequency scale. The Mel Spectrogram is a normal Spectrogram, but with a Mel Scale on the y axis."
### Harmonics and Perceptrual\n\n* Harmonics are characteristichs that human years can't distinguish (represents the sound color)\n* Perceptrual understanding shock wave represents the sound rhythm and emotion
### Tempo BMP (beats per minute)\n\nDynamic programming beat tracker.
"### Spectral Rolloff\n* is a measure of the shape of the signal. It represents the frequency below which a specified percentage of the total spectral energy, e.g. 85%, lies"
### Mel-Frequency Cepstral Coefficients:\n\n* The Mel frequency cepstral coefficients (MFCCs) of a signal are a small set of features (usually about 10‚Äì20) which concisely describe the overall shape of a spectral envelope. It models the characteristics of the human voice.
### Mel-Frequency Cepstral Coefficients:\n\n* The Mel frequency cepstral coefficients (MFCCs) of a signal are a small set of features (usually about 10‚Äì20) which concisely describe the overall shape of a spectral envelope. It models the characteristics of the human voice.
Data needs to be scaled:
Data needs to be scaled:
### Chroma Frequencies\n\n* Chroma features are an interesting and powerful representation for music audio in which the entire spectrum is projected onto 12 bins representing the 12 distinct semitones (or chroma) of the musical octave.
### Chroma Frequencies\n\n* Chroma features are an interesting and powerful representation for music audio in which the entire spectrum is projected onto 12 bins representing the 12 distinct semitones (or chroma) of the musical octave.
"## EDA\n\nEDA is going to be performed on the `features_30_sec.csv`. This file contains the mean and variance for each audio file fo the features analysed above. \n\nSo, the table has a final of 1000 rows (10 genrex x 100 audio files) and 60 features (dimensionalities)."
### Correlation Heatmap for feature means
### Box Plot for Genres Distributions
### Box Plot for Genres Distributions
### Principal Component Analysis - to visualize possible groups of genres\n\n1. Normalization\n2. PCA\n3. The Scatter Plot
### XGBoost is the winner - 90% accuracy\n\n* create the final model\n* compute confusion matrix\n* Compute Feature Importance
### Feature Importance
"Looking at below count plot, looks like very less number of females attended the black friday sale. \nBut it could also mean less number of females paid for the products and may be their spouse paid for them. "
"Now, on plotting a count plot for age, seems like the majority of the population in the ages group 26-35 attended the sale."
"Now, on plotting a count plot for age, seems like the majority of the population in the ages group 26-35 attended the sale."
"Further, I could also check among the age groups, which gender was a majority by adding a hue.\nAnd as seen below, more males spent in the sale than females."
"Further, I could also check among the age groups, which gender was a majority by adding a hue.\nAnd as seen below, more males spent in the sale than females."
May be we could check further - how many of these males were actually married?\nFor this lets create a column that represents gender+married status and then use it as hue.
"As we see above, there are no bars for the married in the 0-17 range which makes sense. And then if we look at the 46 and above groups, females are very less. But on the other hand, married males paying in range 46-55 are also comparatively more than married females. So it could also imply that though ladies do shop a lot, their spouses are possibly paying for it and hence data reflects that men shopped more.\nIf we had more categorical data defining what kind of products were purchased by men, we could dig in this statement further. However, since in this dataset we don't know if there is a category that implies feminine products/clothes we cannot further explore this case."
Even below plots don't provide any hint of whether some products are particularly being purchased by either females or married males.
Even below plots don't provide any hint of whether some products are particularly being purchased by either females or married males.
Let us check back rest of the columns again to see what next we could explore.
"![](https://i.imgur.com/LMDjyOy.jpg)\n\n# 1 | INTRODUCTION\n\n\nNOTEBOOK AIM\n\n- The aim of this notebook is to provide a brief overview on what type of geospatial library tools we can use to visualise & analyse map geospatial data\n- As the title suggests, the notebook is aimed at exploring Australian based maps & visualisation data, subsequent data sources for map geometry & visualisation data is specified\n- Various visualisation approaches are discussed & some examples are also shown of how we can apply the visualisation method\n\n\nGEOSPATIAL ANALYSIS TOOLS\n\nThere are a number of useful geospatial tools we can use to visualise & analyse data, **Plotly** & **GeoPandas** pretty much contains everything we need:\n\n> - **Choropleth Maps** - Region based visualisation | Requires Boundary Locations (unique boundary identifier)\n> - **Hexbin Maps** - Hexagonal region based point counting | Requires Point Locations (long,lat)\n> - **Cluster Maps** - Standard Scatter Plot | Requires Point Locations (long,lat)\n> - **Density Heatmaps** - Model based approximation for scatter data | Requires Point Locations (long,lat)\n> - We will also look at an interpolation method commonly used for geospatial analysis called **Kriging**\n"
"###  2.2 | Unemployment rate in Queensland \n\nPROBLEM AIM\n\n- We are interested in the __unemployment statistics__ of a specific demographic (Both Male & Female) in the state of Queensland on a __Local Government Area__ level.\n- The user wants the ability to explore the exact unemployment value for their own purposes (if possible) and understand any overall differences between male and female unemployment.\n\nSOME OBSTACLES\n\n- Unfortunately Australian internal boundary maps are not integrated into __Plotly__ or __Geopandas__ \n- We'll be requried to look for this __boundary segment__ data & combine it with our visualisation data we wish to display. \n- Both our downloaded __boundary data__ & __visualisation data__ should simply have an index corresponding to the unique boundary identifier & fortunately, pandas is our friend.\n- Its not really relevant what it is, it could be names (sometimes you'd have to clean the name column a little), or more often it is a __unique code__.\n\nWHAT WE WANT TO SHOW\n\n\nWe probably should know what type of geospatial boundaries we are interested in:\n\n> - We might be interested in __State Bondaries__ (eg. Victoria...) (**[State Boundaries AUG2020](https://www.data.gov.au/dataset/ds-dga-bdcf5b09-89bc-47ec-9281-6b8e9ee147aa/distribution/dist-dga-ee6c0f18-3f4b-4275-932e-0b6e434e316f/details?q=)**)\n> - We might be interested in __Local Government Boundaries__ (eg. Boroondara ... ) (**[Local Government Areas NOV2020](https://www.data.gov.au/dataset/ds-dga-bdcf5b09-89bc-47ec-9281-6b8e9ee147aa/distribution/dist-dga-6b4e69ed-6f7f-4422-854d-1013ac716bbe/details?q=)**)\n> - We might be interested in __Suburb Boundaries__ (eg. Carlton ... ) (**[All State Suburbs (November 2020)](https://www.data.gov.au/dataset/ds-dga-bdcf5b09-89bc-47ec-9281-6b8e9ee147aa/distribution/dist-dga-2d59ddfc-1c0f-41a3-8de8-06fa5d11e72f/?q=)**)\n> - Others might include Electoral Boundaries, or you might even have your own.\n\nDATA SOURCES\n\n- **[Data.gov](https://www.data.gov.au)** , has a very wide range of Geographical datasets; including **[Geoscape Administrative Boundaries](https://www.data.gov.au/dataset/ds-dga-bdcf5b09-89bc-47ec-9281-6b8e9ee147aa/details?q=)**, in this example we are interested in __LGA__ boundaries, which we can find in the above link or a direct link [here](https://www.data.gov.au/dataset/ds-dga-bdcf5b09-89bc-47ec-9281-6b8e9ee147aa/distribution/dist-dga-6b4e69ed-6f7f-4422-854d-1013ac716bbe/details?q=) ( Just note that the data is already split by state )\n- **[Australian Bureau of Statistics](https://www.abs.gov.au)**, which contains two .shp files (for Local Government & State Electoral Divisions), found specifically [here](https://www.abs.gov.au/AUSSTATS/abs@.nsf/DetailsPage/1270.0.55.003June%202020?OpenDocument)\n- In this example, we will use __Local Government Areas ASGS Ed 2020 Digital Boundaries in ESRI Shapefile Format__ from __ABS.gov__ & select our specific state (Queensland)\n\nBOUNDARY FORMAT\n\n\n- I think it's much more straightforward to use .shp & simply convert them with geopandas, purely because GeoPandas is very straightforward to use and visualise the geopandas dataframe, like normal pandas dataframes.\n- json files on the other hand, may need a little bit of tweaking to get the read format correct, especially if you are using .read_json from the Pandas library in order to understand what your data contains.\n- The above mentioned sources all contain .shp formats, which is an indicator that it is quite popular, neverthelesss __json__ variants are also available, mainly __Data.gov__\n- Let's load & read the .shp file, noting that all the other files that come with the .shp are needed. The current file contains __LGA__ of all states (as it might be obvious by the STE_NAME16, so we need to select only a subset of the data."
"INTERACTIVE CHOROPLETH MAPS\n\n\n- Queensland contains a variety of boundary segment sizes, this makes it difficult to visualise smaller region values if the map is static & zoomed out.\n- Interactive maps allow us to explore the region and outline any notable key difference.\n- Let's plot the __Female Demographic in the Plotly Go__ plot & __Male Demographic in the Plotly Express__ plot.\n\n**INTERACTIVE** **PLOTLY GO w/ MAPBOX ACCESS TOKEN**\n\n- We can use Plotly Go with or/without a MapBox Access token.\n- MapBox tokens are useful to __outline key area names__ on top of the choropeth map plot, we can do this by modifying the layout.\n- Let's plot the female demographic on the plot with the __mapbox layout__."
"**INTERACTIVE** **  PLOTLY EXPRESS W/O MAPBOX ACCESS TOKEN**\n\n- We can use plotly express without having to use a MapBox Access Token (if you don't need the names to go on top), this may make the map let useful from an EDA point of view, nevertheless we can always refer to the hover data to get the names.\n- Let's plot the male demographic using the plotly express version. \n- Plotly express versions can also be used with __mapbox__ as shown later using the fig.update_layout() option."
"**INTERACTIVE** **  PLOTLY EXPRESS W/O MAPBOX ACCESS TOKEN**\n\n- We can use plotly express without having to use a MapBox Access Token (if you don't need the names to go on top), this may make the map let useful from an EDA point of view, nevertheless we can always refer to the hover data to get the names.\n- Let's plot the male demographic using the plotly express version. \n- Plotly express versions can also be used with __mapbox__ as shown later using the fig.update_layout() option."
"- In this way, we can create choropeth maps in separate code cells and extract data we need from these interactive maps. \n- We could also use a button based load option, which Plotly also offers. An example will be shown below in Section 2.4.\n\n**STATIC** **CHOROPLETH MAP**\n\n- Static plots are more than sufficient as long as they can get the specific point across which was intended.\n- Perhaps a little less insightful comapared to __interactive plots__ when it comes to trying to __show resuts for two very different sized bundaries__. \n- We may have a very big difference in boundary sizes, so regions like Woorabinda, will be hard to spot on a static map that tries to shows the entire state.\n- The aim of the static plot below is to demonstrate that the __level of unemloyment is sightly higher for males than for females__ in the Queensland for the __age group 20-24__, in all statistical quadrant data, which overall we can see, even on static plots. We can also use statistics data in the form of tables to complement our choropleth maps."
"Instead of loading only one dataset into go.Figure(data=data, layout=layout), we simply need to load them into a list."
"To be able to select multiple data, we need to change the __layout__ to include __update_menus__ that will indicate which feature to load via the __visible__ argument, information which is passed through via the list, lst."
"PLOTTING A TIME SERIES SCATTER CLUSTER MAP\n\n\n- Let's look at a time series evolution of __maximum temperature__ for all regions where data is available using a cluster map.\n- We have quite a few datapoints all across Australia, let's focus our attention to the __Victoria__ when creating a model. "
"- In the above plot we can note some very __clear oscilations__ in maximum temperature, which have a tendency to occur periodically every decade, as well as a very small increase in temperature tendency.\n\nPLOTTING YEARLY MEAN OF MAX TEMPERATURE\n\n- Let's plot the __overall mean trend__ using all data sampling stations, from the current data, we can see that there is a very minimal increase in mean maximum temperature.\n- Geospatial maps often work well together with figure plots to more clearly show the animal dynamic on the map (especially for stationally points). Let's use groupby and evaluate the __mean value__ for all locations."
"- In the above plot we can note some very __clear oscilations__ in maximum temperature, which have a tendency to occur periodically every decade, as well as a very small increase in temperature tendency.\n\nPLOTTING YEARLY MEAN OF MAX TEMPERATURE\n\n- Let's plot the __overall mean trend__ using all data sampling stations, from the current data, we can see that there is a very minimal increase in mean maximum temperature.\n- Geospatial maps often work well together with figure plots to more clearly show the animal dynamic on the map (especially for stationally points). Let's use groupby and evaluate the __mean value__ for all locations."
"GEOSPATIAL MAX TEMPERATURE INTERPOLATION\n\n\nNext, let's use Kriging to predict maximum temperature in different regions of __Victoria__, we'll need to load the location data first as well. "
###  4.4 | Visualising high floor area properties \n\nPROBLEM AIM\n\n- Let's revisit the [Perth Housing Dataset](https://www.kaggle.com/syuzai/perth-house-prices).\n- We are interested in different properties that were sold near the Perth CBD & their corresponding property FLOOR_AREA.\n- An interactive maps allow us to investigate the different properties using the interactive window & outline for example key regions with high FLOOR_AREA.
"# 5 | DENSITY HEATMAPS\n\n###  5.1 | Density heatmap overview \n\n- Density heatmaps allow us to visuaise __clustermap__ data in the form of a continuous function. \n- The issue with heavily concentrated __clustermaps__ is that points in close proximity tend to overlap; it may be difficult to distinguish them apart.\n- Continuous functions used to define the heatmap allows us to visualise the overall local value tendencies, similar to how a bias/variance balanced model. \n- The benefit of interactive heatmaps also lies in the ability to use hover_name which comes in handy as well.\n\nDENSITY HEATMAPS REQUIRE\n\n- longitude & latitude spatial point & size/color visualisation data.\n\n\n\n###  5.2 | Visualising high floor area property trends \n\nPROBLEM AIM\n\n- Let's revisit the **[Perth Housing Dataset](https://www.kaggle.com/syuzai/perth-house-prices)** once again. We are interested in regional trends this time of different properties that were sold near the Perth.\n- This time, we aren't too interested in individual property EDA, rather we use heatmaps to find general trends in different parts of Perth, this is quite useful to get an overall picture."
"# 5 | DENSITY HEATMAPS\n\n###  5.1 | Density heatmap overview \n\n- Density heatmaps allow us to visuaise __clustermap__ data in the form of a continuous function. \n- The issue with heavily concentrated __clustermaps__ is that points in close proximity tend to overlap; it may be difficult to distinguish them apart.\n- Continuous functions used to define the heatmap allows us to visualise the overall local value tendencies, similar to how a bias/variance balanced model. \n- The benefit of interactive heatmaps also lies in the ability to use hover_name which comes in handy as well.\n\nDENSITY HEATMAPS REQUIRE\n\n- longitude & latitude spatial point & size/color visualisation data.\n\n\n\n###  5.2 | Visualising high floor area property trends \n\nPROBLEM AIM\n\n- Let's revisit the **[Perth Housing Dataset](https://www.kaggle.com/syuzai/perth-house-prices)** once again. We are interested in regional trends this time of different properties that were sold near the Perth.\n- This time, we aren't too interested in individual property EDA, rather we use heatmaps to find general trends in different parts of Perth, this is quite useful to get an overall picture."
"# 6 | SUMMARY\n\n- In this notebook, we looked at different tools that can be used for geospatial data analysis, mainly Choropleth,Hexbin,Cluster & Density Heatmaps.\n- Greater attention was payed to Choropleth Maps, due to their more complex data input structure, requiring __boundary geometry__ data alongside with the data which is desired to be shown. Compared to Hexbin,Cluster & Density Heatmaps, which require only __point coordinates (longitude & latitude)__.\n\nBOUNDARY BASED VISUALISATION\n\n- Interactive Choropeth were shown to be quite effective at portraying data, especially when the difference in boundary sizes to be shown is very big. We also plotted static maps using __geopandas__, although most of the regions were visible, additional plots were needed in order to show all region data clearly.\n- One slightly issue arose when plotting Australian Choropleth maps, we needed to know specifically where to get the boundary data, which is a required step for plotting Australian based Choropleth Maps. These sources were outlined and a specific example for the __unemployment rate__ of specific demographics were shown and compared to one another.\n- One of the more compex parts of plotting choropleth maps, was the integration of two separate dataframe (boundary & visualisation), which required some data wrangling to combine and join indicies.\n\nCOORDINATE BASED VISUALISATIONS\n\n\n- Hexbin,cluster & density heatmaps, all require geospatial point data (longitude,latitude)\n- Out of the three, cluster maps are probably most useful due to their ability to pinpoint data at different locations. They are also commonly used with data interpolation methods, to estimate data at points we don't yet have data.\n- We also saw that cluster maps, especially when zoomed out tend to start overlapping as demonstrated in the __Perth Housing__ dataset, in such cases, density heatmaps are quite useful, in order to plot continuous data."
\n Import Libraries 
\n Import Dataset \n
"Before we begin with anything else,let's check the class distribution.There are only two classes 0 and 1."
There are more tweets with class 0 ( No disaster) than class 1 ( disaster tweets)
Number of characters in tweets
The distribution of both seems to be almost same.120 t0 140 characters in a tweet are the most common among both.
**Number of words in a tweet**
**Average word length in a tweet**
**Average word length in a tweet**
**Frequencies**\nNow we want to count the frequency of each word in our corpus.
Let's take a look to the punctuations in our tweets : 
In details about each target 
"1. ***Missing Values***\n\nBoth training and test set have same ratio of missing values in keyword and location.\n\n0.8% of keyword is missing in both training and test set\n33% of location is missing in both training and test set\nSince missing value ratios between training and test set are too close, they are most probably taken from the same sample. Missing values in those features are filled with no_keyword and no_location respectively."
"2. Cardinality and Target Distribution\n\nLocations are not automatically generated, they are user inputs. That's why location is very dirty and there are too many unique values in it. It shouldn't be used as a feature.\n\nFortunately, there is signal in keyword because some of those words can only be used in one context. Keywords have very different tweet counts and target means. keyword can be used as a feature by itself or as a word added to the text. Every single keyword in training set exists in test set. If training and test set are from the same sample, it is also possible to use target encoding on keyword."
# **Target Variable Distribution**
# **Distribution of Meta Features vs Target **
# **Distribution of Meta Features vs Target **
# **Frequency Distribution of Meta Features**
# **Frequency Distribution of Meta Features**
# **Clustering**
# **t-SNE**
# **K - Nearest Neighbours**
# **K - Nearest Neighbours**
# **PCA**
# **PCA**
"# **Visualize Dataset Interactively using W&B Tables**\n\nIt only requires 5 lines of extra code to get the power of W&B Tables. \n\n1. You first need to initialize a W&B run using `wandb.init` API. This step is common for any W&B Logging.\n2. Create a `wandb.Table` object. Imagine this to be an empty Pandas Dataframe. \n3. Iterate through each row of the `train.csv` file and `add_data` to the `wandb.Table` object. Imagine this to be appending new rows to your Dataframe. \n4. Log the W&B Tables using `wandb.log` API. You will use this API to log almost anything to W&B.\n5. In a Juypter like interactive session, you need to call `wandb.finish` to close the initialized W&B run. \n\nSource : Content copied from Ayush notebook\n"
### Import modules
### Reading in input file
### Distributions of attributes
"__Notes for Data Cleaning & Preprocessing:__ \nUni-modal, skewed distributions could potentially be log transformed: \n> LotFrontage, LotArea, 1stFlrSF, GrLivArea, OpenPorchSF\n\nAfter-note: This will be a future addition."
__Univariate analysis - box plots for numerical attributes__
__Bivariate analysis - scatter plots for target versus numerical attributes__
"### Assess correlations amongst attributes\nThe linear correlation between two columns of data is shown below. There are various correlation calculation methods, but the Pearson correlation is often used and is the default method. It may be useful to note that:\n1. A combination of the correlation figure and a scatter plot can support the understanding of whether there is a non-linear correlation (i.e. depending on the data, this may result in a low value of linear correlation, but the variables may still be strongly correlated in a non-linear fashion)\n2. Correlation values may be heavily influenced by single outliers! \n\n\nSeveral authors have suggested that _""to use linear regression for modelling, it is necessary to remove correlated variables to improve your model""_, and _""it's a good practice to remove correlated variables during feature selection""_\n\nBelow is a heatmap of the correlation of the numerical columns:"
"With reference to the target SalePrice, the top correlated attributes are:"
"Show scatter plots for each numerical attribute (again, but different, less-efficient code) and show correlation value:"
"__Notes for Feature Selection & Engineering:__\nBased on the scatter plots and correlation figures above, consider:\n* Excluding GarageArea - highly (0.88) correlated with GarageCars, which has a higher corr with Price\n* Excluding GarageYrBlt - highly (0.83) correlated with YearBuilt\n* Excluding all attributes with low corr with Price and unclear non-linear correlation - e.g. MSSubClass, MoSold, YrSold, MiscVal, BsmtFinSF2, BsmtUnfSF, LowQualFinSF?"
"### Considering highly-correlated features\nFeeding highly-correlated features to machine algorithms may cause a reduction in performance. Hence, these are addressed below:"
Highly-correlated attributes include (left attribute has higher correlation with SalePrice_log):\n* GarageCars and GarageArea (0.882)\n* YearBuilt and GarageYrBlt (0.826)\n* GrLivArea_log1p and TotRmsAbvGrd (0.826)\n* TotalBsmtSF and 1stFlrSF_log1p (0.780)\n\nPerhaps choose to drop the column with the lower correlation against SalePrice_log from the above pairs with more than 0.8 correlation.
The kernels below helped me in writing this kernel. Thanks!\n\nAndrew Lukyanenko: https://www.kaggle.com/artgor/eda-and-models\n\nLeonardo Ferreira: https://www.kaggle.com/kabure/extensive-eda-and-modeling-xgb-hyperopt\n\nKonstantin Yakovlev: https://www.kaggle.com/kyakovlev/ieee-gb-2-make-amount-useful-again\n\nKonstantin Yakovlev: https://www.kaggle.com/kyakovlev/ieee-simple-lgbm
The functions used for visualization are below.
The functions used for visualization are below.
First let's check the sample submission.
### Date
"Above, it is shown that the dates of Train and Test data have an empty intersection."
### Importing Libraries
## Data Acquisition
To start the univariate analysis we will plot histograms for the 'redshift' feature column for each class.\n\nThis will tell us how the redshift values are distributed over their range.
"This is an interesting result.\n\nWe can cleary tell that the redshift values for the classes quite differ. \n\n* **Star:** The histogram looks like a truncated zero-centered normal distribution.\n\n* **Galaxy:** The redshift values may come from a slightly right-shifted normal distribution which is centered around 0.075.\n\n* **QSO:** The redshift values for QSOs are a lot more uniformly distributed than for Stars or Galaxies. They are roughly evenly distributed from 0 to 3, than the occurences decrease drastically. For 4 oder ~5.5 there are some outliers.\n\n**The redshift can be an estimate(!) for the distance from the earth to a object in space.**\n\nHence the distplot tells us that most of the stars observed are somewhat closer to the earth than galaxies or quasars. Galaxies tend to be a little further away and quasars are distant from very close to very far.  \n\nPossible rookie explanation: Since galaxies and quasars radiate stronger due to their size and physical structure, they can be observed from further away than ""small"" stars.\n\nAs we can distinct the classes from each other just based on this column - 'redshift' is very likely to be helping a lot classifying new objects."
Let's lvplot the values of dec (Recall: position on celestial equator)!
**First of all: what does this plot tell us?**\n\nThe Letter value (LV) Plot show us an estimate of the distribution of the data. It shows boxes which relate to the amount of values within the range of values inside the box.\n\nIn this case we can observe a clear distinction between Stars and the other two classes. The difference between Galaxies and Quasars is smaller.\n\n* **Star:** The largest part of the data points lay within a 0 to 10 range. Another large part consists of values between about 10 to 55. Only small amounts of the data are lower or higher than these ranges.\n\n* **Galaxy:** The largest part of values lays between 0 and 45. There is a smaller amount of values in the range of 45 to 60. The rest of the data has smaller or higher values.\n\n* **QSO:** This plot looks quite similiar to the GALAXY plot. Only the amount of data points in the range of 0 to 60 is even bigger.\n\nSide Note: The fact that the distribution of dec values of galaxies und quasar objects is almost the same might indicate that one can find both galaxies and quasars at smiliar positions in the night sky.
"Recall: u, g, r, i, z represent the different wavelengths which are used to capture the observations.\n\nLet's find out how much they are correlated."
"Right of the top we observe that the correlation matrices look very similiar for every class.\n\nWe can tell that there are high correlations between the different bands. This feels not really suprising - intuitively one would think that if one of the bands captures some object, the other bands should capture something aswell.\n\nTherefore it is interesting to see that band 'u' is less correlated to the other bands. \n\nRemember: u, g, r, i, z capture light at wavelengths of 354, 476, 628, 769 and 925 nm.\n\nThis might indicates that galaxies, stars and quasar objects shine brighter at wavelengths from 476 - 925 nm. Don't quote me on that though.\n\n**But:** as we can see - the correlation is roughly the same for every class...the different bands behave the same for the different classes!"
We will now plot the right ascension versus the declination depending on the class 
"As we can clearly observe the equatorial coordinates do not differ significantly between the 3 classes. There are some outliers for stars and galaxies but for the bigger part the coordinates are within the same range.\n\nWhy is that?\n\nAll SDSS images cover the same area of the sky. The plot above tells us that stars, galaxies and quasars are observed equally at all coordinates within this area. So whereever the SDSS ""looks"" - the chance of observing a star or galaxy or quasar is always the same.  \n\n**This contradicts our interpretation of the letter value plot of dec from the univariate analysis.**"
**some useful functions**
**loading the data**
"### Seaborn Distplots \n**Distribution of Age as function of Pclass, Sex and Survived**"
"Best chances to survive for male passengers was in Pclass 1 or being below 5 years old.  \nLowest survival rate for female passengers was in Pclass 3 and being older than 40.  \nMost passengers were male, in Pclass 3 and between 15-35 years old."
"Default mode for seaborn barplots is to plot the mean value for the category.  \nAlso, the standard deviation is indicated.  \nSo, if we choose Survived as y-value, we get a plot of the survival rate as function   \nof the categories present in the feature chosen as x-value."
"As we know from the first Titanic kernel, survival rate decreses with Pclass.  \nThe hue parameter lets us see the difference in survival rate for male and female. "
"As we know from the first Titanic kernel, survival rate decreses with Pclass.  \nThe hue parameter lets us see the difference in survival rate for male and female. "
Highest survival rate (>0.9) for women in Pclass 1 or 2.  \nLowest survival rate (<0.2) for men in Pclass 3.
Highest survival rate (>0.9) for women in Pclass 1 or 2.  \nLowest survival rate (<0.2) for men in Pclass 3.
"Passengers embarked in ""S"" had the lowest survival rate, those embarked in ""C"" the highest.  \nAgain, with hue we see the survival rate as function of Embarked and Pclass."
"Passengers embarked in ""S"" had the lowest survival rate, those embarked in ""C"" the highest.  \nAgain, with hue we see the survival rate as function of Embarked and Pclass."
But survival rate alone is not good beacuse its uncertainty depends on the number of samples.  \nWe also need to consider the total number (count) of passengers that embarked.
But survival rate alone is not good beacuse its uncertainty depends on the number of samples.  \nWe also need to consider the total number (count) of passengers that embarked.
"Passengers embarked in ""C"" had largest proportion of Pclass 1 tickets.  \nAlmost all Passengers embarked in ""Q"" had Pclass 3 tickets.  \nFor every class, the largest count of Passengers  embarked in ""S""."
"Here, the high survival rate for kids in Pclass 2 is easily observed.  \nAlso, it becomes more obvious that for passengers older than 40 the best chance to survive is in Pclass 1,  \nand smallest chance in Pclass 3   "
This violinplot shows exactly the same info like the swarmplot before.
**Looks like there is very strong correlation of Survival rate and Name length**
**Chance to survive increases with length of name for all Passenger classes**
**Chance to survive increases with length of name for all Passenger classes**
**Increase of survival rate with length of name most important for male passengers**
"**RandomizedSearchCV  and GridSearchCV apply k fold cross validation on a chosen set of parameters**\n**and then find the parameters that give the best performance.**  \nFor GridSearchCV, all possible combinations of the specified parameter values are tried out, resulting in a parameter grid.  \nFor RandomizedSearchCV, a fixed number of parameter settings is sampled from the specified distributions. The number of parameter settings that are tried is given by n_iter."
"**In the following we apply GridSearchCV and RandomizedSearchCV for these Classification models:**  \n**KNN, Decision Tree, Random Forest, SVC**"
"# 1. Introduction\n\n\n    1.1 Objectives\n\n\nI'm very excited to participate in kaggle's first **unsupervised clustering** TPS competition. The goal is to **predict** the cluster each sample belongs to. However, we are not even given the number of clusters there should be beforehand. \n\nI will try to answer the following questions:\n* *How **many clusters** should we use?*\n* *What is the **competition metric** and where does it come from?*\n* *What is the **best model** for the data*?\n* *How do we **ensemble** predictions together?*\n\n\n    1.2 Libraries\n"
"# 2. Data\n\n\n    2.1 Load data\n\n\n* There are **29** features, all of them masked.\n* There are **almost 100,000** data points."
"# 3. EDA\n\n\n    3.1 Discrete features\n\n\n* There are **7** discrete features: *f_07* to *f_13*.\n* Values are **non-negative**. \n* Distributions are all similar, perhaps **Poisson**."
"\n    3.2 Continuous features\n\n\n* There are **22** continuous features: *f_00* to *f_06* and *f_14* to *f_28*\n* Distributions are all **Normal**, usually with mean 0 and standard deviation 1.\n* Values typically lie between -5 and +5."
"\n    3.2 Continuous features\n\n\n* There are **22** continuous features: *f_00* to *f_06* and *f_14* to *f_28*\n* Distributions are all **Normal**, usually with mean 0 and standard deviation 1.\n* Values typically lie between -5 and +5."
"\n    3.3 Hypothesis testing\n\n\n**Shapiro-Wilk Test** (Copied from [Francisco Javier Gallego & Torch me](https://www.kaggle.com/code/javigallego/outliers-eda-clustering-tutorial))\n\nThis test is used to test whether a dataset is distributed **normally** or not. The null hypothesis is that a sample $$x_1\hspace{0.1cm},\hspace{0.1cm}\cdots\hspace{0.1cm},\hspace{0.1cm}x_n$$ comes from a normally distributed population. It was published in 1965 by Samuel Shapiro and Martin Wilk and **is considered to be one of the most powerful tests for normality testing.** The test statistic is \n\n$$W = \frac{(\sum_{i=1}^{n}a_{i}x_i)^2}{\sum_{i=1}^{n}(x_i - \bar{x})^2}$$\n\nwhere\n\n* $x_i$ is the number from the i-th data point (where the sample is ordered from smallest to largest).\n* $\bar{x}$ is the sample mean. \n* Variables $a_i$ are calculated via\n\n$$(a_1, ... , a_n) = \frac{m^T V^{-1}}{(m^T V^{-1}V^{-1}m)^{1/2}} \hspace{2cm}m = (m_1 , ... , m_n)$$\n\nwhere $m_1 , ... , m_n$ are the mean values of the ordered statistic, of independent and identically distributed random variables, sampled from normal distributions and $V$ denotes the covariance matrix of that order statistic. **The null hypothesis is rejected if W is too small. The value of W can range from 0 to 1.**"
"\n    3.4 Q-Q plots\n\n\nQ-Q plots, aka **Quantile-Quantile** plots, are used to **visually compare** how similar two distributions are to each other. They consist of plotting the quantiles (i.e. regular intervals) of the **observed** distribution against the quantiles of the **theoretical** distribution. The closer the Q-Q plots are to forming a **straight line**, the more confident you can be that the observed and theoretical distributions are the **same**. \n\n**Normal Q-Q plots**"
"Even though the features *f_22* to *f_28* failed the Shapiro-Wilk test, they still appear to be quite close to being normally distributed. This behaviour could be because these features are made up of a **mixture** of normal distributions. There is not an easy way to verify this however.\n\n\n\n\n**Poisson Q-Q plots**"
"Even though the features *f_22* to *f_28* failed the Shapiro-Wilk test, they still appear to be quite close to being normally distributed. This behaviour could be because these features are made up of a **mixture** of normal distributions. There is not an easy way to verify this however.\n\n\n\n\n**Poisson Q-Q plots**"
"We can see more clearly that these features are not distributed according to independent Poisson distributions. However, some of them are quite close. It could be also that these features are made up of a **mixture** of Poisson distributions. Unfortunately, there isn't an easy way to verify this.\n\n\n\n    3.5 Correlations\n\n\n* Features *f_00* to *f_06* and *f_14* to *f_21* are **independent** of all other features.\n* Discrete features (*f_07* to *f_13*) and features *f_22* to *f_28* are **weakly dependent** of each other."
"We can see more clearly that these features are not distributed according to independent Poisson distributions. However, some of them are quite close. It could be also that these features are made up of a **mixture** of Poisson distributions. Unfortunately, there isn't an easy way to verify this.\n\n\n\n    3.5 Correlations\n\n\n* Features *f_00* to *f_06* and *f_14* to *f_21* are **independent** of all other features.\n* Discrete features (*f_07* to *f_13*) and features *f_22* to *f_28* are **weakly dependent** of each other."
"# 4. Elbow method\n\n\n    4.1 How it works\n\n\n\nThe **elbow method** is a practical way to determine the number of clusters in a dataset. It works by plotting the **inertia** (or sometimes distortion) against the **number of clusters**, where **inertia** is defined to be the sum of squared distances of samples to their closest cluster center, i.e. a measure of the models bias.  The '**elbow**' (point of sudden flattening) of the curve is then chosen to be the optimal number of clusters in the dataset.\n\n\n\n\n\nThe idea is that we want **low inertia** (because that means we have a good model), but not too low otherwise this will lead to **overfitting** (since if k=number of samples then every point is a cluster and the inertia is 0). The elbow usually represents the point of **diminishing returns** and therefore is a good **heuristic** for the optimal number of clusters.\n\n\n    4.2 Applying it \n\n\nSee my [discussion post](https://www.kaggle.com/competitions/tabular-playground-series-jul-2022/discussion/335079) where I used 50 clusters and the whole dataset. To save time here, we will just use 30 clusters and 10% of the data."
"# 4. Elbow method\n\n\n    4.1 How it works\n\n\n\nThe **elbow method** is a practical way to determine the number of clusters in a dataset. It works by plotting the **inertia** (or sometimes distortion) against the **number of clusters**, where **inertia** is defined to be the sum of squared distances of samples to their closest cluster center, i.e. a measure of the models bias.  The '**elbow**' (point of sudden flattening) of the curve is then chosen to be the optimal number of clusters in the dataset.\n\n\n\n\n\nThe idea is that we want **low inertia** (because that means we have a good model), but not too low otherwise this will lead to **overfitting** (since if k=number of samples then every point is a cluster and the inertia is 0). The elbow usually represents the point of **diminishing returns** and therefore is a good **heuristic** for the optimal number of clusters.\n\n\n    4.2 Applying it \n\n\nSee my [discussion post](https://www.kaggle.com/competitions/tabular-playground-series-jul-2022/discussion/335079) where I used 50 clusters and the whole dataset. To save time here, we will just use 30 clusters and 10% of the data."
"It is **hard** to tell exactly what the optimal value for the number of clusters should be since the curve is quite smooth. We will go with **k=7** for now, but it might be worth experimenting with different values of k as well. \n\n# 5. Competition metric\n\nIt is worth spending some time trying to understand the competition metric. This is called the **Adjusted Rand Index (ARI)**. But to do this, we first need to look at the **Rand Index (RI)**.\n\n\n    5.1 Rand Index\n\n\nThe Rand Index (named after **William Rand** from 1971) is a measure of **similarity** between the predicted clusters and the ground truth clusters. It looks at whether **pairs** of data points are in the same or different clusters. Let's work through an **example** to see how it works.\n\n$$\Large RI = \frac{a+b}{{n \choose 2}}$$\n\n\n\n\n\nFirst note that there are $n=5$ data points (denoted by greek letters, alpha to epsilon). The prediction is made up of **3 clusters**, whereas the ground truth is made up of **2 clusters**.\n\nThe combinatorial **formula** for the total **number of pairs** of data points is given by ${n \choose 2} = \frac{n(n-1)}{2}$. So for $n=5$, there are 10 total pairs. These are:\n\n$\{\alpha, \beta\}, \{\alpha, \gamma\}, \{\alpha, \delta\}, \{\alpha, \epsilon\}, \{\beta, \gamma\}, \{\beta, \delta\}, \{\beta, \epsilon\}, \{\gamma, \delta\}, \{\gamma, \epsilon\}, \{\delta, \epsilon\}$.\n\n\n\nTo work out the Rand Index, we need to calculate **two quantities**:\n* $a$ = # pairs in the **same** cluster in the prediction and the **same** cluster in the ground truth.\n* $b$ = # pairs in **different** clusters in the prediction and **different** clusters in the ground truth.\n\nThis can be a **bit confusing** but for example, the points ${\color{orange} \alpha}, {\color{orange} \beta}$ are in the same cluster in the prediction (orange) and in the same cluster in the ground truth (orange), so the pair $\{{\color{orange} \alpha}, {\color{orange} \beta}\}$ counts towards $a$. On the other hand, the points ${\color{orange} \alpha}, {\color{green} \delta}$ are in different clusters in both the prediction and ground truth (orange, green) so the pair $\{{\color{orange} \alpha}, {\color{green} \delta}\}$ counts towards $b$. \n\n\n\nIf we continue like this (**check this yourself**), you will find that the pairs $\{{\color{orange} \alpha}, {\color{orange} \beta}\}, \{{\color{green} \delta}, {\color{green} \epsilon}\}$ are in the **same** cluster for both prediction and ground truth so $a=2$ and the pairs $\{{\color{orange} \alpha}, {\color{green} \delta}\}, \{{\color{orange} \alpha}, {\color{green} \epsilon}\}, \{{\color{orange} \beta}, {\color{green} \delta}\}, \{{\color{orange} \beta}, {\color{green} \epsilon}\}, \{{\color{red} \gamma}, {\color{green} \delta}\}, \{{\color{red} \gamma}, {\color{green} \epsilon}\}$ are in **different** clusters for both prediction and ground truth so $b=6$.\n\nGreat, so putting the numbers in we find that $RI=\frac{2+6}{10}=0.8$.\n\n\n    5.2 Properties of RI\n\n\n* RI lies **between 0 and 1**. The closer to 1 the better.\n* If the prediction is **perfect**, i.e. equal to the ground truth, then **RI = 1**.\n* We say a pair of points is in '**agreement**' if they count towards a or b (above), and in '**disagreement**' otherwise. If we pick two points at **random**, RI gives the **probability** that this pair of points is in agreement. (i.e. the predicted 'state' of the pair is 'correct')\n* RI is equivalent to **accuracy** when viewed from a **binary classification** problem over the **pairs** of data points. In particular, each pair is either in agreement (1) or in disagreement (0), in which case $a=\text{True Positives} \, (TP)$ and $b=\text{True Negatives} \, (TN)$ so the Rand Index becomes:\n\n$$RI = \frac{TP + TN}{TP + FP + FN + TN} = \, \text{accuracy of pairs}$$\n* The only main **downside** to RI is that the **expected value** of RI, $\mathbb{E}(RI)$, isn't the same for different clustering problems. That means, some problems are **easier** to get a good RI score than others so we can't really **compare** RI between different problems. This is where the adjusted RI comes in. \n\n\n    5.3 Adjusted Rand Index\n\n\nThe **Adjusted Rand Index** (ARI) (introduced by **Hubert** and **Arabie** in 1985) is a 'corrected-for-chance' version of the Rand Index. It substracts RI by the expected value of RI for the specific clusterting problem. It then scales this number so that it has a maximum value of 1. \n\n$$\Large ARI = \frac{RI - \mathbb{E}(RI)}{max(RI)-\mathbb{E}(RI)}$$\n\n**Properties of ARI:**\n* It has a **maximum value of 1** but **no minimum** value (it can be negative). \n* If the prediction is **perfect**, i.e. equal to the ground truth, then **ARI = 1**.\n* A score of **0**, means the prediction is as good as picking all the clusters at **random**. \n* It is **comparable** between different clustering problems as its expected value is **constant** (0).\n\n\n\nWe know how to work out the RI and also that $max(RI)=1$, but working out the expectation $\mathbb{E}(RI)$ is much **trickier**. Hubert derived the following (rather complicated) **formula** for the entire ARI. See the **appendix** if you are interested to see the derivation.\n\n$$\n\large ARI = \frac{ \left. \sum_{ij} \binom{n_{ij}}{2} - \left[\sum_i \binom{a_i}{2} \sum_j \binom{b_j}{2}\right] \right/ \binom{n}{2} }{ \left. \frac{1}{2} \left[\sum_i \binom{a_i}{2} + \sum_j \binom{b_j}{2}\right] - \left[\sum_i \binom{a_i}{2} \sum_j \binom{b_j}{2}\right] \right/ \binom{n}{2} }\n$$\n\n**Note:** This is equivalent to the ARI formula above. \n\n\n    5.4 ARI example\n\n\nFirst, we start by **formalising** the clustering problem. We denote our dataset with $n$ objects by $S = \{o_1, o_2, \ldots, o_n \}$ (Each element is just a data point). Let's represent the ground truth and predicted clustering by two **partitions**: $X=\{X_1, \ldots, X_r\}$ and $Y=\{Y_1, \ldots, Y_s\}$, respectively.\n\nContinuing from our previous example, $X=\{X_1, X_2\}$ with $X_1=\{{\color{orange} \alpha},{\color{orange} \beta},{\color{orange} \gamma}\}$, $X_2=\{{\color{green} \delta},{\color{green} \epsilon}\}$ and $Y=\{Y_1, Y_2, Y_3\}$ with $Y_1=\{{\color{orange} \alpha},{\color{orange} \beta}\}$, $Y_2=\{{\color{red} \gamma}\}$, $Y_3=\{{\color{green} \delta},{\color{green} \epsilon}\}$.\n\n\n\nThen we draw a **contingency** table. Each entry, $n_{i,j}$, denotes how many data points there are in common between the ground truth cluster $X_i$ and the predicted cluster $Y_j$. Mathematically, the formula is $n_{i,j} = |X_i \cap Y_j |$, i.e. the **intersection**.\n\n$$ \n\begin{array}{c|cccc|c}\n{{} \atop X}\!\diagdown\!^Y &\nY_1&\nY_2&\n\cdots&\nY_s&\n\text{sums}\n\\\n\hline\nX_1&\nn_{11}&\nn_{12}&\n\cdots&\nn_{1s}&\na_1\n\\\nX_2&\nn_{21}&\nn_{22}&\n\cdots&\nn_{2s}&\na_2\n\\\n\vdots&\n\vdots&\n\vdots&\n\ddots&\n\vdots&\n\vdots\n\\\nX_r&\nn_{r1}&\nn_{r2}&\n\cdots&\nn_{rs}&\na_r\n\\\n\hline\n\text{sums}&\nb_1&\nb_2&\n\cdots&\nb_s&\n\end{array}\n$$\n\nFor example, to work out $n_{11}$, we look at clusters $X_1=\{{\color{orange} \alpha},{\color{orange} \beta},{\color{orange} \gamma}\}$ and $Y_1=\{{\color{orange} \alpha},{\color{orange} \beta}\}$ and find the points which appear in both of them. In this case, there are 2:  $X_1 \cap Y_1 = \{{\color{orange} \alpha},{\color{orange} \beta}\}$ so $n_{11}=2$. If we continue like this (**check this yourself**) we get:\n\n$$ \n\begin{array}{c|ccc|c}\n{{} \atop X}\!\diagdown\!^Y &\nY_1&\nY_2&\nY_3&\n\text{sums}\n\\\n\hline\nX_1&\n2&\n1&\n0&\n3\n\\\nX_2&\n0&\n0&\n2&\n2\n\\\n\hline\n\text{sums}&\n2&\n1&\n2&\n\end{array}\n$$\n\n\n\nWe are almost there. To work out ARI, we need to calculate these 3 quantities: $\sum_{i,j} {n_{ij} \choose 2}$, $\sum_{i} {a_{i} \choose 2}$, $\sum_{j} {b_{j} \choose 2}$.\n\nFirst recall the formula ${n \choose 2} = \frac{n(n-1)}{2}$, which we will be using a lot. E.g. ${5 \choose 2} = 10$\n\n1. $\sum_{i,j} {n_{ij} \choose 2} = {2 \choose 2} + {1 \choose 2} + {0 \choose 2} + {0 \choose 2} + {0 \choose 2} + {2 \choose 2} = 1 + 0 + 0 + 0 + 0 + 1 = 2$.\n\n2. $\sum_{i} {a_{i} \choose 2} = {3 \choose 2} + {2 \choose 2} = 3 + 1 = 4$.\n\n3.  $\sum_{j} {b_{j} \choose 2} = {2 \choose 2} + {1 \choose 2} + {2 \choose 2} = 1 + 0 + 1 = 2$.\n\nSo if we plug everything into the formula, we get $ARI = \frac{2 - (4 \times 2) / 10}{(4 + 2)/2 - (4 \times 2)/10} = 0.55$. \n\nThe ARI score (0.55) is quite a bit smaller than the RI score (0.80) we got earlier. This is somewhat expected though with such a small clustering problem; since the number of points $n$ is small, the problem is relatively easy so the ARI makes a **large adjustment**.\n\n# 6. Modelling\n\n\n    6.1 Scaling\n\n\nIt is always important to scale the data for clustering problems so that it is easier to compare the distance between data points. \n\n\n\n\n\nThere are several ways to do this, e.g.\n\n* *StandardScaler*: scales each column independently to have mean 0 and standard deviation 1, by subtracting by the column **mean** and dividing by the column **standard deviation**.\n* *RobustScaler*: does the same as above but uses statistics that are **robust to outliers**, i.e. it subtracts by the **median** and divides by the **interquartile range**. \n* *PowerTransformer*: makes columns more gaussian like by **stabilising variance** and **minising skew**. "
GMM performs much better than k-Means (around ARI=0.49 on public leaderboard). \n\nWe can plot the position of the **center** of each cluster to visualise how well each feature is able to **separate** the different clusters.
From this plot we can see that the features *f_00* to *f_06* and *f_14* to *f_21* **don't separate** the clusters at all! This means we might as well **drop** these features as they are not helping us in any way.
Bayesian GMM performs the **best** out of all of the models we've tried so far (it scores around ARI=0.59 on the public leaderboard). It does take **longer** to run though.\n\n# 7. Visualise predictions\n\n\n    7.1 Label distribution\n
"\n    7.2 Cluster distributions\n\n\nTo get insight into the underlying distributions, we can plot cluster-wise histograms of each of the remaining features.\n\n**Continuous: (f_22 to f_28)**"
"\n    7.2 Cluster distributions\n\n\nTo get insight into the underlying distributions, we can plot cluster-wise histograms of each of the remaining features.\n\n**Continuous: (f_22 to f_28)**"
**Discrete: (f_07 to f_13)**
**Discrete: (f_07 to f_13)**
"\n    7.3 Principle Component Analysis (PCA)\n\n\n**Principal Component Analysis (PCA)** was the first dimensionality reduction technique discovered (by Karl Pearson - yes, the guy from Pearson's correlation coefficient) and dates back to as early as **1901**. It is very popular because it is **fast**, **easy to implement** and **easy to interpret**. \n\nPCA works by finding a low dimensional subspace that **maximises the variance** of the data in that subspace and performing a **linear projection**. This basically means the data will be as **spread out** as possible, without changing the relationship between the data points. This allows us to find patterns in dimensions we can visualised."
"\n    7.3 Principle Component Analysis (PCA)\n\n\n**Principal Component Analysis (PCA)** was the first dimensionality reduction technique discovered (by Karl Pearson - yes, the guy from Pearson's correlation coefficient) and dates back to as early as **1901**. It is very popular because it is **fast**, **easy to implement** and **easy to interpret**. \n\nPCA works by finding a low dimensional subspace that **maximises the variance** of the data in that subspace and performing a **linear projection**. This basically means the data will be as **spread out** as possible, without changing the relationship between the data points. This allows us to find patterns in dimensions we can visualised."
"**Explained variance** shows how much of the variance/spread of the data is captured in each dimension, i.e. how **important** each additional **principal component** is to the original data representation."
"**Explained variance** shows how much of the variance/spread of the data is captured in each dimension, i.e. how **important** each additional **principal component** is to the original data representation."
"\n    7.4 t-SNE\n\n\n**t-SNE** (pronounced tiz-knee) stands for **t-distributed Stochastic Neighbor Embedding** and was proposed much more recently by Laurens van der Maaten and Geoffrey Hinton in their [2008 paper](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf). \nThis works in a similar way to PCA but has some key differences:\n* Firstly, this is a **stochastic method**. So if you run multiple t-SNE plots on the same dataset it can look different.\n* Another difference is that this is an **iterative method**. It works by repeatedly moving datapoints closer or further away from each other depending on how 'similar' they are. \n* The new representation is **non-linear**. This makes it harder to interpret but it can be very effective at 'unravelling' highly non-linear data.\n\nThe main downside to t-SNE is that is **very slow** compared to the other dimensionality techniques. This is because it makes calculations on a pair-wise basis, which does not scale well with large datasets."
"\n    7.4 t-SNE\n\n\n**t-SNE** (pronounced tiz-knee) stands for **t-distributed Stochastic Neighbor Embedding** and was proposed much more recently by Laurens van der Maaten and Geoffrey Hinton in their [2008 paper](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf). \nThis works in a similar way to PCA but has some key differences:\n* Firstly, this is a **stochastic method**. So if you run multiple t-SNE plots on the same dataset it can look different.\n* Another difference is that this is an **iterative method**. It works by repeatedly moving datapoints closer or further away from each other depending on how 'similar' they are. \n* The new representation is **non-linear**. This makes it harder to interpret but it can be very effective at 'unravelling' highly non-linear data.\n\nThe main downside to t-SNE is that is **very slow** compared to the other dimensionality techniques. This is because it makes calculations on a pair-wise basis, which does not scale well with large datasets."
Even with just 5% of the data it still takes several minutes to run.
"\n    7.5 UMAP\n\n\n**UMAP**, which stands for **Uniform Manifold Approximation and Projection** was proposed by Leland McInnes, John Healy and James Melville in their [2018 paper](http://gobie.csb.pitt.edu/SML/umap.pdf).\n\nIt is similar to t-SNE in that it learns a non-linear mapping that preserves clusters but its main advantage is that it is **significantly faster**. It also tends to do better at preserving **global structure** of the data compared to t-SNE. \n\nReference: https://pair-code.github.io/understanding-umap/ "
UMAP's **connectivity plot** is a weighted graph that gives insight into the representation of the embedding. It basically shows which connections were most important when creating the projection. 
"* Our data is sign language digits dataset.\n* Shape of our data is (2062,64,64). \n    * 2062 is number of images.\n    * There are 64x64 pixels.\n* Lets load the data. We will use all images that are from zero to nine.\n* There are totally 2062 images.\n* I will plot one sample from each digits."
"* As a training set we will use all images.\n* As a test set we will choose ten images and use them.\n* reshape(-1) :  It simply means that it is an unknown dimension and we want numpy to figure it out\n* Difference between ""//"" and ""/"": for example 4097/2 = 2048.5 (division) and 4097//2 = 2048\n "
"## Principle Componenet Analysis (PCA)\n* I think it is very cool and good understanding way of PCA. \n* Fundemental dimension reduction technique\n* It is real life example. I hope it makes more sence for you.\n* Now lets try one more thing. \n* As you remember training time is almost 45 second. I think it is too much time. The reason of this time is number of sample(2062) and number of feature(4096). \n* There is two way to decrease time spend.\n    1. Decrease number of sample that I do no recommend.\n    1. Decrease number of features. As you see from images all signs are in the middle of the frames. Therefore, around signs are same for all signs. \n    * Think that all of the pixels are feature. Features which are out of the red frame is useless but pixels in red frame take part in  training and prediction steps.  Therefore, we need to make dimension reduction. \n    * PCA: Principle componenet analysis\n        * One of the most popular dimension reduction technique.\n        * PCA  uses high variances. It means that it likes diversity. For example compare two images above. oUt of red frame there is no diversity (high variance). On the other hand, in red frames there is diversity.\n            * first step is decorrelation:\n                * rotates data samples to be aligned with axes\n                * shifts data SAmples so they have mean zero\n                * no information lost\n                * fit() : learn how to shift samples\n                * transform(): apply the learned transformation. It can also be applies test data( We do not use here but it is  good to know it.)\n                * Resulting PCA features are not linearly correlated\n                * Principle components: directions of variance\n            * Second step: intrinsic dimension: number of feature needed to approximate the data essential idea behind dimension reduction\n                * PCA identifies intrinsic dimension when samples have any number of features\n                * intrinsic dimension = number of PCA feature with significant variance\n* Lets apply PCA and visualize what PCA says to us."
"* In sign language digits, most important things are fingers\n* After PCA, as you can see fingers are emphasized.\n* Try other images and play with n_components"
### 1.3 - Exploring
"SibSp and Parch don't seem to have a clear relationship with the target, so put them together can be a good idea.\nFor Ticket and Cabin a good strategie can be count the number of caracteres."
## Data Interaction\n* Scatter plot
## Data Visualization\n* Box and density plots
## Data Visualization\n* Box and density plots
## Data Visualization\n* Grouping of One hot encoded attributes
## Data Visualization\n* Grouping of One hot encoded attributes
## Data Cleaning\n* Remove unnecessary columns
## Feature selection\n* RFE
#Feature Selection\n* SelectPercentile
#Feature Selection\n* SelectPercentile
#Feature Selection\nRanking summary
#Feature Selection\nRanking summary
#Feature Selection\nRank features based on median
# 1. Importing the necessary libraries
#  2. Reading the datasets
## Exploring the 'keyword' column\nThe keyword column denotes a keyword from the tweet.Let's look at the top 20 keywords in the training data
Let's see how often the word 'disaster' come in the dataset and whether this help us in determining whether a tweet belongs to a disaster category or not.
"## Exploring the 'location' column\nEven though the column `location` has a number of missing values, let's see the top 20 locations present in the dataset. Since some of the locations are repeated, this will require some bit of cleaning."
"> #  4. Text Data Preprocessing\n\n## 1. Data Cleaning\n\nBefore we start with any NLP project we need to pre-process the data to get it all in a consistent format.We need to clean, tokenize and convert our data into a matrix. Some of the  basic text pre-processing techniques includes:\n\n* Make text all **lower case** or **uppercase** so that the algorithm does not treat the same words in different cases as different\n* **Removing Noise** i.e everything that isn‚Äôt in a standard number or letter i.e Punctuation, Numerical values,  common non-sensical text (/n)\n* **Tokenization**: Tokenization is just the term used to describe the process of converting the normal text strings into a list of tokens i.e words that we actually want. Sentence tokenizer can be used to find the list of sentences and Word tokenizer can be used to find the list of words in strings.\n* **Stopword Removal**: Sometimes, some extremely common words which would appear to be of little value in helping select documents matching a user need are excluded from the vocabulary entirely. These words are called stop words\n\n### More data cleaning steps after tokenization:\n\n* **Stemming**: Stemming is the process of reducing inflected (or sometimes derived) words to their stem, base or root form‚Ää‚Äî‚Äägenerally a written word form. Example if we were to stem the following words: ‚ÄúStems‚Äù, ‚ÄúStemming‚Äù, ‚ÄúStemmed‚Äù, ‚Äúand Stemtization‚Äù, the result would be a single word ‚Äústem‚Äù.\n* **Lemmatization**: A slight variant of stemming is lemmatization. The major difference between these is, that, stemming can often create non-existent words, whereas lemmas are actual words. So, your root stem, meaning the word you end up with, is not something you can just look up in a dictionary, but you can look up a lemma. Examples of Lemmatization are that ‚Äúrun‚Äù is a base form for words like ‚Äúrunning‚Äù or ‚Äúran‚Äù or that the word ‚Äúbetter‚Äù and ‚Äúgood‚Äù are in the same lemma so they are considered the same.\n* Parts of speech tagging\n* Create bi-grams or tri-grams\nAnd more...\n\nHowever, it is not necessary that you would need to use all these steps. The usage depends on your problem at hand. Sometimes removal of stop words helps while at other times, this might not help.Here is a nice table taken from the blog titled : [All you need to know about Text Preprocessing for Machine Learning & NLP](https://kavita-ganesan.com/text-preprocessing-tutorial/#.Xi2BhhczZTY) that summarizes how much preprocessing you should be performing on your text data:\n\n![](https://kavita-ganesan.com/wp-content/uploads/2019/02/Screen-Shot-2019-02-23-at-1.36.52-PM-590x270.png)"
Just for fun let's create a wordcloud of the clean text to see the most dominating words in the tweets.
"## 2. Tokenization\n\nTokenization is a process that splits an input sequence into so-called tokens where the tokens can be a word, sentence, paragraph etc. Base upon the type of tokens we want, tokenization can be of various types, for instance"
"## 4. Token normalization\n\nToken normalisation means converting different tokens to their base forms. This can be done either by:\n\n- **Stemming** :  removing and replacing suffixes to get to the root form of the word, which is called the **stem** for instance cats - cat, wolves - wolv \n- **Lemmatization** : Returns the base or dictionary form of a word, which is known as the **lemma** \n\n[*source*](https://www.coursera.org/learn/language-processing/lecture/SCd4G/text-preprocessing)"
"It is important to note here that stemming and lemmatization sometimes donot necessarily improve results as at times we donot want to trim words but rather preserve their original form. Hence their usage actually differs from problem to problem. For this problem, I will not use these techniques."
## Importing all necessary libraries\n
### Load the data
" 2.1 Box Plots\n\nBecause the foremost goal of modeling is to understand variation in the attribute, the first step should be to understand the distribution of the attribute. For a continuous attribute such as the Free Sulfur Dioxide attribute, it is important to understand if the attribute has a symmetric distribution, if the distribution has a decreasing frequency of larger observations (i.e., the distribution is skewed), if the distribution appears to be made up of two or more individual distributions (i.e., the distribution has multiple peaks or modes), or if there appears to be unusually low or high observations (i.e outliers).\n\n**Box Plot** help us visualize distribution of single attribute which further help us understanding dataset. \n\n![](https://miro.medium.com/max/1400/1*2c21SkzJMf3frPXPAR_gZA.png)\n\nMoving towards technical definition of Box Plots it is a method for graphically demonstrating the locality, spread and skewness groups of numerical data through their quartiles. In addition to the box on a box plot, there can be lines (which are called whiskers) extending from the box indicating variability outside the upper and lower quartiles, thus, the plot is also termed as the box-and-whisker plot and the box-and-whisker diagram. Outliers that differ significantly from the rest of the dataset may be plotted as individual points beyond the whiskers on the box-plot. \n\n\n**Median**\nThe median (middle quartile) marks the mid-point of the data and is shown by the line that divides the box into two parts. Half the scores are greater than or equal to this value and half are less.\n\n**Inter-quartile range(IQR)**\nThe middle ‚Äúbox‚Äù represents the middle 50% of scores for the group. The range of scores from lower to upper quartile is referred to as the inter-quartile range. The middle 50% of scores fall within the inter-quartile range.\n\n**Upper quartile**\nSeventy-five percent of the scores fall below the upper quartile.\n\n**Lower quartile**\nTwenty-five percent of scores fall below the lower quartile.\n\n**Whiskers**\nThe upper and lower whiskers represent scores outside the middle 50%. Whiskers often (but not always) stretch over a wider range of scores than the middle quartile groups."
If we try to interpret above box plot it has lots of outliers all values greater thar 42 are considered as outlier. In terms of skewness if we try to analyze it is right skewed or positive skewed means density of data is more near the origin.
If we try to interpret above box plot it has lots of outliers all values greater thar 42 are considered as outlier. In terms of skewness if we try to analyze it is right skewed or positive skewed means density of data is more near the origin.
Above Box plot again has lots of outliers as lots of values are greater than (Q3 + 1.5IQR) and some vales are less than (Q1 - 1.5IQR).
Above Box plot again has lots of outliers as lots of values are greater than (Q3 + 1.5IQR) and some vales are less than (Q1 - 1.5IQR).
Lets look at the **drawbacks of box plot**
Lets look at the **drawbacks of box plot**
"A **drawback of the box plot** is that it is not effective at identifying distributions that\nhave multiple peaks or modes. As an example, consider the distribution of citric acid. Part (a) of the figure above is a histogram of the\ndata. Like box plots, histograms are simple to create, and these figures offer the ability to see additional distributional characteristics. In the citric acid distribution, there are more than one peak can be seen The box plot (b) is unable to capture this important nuance. To achieve a compact visualization of the distribution that retains histogram-like characteristics, Hintze and Nelson (1998) developed the violin plot (c)."
" 2.2 Violin Plots\n\nViolin plot is created by generating a density or distribution of the data and its mirror image. In the above figure we can see the violin plot, where we can now see the many distinct peaks in citric acid distribution. The lower quartile, median, and upper quartile can be added to a violin plot to also consider this information in the overall assessment of the distribution."
"We can read above violin plot like we read the box plots, the difference is just in violin plot we can also get density of attribute which help us understand more about the attribute. The\n\nFor more deeper understanding you can refer this blog post [Violin plots explained](https://towardsdatascience.com/violin-plots-explained-fb1d115e023d)"
" 2.3 Histograms\n\nA histogram is a graphical representation that organizes a group of data points into user-specified ranges. Similar in appearance to a bar graph, the histogram condenses a data series into an easily interpreted visual by taking many data points and grouping them into logical ranges or bins.\n\n\nHistograms are commonly used in statistics to demonstrate how many of a certain type of variable occurs within a specific range. For example, a census focused on the demography of a country may use a histogram to show how many people are between the ages of 0 - 10, 11 - 20, 21 - 30, 31 - 40, 41 - 50, etc. This histogram would look similar to the example below."
"We can consider how to define the y-axis. The most basic label is to use the frequency of occurrences observed in the data, but one could also use percentage of total or density instead."
" 3.1 1:1 Transformations\n\nThere are a variety of modifications that can be made to an individual attribute that might improve its utility in a model. The first type of transformations to a single attribute discussed here are those that change the scale of the data. A good example is the transformation described in plot below. In that case, the attribute residual sugar had very skewed distributions and it was shown that using the inverse of the log values improves distribution."
" 3.1.1 Box-Cox and Yeo-Johnson Transformation\n\n\nA Box-Cox transformation (Box and Cox, 1964) was used to estimate this transformation. The Box-Cox procedure, originally intended as a transformation of a model‚Äôs outcome, uses maximum likelihood estimation to estimate a transformation\nparameter $\lambda$ in the equation\n\n\n![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRkHjrl213TAXMJVBHK3yVaRLxxi-_dUqPJxg&usqp=CAU)\n\nIn this procedure, $\lambda$ is estimated from the data. Because the parameter of interest is in the exponent, this type of transformation is called a **power transformation**. Some values of $\lambda$ map to common transformations, such as $\lambda$ = 1 (no transformation), $\lambda$ = 0 (log), $\lambda$ = 0.5 (square root), and $\lambda$ = ‚àí1 (inverse). As you can see, the Box-Cox transformation is quite flexible in its ability to address many different data distributions. \n\n**It is important to note that the Box-Cox procedure can only be applied to data that is strictly positive.**\n\nTo address this problem, **Yeo and Johnson (2000)** devised an analogous procedure that can be used on any numeric data.\n\nAlso, note that both transformations are unsupervised since, in this application, the outcome is not used in the computations. While the transformation might improve the attribute distribution, it has no guarantee of improving the model. However, there are a variety of parametric models that utilize polynomial calculations on the attribute data, such as most linear models, neural networks, and support vector machines. In these situations, a skewed attribute distribution can have a harmful effect on these models since the tails of the distribution can dominate the underlying calculations.\n"
" 3.1.2 Logit Transformation\n\nAnother important transformation to an individual variable is for a variable that has values bounded between zero and one, such as proportions. The problem with modeling this type of outcome is that model predictions might may not be guaranteed to be within the same boundaries. For data between zero and one, the logit transformation could be used. If p is the variable, the logit transformations is\n\n![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcT82WhzCNQVRXt1NE-rnzE_6sqsQ7yZxK-tXg&usqp=CAU)\n\nThis transformation changes the scale from values between zero and one to values between negative and positive infinity. On the extremes, when the data are absolute zero or one, a small constant can be added or subtracted to avoid division by zero. Once model predictions are created, the inverse logit transformation can be used to place the values back on their original scale. An alternative to the logit transformation is the arcsine transformation. This is primarily used on the square root of the proportions (e.g., y = arcsine(sqrt(p))."
" 3.1.3 Centring\n\n\nAnother common technique for modifying the scale of a predictor is to standardize its value in order to have specific properties. Centering a predictor is a common technique. The predictor‚Äôs training set average is subtracted from the predictor‚Äôs individual values. When this is applied separately to each variable, the collection of variables would have a common mean value (i.e., zero). Similarly, scaling is the process of dividing a variable by the corresponding training set‚Äôs standard deviation. This ensures that that variables have a standard deviation of one. \n\n![](https://cdn-images-1.medium.com/max/1600/0*aR0ivCUZJjM9DFDw.png)\n\nAlternatively, range scaling uses the training set minimum and maximum values to translate the data to be within an arbitrary range (usually zero and one). Again, it is emphasized that the statistics required for the transformation (e.g., the mean) are estimated from the training set and are applied to all data sets (e.g., the test set or new samples). These transformations are mostly innocuous and are typically needed when the model requires the predictors to be in common units. For example, when the distance or dot products between predictors are used (such as K-nearest neighbors or support vector machines) a standardization procedure is essential."
" 3.1.3 Centring\n\n\nAnother common technique for modifying the scale of a predictor is to standardize its value in order to have specific properties. Centering a predictor is a common technique. The predictor‚Äôs training set average is subtracted from the predictor‚Äôs individual values. When this is applied separately to each variable, the collection of variables would have a common mean value (i.e., zero). Similarly, scaling is the process of dividing a variable by the corresponding training set‚Äôs standard deviation. This ensures that that variables have a standard deviation of one. \n\n![](https://cdn-images-1.medium.com/max/1600/0*aR0ivCUZJjM9DFDw.png)\n\nAlternatively, range scaling uses the training set minimum and maximum values to translate the data to be within an arbitrary range (usually zero and one). Again, it is emphasized that the statistics required for the transformation (e.g., the mean) are estimated from the training set and are applied to all data sets (e.g., the test set or new samples). These transformations are mostly innocuous and are typically needed when the model requires the predictors to be in common units. For example, when the distance or dot products between predictors are used (such as K-nearest neighbors or support vector machines) a standardization procedure is essential."
 3.2 1:Many Transformations\n\n1:Many transformations can be made on a single numeric predictor to expand it to many predictors. These one-to-many transformations of the data can be used to improve model performance. Will discuss following 1:Many transformation methods:-\n\n- Nonlinear Features via Basis Expansions and Splines\n- Discretize the attributes
Lets try to fetch information of linear correlated attributes from the data.
"From above heatmap of correlation matrix we can interpret that many attributes are showing linear correlation with each other. So, Lets try to reduce redundancy from the data.\n\nJust to make things simple lets predict the attribute 'density' instead of attribute 'quality'."
\n## **1. Library and data loading** ##
\n## **2. Data cleaning** ##
\n## **4. Covariance Matrix. Variability comparison between categories of variables** 
\n## **5. Some charts to see data relationship** 
Distribiution and density by Age
Separate by treatment
Separate by treatment
How many people has been treated?
How many people has been treated?
Draw a nested barplot to show probabilities for class and sex
Draw a nested barplot to show probabilities for class and sex
Barplot to show probabilities for family history
Barplot to show probabilities for family history
Barplot to show probabilities for care options
Barplot to show probabilities for care options
Barplot to show probabilities for benefits
Barplot to show probabilities for benefits
Barplot to show probabilities for work interfere
Barplot to show probabilities for work interfere
\n## **6. Scaling and fitting** ##\n\n
### **Tuning with cross validation score**
### **Tuning with GridSearchCV** ###
### **Tuning with GridSearchCV** ###
### **Tuning with RandomizedSearchCV** ###
\n\n\n\n0¬†¬†IMPORTS¬†¬†¬†¬†‚§í
\n\n\n\n1¬†¬†SETUP¬†¬†¬†¬†‚§í\n\n---\n
\n\n3.2 CREATE TF.DATA.DATASET\n\n---\n\n**INPUT**\n* Raw Image (256x256x3)\n\n**OUTPUT/TARGET**\n* Segmented Image (256x256x[3|1])\n\n---\n
\n\n\n\n\n\n\n    4¬†¬†MODEL TRAINING¬†¬†¬†¬†‚§í\n\n\n---
\n\n4.2 TRAINING THE MODEL\n\n[REF]\n\n---
\n\n4.3 VALIDATE AND VISUALIZE\n\n---
\n\n4.3 VALIDATE AND VISUALIZE\n\n---
\n\n\n\n\n\n\n    5¬†¬†MODEL INFERENCE¬†¬†¬†¬†‚§í\n\n\n---\n\n**Only run this when things are done as masks will be deleted**
"##  Classification Metrices \n\n- Dataset: Pima Indians onset of diabetes dataset.\n- Evaluation Algorithm: Logistic Regression, SGDClassifier, RandomForestClassifier."
"* Quick Note : SkLearn's ""predict_log_proba"" gives the logarithm of the probabilities, this is often handier as probabilities can become very, very small."
"###  ROC Curve \n\nROC can be broken down into sensitivity and specificity. Choosing the best model is sort of a balance between predicting 1's accurately or 0's accurately. In other words sensitivity and specificity.\n\n- True Positive Rate (Sensitivity/ Recall) : True Positive Rate is defined as TP/ (FN+TP). True Positive Rate corresponds to the proportion of positive data points that are correctly considered as positive, with respect to all positive data points.\n\n- False Positive Rate (Specificity) : False Positive Rate is defined as FP / (FP+TN). False Positive Rate corresponds to the proportion of negative data points that are mistakenly considered as positive, with respect to all negative data points.\n\nTrue Positive Rate and False Positive Rate both have values in the range [0, 1]. TPR and FPR both are computed at threshold values such as (0.00, 0.02, 0.04, ‚Ä¶., 1.00) and a graph is drawn."
"#### Interpreting ROC Plot:\n\nInterpreting the ROC plot is very different from a regular line plot. Because, though there is an X and a Y-axis, we don't read it as: for an X value of 0.25, the Y value is .9.\n\nInstead, what we have here is a line that traces the probability cutoff from 1 at the bottom-left to 0 in the top right.\n\nThis is a way of analyzing how the sensitivity and specificity perform for the full range of probability cutoffs, that is from 0 to 1.\n\nIdeally, if we have a perfect model, all the events will have a probability score of 1 and all non-events will have a score of 0. For such a model, the area under the ROC will be a perfect 1.\n\nSo, if we trace the curve from bottom left, the value of probability cutoff decreases from 1 towards 0. If we have a good model, more of the real events should be predicted as events, resulting in high sensitivity and low FPR. In that case, the curve will rise steeply covering a large area before reaching the top-right.\n\nTherefore, the larger the area under the ROC curve, the better is the model.\n\nThe ROC curve is the only metric that measures how well the model does for different values of prediction probability cutoffs.\n\n"
"This confirms that raising the threshold decreases recall. The instance actually represents a 1(True), and the classifier detects it when the threshold is 0, but it misses it when the threshold is increased to 2.\n\nTo decide which threshold to use, we first need to get the scores of all instances in the training set using the cross_val_predict() function again, but this time specifying that you want it to return decision scores/probability instead of class:"
"Now we can simply select the threshold value that gives us the best precision/recall tradeoff for our task. let‚Äôs suppose you decide to aim for 80% recall. You look up the first plot (zooming in a bit) and find that you need to use a threshold of about 0.32. To make predictions (on the training set for now), instead of calling the classifier‚Äôs predict() method, you can just run this code:"
##  Regression Metrices \n\n- Dataset:  Boston House Price dataset.\n- Evaluation Algorithm: Logistic Regression.
"###  Mean Absolute Error \n- Average of the difference between the Original Values and the Predicted Values.\n- Do not gives any idea of the direction of the error i.e. whether we are under predicting the data or over predicting the data.\n- Smaller the MAE, better is the model.\n- Robust to outliers\n- Range (0, + infinity]\n\n$$ Mean\ Absolute\ Error = \frac{1}{N} \sum_{i=1}^{N} |y_{i} -  \hat{y_{i}}|$$"
### FARE & SURVIVAL
### 'SibSp' & 'Parch' PLOTS W.R.T 'SURVIVED' 
### 'SibSp' & 'Parch' PLOTS W.R.T 'SURVIVED' 
- FILL THE MISSING VALUES IN THE DATA 
### AGE & SURVIVAL
### FARE & AGE w.r.t SURVIVED
### INITIALS & AGE GROUP
### SURVIVAL vs INITIALS 
### SURVIVAL vs INITIALS 
"## INSIGHTS\n- FEMALE PASSENGERS WERE SAVED MORE THAN MALE PASSENGERS\n- CLASS 1, CLASS 2, CLASS 3 WAS ALSO CONSIDERED WHILE SAVING THE PASSENGERS\n- NO SUCH PREFERENCE CAN BE SAID ABOUT THE PASSENGERS BASED ON PORT OF EMBARKATION\n- 'Parch' & 'SibSp' DON'T GIVE US MUCH ABOUT THE DATA [MAYBE I NEED TO WORK ON THIS FACTOR ]\n- DISTRIBUTION OF 'FARE' SHOWS PREFERENCE GIVEN TO PEOPLE HAVING PAID MORE FOR FARE THAN OTHERS\n- PROBABLY BECAUSE OF CLASS 1, CLASS 2, CLASS 3\n- WE ALSO CHECKED THE SURVIVAL OF PASSENGERS W.R.T THEIR INITIALS WHICH CAN HELP FOR BETTER RESULTS\n- 'AGE' CLEARLY SHOWS A BULGE AT VALUE 28 WHICH WE FILLED IN PLACE OF 'NAN'\n- WE WILL DROP 'PassengerId','TICKET','CABIN'"
This is likely a very valuable feature for our model. If we know how many people are in a match we can normalize other features and get stronger predictions on individual players.
There are a few matches with fewer than 75 players that are not displayed here. As you can see most of the matches are nearly packed a have nearly 100 players. It is nevertheless interesting to take these features into our analysis.
Let's plot the total kills for every player first. It doesn't look like there are too many outliers.
Let's take a closer look.
Most kills are made from a distance of 100 meters or closer. There are however some outliers who make a kill from more than 1km away. This is probably done by cheaters.
Let's take a look at the players who make these shots.
**Feature importance for top features**
## Correlations
**Predictive quality of kills**
**Predictive quality of walkDistance**
**Predictive quality of walkDistance**
# Final Random Forest Model 
Let's look at the point of visualization
As you see the sale price value is right skewed. We need to make this normal distributed.
As you see the sale price value is right skewed. We need to make this normal distributed.
we can see the most corelated parameters in numerical values above plotting. And we can pick these as features for our macine learning model.
"Weight decay, or *L2 regularization*, consists in adding to your loss function the sum of all the weights squared. Why do that? Because when we compute the gradients, it will add a contribution to them that will encourage the weights to be as small as possible.\n\nWhy would it prevent overfitting? The idea is that the larger the coefficients are, the sharper canyons we will have in the loss function. If we take the basic example of a parabola, `y = a * (x**2)`, the larger `a` is, the more *narrow* the parabola is:"
"So, letting our model learn high parameters might cause it to fit all the data points in the training set with an overcomplex function that has very sharp changes, which will lead to overfitting.\n\nLimiting our weights from growing too much is going to hinder the training of the model, but it will yield a state where it generalizes better. Going back to the theory briefly, weight decay (or just `wd`) is a parameter that controls that sum of squares we add to our loss (assuming `parameters` is a tensor of all parameters):\n\n``` python\nloss_with_wd = loss + wd * (parameters**2).sum()\n```\n\nIn practice, though, it would be very inefficient (and maybe numerically unstable) to compute that big sum and add it to the loss. If you remember a little bit of high school math, you might recall that the derivative of `p**2` with respect to `p` is `2*p`, so adding that big sum to our loss is exactly the same as doing:\n\n``` python\nparameters.grad += wd * 2 * parameters\n```\n\nIn practice, since `wd` is a parameter that we choose, we can just make it twice as big, so we don't even need the `*2` in this equation. To use weight decay in fastai, just pass `wd` in your call to `fit` or `fit_one_cycle`:"
"So, for instance, even if you don't normally enjoy detective movies, you might enjoy *LA Confidential*!\n\nIt is not quite so easy to directly interpret the embedding matrices. There are just too many factors for a human to look at. But there is a technique that can pull out the most important underlying *directions* in such a matrix, called *principal component analysis* (PCA). We will not be going into this in detail in this book, because it is not particularly important for you to understand to be a deep learning practitioner, but if you are interested then we suggest you check out the fast.ai course [Computational Linear Algebra for Coders](https://github.com/fastai/numerical-linear-algebra). Here's what our movies look like based on two of the strongest PCA components."
"We can see here that the model seems to have discovered a concept of *classic* versus *pop culture* movies, or perhaps it is *critically acclaimed* that is represented here."
### Sidebar: kwargs and Delegates
### End sidebar
"### * From above two plots it is seen that 'Global_intensity' and 'Global_active_power' correlated. But 'Voltage', 'Global_active_power' are less correlated. This is important observation for machine learning purpose. "
# Correlations among features
"## **Hello Guys! I'm trying Glove embed, Build lstm model and predict output**\n\n### **Features selection: I'm simply take x is target and y is score**\n"
### **Count the score attribute**
### **Count the score attribute**
### **Feature selection**
"## If you like the content of this notebook, please consider upvoting it.\n\nNot only it will show to visitors that this notebook have valuable information, but it will also encourage me to produce more quality notebooks. :)"
"# Domain Specific Language (DSL)\n\nWe will build a domain specific language specialized on processing list of images. To allow easy chaining of keyword from this language together, each *function* provided by this language will be take one or more images and transform it to none, one or more. The final result of our program will then be a list of images.\n\nThe DSL is so constituted by a collection of functions of type `np.array -> [np.array]` and `[np.array] -> [np.array]`.\n\nThe first kind of function take an image, and produce a list of images (for example, the image split by different colors). The second type of function take a list of images and produce a new list (for exemple, intersect).\n[](http://)"
\n\n\nQuick Navigation\n\n* [Overview](#1)\n* [Annotations](#2)\n    \n    \n    \n* [ETT - Abnormal](#4)\n* [ETT - Borderline](#5)\n* [ETT - Normal](#6)\n* [NGT - Abnormal](#7)\n* [NGT - Borderline](#8)\n* [NGT - Incompletely Imaged](#9)\n* [NGT - Normal](#10)\n* [CVC - Abnormal](#11)\n* [CVC - Borderline](#12)\n* [CVC - Normal](#13)\n* [Swan Ganz Catheter Present](#14)\n    \n\n* [Venn Diagrams](#50)\n    \n    \n* [Submission](#100)
\nOverview
# 1. Importing necessary modules
# 2. Importing Dataframes
# 3. Target Value Distribution
*  We can observe that about 43% of tweets in the dataframe is about real disaster.
## 4.1 Number of tweets according to location(top 20)
"*  Most of the tweets are from USA,London,Canada."
## 4.2 Number of tweets according to location per class (0 or1)
* The graph says it all!!
# 5. Word clouds of each class
"*  By observing the above word cloud we can see some words like earthquake,fire,wildfires etc.., which refer to real disasters."
"*  By observing the above word cloud we can see some words like earthquake,fire,wildfires etc.., which refer to real disasters."
"*  By examining we can observe words like disney,Wrecked etc.., we need to explore more."
## 6.1 Word Frequency 
## 6.2 Bigram plots
## 7.1 Plotting of meta features vs each target class (0 or 1)
# 8. Histogram plots
## 8.1 Histogram Plots of number of words per each class (0 or 1)
## 8.2 Histogram Plots of number of characters per each class (0 or 1)
## 8.2 Histogram Plots of number of characters per each class (0 or 1)
## 8.3 Histogram Plots of number of punctuations per each class (0 or 1)
## 8.3 Histogram Plots of number of punctuations per each class (0 or 1)
## 8.4 Histogram plots of number of words in train and test sets
## 8.4 Histogram plots of number of words in train and test sets
## 8.5 Histogram plots of number of chars in train and test sets
## 8.5 Histogram plots of number of chars in train and test sets
## 8.6 Histogram plots of number of punctuations in train and test sets
## 8.6 Histogram plots of number of punctuations in train and test sets
# 9. Readability features 
"\n    10.2 | Main training function\n\n\n`train_val` is the main function used to train the model on the **training set** `train_dl` & evaluate on the **validation set** `val_dl`\n\n#### **FUNCTION INPUTS**\n\nThe function requires\n- **PyTorch Classifier** , `cnn_model` (we visualised in section 7)\n- Training **parameter dictionary** `params_train` (which contains both hyperparameters & input data)\n\n#### **FUNCTION PARAMETER DICTIONARY**\n\nThe parameter dictionary requires:\n- Number of training **iterations**,`epochs`\n- Training & validation **data loaders**, `train_dl`, `val_dl`\n- **Optimiser** & **loss function**,  `opt` & `loss_func`\n- **Learning rate adjustor** (on the fly) `lr_change`\n\n#### **POST TRAINING OUTPUT**\n\n`train_val` returns:\n- The best performing model on the validation dataset\n- The Loss per iteration\n- The Evaluation Metric per iteration (which is accuracy)"
"\n    10.3 | Training Process \n\n\n#### **SET PARAMETER DICTIONARY**\n\n- Define the parameters for training `params_train` & train classifier\n- We'll be training the classifier for **50 iterations** using the **Adam optimiser** & the **negative log likelihood loss** function\n- The learning rate will be adjusted on the fly using `ReduceLROnPlateau`, with a factor of 1/2"
### Import Libraries
### Reading Data
### **CNN related videos from:** https://www.appliedaicourse.com/\n### http://cs231n.github.io/convolutional-networks/\n### https://www.mathworks.com/solutions/deep-learning/convolutional-neural-network.html\n### https://blog.floydhub.com/building-your-first-convnet/\n### https://medium.com/@gopalkalpande/biological-inspiration-of-convolutional-neural-network-cnn-9419668898ac\n### https://blog.datawow.io/interns-explain-cnn-8a669d053f8b\n### https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5
\n![](https://i.imgur.com/Qaong5c.png)\n
\n    Libraries\n
---
\n    Colors\n
---
\n    Visualizations\n
Let's check out the distribution of the features.
"**Hardenss of water**: The simple definition of water hardness is the amount of dissolved calcium and magnesium in the water. Hard water is high in dissolved minerals, largely calcium and magnesium. You may have felt the effects of hard water, literally, the last time you washed your hands. Depending on the hardness of your water, after using soap to wash you may have felt like there was a film of residue left on your hands. In hard water, soap reacts with the calcium (which is relatively high in hard water) to form ""soap scum"". When using hard water, more soap or detergent is needed to get things clean, be it your hands, hair, or your laundry."
"**pH level:** The  pH  of  water  is  a  measure  of  the  acid‚Äìbase  equilibrium  and,  in  most  natural  waters,   is   controlled   by   the   carbon   dioxide‚Äìbicarbonate‚Äìcarbonate   equilibrium   system. An increased carbon dioxide concentration will therefore lower pH, whereas a decrease will cause it to rise. Temperature will also affect the equilibria and the pH. In pure  water,  a  decrease  in  pH  of  about  0.45  occurs  as  the  temperature  is  raised  by  25  ¬∞C.  In  water  with  a  buffering  capacity  imparted  by  bicarbonate,  carbonate  and  hydroxyl  ions,  this  temperature  effect  is  modified  (APHA,  1989).  The  pH  of  most  drinking-water lies within the range 6.5‚Äì8.5. Natural waters can be of lower pH, as a result of, for example, acid rain or higher pH in limestone areas."
"**pH level:** The  pH  of  water  is  a  measure  of  the  acid‚Äìbase  equilibrium  and,  in  most  natural  waters,   is   controlled   by   the   carbon   dioxide‚Äìbicarbonate‚Äìcarbonate   equilibrium   system. An increased carbon dioxide concentration will therefore lower pH, whereas a decrease will cause it to rise. Temperature will also affect the equilibria and the pH. In pure  water,  a  decrease  in  pH  of  about  0.45  occurs  as  the  temperature  is  raised  by  25  ¬∞C.  In  water  with  a  buffering  capacity  imparted  by  bicarbonate,  carbonate  and  hydroxyl  ions,  this  temperature  effect  is  modified  (APHA,  1989).  The  pH  of  most  drinking-water lies within the range 6.5‚Äì8.5. Natural waters can be of lower pH, as a result of, for example, acid rain or higher pH in limestone areas."
"**TDS**: TDS means concentration of dissolved particles or solids in water. TDS comprises of inorganic salts such as calcium, magnesium, chlorides, sulfates, bicarbonates, etc, along with many more inorganic compounds that easily dissolve in water. "
"**TDS**: TDS means concentration of dissolved particles or solids in water. TDS comprises of inorganic salts such as calcium, magnesium, chlorides, sulfates, bicarbonates, etc, along with many more inorganic compounds that easily dissolve in water. "
**Chloramines**: Chloramines (also known as secondary disinfection) are disinfectants used to treat drinking water and they:\n\n* Are most commonly formed when ammonia is added to chlorine to treat drinking water.\n* Provide longer-lasting disinfection as the water moves through pipes to consumers.\n\nChloramines have been used by water utilities since the 1930s.
**Chloramines**: Chloramines (also known as secondary disinfection) are disinfectants used to treat drinking water and they:\n\n* Are most commonly formed when ammonia is added to chlorine to treat drinking water.\n* Provide longer-lasting disinfection as the water moves through pipes to consumers.\n\nChloramines have been used by water utilities since the 1930s.
"**Sulfate**: Sulfate (SO4) can be found in almost all natural water. The origin of most sulfate compounds is the oxidation of sulfite ores, the presence of shales, or the industrial wastes.\nSulfate is one of the major dissolved components of rain. High concentrations of sulfate in the water we drink can have a laxative effect when combined with calcium and magnesium, the two most common constituents of hardness."
"**Sulfate**: Sulfate (SO4) can be found in almost all natural water. The origin of most sulfate compounds is the oxidation of sulfite ores, the presence of shales, or the industrial wastes.\nSulfate is one of the major dissolved components of rain. High concentrations of sulfate in the water we drink can have a laxative effect when combined with calcium and magnesium, the two most common constituents of hardness."
"**Conductivity**: Conductivity is a measure of the ability of water to pass an electrical current. Because dissolved salts and other inorganic chemicals conduct electrical current, conductivity increases as salinity increases. Organic compounds like oil do not conduct electrical current very well and therefore have a low conductivity when in water. Conductivity is also affected by temperature: the warmer the water, the higher the conductivity."
"**Conductivity**: Conductivity is a measure of the ability of water to pass an electrical current. Because dissolved salts and other inorganic chemicals conduct electrical current, conductivity increases as salinity increases. Organic compounds like oil do not conduct electrical current very well and therefore have a low conductivity when in water. Conductivity is also affected by temperature: the warmer the water, the higher the conductivity."
"**Organic Carbon**: Organic contaminants (natural organic substances, insecticides, herbicides, and other agricultural chemicals) enter waterways in rainfall runoff. Domestic and industrial wastewaters also contribute organic contaminants in various amounts. As a result of accidental spills or leaks, industrial organic wastes may enter streams. Some of the contaminants may not be completely removed by treatment processes; therefore, they could become a problem for drinking water sources. It is important to know the organic content in a waterway."
"**Organic Carbon**: Organic contaminants (natural organic substances, insecticides, herbicides, and other agricultural chemicals) enter waterways in rainfall runoff. Domestic and industrial wastewaters also contribute organic contaminants in various amounts. As a result of accidental spills or leaks, industrial organic wastes may enter streams. Some of the contaminants may not be completely removed by treatment processes; therefore, they could become a problem for drinking water sources. It is important to know the organic content in a waterway."
"**Trihalomethanes**: Trihalomethanes (THMs) are the result of a reaction between the chlorine used for disinfecting tap water and natural organic matter in the water. At elevated levels, THMs have been associated with negative health effects such as cancer and adverse reproductive outcomes."
"**Trihalomethanes**: Trihalomethanes (THMs) are the result of a reaction between the chlorine used for disinfecting tap water and natural organic matter in the water. At elevated levels, THMs have been associated with negative health effects such as cancer and adverse reproductive outcomes."
"**Turbidity**: Turbidity is the measure of relative clarity of a liquid. It is an optical characteristic of water and is a measurement of the amount of light that is scattered by material in the water when a light is shined through the water sample. The higher the intensity of scattered light, the higher the turbidity. Material that causes water to be turbid include clay, silt, very tiny inorganic and organic matter, algae, dissolved colored organic compounds, and plankton and other microscopic organisms."
"**Turbidity**: Turbidity is the measure of relative clarity of a liquid. It is an optical characteristic of water and is a measurement of the amount of light that is scattered by material in the water when a light is shined through the water sample. The higher the intensity of scattered light, the higher the turbidity. Material that causes water to be turbid include clay, silt, very tiny inorganic and organic matter, algae, dissolved colored organic compounds, and plankton and other microscopic organisms."
Scatter Plot Matrix helps in finding out the correlation between all the features.
Scatter Plot Matrix helps in finding out the correlation between all the features.
"As we can see, there seems to be very less correlation between all the features."
Let's make a Heatmap to visualize the correlation.
---
# Gathering a Basic Insight of our Data:\n\n\n\n\n\n## Summary:\n\n The distribution of  house prices  is right skewed.\n There is a drop in the number of houses sold during the year of 2010. \n
"## Right-Skewed Distribution Summary:\nIn a right skew or positive skew the mean is most of the times to the right of the median. There is a higher frequency of occurence to the left of the distribution plot leading to more exceptions (outliers to the right). Nevertheless, there is a way to transform this histogram into a normal distributions by using log transformations which will be discussed further below."
"## Right-Skewed Distribution Summary:\nIn a right skew or positive skew the mean is most of the times to the right of the median. There is a higher frequency of occurence to the left of the distribution plot leading to more exceptions (outliers to the right). Nevertheless, there is a way to transform this histogram into a normal distributions by using log transformations which will be discussed further below."
" Economic Activity: \n\n\nWe will visualize how the housing market in **Ames, IOWA** performed during the years 2006 - 2010 and how bad it was hit by the economic recession during the years of 2007-2008.  \n\n## Level of Supply and Demand (Summary):\n\nJune and July were the montnths in which most houses were sold. \n The  median house price  was at its peak in 2007 (167k) and it was at its lowest point during the year of 2010 (155k) a difference of 12k. This might be a consequence of the economic recession. \n Less houses were sold and built during the year of 2010 compared to the other years. \n\n\n"
## What Garages tells us about each Price Category:\n\n
# Miscellaneous and Utilities:\n
"\n\n## Interesting insights:\n\n1) **Overall Condition**: of the house or building, meaning that further remodelations are likely to happen in the future, either for reselling or to accumulate value in their real-estate.. \n2) **Overall Quality**: The quality of the house is one of the factors that mostly impacts SalePrice. It seems that the overall material that is used for construction and the finish of the house has a great impact on SalePrice. \n3) **Year Remodelation**: Houses in the **high** price range remodelled their houses sooner. The sooner the remodelation the higher the value of the house. \n"
## Which Material Combination increased the Price of Houses?\n\n\n Roof Material: Hip and Gable was the most expensive since people who bought high value houses tended to buy this material bor he rooftop.\n House Material: Houses made up of stone tend to influence positively the price of the house. (Except in 2007 for High Price House Values. )  \n\n
## Which Material Combination increased the Price of Houses?\n\n\n Roof Material: Hip and Gable was the most expensive since people who bought high value houses tended to buy this material bor he rooftop.\n House Material: Houses made up of stone tend to influence positively the price of the house. (Except in 2007 for High Price House Values. )  \n\n
"**Note:** Interestingly, the Masonry Veneer type of stone became popular after 2007 for the houses that belong to the **high** Price Range category. I wonder why? \n**For some reason during the year of 2007, the Saleprice of houses within the high range made of stone dropped drastically! \n\n"
"**Note:** Interestingly, the Masonry Veneer type of stone became popular after 2007 for the houses that belong to the **high** Price Range category. I wonder why? \n**For some reason during the year of 2007, the Saleprice of houses within the high range made of stone dropped drastically! \n\n"
 Quality of Neighborhoods \n\n\n\n## Which Neighborhoods had the best Quality houses?\n
"## Bivariate Analysis (Detecting outliers through visualizations):\n\n**There are some outliers in some of this columns but there might be a reason behind this, it is possible that these outliers in which the area is high but the price of the house is not that high, might be due to the reason that these houses are located in agricultural zones.**"
 Feature Engineering \n\n## Dealing with Missing Values:\n
## Categorical Encoding Class:\n\nThis is a way to encode our features in a way that it avoids the assumption that two nearby values are more similar than two distant values. This is the reason we should avoid using LabelEncoder to scale features (inputs) in our dataset and in addition the word **LabelEncoder** is used for scaling labels (outputs). This could be used more often in **binary classification problems** were no *association* exists between the outputs.
## Combine Attribute Class:\n\nThis class will help us to include the total area variable into our pipeline for further scaling.
**Let's compare the results with the ones found via Decision Tree.**
\n7.4.e ROC (Receiver Operating Curve) and AUC (Area Under Curve)\n\nTable of Contents
\n7.4.f The Visualization of the Tree\n\nTable of Contents
\n7.5 The Implementation of K-Nearest Neighbor (KNN)\n\nTable of Contents
\n8) THE COMPARISON OF MODELS\n\nTable of Contents
\n9) CONCLUSION\n\nTable of Contents
Load Packages
Import the Data
### Check for Class Imbalance
Classification augment
"**Created by *Peter Nagy* February 2017**  \n[github][1] \n[Linkedin](https://www.linkedin.com/in/peternagyjob/) \n[My other kernel on LSTM](https://www.kaggle.com/ngyptr/lstm-sentiment-analysis-keras) \n\n**Sentiment Analysis:**\nthe process of computationally identifying and categorizing opinions expressed in a piece of text, especially in order to determine whether the writer's attitude towards a particular topic, product, etc. is positive, negative, or neutral.\n\n  [1]: https://github.com/nagypeterjob"
"I decided to only do sentiment analysis on this dataset, therfore I dropped the unnecessary colunns, keeping only *sentiment* and *text*."
"As a next step I separated the Positive and Negative tweets of the training set in order to easily visualize their contained words.  After that I cleaned the text from hashtags, mentions  and links. Now they were ready for a WordCloud visualization which shows only the most emphatic words of the Positive and Negative tweets."
"Interesting to notice the following words and expressions in the positive word set:\n **truth**, **strong**, **legitimate**,  **together**, **love**, **job**\n\nIn my interpretation, people tend to believe that their ideal candidate is truthful, legitimate, above good and bad.\n\n\n----------\n\n\nAt the same time, negative tweets contains words like:\n**influence**, **news**, **elevator music**, **disappointing**, **softball**, **makeup**, **cherry picking**, **trying**\n\nIn my understanding people missed the decisively acting and considered the scolded candidates too soft and cherry picking."
Hereby I plotted the most frequently distributed words. The most words are centered around debate nights.
Using the nltk NaiveBayes Classifier I classified the extracted tweet word features.
![image.png](attachment:image.png)
 Objective  \n\nGoal of this kernel is following:\n- Basic Exploratory Data Analysis.\n- Feature Analysis
\n   Prerequisites 
\n   Load and Check Data  
\n Total number of matches Win by Each Teams
\nDoes toss winning affects the match winner ?
\nDoes toss winning affects the match winner ?
\nToss/Win Ratio
\nToss/Win Ratio
\nDo you know who won the most player of the match?
\nDo you know who won the most player of the match?
\nSuccess rate of winning matches\n
\nSuccess rate of winning matches\n
\n Most Titles Wins 
\n Most Titles Wins 
\nTop 10 Playes with Most Runs
\nTop 10 Playes with Most Runs
\nTop 10 Best Performances in a match\n
\nTop 10 Best Performances in a match\n
\nTop 10 bowlers till 2020
\nTop 10 bowlers till 2020
\nTop 10 Bowling Performance till 2020\n
\nTop 10 Bowling Performance till 2020\n
\n Top 10 Cities by Number Of matches\n
\n Top 10 Cities by Number Of matches\n
"\n Results Based on Duckworth-Lewis \n\n*Duckworth-Lewis is based on the idea of compensating rain-affected teams for the loss of ""run-scoring resources"". The D/L method works on the basis that teams have two resources to make runs with: the number of overs to be bowled and the number of wickets in hand.*"
"\n Results Based on Duckworth-Lewis \n\n*Duckworth-Lewis is based on the idea of compensating rain-affected teams for the loss of ""run-scoring resources"". The D/L method works on the basis that teams have two resources to make runs with: the number of overs to be bowled and the number of wickets in hand.*"
\nTop 10 Umpire to feature in max number of matches 
\nTop 10 Umpire to feature in max number of matches 
\n Season wise match summary of matches won by runs 
\n Season wise match summary of matches won by runs 
\n Top Most Dissmisal Reason 
\n Top Most Dissmisal Reason 
\n Top 10 Best Fielders in the Field 
\n Top 10 Best Fielders in the Field 
\n Toss Decision 
\n Toss Decision 
\n Analysis Over by Over 
\n Analysis Over by Over 
"### If these kernels impress you,give them an Upvote."
The correlation matrix and the distance matrix look like this
"This is an interesting chessboard. We can see that there are highly (anti-)correlated features. In particular, three clusters stand out: `ft_17` to `ft_40`, the two overlapping squares in `ft_73` to `ft_93` and `ft_85` to `ft_117`, and the *chessboard* between `ft_121` and `ft_129`. A very important **caveat** at this point: we are examining correlations for the first thirty days of trading. Correlations are not static entities, on the contrary, they tend to change and evolve depending on the time period and the market regime. Analyzing the stability of these clusters over time is an interesting point that could be analyzed. Stable clusters could be thought as intrinsic properties of the dataset rather than temporary events."
"### 4.1 Graph representation\n\nAs in the correlation-based case, we can create a minumum spanning tree to visualize the *features network*."
"The spanning tree calculated from the information-based metric seems more *stretched*. The features seem to be further apart and less clustered. However, the features that we had identified as highly correlated are also extremely close here (`ft_60`-`ft_61`, `ft_65`-`ft_66`, `ft_62`-`ft_63`, `ft_67`-`ft_68` and so on. Let's see what happens when we plot the distance matrix and compare it to the distance matrix obtained from the correlation based distance."
"### 4.2 Information-based clustering\n\nAs in the correlation-based case, we can use the distance matrix to obtain a linkage matrix and the final clustering. Before proceeding further, it is interesting to compare the correlation-based distance matrix and with the information-based distance matrix."
"Correlation-based and information-based distance matrices show very similar patterns. The information-based distance is on average higher than its correlation-based counterpart (see chart below). It would be interesting to investigate this result further: Does it mean that the information distance is ""less sensitive""? Or perhaps that it is better able to detect meaningful relationships?"
"Correlation-based and information-based distance matrices show very similar patterns. The information-based distance is on average higher than its correlation-based counterpart (see chart below). It would be interesting to investigate this result further: Does it mean that the information distance is ""less sensitive""? Or perhaps that it is better able to detect meaningful relationships?"
Let's proceed with the linkage matrix.
\n\n# Table of Contents\n\n* [0. Introduction](#0)   \n* [1. Templates](#1)\n* [2. Basic Plots](#2)\n    * [2.1 Histogram: basic](#2.1)\n    * [2.2 Histogram: ordered](#2.2)\n    * [2.3 Histogram: oredered and highlighted](#2.3)\n    * [2.4 Bar plots/horizontal histogram: (+ annotation)](#2.4)\n    * [2.5 Scatter/line plots](#2.5)\n    * [2.6 Pie Charts](#2.6)\n* [3. Distribution/kde plot](#3)\n* [4. Correlation Heatmaps](#4)\n    * [4.1 Correlation Heatmaps:full matrix ](#4.1)\n    * [4.2 Correlation Heatmaps:lower-triangular matrix ](#4.2)\n    * [4.3 Correlation Heatmaps:annotated matrix ](#4.3)\n* [5. Annotations](#5)\n* [6. Subplots](#6)\n    * [6.1 Mixed plots/subplots](#6.1)\n    * [6.2 Facet grids/pairplots](#6.2)\n    * [6.3 Daigonal Facet grids (custom made)](#6.3)\n* [7. Radar Charts](#7)\n* [8. 3D Plots](#8)\n* [9. Choropleth Map (Animation)](#9)\n* [10. Bonus: Getting creative with annotated heatmaps](#10)\n* [11. Reference](#11)
 Import libraries and load datasets 
 Import libraries and load datasets 
"\n1. Templates\n\nAs good a plotting library as plotly is, its default template is not pleasing to the eye and we often need to customize in order to have an informative as well as better looking plots and charts. Plotly offers the following background/plot-area templates to choose from. \n\n >- `plotly` \n >- `ggplot2`     \n >- `seaborn`\n >- `plotly_dark`\n >- `simple_white`\n     \n**Note**: You can also customize your own backgroud/paper color if you wish to. Let's demonstrate using the titanic dataset.\n"
Back to top\n\n\n2. Basic Plots\n\n\n2.1 Histogram: basic\n
\n2.2 Histogram: ordered\n\n> Ordered bar/histogram according to y-value (percentage of students in each race/ethnicity group).
\n2.2 Histogram: ordered\n\n> Ordered bar/histogram according to y-value (percentage of students in each race/ethnicity group).
\n2.3 Histogram: ordered and highlighted\n\n> Highlight *important* message you want to communicate. We can do so by assigning different colors to specific categories (max and min in this example)
\n2.3 Histogram: ordered and highlighted\n\n> Highlight *important* message you want to communicate. We can do so by assigning different colors to specific categories (max and min in this example)
\n2.4 Bar plots/Horizontal histogram
\n2.4 Bar plots/Horizontal histogram
\n\n2.5 Scatter/line plots\n\n>- Highlight only the head story while keeping others in the background\n>- Add secondary y-axis\n
\n\n2.5 Scatter/line plots\n\n>- Highlight only the head story while keeping others in the background\n>- Add secondary y-axis\n
"\n2.6 Pie Charts\n\nGood practice while using Pie charts:\n>- Avoid using/displaying `too many categories` in a pie-chart. If you have to display more that say 10 categories, may be its time to consider other chart types.\n>- Hide a `long-list of legends`. When it is too long it's a distruction. \n>- Use `pulled-sector` only to highlight something interesting, not because it looks cool.\n>- Use `sunburst` charts to your advantage. They are quite useful in communicating hierarchical data.  \n>- If you use `donut` chart, use the center to display useful information. However, try to avoid very thin donut pies."
"\n4. Correlation Heatmaps\n\nCorrelation heatmap is a very good way of summerizing how features of a dataset are related to one another or with the target variable. It gives a helicopter-view of all the features in a compact and beautifull correlation matrix. Often times seaborn offers an excellent plotting functon and it is a go-to option even when people try to make a plolty-only EDA. The main reason is that plotly's default heatmap is not pleasing to the eye. However, when cutomized properly plotly can also be beautifully pleasing with the added interactivness as well.\n\nBellow are few examples. \n\n\n4.1 Correlation Heatmap: full matrix"
\n4.2 Correlation Heatmap: annotated full matrix
\n4.2 Correlation Heatmap: annotated full matrix
\n4.3 Correlation Heatmap: lower-triangular & annotated matrix
\n4.3 Correlation Heatmap: lower-triangular & annotated matrix
Back to top\n\n\n5. Annotations\n\nWith annotation you can draw the attention of your reader to the a specific point(s) you would like to focus. Area annotation and text annotation are two examples as demonstated using the examples below. We have seen this already at [section 2.4](#2.4) (text annotation). \n
"\n6.2 Facet grids/pairplots\n\nYou can plot pairplot using the standard plolty function `figure_factory.create_scatterplotmatrix()`. The diagonal plot type can be chosen from *histogram*, *scatter* or *box* types. Background and paper colors can also be customized. "
Another option is to use the `go.Splom` from `plotly.graph_objects`. See the example below which uses the Iris dataset.
Another option is to use the `go.Splom` from `plotly.graph_objects`. See the example below which uses the Iris dataset.
\n6.3 Daigonal Facet grids (custom made)\n\nWe can also make a new pairplot with lower-triangular with a kde diagonal plot. This plot was inspiried by [subinium's](https://www.kaggle.com/subinium)  [notebook](https://www.kaggle.com/subinium/dark-mode-visualization-apple-version) which he did using matplotlib/seaborn library.
\n6.3 Daigonal Facet grids (custom made)\n\nWe can also make a new pairplot with lower-triangular with a kde diagonal plot. This plot was inspiried by [subinium's](https://www.kaggle.com/subinium)  [notebook](https://www.kaggle.com/subinium/dark-mode-visualization-apple-version) which he did using matplotlib/seaborn library.
"Back to top\n\n\n7. Radar Charts\n\nUse `radar chart` when appropriate. When plotting some feature which varies with direction, it can best be represented (plotted) using a radar chart. For example, the effect of wind direction on rainfall (Rainfall in Austrailia dataset) can be a good example. Below I have depicted how this can be demonstrated with line graph and radar chart. In my opinion the radar chart looks/feels right."
\n\n8. 3D Plots\n\nIn 3D plots it helps to use different background colors for the three planes to make the visiblity and visualization a bit more clear. With the help of the legendary iris dataset let's do that. 
\n\n9. Choropleth Map (Animation)\n\n* Use the `scope` argument if you want to focus on one continent only. \n* Africa is selected as the scope argument in the second example. 
"\n\n10. Bonus: Getting creative with annotated heatmaps\n\nIf you haven't seen the periodic table plotted using plotly, you can find it [here](https://plotly.com/python/annotated-heatmap/). Go ahead and check it. It gives you an idea on how to get creative with annotated heatmaps. [@janiobachmann](https://www.kaggle.com/janiobachmann) also used the periodic table structure in his [housing market project](https://www.kaggle.com/janiobachmann/melbourne-comprehensive-housing-market-analysis). Below is a simplified example.\n\nNote: You might have already noticed that the header of this notebook is also an adaptation of the periodic table plot."
\n\n11. Reference\n\n1. [plotly.com](https://plotly.com/) \n2. [Plotly Tutorial for Everyone](https://www.kaggle.com/saurav9786/plotly-tutorial-for-everyone/) by [@saurav9786](https://www.kaggle.com/saurav9786) \n3. [Plotly Tutorial for Beginners](https://www.kaggle.com/kanncaa1/plotly-tutorial-for-beginners) by [@kanncaa1](https://www.kaggle.com/kanncaa1)\n\n**Note**: Ref 2&3 are very good tutorials for beginners on plotly. I recommend them for anyone who's just starting exploring the plotly plotting library.\n\nOther notebooks where I used plotly:\n\n1. [Students Performance: Practice EDA with plotly](https://www.kaggle.com/desalegngeb/students-performance-practice-eda-with-plotly)\n2. [April TPS: Synthanic EDA + Visualizations](https://www.kaggle.com/desalegngeb/april-tps-synthanic-eda-visualizations)\n3. [English PL Players' stat: Data-viz with Plotly](https://www.kaggle.com/desalegngeb/english-pl-players-stat-data-viz-with-plotly)\n
\n# Import libraries
 ‚¨ÜÔ∏èBack to Table of Contents ‚¨ÜÔ∏è
> „Ö§The level distribution of events 
> „Ö§Level and Elapsed_time 
> „Ö§Level and Elapsed_time 
 ‚¨ÜÔ∏èBack to Table of Contents ‚¨ÜÔ∏è
> Page 
 ‚¨ÜÔ∏èBack to Table of Contents ‚¨ÜÔ∏è
> Level_group 
 ‚¨ÜÔ∏èBack to Table of Contents ‚¨ÜÔ∏è
"# **W & B Artifacts**\n\nAn artifact as a versioned folder of data.Entire datasets can be directly stored as artifacts .\n\nW&B Artifacts are used for dataset versioning, model versioning . They are also used for tracking dependencies and results across machine learning pipelines.Artifact references can be used to point to data in other systems like S3, GCP, or your own system.\n\nYou can learn more about W&B artifacts [here](https://docs.wandb.ai/guides/artifacts)\n\n![](https://drive.google.com/uc?id=1JYSaIMXuEVBheP15xxuaex-32yzxgglV)"
# **Feature Scaling**
# **Feature Importance**
# **SHAP VALUES**\n\n**SHAP (SHapley Additive exPlanations**) is a game theoretic approach to explain the output of any machine learning model. It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions
## Lets look at overall survival stats[^](#3_1)
* Sad Story! Only 38% have survived. That is roughly 340 out of 891. 
### Feature: Sex[^](#3_2_1)
"* While survival rate for female is around 75%, same for men is about 20%.\n* It looks like they have given priority to female passengers in the rescue.\n* **Looks like Sex is a good predictor on the survival.**"
"---\n### Feature: Pclass[^](#3_2_2)\n**Meaning :** Ticket class : 1 = 1st, 2 = 2nd, 3 = 3rd"
"* For Pclass 1 %survived is around 63%, for Pclass2 is around 48% and for Pclass2 is around 25%.\n* **So its clear that higher classes had higher priority while rescue.**\n* **Looks like Pclass is also an important feature.**"
---\n### Feature: Age[^](#3_2_3)\n**Meaning :** Age in years
* Survival rate for passenegers below Age 14(i.e children) looks to be good than others.\n* So Age seems an important feature too.\n* Rememer we had 177 null values in the Age feature. How are we gonna fill them?.
"---\n### Feature: Embarked[^](#3_2_4)\n**Meaning :** Port of Embarkation. C = Cherbourg, Q = Queenstown, S = Southampton"
* Majority of passengers borded from Southampton\n* Survival counts looks better at C. Why?. Could there be an influence from sex and pclass features we already studied?. Let's find out 
* Majority of passengers borded from Southampton\n* Survival counts looks better at C. Why?. Could there be an influence from sex and pclass features we already studied?. Let's find out 
* We guessed correctly. higher % of 1st class passegers boarding from C might be the reason.
#### Filling Embarked NaN
"* Since 72.5% passengers are from Southampton, So lets fill missing 2 values using S (Southampton)"
---\n### Features: SibSip & Parch[^](#3_2_5)\n**Meaning :**  \nSibSip -> Number of siblings / spouses aboard the Titanic\n\nParch -> Number of parents / children aboard the Titanic\n\nSibSip + Parch -> Family Size 
"* The barplot and factorplot shows that if a passenger is alone onboard with no siblings, he have 34.5% survival rate. The graph roughly decreases if the number of siblings increase."
Lets combine above and analyse family size. 
* This looks interesting! looks like family sizes of 1-3 have better survival rates than others.
### Correlation Between The Features[^](#3_4)
"---\n# PART 2 : Feature Engineering and Data Cleaning[^](#4)\nNow what is Feature Engineering? Feature engineering is the process of using domain knowledge of the data to create features that make machine learning algorithms work.\n\nIn this section we will be doing,\n1. Converting String Values into Numeric\n1. Convert Age into a categorical feature by binning\n1. Convert Fare into a categorical feature by binning\n1. Dropping Unwanted Features\n"
A confusion matrix is a table that is often used to describe the performance of a classification model. read more [here](https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/)
"* By looking at above matrices we can say that, if we are more concerned on making less mistakes by predicting survived as dead, then Naive Bayes model does better.\n* If we are more concerned on making less mistakes by predicting dead as survived, then Decision Tree model does better."
"## Feature Importance[^](#6)\n\nWell after we have trained a model to make predictions for us, we feel curiuos on how it works. What are the features model weights more when trying to make a prediction?. As humans we seek to understand how it works. Looking at feature importances of a trained model is one way we could explain the decisions it make. Lets visualize the feature importances of the Random forest model we used inside the ensemble above."
"---\n## So That is it...\n\nIts Simple isnt it?.\n\nWe started from **EDA** to see what the data can tell us. Then we moved to **Feature Engineering and Data Cleaning** step where we added few features, Removed redundant features, Converted features into suitable form for modeling. Finally in the **Predictive Modeling** part we tried basic ML algorthms, cross validated, ensemble and Important feature Extraction.\n\nThanks a lot for reading!\nSee you! :)"
 Cheatsheet - Matplotlib Charts 
 Please Upvote my kernel and keep it in your favourite section if you think it is helpful.
 Please Upvote my kernel and keep it in your favourite section if you think it is helpful.
\n\n\nTable of Content\n    \n- Introduction\n- Libraries\n- Data\n- Scatter Plot\n- Line Plot\n- Multiline Plot\n- Histogram\n- Bar Plot\n- Horizontal Bar Plot\n- Error Bar Plot\n- Stacked Bar Plot\n- Box Plot\n- Area Plot\n- Stacked Area Plot\n- Density Plot\n- Hexbin Plot\n- Lollipop Plot
\nIntroduction
\nLibraries
\nLibraries
\nLoad Data
![](https://datavizcatalogue.com/methods/images/anatomy/scatterplot.png)\n Source: [DataVizCatalogue](https://datavizcatalogue.com/methods/images/anatomy/scatterplot.png) 
\nLine Plot
![](https://datavizcatalogue.com/methods/images/anatomy/line_graph.png)\nSource: [DataVizCatalogue](https://datavizcatalogue.com/methods/images/anatomy/line_graph.png)
"\nHistogram\n\nAn histogram is an accurate graphical representation of the distribution of numerical data. It takes as input one numerical variable only. The variable is cut into several bins, and the\nnumber of observation per bin is represented by the height of the bar."
![](https://datavizcatalogue.com/methods/images/anatomy/histogram.png)\nSource: [DataVizCatalogue](https://datavizcatalogue.com/methods/images/anatomy/histogram.png)
"\nBar Plot\n\nA barplot (or barchart) is one of the most common type of plot. It shows the relationship between a numerical variable and a categorical variable. For example, you can display the height of several individuals using bar chart."
\nHorizontal Bar Plot
\nError Bar Plot
\nError Bar Plot
\nBox Plot\n\nA Box and Whisker Plot (or Box Plot) is a convenient way of visually displaying the data distribution through their quartiles.
\nSimple Box Plot
\nMultiple Box Plot
\nMultiple Box Plot
## Adding Xticks to Box Plot
## Adding Xticks to Box Plot
# Horizontal Box Plot
# Horizontal Box Plot
"# Area Plot\n\nAn area chart is really similar to a line chart, except that the area between the x axis and the line is filled in with color or shading. It represents the evolution of a numerical variable following another numerical variable. "
# Horizontal Lollipop
"## Don't forget to upvote if you like it!. \nIf you have any doubt reagrding any part of the notebook, feel free to comment your doubt in the comment box.\n\nThank you!!\n\n## Work in Progress... ‚è≥"
# Loading packages 
# Exploring at the meta data 
"Ok, that's good! We only have one age value per patient or none at all. "
### Insights\n\n* Most of the patients are older than 40 years. \n* It seems that we have two peaks around the age of 50 and close to 70. \n* There is a drop of patients counts after the age of 70. \n* For patients with cancer it's more likely that they are older and above the age of 50. 
## Image features 
### Insights\n\n* The laterality is quite balanced. \n* In the data description we can find that there are usually two views per breast. The most common views are CC and MLO and given these 2 views for the right and left breast we end up with 4 images that most of the patients show.  \n* Only a very few images show implants. \n* Most of the images show medium dense images of category B and C. Nonetheless there are also cases that are very dense (D) or less dense (A). Given the information that it could be more difficult to identify cancer in dense tissues this could be an interesting feature when thinking about validation strategies. 
And let's plot the raw pixelarray distribution for this example and display the related image:
"### Insights\n\n* Uhh! That's interesting! The background seems to be above 3000 this time! So it's competely different to previous competitions.\n* As the raw values depend on the machine, it could be worth it to explore background values dependent on the machine id. "
"At first glance, I would say that the RFR model produced the best predictive results, just looking at the scatter graph plotted. Overall both models, the integer and the dummy encoded models seem to perform relatively similar, although the dummy encoded model has a higher overall predicted mean."
"If we look at what influences the ratings, the top 4 being reviews, size, category, and number of installs seem to have the highest influence. This is quite an interesting observation, while also rationalizable."
"If we look at what influences the ratings, the top 4 being reviews, size, category, and number of installs seem to have the highest influence. This is quite an interesting observation, while also rationalizable."
"Looking at the breakdown even further, it would seem that indeed Reviews, size and number of install remain as a significant contributer to the predictiveness of app ratings.  What's interesting to me is that how the Tools category of apps have such a high level of predictiveness in terms of ratings, as say compared to the Food and Drink category."
"Again with the inclusion of the genre variable, the results do not seem to defer significantly as compared to the previous results."
"From the results, it would seem that the genre section actually plays an important part in the decision tree making. Yet the exclusion of it dosent seem to significantly impact results. This to me is quite interesting."
"\n\n Weights & Biases (W&B) is a set of machine learning tools that helps you build better models faster. Kaggle competitions require fast-paced model development and evaluation. There are a lot of components: exploring the training data, training different models, combining trained models in different combinations (ensembling), and so on.\n\n> ‚è≥ Lots of components = Lots of places to go wrong = Lots of time spent debugging\n\nW&B can be useful for Kaggle competition with it's lightweight and interoperable tools:\n\n* Quickly track experiments,\n* Version and iterate on datasets, \n* Evaluate model performance,\n* Reproduce models,\n* Visualize results and spot regressions,\n* Share findings with colleagues.\n\nTo learn more about Weights and Biases check out this kernel."
# Training Configuration ‚öôÔ∏è
# Training Function
# Validation Function
"If we plot the original data, we can see that one of the classes is linearly separable, but the other two are not."
Let's try to use a Linear SVC to predict the the labels of our test data.
"Below are the imports needed to run the code.  The code has been written and run in Python 3.6 and 3.7 Anaconda environments.  Many of these libraries request a citation when used in an academic paper.  Note the use of the Scikit-Learn (Pedregosa et al. (2011), XGBoost (Chen & Guestrin, 2016) and LightGBM (Ke, et al., 2017) libraries for machine learning and support.  Numpy is utilized to provide many numerical functions for feature creation (van der Walt, Colbert & Varoquaux, 2011). Pandas is very helpful for its ability to support data manipulation and feature creation (McKinney, 2010).  SciPy is utilized to provide signal processing functions, especially filtering and for Pearson's correlation metrics (Jones E., et al, 2001).  The Jupyter environment in which this project is presented is a descendant of the IPython environment originated by P√©rez & Granger (2007)."
Define some constants.\nThe signal constants define how the signal and Fourier transforms will be filtered to produce bandwidth limited features.
"Run a LightGBM model and save for a submission to Kaggle.  This will also output feature importance.  This model scored 1.441 on Kaggle.  For this and the models that follow, remember to adjust the number of jobs(treads or processes) based on the CPU capabilities available.  As noted above, the feature importance from the LightGBM model was abandoned as a feature selection mechanism in favor of Pearson's correlation."
"This is the variant of the model with feature elimination performed by Pearson's correlation.  As noted below, these models usually scored higher individual scores on the Kaggle leader board."
"This is the variant of the model with feature elimination performed by Pearson's correlation.  As noted below, these models usually scored higher individual scores on the Kaggle leader board."
### Feature Selection
### Test Environment
### Author and License Information
"The expected batch length increases with the batch size. It even surpasses the maximum length of 220 at batch_size=32768, and it is significantly smaller than the fixed padding at a reasonable batch size of e. g. 512. When looking at the histogram, you can also see very well that the number of outliers increases when increasing the batch size. Because we are padding to the maximum length, the expected batch size is strongly influenced by outliers.\n\nNote that the difference between the green line and the red line for each batch size does not directly relate to the speedup; there is some small overhead to dynamically padding the sequences."
"When padding to the 95th percentile of batch lengths instead, we can see another interesting pattern. The expected sequence length does not change that much when increasing batch size because it is more robust to outliers. In fact, it very quickly approaches the 95th percentile of lengths in the whole dataset!"
*  Ticket Prices for the Titanic
# Cufflinks:\n \nCufflinks is another library that connects the Pandas data frame with Plotly enabling users to create visualizations directly from Pandas
**Data Cleaning**\n\n* We want to fill in missing age data instead of just dropping the missing age data rows.\n\n* One way to do this is by filling in the mean age of all the passengers (imputation). \n\n* However we can be smarter about this and check the average age by passenger class. For example:
"\nWe can see the wealthier passengers in the higher classes tend to be older,which makes sense.\n\nWe'll use these average age values to impute based on Pclass for Age."
**Now let's check that heat map again!**
**Great! Let's go ahead and drop the Cabin column and the row in Embarked that is NaN.**
"**Objective** is to build a Deep Learning model which can identify if the person is wearing a mask or not, also detecting if people vilating social distancing norms."
"### Using haar cascade to detect faces\n\nObject Detection using Haar feature-based cascade classifiers is an effective object detection method proposed by Paul Viola and Michael Jones in their paper, ""Rapid Object Detection using a Boosted Cascade of Simple Features"" in 2001. It is a machine learning based approach where a cascade function is trained from a lot of positive and negative images. It is then used to detect objects in other images. We'll be using a Haar Cascade Model trained to detect faces in order to obtain the bounding box coordinates of faces in an image."
# What are the strategies to earn discussion medals?
## 1. Introduction\n\nFrom my experience I noticed that silver and gold discussion medals (5 and 10 upvotes respectively) are much harder to get compared to bronze discussion medals (1 upvote). So I began wondering **how forum messages with silver/gold medals are different from messages with bronze medals?**\n\nIn this kernel I will present my analysis and for this purpose will use a number of Python libraries:\n\n- `pandas` - working with DataFrames\n- `BeautifulSoup` and `re` - cleaning messages from HTML tags\n- `spacy` - natural language processing (NLP)\n- `TSNE` and `KMeans` from `sklearn` - dimension reduction and clustering\n- `seaborn` - visualization\n- `Counter` from `collections` - counting words\n- `bokeh` - interactive visualization
## 1. Introduction\n\nFrom my experience I noticed that silver and gold discussion medals (5 and 10 upvotes respectively) are much harder to get compared to bronze discussion medals (1 upvote). So I began wondering **how forum messages with silver/gold medals are different from messages with bronze medals?**\n\nIn this kernel I will present my analysis and for this purpose will use a number of Python libraries:\n\n- `pandas` - working with DataFrames\n- `BeautifulSoup` and `re` - cleaning messages from HTML tags\n- `spacy` - natural language processing (NLP)\n- `TSNE` and `KMeans` from `sklearn` - dimension reduction and clustering\n- `seaborn` - visualization\n- `Counter` from `collections` - counting words\n- `bokeh` - interactive visualization
"In order to use Python in Bokeh callback functions, it is also required to install the module `pscript`."
"Finally, the messages are visualized on the scatter plot."
It looks like there are at least 3 clusters present here. Let's use `KMeans()` to identify these clusters and add the cluster labels to the DataFrame. I have also added message tokens to the DataFrame to analyze the most frequent words in the clusters.
The scatter plot of the labeled clusters is shown below.
"Since `KMeans` assigns labels in random order, they might be completely different after commiting the kernel. Therefore, I print the most common words of all 3 clusters below and discuss them in no particular order.\n\nIn one of the clusters the top words are ""kernel"", ""thank"", ""great"", ""upvote"", ""share"", etc. One could think of possible phrases from these words such as ""great kernel"", ""thanks for sharing"", etc. These messages probably show appreciation to kernels. \n\nIn another cluster the top words are ""thank"", ""work"", ""nice"", ""great"", ""share"", etc. These are very similar to the words from the previous cluster with an exception of ""kernel"". These messages probably show appreciation in general.\n\nIn another cluster the top words are ""datum"" (lemmatized form of ""data""), ""model"", ""thank"", ""kernel"", ""good"", etc. This is probably a mix of messages that discuss models/kernels and show appreciation. Another possible reason for this mix is that the clusters are not separated very well on the plot and `KMeans` might mistakenly assign a part of one cluster to the other."
Let's repeat the same for the messages that got silver and gold medals. The code here is mostly a copy-paste from the previous section without changing the variable names.
It looks like there might be at least 2 clusters here.
It looks like there might be at least 2 clusters here.
"In one of the clusters the most frequent words are ""competition"", ""kernel"", ""submission"", ""time"", ""score"", etc. These messages probably discuss competition scores and related kernels.\n\nIn another cluster the most frequent words are ""model"", ""feature"", ""score"", ""lb"" (short for ""leaderboard""), ""datum"", etc. These messages probably focus on studying model features and improving scores. Note that the word ""competition"" is also quite frequent in this cluster which means that these messages are also related to competitions."
"The number of bronze medals for each forum title is shown below. Notice that bronze medals are predominantly granted in ""Kernels"". This is in line with the previous observation that the messages with bronze medals mostly express appreciation of other people's work. Indeed, this happens a lot in the comments section of kernels."
"The distriubtion of silver/gold medals is significantly different. The majority of medals are granted in the recent Kaggle competitions such as ""Microsoft Malware Prediction"", ""Quora Insincere Questions Classification"", ""Elo Merchant Category Recommendation"", etc. This is in line with the previous observation that the messages with silver/gold medals mostly focus on competitions."
"The distriubtion of silver/gold medals is significantly different. The majority of medals are granted in the recent Kaggle competitions such as ""Microsoft Malware Prediction"", ""Quora Insincere Questions Classification"", ""Elo Merchant Category Recommendation"", etc. This is in line with the previous observation that the messages with silver/gold medals mostly focus on competitions."
## 5. Conclusion
"# 1. Introducing the subject \n**Ahoy**! If you are like me, you've always been looking up to the **top Data Scientists**, the **cream of the community**, the very few that, somehow, through some *magic* only they know, manage to understand, teach and perform like none others.\n\n**But how**? This survey is an opportunity to take this curiosity very *close and personal* in the search for the treasure that might reveal us the steps to reach that greatness.\n\n**Buckle up pirates; the treasure hunt is ON**.\n\nCriteria\n\nUnfortunately, there will need to be some *bias* involved.\n\nIn the Kaggle Survey, there is no feature or clue to identify which of the respondents are Masters/Grandmasters - neither the rank nor any performance within the Kaggle Community.\n\nHence, I had to *define* what is a *successful* data scientist. Some assumptions were:\n\n\n\n.tg  {border-collapse:collapse;\n     border-spacing:0;}\n.tg td{border-color:""#010307"";\n    border-style:solid;\n    border-width:1px;\n    font-family:'Source Code Pro', monospace;\n    font-size:14px;\n    overflow:hidden;\n    padding:10px 5px;\n    word-break:normal;}\n.tg th{border-color:""#010307"";\n    border-style:solid;\n    border-width:1px;\n    font-family:'Source Code Pro', monospace;\n    font-size:14px;\n    font-weight:normal;\n    overflow:hidden;\n    padding:10px 5px;\n    word-break:normal;}\n.tg .tg-c3ow{border-color:""#010307"";\n    text-align:center;\n    vertical-align:top}\n\n\n\n  \n    Criteria\n    Value\n    Strength\n    Flaw\n  \n\n\n  \n    Level of Education\n    Masters / Doctoral\n    Majority of Grandmasters havesome level of upper education\n    Many bright data scientists areself-taught\n  \n  \n    Years of Programming/ML\n    4 - 5+ years\n    Might show increased experience\n    The passing of time doesn't reflecthow much one has learned\n  \n  \n    Pay\n    USD 100,000+\n    Very high pay can signal high skill\n    High pay doesn't guarantee skill;Regional bias also involved\n  \n  \n    Spending\n    USD 100,000+\n    A company investing thousands in MLis investing in bright DS employees\n    How much a company spends on ML doesn'tdefine the skill of the employer\n  \n\n\n\n\n  In the end, I decided to use pay as my delimiter. Firstly, I consider that it has the lowest bias out of all. Secondly, I could lower the regional bias significantly (more of that in the next chapter). I would also believe that highly skilled people usually have very high pay and the exceptions from that rule aren't that many.\n\n\n#### Libraries below ‚¨á"
"# 2. How many McMeal menus can you buy?\n\n## 2.1 The problem\nOk, so we agreed on using the pay as the indicator. However, this feature has many issues on its own:\n\n\n.tg  {border-collapse:collapse;\n     border-spacing:0;}\n.tg td{border-color:""#010307"";\n    border-style:solid;\n    border-width:1px;\n    font-family:'Source Code Pro', monospace;\n    font-size:14px;\n    overflow:hidden;\n    padding:10px 5px;\n    word-break:normal;}\n.tg th{border-color:""#010307"";\n    border-style:solid;\n    border-width:1px;\n    font-family:'Source Code Pro', monospace;\n    font-size:14px;\n    font-weight:normal;\n    overflow:hidden;\n    padding:10px 5px;\n    word-break:normal;}\n.tg .tg-c3ow{border-color:""#010307"";\n    text-align:center;\n    vertical-align:top}\n\n\n\n  \n    No.\n    Problem\n  \n\n\n  \n    1.\n    It doesn't take into account the purchasing power (e.g., a person in Ukraine with 90k a year may buy more ""valuables"" in their country than another person in Japan with 150k a year).\n  \n  \n    2.\n    It might be misunderstood: there might be people that completed the salary in their base currency. Nevertheless, this column is expressed in dollars.\n  \n  \n    3.\n    More than 50% of the respondents didn't respond altogether, so only half remain available.\n  \n  \n    4.\n    There are the occasional trolls who might give a lower or a much higher salary than they actually have.\n  \n\n"
"# 2. How many McMeal menus can you buy?\n\n## 2.1 The problem\nOk, so we agreed on using the pay as the indicator. However, this feature has many issues on its own:\n\n\n.tg  {border-collapse:collapse;\n     border-spacing:0;}\n.tg td{border-color:""#010307"";\n    border-style:solid;\n    border-width:1px;\n    font-family:'Source Code Pro', monospace;\n    font-size:14px;\n    overflow:hidden;\n    padding:10px 5px;\n    word-break:normal;}\n.tg th{border-color:""#010307"";\n    border-style:solid;\n    border-width:1px;\n    font-family:'Source Code Pro', monospace;\n    font-size:14px;\n    font-weight:normal;\n    overflow:hidden;\n    padding:10px 5px;\n    word-break:normal;}\n.tg .tg-c3ow{border-color:""#010307"";\n    text-align:center;\n    vertical-align:top}\n\n\n\n  \n    No.\n    Problem\n  \n\n\n  \n    1.\n    It doesn't take into account the purchasing power (e.g., a person in Ukraine with 90k a year may buy more ""valuables"" in their country than another person in Japan with 150k a year).\n  \n  \n    2.\n    It might be misunderstood: there might be people that completed the salary in their base currency. Nevertheless, this column is expressed in dollars.\n  \n  \n    3.\n    More than 50% of the respondents didn't respond altogether, so only half remain available.\n  \n  \n    4.\n    There are the occasional trolls who might give a lower or a much higher salary than they actually have.\n  \n\n"
"> Hence ... **what do we choose**? We can't just take all people with a salary > 80k - this would be highly bias and inefficient (80k is lots of money in the UK, but not that much in the US). Unfortunately, points *2.*, *3.* and *4.* are systematic issues, so we'll have to go ahead and trust the Kagglers that they completed the *pay* question to the best of their abilities.\n\n## 2.2 The Solution üçü\n\n**Thanks to a good friend** who gave me this idea, we can all have a snack break now.\n\n\n\nA McMeal may solve our regional bias problem. Instead of using pay, we can look around the world at **how many McMeals can one respondent buy with their salary in their own country**. Afterward, we can use the McMeal(units) as our non-bias indicator.\n\nNow, the only thing that remains is where do we draw the line in the McMeal units?"
"> Hence ... **what do we choose**? We can't just take all people with a salary > 80k - this would be highly bias and inefficient (80k is lots of money in the UK, but not that much in the US). Unfortunately, points *2.*, *3.* and *4.* are systematic issues, so we'll have to go ahead and trust the Kagglers that they completed the *pay* question to the best of their abilities.\n\n## 2.2 The Solution üçü\n\n**Thanks to a good friend** who gave me this idea, we can all have a snack break now.\n\n\n\nA McMeal may solve our regional bias problem. Instead of using pay, we can look around the world at **how many McMeals can one respondent buy with their salary in their own country**. Afterward, we can use the McMeal(units) as our non-bias indicator.\n\nNow, the only thing that remains is where do we draw the line in the McMeal units?"
"About 2,300 people have at least 10,000 Meals or more (10,000 threshold was chosen looking at Q3). I will be analyzing these people from now on. However, I will also be segmenting them into three categories:\n\n\n.tg  {border-collapse:collapse;\n     border-spacing:0;}\n.tg td{border-color:""#010307"";\n    border-style:solid;\n    border-width:1px;\n    font-family:'Source Code Pro', monospace;\n    font-size:14px;\n    overflow:hidden;\n    padding:10px 5px;\n    word-break:normal;}\n.tg th{border-color:""#010307"";\n    border-style:solid;\n    border-width:1px;\n    font-family:'Source Code Pro', monospace;\n    font-size:14px;\n    font-weight:normal;\n    overflow:hidden;\n    padding:10px 5px;\n    word-break:normal;}\n.tg .tg-c3ow{border-color:""#010307"";\n    text-align:center;\n    vertical-align:top}\n\n\n\n  \n    Category Name\n    Meaning\n    Number of respondents\n  \n\n\n  \n    High\n    10,000 - 20,000 units: These people can buy more McMeals than more than 75% of our base users.\n    ~1600\n  \n  \n    Very High\n    20,000 - 50,000 units: These people can buy more McMeals than more than 90% of our base users.\n    ~700\n  \n  \n    Crazy High\n    50,000 + units: These people can buy more McMeals than more than 99% of our base users.\n    ~100\n  \n\n"
"About 2,300 people have at least 10,000 Meals or more (10,000 threshold was chosen looking at Q3). I will be analyzing these people from now on. However, I will also be segmenting them into three categories:\n\n\n.tg  {border-collapse:collapse;\n     border-spacing:0;}\n.tg td{border-color:""#010307"";\n    border-style:solid;\n    border-width:1px;\n    font-family:'Source Code Pro', monospace;\n    font-size:14px;\n    overflow:hidden;\n    padding:10px 5px;\n    word-break:normal;}\n.tg th{border-color:""#010307"";\n    border-style:solid;\n    border-width:1px;\n    font-family:'Source Code Pro', monospace;\n    font-size:14px;\n    font-weight:normal;\n    overflow:hidden;\n    padding:10px 5px;\n    word-break:normal;}\n.tg .tg-c3ow{border-color:""#010307"";\n    text-align:center;\n    vertical-align:top}\n\n\n\n  \n    Category Name\n    Meaning\n    Number of respondents\n  \n\n\n  \n    High\n    10,000 - 20,000 units: These people can buy more McMeals than more than 75% of our base users.\n    ~1600\n  \n  \n    Very High\n    20,000 - 50,000 units: These people can buy more McMeals than more than 90% of our base users.\n    ~700\n  \n  \n    Crazy High\n    50,000 + units: These people can buy more McMeals than more than 99% of our base users.\n    ~100\n  \n\n"
"## The Pareto Principle\n\nOur McMeal units distribution match the Pareto Principle very well: [roughly 80% of consequences come from 20% of the causes](https://en.wikipedia.org/wiki/Pareto_principle).\n\nHence, we are noticing a **""top 20%"" of the ""top 20%"" of the ""top 20%""** situation, meaning:\n* out of 9,893 total respondents, 2,301 (*25%*) have more than 10,000 units\n* out of the 2,301 respondents, 705 (*~ 20%*) have more than 20,000 units\n* out of the 705 respondents, 100 (*~ 15%*) have more than 50,000 units\n\n> What does this mean? It doesn't matter to which category you look, the distribution is always going to be **skewed to the right**. This is why segmentation is so important here."
"## The Pareto Principle\n\nOur McMeal units distribution match the Pareto Principle very well: [roughly 80% of consequences come from 20% of the causes](https://en.wikipedia.org/wiki/Pareto_principle).\n\nHence, we are noticing a **""top 20%"" of the ""top 20%"" of the ""top 20%""** situation, meaning:\n* out of 9,893 total respondents, 2,301 (*25%*) have more than 10,000 units\n* out of the 2,301 respondents, 705 (*~ 20%*) have more than 20,000 units\n* out of the 705 respondents, 100 (*~ 15%*) have more than 50,000 units\n\n> What does this mean? It doesn't matter to which category you look, the distribution is always going to be **skewed to the right**. This is why segmentation is so important here."
"# 3. On our way to finding the treasure\n\nAlright! We established our feature, normalized it, excluded the regional bias, and segmented our target respondents. We can say **our map** is already laid out in front of us (well structured and ready, but still empty); hence we now need to follow the steps to our treasure.\n\n\n  The map is BLANK for the moment, but it will start revealing itself once we begin discovering new Realms.\n\n\n\n*üìå Note: From now on, when I'll mention **pay**, I will refer to the units we got after the pay normalization on the McMeal units :)*\n\n3.1 The Personal Profile\n\nWhere do they reside?\n\nSome pointers we observe here:\n* The top countries do **match the profile** of the majority **of masters/grandmasters** on Kaggle.\n* The top 2 countries for all categories are **the USA** and **India**.\n* **Japan, China, Indonesia, Russia, Canada, and the UK** are the other 6 places where these highly skilled people reside."
"# 3. On our way to finding the treasure\n\nAlright! We established our feature, normalized it, excluded the regional bias, and segmented our target respondents. We can say **our map** is already laid out in front of us (well structured and ready, but still empty); hence we now need to follow the steps to our treasure.\n\n\n  The map is BLANK for the moment, but it will start revealing itself once we begin discovering new Realms.\n\n\n\n*üìå Note: From now on, when I'll mention **pay**, I will refer to the units we got after the pay normalization on the McMeal units :)*\n\n3.1 The Personal Profile\n\nWhere do they reside?\n\nSome pointers we observe here:\n* The top countries do **match the profile** of the majority **of masters/grandmasters** on Kaggle.\n* The top 2 countries for all categories are **the USA** and **India**.\n* **Japan, China, Indonesia, Russia, Canada, and the UK** are the other 6 places where these highly skilled people reside."
"Who are they?\n\nLet's take it step by step:\n* As the percentage of the entire population of the survey is **mostly formed by males**, the ""high end"" people we're studying match accordingly.\n* Looking at age, we can observe that the *average* age increases by category:\n    * High Pay: the average age is around *35* yo\n    * Very High Pay: average age starts moving towards *40 yo and 45 yo* (these bars start to rise)\n    * Crazy High Pay: the 30s drop suddenly, whereas the beginning of *40s* stays the same. Later age (*45, 50s*) are also visible.\n* Hence, there is a clear, **direct correlation between high pay and age** - the older, the wiser, the wealthier.\n\nMy personal opinion is that the 20s and 30s are hectic anyway, and the golden ages are still after 40. Glad that this survey also reflects that. üòÅ [@Dieter](https://www.kaggle.com/christofhenkel) might agree with me as well. üëÄ"
"\n  Takeaway: We now know that the high end of respondents is mostly from the US and India, but also located in Japan, China, Indonesia, UK, Russia, or Canada, and have a Male 30-45 yo profile.\n  However, these ""personal"" aspects don't define an outstanding Data Scientist. Hence, we'll start from now on to look at education, expertise, work ethic, and knowledge, rather than focusing on biological, racial, or other environmental aspects.\n\n\n> Oh, and look! The first portion of the map is clear now!\n\n\n\n> Closer look üîé\n\n\n3.2 The Education\n\nAs for education, the 3 Pay Categories differentiate through:\n* *High Pay* and *Very High Pay* have very similar distributions. **50% of the respondents** have or plan to complete a **Masters's degree** in the next 2 years.\n* However, *Crazy High Pay* steals ~15 percentage points from the *Master* category and adds to the **Doctoral** category.\n* Hence, the majority of extremely well-paid respondents choose to continue their superior studies to a Doctoral."
"\n  Takeaway: The high-end respondents have mostly a high to very high education; the majority have at least a Masters's complete. The difference between High Pay and Crazy High Pay is in majority's choice to pursue their passion further to a Doctoral.\n  As you may know, indeed, many Masters/Grandmasters discuss (on Twitter, podcasts, etc.) that they have a Doctoral in a field supported by Data Science work.\n\n\n> And the Education Mountains revealed themselves!\n\n\n\n> Closer look üîé\n\n\n3.3 The Expertise and Work Environment\n\nHere is our chance to take a glimpse into the work environment and skills acquired. How long does it take to get there? How hard do we need to work? What job roles should we pursue?\n\nFor how long have they been practicing?\n\nThe most important points here:\n* These exceptional people have **more coding experience** than ML in terms of time.\n* Looking at the coding expertise, we see more than 50% of them having **10+ years of coding experience**.\n* Looking at the ML expertise, the donut is more evenly split between **1 and 10 years**, with *less than 10% having 10+ years of experience*.\n* What does this mean? It means that exceptional people do have lots of coding experience but NOT ML necessarily, and most of them are on Kaggle because they might have recently (or in the last years) found their passion in data."
"\n  Takeaway: The high-end respondents have mostly a high to very high education; the majority have at least a Masters's complete. The difference between High Pay and Crazy High Pay is in majority's choice to pursue their passion further to a Doctoral.\n  As you may know, indeed, many Masters/Grandmasters discuss (on Twitter, podcasts, etc.) that they have a Doctoral in a field supported by Data Science work.\n\n\n> And the Education Mountains revealed themselves!\n\n\n\n> Closer look üîé\n\n\n3.3 The Expertise and Work Environment\n\nHere is our chance to take a glimpse into the work environment and skills acquired. How long does it take to get there? How hard do we need to work? What job roles should we pursue?\n\nFor how long have they been practicing?\n\nThe most important points here:\n* These exceptional people have **more coding experience** than ML in terms of time.\n* Looking at the coding expertise, we see more than 50% of them having **10+ years of coding experience**.\n* Looking at the ML expertise, the donut is more evenly split between **1 and 10 years**, with *less than 10% having 10+ years of experience*.\n* What does this mean? It means that exceptional people do have lots of coding experience but NOT ML necessarily, and most of them are on Kaggle because they might have recently (or in the last years) found their passion in data."
"Role and Duties at Work\n\nPointers here:\n* The most frequent jobs for the top respondents are Data Scientist, ML Engineer or Software Engineer (Analyst incorporates 3 jobs - Data Analyst, Business Analyst, and Statistician).\n* Some of the most crucial duties are:\n    * *Software Engineer*: more oriented towards **data infrastructure, exploration, and creating ML models**.\n    * *ML Engineer*: most prominent duties are for **building and exploring new ideas for ML models**.\n    * *Data Scientist*: the **most versatile** out of all, it incorporates almost equally all the duties (however, lower in the research areas). Basically, they need to know everything üëÄ."
"Role and Duties at Work\n\nPointers here:\n* The most frequent jobs for the top respondents are Data Scientist, ML Engineer or Software Engineer (Analyst incorporates 3 jobs - Data Analyst, Business Analyst, and Statistician).\n* Some of the most crucial duties are:\n    * *Software Engineer*: more oriented towards **data infrastructure, exploration, and creating ML models**.\n    * *ML Engineer*: most prominent duties are for **building and exploring new ideas for ML models**.\n    * *Data Scientist*: the **most versatile** out of all, it incorporates almost equally all the duties (however, lower in the research areas). Basically, they need to know everything üëÄ."
"How large are the company and the team?\n\nThe 2 plots match very well the overall distribution of the respondents ([you can see the overall summary here](https://www.kaggle.com/kaggle-survey-2020)):\n* On average, most respondents (regardless of their pay) are located in **small companies in 20% of the cases**, and more than **50% of the cases in large ones** (1000+ employees).\n* Also, the team size is **half the time bigger than 10 people**; however, **40% of cases are in small teams, of a maximum of 4 people**. This is a direct implication of the companies' size (there are many respondents in tiny companies, hence smaller DS teams and vice versa)."
"How large are the company and the team?\n\nThe 2 plots match very well the overall distribution of the respondents ([you can see the overall summary here](https://www.kaggle.com/kaggle-survey-2020)):\n* On average, most respondents (regardless of their pay) are located in **small companies in 20% of the cases**, and more than **50% of the cases in large ones** (1000+ employees).\n* Also, the team size is **half the time bigger than 10 people**; however, **40% of cases are in small teams, of a maximum of 4 people**. This is a direct implication of the companies' size (there are many respondents in tiny companies, hence smaller DS teams and vice versa)."
"The Interest of the Company in Machine Learning\n\nThis question *might have some bias* because some people could have guessed a rough estimate, as they might not know of the ""business side"" of the company. However, because we're talking about high-end Data Scientists, who most certainly are involved in their projects' finances, this bias might be lower.\n\n*Or are they?*\n\nThe following graphs show some fascinating insights:\n* Firstly, respondents around **all groups** agreed that their company is either using  well established ML - or - they've just started implementing ML into the business ([and these numbers are increasing as the years pass, according to the general summary](https://www.kaggle.com/kaggle-survey-2020)).\n* We can see that the respondents with **High Pay** are located in companies that spend much less on ML than the other 2 groups. In the **Very High Pay** and **Crazy High Pay** groups, *more than 50% of the respondents* are employed in companies that *spend tens of thousands of dollars* on their ML equipment and team.\n* Hence, the graph shows that the bigger the individual pay, the more the company invests in ML in its business model."
"The Interest of the Company in Machine Learning\n\nThis question *might have some bias* because some people could have guessed a rough estimate, as they might not know of the ""business side"" of the company. However, because we're talking about high-end Data Scientists, who most certainly are involved in their projects' finances, this bias might be lower.\n\n*Or are they?*\n\nThe following graphs show some fascinating insights:\n* Firstly, respondents around **all groups** agreed that their company is either using  well established ML - or - they've just started implementing ML into the business ([and these numbers are increasing as the years pass, according to the general summary](https://www.kaggle.com/kaggle-survey-2020)).\n* We can see that the respondents with **High Pay** are located in companies that spend much less on ML than the other 2 groups. In the **Very High Pay** and **Crazy High Pay** groups, *more than 50% of the respondents* are employed in companies that *spend tens of thousands of dollars* on their ML equipment and team.\n* Hence, the graph shows that the bigger the individual pay, the more the company invests in ML in its business model."
"\n  Takeaway: We've learned that our high-end respondents are usually Data Scientists or Software/Machine Learning Engineers, with lots of coding experience, but not necessarily ML seniority.\n   They come from large and small companies, but the higher the investment and interest of the company in ML, the higher the individual income received.\n\n\n> And another area of the map has revealed itself: the Work Habitat!\n\n\n\n> Closer look üîé\n\n\n3.4 The Coding Preferences\n\nNow we know that more than half of our respondents have 10+ years of coding experience. But let's discover what languages they use most, what is their DS setup, how do they deal with cloud and big data, and what advice they have to share.\n\nLanguages Used and Advice to Community\n\nMost coders secretly root for Python, but are all the other languages obsolete?\n* **Python** is by far the most popular language in most areas, both in **usage** and as **a recommendation** for future users.\n* The next 3 most used languages are **SQL, R, and Bash**, all very useful in the Data Science discipline.\n* Hence, all our top respondents (regardless of the Pay Category) use and recommend the most Python, followed by SQL, R, and Bash."
"\n  Takeaway: We've learned that our high-end respondents are usually Data Scientists or Software/Machine Learning Engineers, with lots of coding experience, but not necessarily ML seniority.\n   They come from large and small companies, but the higher the investment and interest of the company in ML, the higher the individual income received.\n\n\n> And another area of the map has revealed itself: the Work Habitat!\n\n\n\n> Closer look üîé\n\n\n3.4 The Coding Preferences\n\nNow we know that more than half of our respondents have 10+ years of coding experience. But let's discover what languages they use most, what is their DS setup, how do they deal with cloud and big data, and what advice they have to share.\n\nLanguages Used and Advice to Community\n\nMost coders secretly root for Python, but are all the other languages obsolete?\n* **Python** is by far the most popular language in most areas, both in **usage** and as **a recommendation** for future users.\n* The next 3 most used languages are **SQL, R, and Bash**, all very useful in the Data Science discipline.\n* Hence, all our top respondents (regardless of the Pay Category) use and recommend the most Python, followed by SQL, R, and Bash."
"What's the best setup?\n\nThe graphs below are about the same for all 3 Pay Categories:\n* Most work on their **personal laptop/computer**.\n* When working on the personal computer, the usage is usually oriented towards the classics: **Jupyter, RStudio, and PyCharm**.\n* They also use cloud computing services, like **Google Cloud Datalab/ AI Platform**, but most excessively **Colab** or **Kaggle Notebooks**.\n* To conclude, the top data scientists have *similar behavior in terms of environments and IDE with the average Kaggler*: use the most the personal gear, Colab and Kaggle Notebooks and work on a combination of Jupyter, PyCharm, and RStudio environments."
"What's the best setup?\n\nThe graphs below are about the same for all 3 Pay Categories:\n* Most work on their **personal laptop/computer**.\n* When working on the personal computer, the usage is usually oriented towards the classics: **Jupyter, RStudio, and PyCharm**.\n* They also use cloud computing services, like **Google Cloud Datalab/ AI Platform**, but most excessively **Colab** or **Kaggle Notebooks**.\n* To conclude, the top data scientists have *similar behavior in terms of environments and IDE with the average Kaggler*: use the most the personal gear, Colab and Kaggle Notebooks and work on a combination of Jupyter, PyCharm, and RStudio environments."
"Accelerators : Yay or Nay?\n\nFirst, let's understand some concepts:\n\n.tg  {border-collapse:collapse;\n     border-spacing:0;}\n.tg td{border-color:""#010307"";\n    border-style:solid;\n    border-width:1px;\n    font-family:'Source Code Pro', monospace;\n    font-size:14px;\n    overflow:hidden;\n    padding:10px 5px;\n    word-break:normal;}\n.tg th{border-color:""#010307"";\n    border-style:solid;\n    border-width:1px;\n    font-family:'Source Code Pro', monospace;\n    font-size:14px;\n    font-weight:normal;\n    overflow:hidden;\n    padding:10px 5px;\n    word-break:normal;}\n.tg .tg-c3ow{border-color:""#010307"";\n    text-align:center;\n    vertical-align:top}\n\n\n\n  \n    Accelerator\n    Explanation\n  \n\n\n  \n    GPU (Graphics Processing Unit)\n    Designed to rapidly manipulate and alter memory to accelerate the creation of images. Used greatly in all areas of Data Science.\n  \n  \n    TPU (Tensor Processing Unit)\n    AI accelerator developed by Google specifically for neural network machine learning. Mostly used in Deep Learning problems.\n  \n\n\n\n\nIs GPU better? Technically no - either if you choose CPU, GPU or TPU, they all outperform in some areas and underperform in others. [Learn more about this comparison here.](https://analyticsindiamag.com/tpu-vs-gpu-vs-cpu-which-hardware-should-you-choose-for-deep-learning/#:~:text=TPU%20vs%20GPU%20vs%20CPU%3A%20A%20Cross%2DPlatform%20Comparison&text=TPU%3A%20Tensor%20Processing%20Unit%20is,small%20batches%20and%20nonMatMul%20computations.)\n\nSo, what are the top Data Scientists using to move fast during competitions and achieve the best scores? We know that they mostly use their personal laptop/ workstation, but how do these look like?\n* The visualization below is *representative* of all 3 Pay Categories.\n* Respondents are *split in half* (with very few exceptions): **~44% use no Acceleration**, while the other **44% use GPUs** most often.\n* Most of this **88% percent have never even tried TPUs** before, and if they did, they only used it between 2 to 5 times.\n* TPU users are **~ 7%** out of all respondents; however, half of them have been using TPUs more than 6 times (and around a quarter of them more than 25 times).\n* Hence, the vast majority of our respondents use GPU acceleration; however, there are a few very experienced TPU users that rely mostly on TPU during their work/competitions. Moreover, this graph is also a relief for beginners: you don't necessarily need heavy computing power to be very good or earn well."
"\n  Takeaway: We've learned that our high-end respondents are Python people, usually coding on their personal gear. The usual environments used are Jupyter Notebooks, PyCharm, Colab, and Kaggle.\n  They most often use GPUs, but no worries, No Accelerator whatsoever is a popular choice as well - meaning skill only can bring you some pretty awesome results as well. ;)\n\n\n> The Code Waters is now visible!\n\n\n\n> Closer look üîé\n\n\n3.5 The Machine Learning Preferences\n\nIn this last chapter, we'll find out what machine learning tools and resources these Data Science veterans use, as well as what we need to focus on when we start our Machine Learning journey.\n\nLet the hunt conclude in elegance!\n\nReliable Sources to Keep Informed\n\nThe learnings below are representative for all 3 levels of Pay Categories (meaning that all behave in the same manner, so the split wasn't shown):\n* For *learning*, the most popular choice was **Coursera**; as you may know, Coursera has the most popular [Machine Learning course](https://www.coursera.org/learn/machine-learning) from Stanford University, with the top instructor being the King of ML, **Andrew Ng**. This aspect might have significantly influenced the overall decision.\n* Other very popular Course platforms were **Kaggle Learn, Udemy, and University-specific courses**.\n* As *resources for getting the daily Data Science intake*, **Youtube** had the lead (possibly from the multitude of videos on different abstract topics, which are better understood from animated content rather than by only reading them). **Blogs** (such as Towards Data Science), **Twitter, and Publications** were a popular choice as well.\n* Hence, the ""go-to"" reliable sources for getting that Data Science information were Coursera, Kaggle Learn, Udemy, Youtube, and Blogs."
"What Frameworks to have in your Pocket?\n\nThe below graphs and explanations are representative for all 3 Pay Categories:\n* For the *overall Frameworks*, **Scikit-learn** has the lead. This is also **the oldest** (13 years), the **most volatile and versatile** library, so the fact that it has the most popularity isn't a shock.\n* Out of the deep learning frameworks, **Tensorflow** is the most popular, but PyTorch is starting to gain some visibility itself (it is *1 year younger* than Tensorflow, so it's only natural that it is a little bit behind).\n* As for the *visualization libraries*, our top-end respondents mainly prefer and use **the originals** Matplotlib, Seaborn, and Plotly, with Ggplot for R.\n* Hence, the industry's top data scientists use lots of Scikit-learn, Tensorflow, Matplotlib, and Seaborn. However, we mustn't lose sight of the emerging libraries that have the potential to overcome soon the giants, such as PyTorch (can you already tell I'm a fan of PyTorch?üëÄ)"
"What Frameworks to have in your Pocket?\n\nThe below graphs and explanations are representative for all 3 Pay Categories:\n* For the *overall Frameworks*, **Scikit-learn** has the lead. This is also **the oldest** (13 years), the **most volatile and versatile** library, so the fact that it has the most popularity isn't a shock.\n* Out of the deep learning frameworks, **Tensorflow** is the most popular, but PyTorch is starting to gain some visibility itself (it is *1 year younger* than Tensorflow, so it's only natural that it is a little bit behind).\n* As for the *visualization libraries*, our top-end respondents mainly prefer and use **the originals** Matplotlib, Seaborn, and Plotly, with Ggplot for R.\n* Hence, the industry's top data scientists use lots of Scikit-learn, Tensorflow, Matplotlib, and Seaborn. However, we mustn't lose sight of the emerging libraries that have the potential to overcome soon the giants, such as PyTorch (can you already tell I'm a fan of PyTorch?üëÄ)"
"Machine Learning & Deep Learning Basics\n\nThe below graphs and explanations are representative for all 3 Pay Categories:\n* Awkwardly enough, the most frequent choices for *ML Methods* were **Regressions and Tree-Based methods**, which are more straightforward approaches than Neural Nets. Hence, complicated methodologies aren't necessarily better.\n* **Classification** and **General Image Methods** (like cv2, PIL, or skimage) are the most popular in the *Computer Vision* department.\n* Regarding *NLP*, our top respondents were **Word Embeddings** and **Transformers**.\n* Hence, the most popular choices were the simpler ones, such as Regressions, Tree-Based Methods, General & Classification methods, and Word Embeddings."
"Machine Learning & Deep Learning Basics\n\nThe below graphs and explanations are representative for all 3 Pay Categories:\n* Awkwardly enough, the most frequent choices for *ML Methods* were **Regressions and Tree-Based methods**, which are more straightforward approaches than Neural Nets. Hence, complicated methodologies aren't necessarily better.\n* **Classification** and **General Image Methods** (like cv2, PIL, or skimage) are the most popular in the *Computer Vision* department.\n* Regarding *NLP*, our top respondents were **Word Embeddings** and **Transformers**.\n* Hence, the most popular choices were the simpler ones, such as Regressions, Tree-Based Methods, General & Classification methods, and Word Embeddings."
"Analysis at Work and Deployment\n\nThe visualization and discussion below are representative for all 3 Pay Categories:\n* Regarding *Deployment*, most of our top respondents prefer to either deploy on **GitHub** or **keep their models locally on their personal gear**. **Kaggle and Colab**, however are popular options as well.\n* For *Tools Analysis*, the most popular are the **Local Development Environments**, like *Jupyter Notebooks* or *R Studio*. \n* So, our top respondents analyze data mostly on Local Environments and deploy models either on GitHub, or keep them on their local machine."
"Analysis at Work and Deployment\n\nThe visualization and discussion below are representative for all 3 Pay Categories:\n* Regarding *Deployment*, most of our top respondents prefer to either deploy on **GitHub** or **keep their models locally on their personal gear**. **Kaggle and Colab**, however are popular options as well.\n* For *Tools Analysis*, the most popular are the **Local Development Environments**, like *Jupyter Notebooks* or *R Studio*. \n* So, our top respondents analyze data mostly on Local Environments and deploy models either on GitHub, or keep them on their local machine."
"Machine Learning - Miscellaneous Tools\n\nFinally, the visualization and discussion below are representative for all 3 Pay Categories as well:\n* For *ML Products*, there is a general lack of interest for all the presented options, as **None** was the most popular choice. However, most respondents say that they would like to gain knowledge of **Google Cloud Products** in the next 2 years.\n* Regarding *ML Experiments*, there is even a more prominent lack of interest, as **None** is the predominant answer, followed from far by **TensorBoard**. The **carelessness continues** in the projection over the next 2 years. So, ML Experiments, not so popular at the moment.ü§∑‚Äç‚ôÄÔ∏è\n* There is a lack of frequent usage in *Automated ML* as well; however, there can be seen a general trend for **Automated Model Selection** for the next 2 years.\n* Hence, *ML products, Experiments, or Automated ML* aren't a popular choice between our top respondents. A reason for that could be the inclination towards old school coding, as they might prefer writing everything from scratch, going through the data, the patterns, and the models with ""their own hands"", instead of relying on a service. But I am sure this fact will change over the years."
"Machine Learning - Miscellaneous Tools\n\nFinally, the visualization and discussion below are representative for all 3 Pay Categories as well:\n* For *ML Products*, there is a general lack of interest for all the presented options, as **None** was the most popular choice. However, most respondents say that they would like to gain knowledge of **Google Cloud Products** in the next 2 years.\n* Regarding *ML Experiments*, there is even a more prominent lack of interest, as **None** is the predominant answer, followed from far by **TensorBoard**. The **carelessness continues** in the projection over the next 2 years. So, ML Experiments, not so popular at the moment.ü§∑‚Äç‚ôÄÔ∏è\n* There is a lack of frequent usage in *Automated ML* as well; however, there can be seen a general trend for **Automated Model Selection** for the next 2 years.\n* Hence, *ML products, Experiments, or Automated ML* aren't a popular choice between our top respondents. A reason for that could be the inclination towards old school coding, as they might prefer writing everything from scratch, going through the data, the patterns, and the models with ""their own hands"", instead of relying on a service. But I am sure this fact will change over the years."
"\n  Takeaway: Our top respondents use Youtube, Personal Blogs, and Twitter to keep informed on the latest Data Science ""gossips"" while having lots of trust in Coursera as a reliable source of learning.\n  As for tools and methods, even though one would expect some complicated answers, they still use and rely the most on simple structures, like Regressions, Tree-Based models, Image Classification, or Word Embeddings for NLP. Most loved frameworks are Scikit-learn, Tensorflow, and Matplotlib, with no complicated or additional current preference for miscellaneous tools.\n  There IS something to learn here. There was a tweet from Grandmaster Bojan Tunguz that was sarcastically stating that all models besides Neural Nets are obsolete and you should focus only on these. The takeaway here is that no simple method, tool, or model is ""redundant"", not even for the great of the great. And this chapter shows precisely that.\n\n\n\n\n> And finally, our map is completely revealed!\n\n\n\n> Closer look üîé\n\n\n\n# 4. Conclusion\n\nI for one would have loved to see a survey like this when I started, some 1 year and a half ago. With no experience and no slight clue what data science is, what programming language do I need to know, how do you do machine learning, what is a notebook, how deep is deep learning, and ... for the love of God, GPUs? And, with the internet full of healthy and diverse opinions, it is easy to get lost and start not to trust the sources.\n\nHence, I truly hope that this analysis will bring some light for anybody in any query they might have. However small or big, I think that it's best to ask a professional, a great veteran in the discipline, a guru, a Grandmaster :) what is the best way to learn that specific subject? And if this notebook helped you just a bit getting closer to your answers, or a mentor, I can declare myself happy and fulfilled. I know it helped me, at least.\n\nGood luck, and may we all succeed with grace. Happy Data Sciencin'!\n\n# 5. References üìú\n\n* [Numbero Cost of Living Index](https://www.numbeo.com/cost-of-living/rankings_by_country.jsp)\n* [Pareto Principle](https://en.wikipedia.org/wiki/Pareto_principle)\n* [TPU vs GPU](https://analyticsindiamag.com/tpu-vs-gpu-vs-cpu-which-hardware-should-you-choose-for-deep-learning/#:~:text=TPU%20vs%20GPU%20vs%20CPU%3A%20A%20Cross%2DPlatform%20Comparison&text=TPU%3A%20Tensor%20Processing%20Unit%20is,small%20batches%20and%20nonMatMul%20computations.)\n* [Machine Learning Course - Stanford](https://www.coursera.org/learn/machine-learning)\n\n\n\n# Specs ‚å®Ô∏èüé®\n### (*tools that helped visualisation & creating the pirate map*)\n* Z8 G4 Workstation üñ•\n* 2 CPUs & 96GB Memory üíæ\n* NVIDIA Quadro RTX 8000 üéÆ\n* RAPIDS version 0.17 üèÉüèæ‚Äç‚ôÄÔ∏è"
**Imports**
# Setup TPU configuration
## 1.2 Check the target
"- Wow, very-well balanced target! Fun with this competition :)."
**Library and Data **
**Model with plots and accuracy**
**Model with plots and accuracy**
"# Logistic Regression \n**It‚Äôs a classification algorithm, that is used where the response variable is categorical. The idea of Logistic Regression is to find a relationship between features and probability of particular outcome.**   \n* odds= p(x)/(1-p(x)) = probability of event occurrence / probability of not event occurrence \n\n**Example- When we have to predict if a student passes or fails in an exam when the number of hours spent studying is given as a feature, the response variable has two values, pass and fail. \n**\n![multinomial-logistic-regression-with-apache-spark-4-638.jpg](attachment:multinomial-logistic-regression-with-apache-spark-4-638.jpg)"
**Checking for number of clusters**
**Fitting Model**
**Plotting Clusters**
# CNN
**Preprocessing and Data Split**
**Model**
"**It's an important method for dimension reduction.It extracts low dimensional set of features from a high dimensional data set with a motive to capture as much information as possible and to visualise high-dimensional data, it also reduces noise and finally makes other algorithms to work better because we are injecting fewer inputs.**\n* Example: When we have to bring out strong patterns in a data set or to make data easy to explore and visualize"
# Apriori
**Library and Data**
**Model and Forecast**
**Prediction**
# **Evaluate Algorithms** \n**The evaluation of algorithm consist three following steps:- **\n1. Test Harness  \n2. Explore and select algorithms \n3. Interpret and report results \n\n
"Thanks to https://www.kaggle.com/startupsci/titanic-data-science-solutions\n\nNow we are ready to train a model and predict the required solution. There are 60+ predictive modelling algorithms to choose from. We must understand the type of problem and solution requirement to narrow down to a select few models which we can evaluate. Our problem is a classification problem. With these two criteria - Supervised Learning, we can narrow down our choice of models to a few. These include:\n\n- Linear Regression, Logistic Regression\n- Naive Bayes \n- k-Nearest Neighbors algorithm\n- Neural network with Keras\n- Support Vector Machines and Linear SVC\n- Stochastic Gradient Descent, Gradient Boosting Classifier, RidgeCV, Bagging Classifier\n- Decision Tree Classifier, Random Forest Classifier, AdaBoost Classifier, XGB Classifier, LGBM Classifier, ExtraTrees Classifier \n- Gaussian Process Classification\n- MLP Classifier (Deep Learning)\n- Voting Classifier\n\nEach model is built using cross-validation (except LGBM). The parameters of the model are selected to ensure the maximum matching of accuracy on the training and validation data. A plot is being built for this purpose with [learning_curve](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html?highlight=learning_curve#sklearn.model_selection.learning_curve) from sklearn library."
### 5.1 Linear Regression \n\n[Back to Table of Contents](#0.1)
"Once trained our model, we can then visualize how changing some of its Hyperparameters can affect the overall model accuracy. In this case, I decided to observe how changing the number of estimators and the criterion can affect our Random Forest accuracy."
"We can now evaluate how our model performed using Random Search. In this case, using Random Search leads to a consistent increase in accuracy compared to our base model."
And this is distribution of # of sub-questions.
"## 1. Simple Distribution (Age, Gender, Country)\n\n### About Q1, Q2, Q3\n\n- Q1. What is your age (# years)?\n- Q2. What is your gender? - Selected Choice\n- Q3. In which country do you currently reside?\n\n\nLet's look at a simple distribution first.\n\nWhat kind of distribution does Kagler have? \n\nI have an [AI-related Facebook page](https://web.facebook.com/AI.Lookbook) with about 2000 followers in Korea and compare it lightly with the distribution.\n"
"I can certainly see that the proportion of female is lower than that of male.\n\nThe distribution of my Facebook page is:\n\n![img](https://i.imgur.com/00tja7U.png)\n\nBoth data vary by about 5 to 6 times, depending on gender.\n\nThis distribution corresponds to the entire engineering world, including AI.\n\nI hope to see more female AI researchers.\n\n"
"India and USA are overwhelming compared to other countries.\nIt's almost the opposite of Earth, but it's fun, although it's not special. (It reminds me of Sheldon and Rajesh of the *Big Bang Theory*.)\n\n\n![bigbang](https://media.giphy.com/media/SXJfIASq4Ayxq/giphy.gif)\n> img from https://giphy.com/gifs/the-big-bang-theory-sheldon-cooper-jim-parsons-SXJfIASq4Ayxq\n\nLet's look at the map and graph of the ratio of men and women by country."
Let's look at the relationship between models and frameworks.
"I can observe the following parts:\n\n- **Scikit-Learn** : Linear/Logistic Regression, Decision Trees, Random Forest\n- **Keras, Tensorflow** : CNN \n- **Pytorch** : More wide deep Learning task\n- GBMs\n\nIf you're a newbie, this library is a good reference.\n\n- [scikit-learn](https://scikit-learn.org/stable/)\n- [Keras](https://keras.io/)\n- [Tensorflow](https://www.tensorflow.org/)\n- [PyTorch](https://pytorch.org/)\n- [XGBoost](https://xgboost.readthedocs.io/en/latest/)\n- [LightGBM](https://lightgbm.readthedocs.io/en/latest/Python-API.html)\n\nAfter all, it's important to know everything.\n\nIf you are a beginner, we recommend studying in the following order.\n\n- Classic Machine Learning \n    - KNN, Linear/Logistic Regression, Decision Tree \n    - with Scikit-Learn\n    - The application of ML is important, but the content of it is important. That's why it's important to learn intuition and mathematical concepts in the classic ML.\n- Ensemble \n    - stacking, bagging, GBMs\n    - XGBoost -> LightGBM -> CatBoost\n    - LGBM seems to be the trend now.\n- Deep learning \n    - Personally I recommend starting with Keras or Pytorch.\n"
"## 6. What is the relationship between Programming Career and Language recommendations?\n\n> Q15. How long have you been writing code to analyze data (at work or at school)?\n\nIn fact, the person doing the analysis here expects to have almost the same distribution because there are many ML workers, but let's do it once."
"## 7. Personal thoughts about the visualization library\n\n> Q20. What data visualization libraries or tools do you use on a regular basis? (Select all that apply) - Selected Choice\n\nI like visualization the most in data analysis.\n\nSo I have made the following kernel : \n\n- [Road to Viz Expert (1) - Unusual tools](https://www.kaggle.com/subinium/road-to-viz-expert-1-unusual-tools)\n- [Road to Viz Expert (2) - Plotly & Seaborn](https://www.kaggle.com/subinium/road-to-viz-expert-2-plotly-seaborn)\n- [Road to Viz Expert (3) - Geo with Plotly Express](https://www.kaggle.com/subinium/road-to-viz-expert-3-geo-with-plotly-express)\n- [Road to Viz Expert (4) - Unusual Tools II](https://www.kaggle.com/subinium/road-to-viz-expert-4-unusual-tools-ii)\n\n- [3D Interactive Carüöó with Plotly](https://www.kaggle.com/subinium/3d-interactive-car-with-plotly)\n- [Weather Dashboard : EDA & Visualization ‚õÖüå°Ô∏è](https://www.kaggle.com/subinium/weather-dashboard-eda-visualization)\n- [Mask visualization, managing with buttons!](https://www.kaggle.com/subinium/mask-visualization-managing-with-buttons)\n\nThe various visualizations of this kernel can be found in my various kernels.\n\nLet's see survey again."
"Here is the link to the editor.\n\n- [Jupyter](https://jupyter.org/) : Project Jupyter exists to develop open-source software, open-standards, and services for interactive computing across dozens of programming languages.\n- [RStudio](https://rstudio.com/) : Open source and enterprise-ready professional software for data science.\n- [PyCharm](https://www.jetbrains.com/pycharm/) : The Python IDE for Professional Developers\n- [Atom](https://atom.io/) : A hackable text editor for the 21st Century\n- [MATLAB](https://www.mathworks.com/products/matlab.html) : Math. Graphics. Programming.\n- [Visual Studio / Visual Studio Code](https://code.visualstudio.com/) : Code editing. Redefined. Free. Built on open source. Runs everywhere.\n- [Spyder](https://www.spyder-ide.org/) : Spyder is a powerful scientific environment written in Python, for Python, and designed by and for scientists, engineers and data analysts.\n- Vim / Emacs\n- [Notepad++](https://notepad-plus-plus.org/) : Notepad++ is a free source code editor and Notepad replacement that supports several languages. \n- [Sublime Text](https://www.sublimetext.com/) : A sophisticated text editor for code, markup and prose\n\nI have used jupyter, pycharm, atom, vs / vscode, vim, notepad++, sublime text.\nCurrently I am using jupyter and vs code.\n\nIn the DS world where you need to keep testing, ipython environments like jupyter are a good fit. And it's very easy to have another server. And other than that, it's lightweight and the library uses a lot of vs code."
"Recently, the Colab GPU was upgraded to P100. Personally, I prefer the Kaggel notebook because of CSS. If you have a good UI / UX environment, please recommend."
"You can find some interesting facts by checking the treemap.\n\nFor most occupations, Python ranks first and SQL second.\n\nHowever, only statisticians rank R first.\n\nOf course, people could choose multiple languages ‚Äã‚Äãfor one job, but it is surprising that this trend is emerging.\n\n\n### 10-2. Jobs & Framework"
"## 11. Is salary high depending on career? How about educational background? \n\n> Q4. What is the highest level of formal education that you have attained or plan to attain within the next 2 years?\n\n> Q10. What is your current yearly compensation?\n\n> Q15. How long have you been writing code to analyze data (at work or at school)?\n\nIn fact, money questions are always fun.\n\n![money](https://media.giphy.com/media/xTiTnqUxyWbsAXq7Ju/giphy.gif)\n> img from https://giphy.com/gifs/yosub-money-donald-duck-cash-xTiTnqUxyWbsAXq7Ju\n\n\nLet's look at the annual salary.\n\n\nMost graphs will be viewed in the following order:\n\n- Distribution over the whole figure,\n- Percentage in percent\n- Average value\n\nAs a statistician, you shouldn't have complete confidence in the mean, but the visualization below shows the trend."
First let's look at the salary distribution.
"\n### 11-1. Educational background & Average Salary\n\nThe distribution is more diverse than I thought. I expected a normal distribution, but that's a shame.\n\nNow let's see what distribution this has for each condition.\n\nLet's look at a typical counting distribution and scaled distribution based on that condition."
"The average amount is estimated as follows. (If the distribution within the interval is normally distributed, we thought we would use the median.)"
"Those who earned a Ph.D. in the second half of the graph show that the salary is somewhat high. \n\nEven if you look at the average, degree holders can see some linearity.\n\nThis is why We have to go to a Ph.D.\n\n> I think that no formal education past high school is actually a successful dropout like Steve Jobs or Bill Gates. In fact, the answer may really be outside of school. Of course it would be different if it was a Ph.D.\n\n### 11-2. Career & Averagy Salary"
"Here you can see that there is about 25% difference between male/female.\n\nBefore looking at the country differences below, let's take a look at the percentage differences between male and female.\n\n- Formula : Female Wages-Male Wages / Male Wages\n\nAnd I was surprised to do this work. In this survey, My home country, Korea, was divided into South Korea and Republic of Korea. Korea also has wage gaps, but not as bad as the statistics show. Please see South Korea."
### 12-3. Country & Salary
"## 14. Who paid a lot?\n\n> Q11. Approximately how much money have you spent on machine learning and/or cloud computing products at your work in the past 5 years?\n\nAlthough it was not graphed, it did not have much correlation with the following areas. I preprocessed to log scale and looked at corr, but it didn't matter much. (I checked th correlation by corr method)\n\n- ML Library \n- Basis Algorithm\n\n\nI thought *deep learning* would definitely spend more money, but that wasn't it.\nWho is spending a lot of money?\n\n"
I will show it as a percentage graph according to the feature.
I will show it as a percentage graph according to the feature.
"On average, women use more. but i don't know why"
"On average, women use more. but i don't know why"
"Obviously, you can see that the size of your spending grows with age.\n\nThere seems to be a difference between having a job and not having one."
Table of Content:\n\n1. Data Handling\n1.1. Getting sense of the data\n1.2. Univariate and Bivariate Data Exploration\n1.3. Treatment of data for missing values\n1.4. Categorical Data Essense\n1.4.1. About Categorical Data\n1.4.2. Treatment Techniques for Categorical Data\n1.4.3. One hot encoding implementation\n1.5. Data Scaling\n2. Model Training\n2.1. What is Regression?\n2.2. Linear Regression\n2.3. Ridge and Lasso Regression\n2.3.1. Lasso Regression Implementation\n2.3.2. Ridge Regression Implementation\n3. Stats Model Interpretation and Backward Elimination Technique\n3.1. Selection Techniques in Multiple Regression\n3.2. How to perform backward elimination?\n3.3. Why Stats Model Library\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n
# 1. Data Handling\n
## 1.2. Univariate and Bivariate Data Exploration\n
"### Few observations from the above plot\n1. x,y, and z have a very strong relation with price but surprisingly depth (which comes from x,y, and z) doesn't has a significant relation with price.\n2. Carat has a strong relation with price\n3. Table doesn't have a significant relation with price or any other variable as well ( We can try dropping that when making the model) "
Since the dataset is big enough dropping 20 rows shouldn't cost us much hence the nans have been dropped.
## 1.4. Categorical Data Essense\n
* The figure above shows the decision boundary of a decision tree and k-NN classifiers along with their bagging ensembles applied to the Iris dataset. The decision tree shows axes parallel boundaries while the $k=1$ nearest neighbors fits closely to the data points. The bagging ensembles were trained using $10$ base estimators with $0.8$ subsampling of training data and $0.8$ subsampling of features. The decision tree bagging ensemble achieved higher accuracy in comparison to k-NN bagging ensemble because k-NN are less sensitive to perturbation on training samples and therefore they are called *stable learners*. Combining stable learners is less advantageous since the ensemble will not help improve generalization performance.
* The figure above shows learning curves for the bagging tree ensemble. We can see an average error of $0.3$ on the training data and a U-shaped error curve for the testing data. The smallest gap between training and test errors occurs at around $80\%$ of the training set size.
# Importing all Libraries...
"# Here is THE Big TRICK...\n\nBy default running the `fetch_lfw_people()` function downloads the data into the '**~/scikit_learn_data**' subfolders. This is not a big deal when you run the notebook on your **Local PC**. But, this becomes ***Shooting a STAR*** when you do the same in a **KAGGLE KERNEL**.\n\nIts *hard to locate* where your data gets downloaded when you run the above function. Matter of relief, I have collected the data from Sklearn's dataset folder, and uploaded here. So, you can easily get to know that data is available at location: '**../input**'.\n\nAgain, you get *a hit on your nose*; Because, You can not fetch and process data there as '**../input/lfw_people/**' has '***READ ONLY***' permission.\n\n### Solution:\n\nI ***created*** a folder named '**../LFW/lfw_people**'. And, set it as the **path**. So, the next time my fetching function will access and process data here. Then, I **copied** my complete dataset to this location(* Moving is also a great option*).\n\nNow, It is behaving exactly like its running on your Local PC. Hurray !\n"
#### Visualizing the Eigen Faces (reduced components of faces).
#### Visualising the prediction by plotting with Faces and train-test Prediction pairs.
"#  About Kiva and the challenge\n***\n\nKiva is a non-profit organization that allows anyone to lend money to people in need in over 80 countries. When you go to kiva.org, you can choose a theme (Refugees, Shelter, Health ...) or a country and you'll get a list of all the loans you can fund with a description of the borrower, his needs and the time he'll need for repayment. So far, Kiva has funded more than 1 billion dollars to 2 million borrowers and is considered a major actor in the fight against poverty, especially in many African countries.\n\nIn this challenge, the ultimate goal is to obtain as precise informations as possible about the poverty level of each borrower / region because that would help setting investment priorities. Kagglers are invited to use Kiva's data as well as any external public datasets to build their poverty estimation model.  \nAs for Kiva's data, here's what we've got : \n* **kiva_loans** : That's the dataset that contains most of the informations about the loans (id of borrower, amount of loan, time of repayment, reason for borrowing ...)\n* **kiva_mpi_region_locations** : This dataset contains the MPI of many regions (subnational) in the world.\n* **loan_theme_ids** : This dataset has the same unique_id as the kiva_loans (id of loan) and contains information about the theme of the loan.\n* **loan_themes_by_region** : This dataset contains specific informations about geolocation of the loans.\n\nThis notebook will be divided into two parts : \n1. First I will conduct an EDA using mainly the 4 datasets provided by Kiva. \n2. After that, I'll try to use the informations I got from the EDA and external public datasets to build a model for poverty level estimation."
"# 1. Exploratory Data Analysis\n\n*** \nIn this part, the goal is to understand the data that was given to us through plots and statistics, draw multiple conclusions and see how we can use those results to build the features that will be needed for our machine learning model. \n\nLet's first see what this data is about."
"Philippines is the country with most borrowers with approximately 25% of all users being philippinians. Elliott Collins, from the Kiva team, explained that this is due to the fact that a couple of Philippine field partners tend to make smaller short-term loans (popular low-risk loans + fast turnover rate). \n\n\nWe also notice that several african countries are in the list such as *Kenya, Mali, Nigeria, Ghana ...* and no european union country at all !     \nFor me, the most surprising was actually the presence of the US in this list, as it doesn't have the same poverty rate as the other countries but it turns out it's indeed a specific case, **I'll explain that in 1.4**.\n\nLet's now move on to the genders."
"In many loans (16.4% as you can see), the borrower is not actually a single person but a group of people that have a project, here's an [example](https://www.kiva.org/lend/1440912). In the dataset, they're listed as 'female, female, female' or 'male, female' ... I decided to use the label *mixed group* to those borrowers on the pie chart above.\n\nYou can see that most borrowers are female, I didn't expect that and it was actually a great surprise. This means that **women are using Kiva to get funded and work on their projects in countries (most of them are third world countries) where breaking in as a woman is still extremely difficult.**"
"## 1.3 Activities, sectors and funding amounts\n***\n\nNow let's take a peek at what people are needing loans for and what's the amounts they're asking for. Let's start with the sectors. There were 15 unique sectors in the summary we've seen above, let's see how each of them fare."
"**The most dominant sector is Agriculture**, that's not surprising given the list of countries that heavily use Kavi. A fast research for Kenya for example shows that all the top page is about agriculture loans, here's a sample of what you would find:  *buy quality seeds and fertilizers to use in farm*, *buy seeds to start a horticulture farming business so as a single mom*, *Purchase hybrid maize seed and fertilizer* ... Food sector occupies an important part too because many people are looking to buy fish, vegetables and stocks for their businesses to keep running.  \nIt's important to note that *Personal Use* occupy a significant part too, this means there are people who don't use Kavi to get a hand with their work but because they are highly in need.\n\nLet's see the more detailed version and do a countplot for **activities**"
"**The most dominant sector is Agriculture**, that's not surprising given the list of countries that heavily use Kavi. A fast research for Kenya for example shows that all the top page is about agriculture loans, here's a sample of what you would find:  *buy quality seeds and fertilizers to use in farm*, *buy seeds to start a horticulture farming business so as a single mom*, *Purchase hybrid maize seed and fertilizer* ... Food sector occupies an important part too because many people are looking to buy fish, vegetables and stocks for their businesses to keep running.  \nIt's important to note that *Personal Use* occupy a significant part too, this means there are people who don't use Kavi to get a hand with their work but because they are highly in need.\n\nLet's see the more detailed version and do a countplot for **activities**"
"This plot is only a confirmation of the previous one, activities related to agriculture come in the top : *Farming, Food production, pigs ...*. All in all, we notice that none of the activities belong to the world of 'sophisticated'. Everything is about basic daily needs or small businesses like buying and reselling clothes ...\n\nHow about the money those people need to pursue their goals ?"
"This plot is only a confirmation of the previous one, activities related to agriculture come in the top : *Farming, Food production, pigs ...*. All in all, we notice that none of the activities belong to the world of 'sophisticated'. Everything is about basic daily needs or small businesses like buying and reselling clothes ...\n\nHow about the money those people need to pursue their goals ?"
"Some outliers are clearly skewing the distribution and the plot doesn't give much information in this form : We need to **truncate the data**, how do we do that ? \n\nWe'll use a basic yet really powerful rule : the **68‚Äì95‚Äì99.7 rule**. This rule states that for a normal distribution :\n* 68.27% of the values $ \in [\mu - \sigma , \mu + \sigma]$\n* 95.45% of the values $ \in [\mu - 2\sigma , \mu + 2\sigma]$\n* 99.7% of the values $ \in [\mu - 3\sigma , \mu + 3\sigma]$     \nwhere $\mu$ and $\sigma$ are the mean and standard deviation of the normal distribution.\n\nHere it's true that the distribution isn't necessarily normal but for a shape like the one we've got, we'll see that applying the third filter will **improve our results radically**.\n"
"Some outliers are clearly skewing the distribution and the plot doesn't give much information in this form : We need to **truncate the data**, how do we do that ? \n\nWe'll use a basic yet really powerful rule : the **68‚Äì95‚Äì99.7 rule**. This rule states that for a normal distribution :\n* 68.27% of the values $ \in [\mu - \sigma , \mu + \sigma]$\n* 95.45% of the values $ \in [\mu - 2\sigma , \mu + 2\sigma]$\n* 99.7% of the values $ \in [\mu - 3\sigma , \mu + 3\sigma]$     \nwhere $\mu$ and $\sigma$ are the mean and standard deviation of the normal distribution.\n\nHere it's true that the distribution isn't necessarily normal but for a shape like the one we've got, we'll see that applying the third filter will **improve our results radically**.\n"
"Well, that's clearly a lot better !    \n* Most of the loans are between 100\$ and 600\$ with a first peak at 300\$.\n* The amount is naturally decreasing but we notice that we have a clear second peak at 1000\$. This suggets that there may be a specific class of projects that are more 'sophisticated' and get funded from time to time, interesting."
"Now first thing first, we'll plot the this difference that we called *time_funding*. To avoid any outliers, we'll apply the same rule for normal distribution as before."
"I was really surprised when I got this plot (and happy too), you'll rarely find a histogram where the distribution fits in this smoothly !   \nOn top of that, getting two peaks was the icing on the cake, it makes perfect sense ! **We've seen above that there are two peaks for loans amounts, at 300\$ and 1000\$, we're basically saying that for the first kind of loan you would be waiting 7 days and for the second kind a little more than 30 days !   **\nThis gives us a great intuition about how those loans work going forward.\n\nLet's be more specific and check for both loan amounts and waiting time country-wise :   \nWe'll build two new DataFrames using the groupby function and we'll aggregate using the median : what we'll get is the median loan amount (respectively waiting time) for each country."
"The US have the largest community of lenders and it is followed by Canada and Australia. On the other hand, the African continent seems to have the lowest number of funders which is to be expected, since it's also the region with highest poverty rates and funding needs.\n\nSo now that we know more about lenders location, let's analyze the textual freeform column *loan_because* and construct a wordcloud to get an insight about their motives for funding proejcts on Kiva."
"Lenders' answers are heartwarming :) Most reasons contain *help people / others* or *want to help*. We also find that it's the *right thing* (to do), it helps *less fortunate* and makes the world a *better place*.  \nKiva provides a platform for people who need help to fund their projects but it also provides a platform for people who want to make a difference by helping others and maybe changing their lives !"
###  Importing all the libraries
### Reading the file 
### Plotting boxplot to see the distribution of the data
**Separating features and label**
# Importing the necessary libraries
# Reading the 3 files from the Titanic Data Set
"# Gotta work a little bit in the Name column, \n# Creating a Title column based on the titles found in Name column, mapping them into numbers and finally removing the Name column"
"# we need to impute age column, basically fill in the blanks, I'm filling the blanks based on their Priority Class means"
# Decision Tree (only to show actual trees)
# XGBOOST
"INVESTIGATING THE DATA and EXPLORATORY DATA ANALSIS\n\nFirst, I install all the libraries that I will use in our application. I install all the libraries in the first part because the algorithms I will use later and the analysis I will make more clearly will be done.Furthurmore, I have investigated the data, presented some visualization and analysed features. Let's write it. I will import necessary Python modules and read the data."
"Now, we are uploading our data set to the data variable using the read_csv function in the pandas library. "
"We will perform analysis on the training data. The relationship between the features found in the training data is observed. In this way, comments about the properties can be made\n"
Age Analysis
"Now, we are going to analyze both the sex and the heart health situation."
"In this section, the rate of disease is seen less when the gender value is male. This is the result of an analysis for us."
"The transactions we perform in this section mean an average age. In this part, taking the average of all transactions is performed."
In this section we will use the groupby function. Our aim here is to obtain the average values of Thalach according to age ranges. Because we're going to do chest pain.
It seems that old people have a very hard job because their values are very high.
"MODEL, TRAINING and TESTING\nAs a result of our initial evaluations, we have used a number of artificial learning algorithms. These are logistic regression, support vector machine (SVM), k close neighborhood (kNN), GradientBoostingClassifier and RandomForestClassifier algorithms. The first algorithm is logistic regression algorithm. To implement this algorithm model, we need to separate dependent and independent variables within our data sets. In addition, we created a combination of features between different features to make different experiments. While creating these parameters, the process of finding the best results was made by giving hyper parameter values."
"First, let's plot the answers to Q15 and Q23 as a standard bar chart (I may not like bar charts, but they are just so useful). "
"They are clearly not the same, but one problem with them is the bins used. Unfortunately, the responses for Q15 and Q23 have not been made equal - Q23 has more granular bins (`'00-01', '01-02', '02-03', '03-04', '04-05', '05-10', '10-15', '20-99'`) than Q15 (`'00-01', '01-02', '03-05', '05-10', '10-20', '20-99'`). To make them more easily comparable, I aggregate the answers to the Q23 so that they correspond 1:1 with the answers to Q15. To do this, I create a new category, `'03-05'` by summing up the categories `'03-04'`, `'04-05'`, and I add the entries from category `'02-03'` to `'01-02'`. It is an arbitrary choice in case of the category `'02-03'`. "
"I am not interested in comparing the two bar plots however. I would like to know how the two questions interrelate. So let's create a pivot table and plot the answers to these survey questions against each other as a heatmap, with insights from Q23 plotted horizontally and from Q15 - vertically. For the sake of comparison, on the sides I draw again the two bar-plots, representing the aggregated values across each dimension. Therefore, the vertical bar plot on the left is a histogram of answers to the Q15, while the horizontal bar plot on the top represents summary of answers to the Q23, with the aggregated bins. Comparing these two bar plots, we can see that the two are not symmetric - there is more experienced coders than experienced ML-practitioners. "
"Plotting the pivot table, confronting the answers to Q15 and Q23, enables us to see much more structure than on a simple bar plot. We will use this heatmap as our anchor, digging deeper in the data. Let's look at it more in detail."
"First of all, we can see that the majority of the Kagglers are concentrated in the top-left corner of the plot. In fact, if we sum all the entries corresponding to less than 2 years of experience using both code to analyze data and machine learning methods, we will cover 52% of the survey responders, and if we increase those limits to less than 5 years in both cases, we cover 75% of participants. "
"There are other features catching our attention here. Let's look at what happens on the diagonal, and off the diagonal. Basing on this criterium, we can split the population of Kagglers into three big groups: \n- Kagglers who have been coding to analyze data for longer than they have been using machine learning (code-first)\n- Kagglers who have been in machine learning for longer than they have been coding for data (ML-first)\n- Kagglers who have similar experience in both coding and machine learning, therefore the two aspects have been likely highly intercorrelated for them (code and ML interrelated)"
"There are other features catching our attention here. Let's look at what happens on the diagonal, and off the diagonal. Basing on this criterium, we can split the population of Kagglers into three big groups: \n- Kagglers who have been coding to analyze data for longer than they have been using machine learning (code-first)\n- Kagglers who have been in machine learning for longer than they have been coding for data (ML-first)\n- Kagglers who have similar experience in both coding and machine learning, therefore the two aspects have been likely highly intercorrelated for them (code and ML interrelated)"
"Because the categories were not identical for both of the questions, and because of the fuzzy nature of the boundaries (*If I started with machine learning in 2017, have I been doing it 1-2 years, or 2-3 years?*), the off-diagonal entries that are very close to the diagonal should be intepreted also with a doze of fuzziness and uncertainty. But it doesn't change the fact that 5 Kagglers said that they have been using machine learning for more than 10 years while having coded for less than a year! (were they using visual or point-and-click tools? or proving theorems on paper? or made a mistake in the answer? or just trolling?)"
"We can also name some simple subgroups here, which will help us better interpret the survey results later on. I will focus on the ""prototypical"" groups, on the extreme sides. "
"The first group are the beginners. They have less than 2 years of experience of both coding and ML methods, and so they likely have started with the topic around 2017 the earliest. As I mentioned before, they make up for around 50% of all survey participants. \n\nThe second group are coders in transition. Those people have a decades-long coding experience for working with data, however they have started working with machine learning only recently. These may be for example software engineers transitioning into data engineers. \n\nThe third group belongs in the lower right corner and these are the machine learning veterans. Those people have been coding since long before the current AI revolution - with 10 or even over 20 years of both ML and coding experience, they may have started to specialize in the topic around 2000s or even late 1990s. These people were doing machine learning before it was cool. They likely know methods that are rare to find in the current data science curriculum, as well as have deep understanding of modern methods. \n\nThe last group to point out is the group in the middle: modern data scientists. They have started to be interested in the topic more less around 2015, so at the beginning of the boom, when ML started to go public. They are the most likely to have some kind of a ""standard"" education in the modern state-of-the-art tools, both for coding and machine learning. They have or are about to have passed the initial learning phase and can provide measurable value. "
"This heatmap in general informs us about the distribution of a specific population of survey takers when it comes to their experience in coding and machine learning. What we drew above corresponds to the total number of survey takers. Instead of using absolute number of counts in each bin, we can normalize it to get percentage of given population falling into each bin. We can then use it as a reference - we can calculate such a code-ML heatmap for a given subgroup of survey responders, and compare the two distributions to better understand the differences between that subgroup and general population. \n\nFor clarity, I leave out the axes descriptions for now, and adjust the color scale to better represent the data. "
"We can now compare this for example with the two heatmaps corresponding to the subpopulation of Kagglers, who defined themselves as ""Students"" and ""Data Scientists"" in Question 5 respectively."
"We can now compare this for example with the two heatmaps corresponding to the subpopulation of Kagglers, who defined themselves as ""Students"" and ""Data Scientists"" in Question 5 respectively."
"Comparing the three heatmaps as they are already brings some insights. \n- Among the group of students, 42% has less than a year experience in coding and using machine learning, while among the group of data scientists only 6% of people belong to this bin. \n- We also see that 2% of data scientists belongs to the group of the extreme veterans (>20 years of experience), while in general population only 1% belongs to this bin. \n- Among data scientists, the majority (almost 50% in total) is concentrated in the middle part of the heatmap, and there is a disproportionally bigger proportion of people especially in groups that use coding for 3-10 years and machine learning for 2-5 years. "
"However, going through each survey response like this would be extremely tiring, so I developed another way of visualizing these differences: \n1. I subtract the reference histogram (depending on a question, it will be either overall population of survey takers, or for example the ones that declared themselves as non-students - following the survey schema). \n2. Because I mostly want qualitative insights, I leave out the numbers and encode values in the difference histogram as colors. \n\nLet's look at an illustrative example: "
"By reducing the complexity of the data representation and encoding the matrix numbers as an easily interpretable colored pattern, we can gather insights much quicker and with less cognitive load. Here for example we immediately see that the Kagglers who describe as data scientists have on average much more ML and coding experience than the overall population, and there is relatively less people only starting to code in this group. A remark: white cells signify no big difference between the selected group and general population - and not necessarily that there is no counts in this cell! "
"With an average house price of $180921, it seems like I should relocated to Iowa!"
"Looks like a normal distribution? Not quite! Looking at the kurtosis score, we can see that there is a very nice peak. However, looking at the skewness score, we can see that the sale prices deviate from the normal distribution. Going to have to fix this later! We want our data to be as ""normal"" as possible."
"With 81 features, how could we possibly tell which feature is most related to house prices? Good thing we have a correlation matrix. Let's do it!"
"It's a nice overview, but oh man is that a lot of data to look at. Let's zoom into the top 10 features most related to Sale Price."
"Well, the most correlated feature to Sale Price is... Sale Price?!? Of course. For the other 9, they are as listed. Here is a short description of each. (Thank you, data_description.txt!)\n\n1. OverallQual: Rates the overall material and finish of the house (1 = Very Poor, 10 = Very Excellent)\n2. GrLivArea: Above grade (ground) living area square feet\n3. GarageCars: Size of garage in car capacity\n4. GarageArea: Size of garage in square feet\n5. TotalBsmtSF: Total square feet of basement area\n6. 1stFlrSF: First Floor square feet\n7. FullBath: Full bathrooms above grade\n8. TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n9. YearBuilt: Original construction date\n\nLet's take a look at how each relates to Sale Price and do some pre-cleaning on each feature if necessary."
What! People pay more for better quality? Nothing new here. Let's move on.
What! People pay more for better quality? Nothing new here. Let's move on.
It makes sense that people would pay for the more living area. What doesn't make sense is the two datapoints in the bottom-right of the plot. \n\n We need to take care of this! What we will do is remove these outliers manually. 
Nice! We got a 0.02 point increase in the Pearson-R Score.
4-car garages result in less Sale Price? That doesn't make much sense. Let's remove those outliers.
That looks much better. Note: removal of data is totally discretionary and may or may not help in modeling. Use at your own preference.
Again with the bottom two data-points. Let's remove those outliers.
"Only 0.01 point Pearson-R Score increase, but looks much better!"
Everything looks fine here.
Everything looks fine here.
Looks good.
Looks good.
It seems like houses with more than 11 rooms come with a $100k off coupon. It looks like an outlier but I'll let it slide.
It seems like houses with more than 11 rooms come with a $100k off coupon. It looks like an outlier but I'll let it slide.
"Although it seems like house prices decrease with age, we can't be entirely sure. Is it because of inflation or stock market crashes? Let's leave the years alone."
# About the notebook\n
# Let's load the required libraries\n
# Let's load the required libraries\n
# Load data set
So there are three types of species \n\nIris-setosa\nIris-versicolor\nIris-virginica
So we have equally distributed species all are of 50
# Corelation between features
# Visualizing species based on Sepal length and width
# Visualizing species based on Sepal length and width
We can easily differentiate setosa based on Sepal but for versicolor and virginica its difficult because the data is scattred.
#  Visualizing species based on petal length and width
Again based on petal we can easily classify setosa and for versicolor and virginica also we can classify but there is a thin line which should be taken care of
# Values distribution based on petal width
# Values distribution based on petal length
# Values distribution based on petal length
# Values distribution based on sepal length
# Values distribution based on sepal length
# Values distribution based on sepal width
# Values distribution based on sepal width
- From the above four graph you can see that the distribution of setosa < vericolor < virginica\n- There are few outliers which can be explained by the scatter plot graph.
# Andrew curves
Andrews curves are a method for visualizing multidimensional data by mapping each observation onto a function.\n\nSource - https://dzone.com/articles/andrews-curves
# Linear regression based on sepal
# Linear regression based on petal
# Linear regression based on petal
We have seen the visualization part\n\nNow lets see the how to apply machine learning to the dataset
# Let's get started!
# Reading the data
The 'freq' column represents number of observations for that patient
The number of oberservations for every unique patient in the train csv ranges from 6 to 10 wherein most of them have 9 observations.
The number of oberservations for every unique patient in the train csv ranges from 6 to 10 wherein most of them have 9 observations.
We notice the range of age to be between 48-88 where we have more records for patients in the age range 64-74.
We notice the range of age to be between 48-88 where we have more records for patients in the age range 64-74.
More number of male patients than female patients.
More number of male patients than female patients.
A big chunk of data is of patients who are Ex-smokers whereas very few patients who currently smoke.
A big chunk of data is of patients who are Ex-smokers whereas very few patients who currently smoke.
Records with patient who have never smoked have equal distribution of male and female patients whereas a large majority of ex-smokers are males.
Records with patient who have never smoked have equal distribution of male and female patients whereas a large majority of ex-smokers are males.
Male and female records are almost distributed throughout the age range.
Male and female records are almost distributed throughout the age range.
> **NOTE: Double click on the side legend to isolate a category**
Patients who currently smoke show only a few occurrence along the age range.
"* The value of FVC for current smokers is mostly concentrated around 3000.\n* For patients who never smoked, the value remains below 4400.\n* For Ex-smokers we see a few higher values around 6000 and a large number of records are between 2000 and 3000"
"* The value of FVC for current smokers is mostly concentrated around 3000.\n* For patients who never smoked, the value remains below 4400.\n* For Ex-smokers we see a few higher values around 6000 and a large number of records are between 2000 and 3000"
> **NOTE: Double click on the side legend to isolate a category**
**HAVING A FIRST LOOK AT THE IMAGE**
**Let's view all of the images for the first patient**
**Let's view all of the images for the first patient**
"> Looking at the previous two visuals, we notice that the **scans** for both the patients are **different**. Let's have a closer look at the scans of both patients."
"> Looking at the previous two visuals, we notice that the **scans** for both the patients are **different**. Let's have a closer look at the scans of both patients."
> Notice that the first scan has a circular border and the second one is regular. 
Import Libraries üìö
"\n\nI will be integrating ```W&B``` for ```visualizations``` and ```logging artifacts```!\n\n[Shopee Project on W&B Dashboard](https://wandb.ai/ruchi798/shopee?workspace=user-ruchi798) üèãÔ∏è‚Äç‚ôÄÔ∏è\n\n* To get the API key, an account is to be created on the website first.\n* Next, use secrets to use API Keys more securelyü§´"
**Visualizing and querying the dataset** with W&B üèãÔ∏è‚Äç‚ôÄÔ∏è\n\n[Documentation](https://docs.wandb.ai/datasets-and-predictions)
This is a snapshot of the table I just created and added to an artifact.\n\n![](https://i.imgur.com/oF7CloS.png)
**Logging an image** of the wordcloud of image titlesüèãÔ∏è‚Äç‚ôÄÔ∏è
"Unigrams, bigrams and trigrams üî¢ "
"**Logging custom bar charts** for unigrams, bigrams and trigramsüèãÔ∏è‚Äç‚ôÄÔ∏è"
Plugging in RAPIDS üèÉ‚Äç‚ôÄÔ∏è \n
 \n## Step 3: Visualising the Data\n\n- Here we will identify if some predictors directly have a strong association with the outcome variable `price`
#### Insights:\n- Toyota seems to be the most favoured cars.\n- Mercury seems to be the least favoured cars.
#### Visualizing the distribution of car prices
"- The plots seems to be right skewed, the prices of almost all cars looks like less than 18000.\n"
#### Visualising Numeric Variables\n\nPairplot of all the numeric variables
"#### Insights:\n- `carwidth` , `carlength`, `curbweight` ,`enginesize` ,`horsepower`seems to have a poitive correlation with price.\n- `carheight` doesn't show any significant trend with price.\n- `citympg` , `highwaympg` - seem to have a significant negative correlation with price."
#### Visualising few more Categorical Variables\n\nBoxplot of all the categorical variables
#### Insights\n- The cars with `fueltype` as `diesel` are comparatively expensive than the cars with `fueltype` as `gas`.\n- All the types of carbody is relatively cheaper as compared to `convertible` carbody.\n- The cars with `rear enginelocation` are way expensive than cars with `front enginelocation`.\n- The price of car is directly proportional to `no. of cylinders` in most cases.\n- Enginetype `ohcv` comes into higher price range cars.\n- `DoorNumber` isn't affecting the price much.\n- HigerEnd cars seems to have `rwd` drivewheel
#### Insights\n- The cars with `fueltype` as `diesel` are comparatively expensive than the cars with `fueltype` as `gas`.\n- All the types of carbody is relatively cheaper as compared to `convertible` carbody.\n- The cars with `rear enginelocation` are way expensive than cars with `front enginelocation`.\n- The price of car is directly proportional to `no. of cylinders` in most cases.\n- Enginetype `ohcv` comes into higher price range cars.\n- `DoorNumber` isn't affecting the price much.\n- HigerEnd cars seems to have `rwd` drivewheel
#### Insights:\n- The number of cylinders used in most cars is `four`.\n- Number of `Gas` fueled cars are way more than `diesel` fueled cars.\n- `Sedan` is the most prefered car type.
#### Relationship between `fuelsystem` vs `price` with hue `fueltype`
#### Relationship between `carbody` vs `price` with hue `enginelocation`
#### Relationship between `carbody` vs `price` with hue `enginelocation`
#### Relationship between `cylindernumber` vs `price` with hue `fueltype`
#### Relationship between `cylindernumber` vs `price` with hue `fueltype`
#### Derived Metrices\n- Average Price
#### Derived Metrices\n- Average Price
"#### Insights:\n- `Jaguar`,`Buick` and `porsche` seems to have the highest average price."
"#### Insights:\n- `Jaguar`,`Buick` and `porsche` seems to have the highest average price."
#### Insights:\n- `hardtop` and `convertible` seems to have the highest average price.
 \n## Step 9: Model Evaluation\n\nLet's now plot the graph for actual versus predicted values.
### RMSE Score
## Step 9: Model Evaluation\n\nLet's now plot the graph for actual versus predicted values.
### RMSE Score
"\n\n Weights & Biases (W&B) is a set of machine learning tools that helps you build better models faster. Kaggle competitions require fast-paced model development and evaluation. There are a lot of components: exploring the training data, training different models, combining trained models in different combinations (ensembling), and so on.\n\n> ‚è≥ Lots of components = Lots of places to go wrong = Lots of time spent debugging\n\nW&B can be useful for Kaggle competition with it's lightweight and interoperable tools:\n\n* Quickly track experiments,\n* Version and iterate on datasets, \n* Evaluate model performance,\n* Reproduce models,\n* Visualize results and spot regressions,\n* Share findings with colleagues.\n\nTo learn more about Weights and Biases check out this kernel."
# Training Configuration ‚öôÔ∏è
# Training Function
# Validation Function
"\n  Table of Contents\n  Phases of Kaggle/Competitive Data Science1\n    My Advice on ""How to get Started with Kaggle""2\n  Step 1 : Understanding the Problem Statement and Gathering Basic(Initial Domain Knowledge) Domain Knowledge3\n  Step 2 : Understanding the Evaluation Metric 4\n  Step 3: Exploratory Data Analysis ,Advanced Domain Knowledge Gathering and Choosing a Reliable CV5 \n  Step 4: Setting up a Baseline and Gathering All your ideas in One place6\n  Step 5: Skimming through all the dicussion threads and Notebooks , writing down the Ideas to try7\n    Step 6: Perform Experiments and repeat8"
"# Phases of Kaggle/Competitive Data Science\n\nIn this section I try to answer How to get started with Kaggle as before learning how to tackle a competitive problem you should know where to begin right. In my experience , I feel there are following four stages of Kaggling or Competitve Data science for anyone :\n\n* Noobie\n* Had the Feel /Intermediate \n* Seasoned/Pro\n* Elite\n\nLet me frame a clear definition for each of the above so that you know on which stage you are currently in :\n\n### Noobie\nYou are a Noobie if :\n* you have read about data science and are really excited to start with it , but you are not sure how to...\n* you have just started your data science journey , have done some courses and now you are not sure where to go...\n* you have good knowledge of python but are getting started with data science \n* you are switching from a different background alltogether\n\nThere can be many more scenarios but you get the idea right?\n\n### Had the Feel / Intermediate\nYou are at intermediate level if:\n* you have participated in other hackathons on other smaller platforms like Analytics Vidhya , MachineHack ,Zindi ,etc but are afraid of Kaggle\n* you are good with tabular data competitions , EDA ,etc but are afraid of kaggle\n* you have done beginner level projects and now want to try your hands at real competitions, etc \n\n### Seasoned / Pro\nYou are a Pro if:\n* you have participated in Competitions before on other platforms and have got very good ranks but are new to kaggle\n* you are experienced data scientists working in big organizations but are new to kaggle\n* you have participated in live Kaggle competitions and managed to be in top 15 percent\n* you have written some very good kernels and are able to read and understand the advanced kernels etc\n* you have won several bronze medals in competitions\n* you are not afraid anymore but don't know how to get better in order to reach gold or silver not by chance but by pure work\n\n### Elite\nYou are an Elite if:\n* There is huge respect for you in the community\n* People follow you , admire you , look up to you\n* Have reached a stage of competition master or grandmaster\n\nThis is the stage which is a dream of every kaggler when they start right? These are the people whom we look upto and whom we want to follow , these are the people who have taught us , helped us , to reach all the way to pro level .\nThe people in Elite level are something different , I am talking about the brilliant Grandmasters whom we follow , whose kernels we drool over , whom we ask all the tough questions and they are kind enough to readily answer them.\n\n# My Advice on ""How to get Started with Kaggle""\n\n Now I believe you need to be atleast in intermediate stage to get started with kaggle but that doesnt mean you cant do competitive data science if you are in the Noobie stage . Below are my advices stage wise on "" How To Get Started With Kaggle "" \n\n### Noobie\n\nIf you are in this stage means you are just starting , your pure focus should be to move into the `intermediate stage`, here is what you will need :-\n\n* Start with Python , main goal should be to atleast get to an intermediate level in python. Sources such as hackerearth and others can used.\n* Along with learning Python , Kaggle Learn courses are very good to transition into data science, focus should be on getting better with main python data science libraries like pandas , numpy ,scikit learn\n* Also you should keep practicing EDA and skills gained by kaggle learn courses ,by taking any dataset and exploring it with whatever knowledge you have with your own intuition and documenting main insights you might have found\n* Once all the above is done till a level that you are able to understand and write basic level codes , you can slowly start moving to starter level competitions , your focus should be doing only tabular data competitions . You can start by choosing playground kaggle competitions or go at a slightly smaller platform and practice there . This step is very crucial as you need to apply all the knowledge from kaggle learn courses and other places now to get a good rank in starter level competitions at sites like analytics vidhya and machine hack\n* Please note that delving into the maths and the algorithms themselves is not necessary at this stage and focus should be more on the applied part, if you are someone who likes the top down approach\n* Once you have gained good amount of confidence at exploring and modelling tabular data , try doing a unique personal project on tabular data and apply all the skills which you have acquired till now . The most important part with data science is your creativity and you must always use free will while dealing with data science projects and hence practice is necessary.\n\nVoila ! you are near about ready to transition into intermediate phase and you have already got a data science project to showcase , not bad , isn't it?\n\n### Intermediate\n\nIf you are here then it means you have sufficient applied knowledge in data science and have proven yourself with tabular data already .Now you want to enter the real competitions and want to make your name:\n\n* Live Kaggle competitions are tough and might be out of your league given your current skillset but that doesn't mean you can't do anything. The easiest way to get started with Kaggle competitions is to read kernels and write your own . Writing good kernels is a great way to boost your creativity, take any live kaggle competition , use whatever knowledge you have and publish a kernel (Don't worry no one is going to judge), read other people's kernels to get some ideas if the data is totally new to you , but be sure to put your original ideas as well in your kernels .Always remember the point of this exercise is to be creative , original and most of all learning to get comfortable with live kaggle comeptitions and not medals .\n* Don't worry if your kernels fail on upvotes ,if you learned something new by writing it then it was worth it , read the most upvoted kernels , try and understand why they got the upvotes .Kaggle community really appreciates original ideas and creativity so if you keep doing this you will surely get rewards . [Here](https://www.kaggle.com/general/89512#post516909) is a discussion thread in which Andrew (former notebooks rank 1) answers how to write good kernels.\n* Along with all this , its important that you now gain theoritical knowledge as well , learn one algorithm at a time , try to finish all the classical ones , read about different evaluation metrics(both for classification and regression) , bias-variance trade off , etc. The point is read atleast 10 articles everyday . Visit previous kaggle competitions , read best kernels there and make sure you are covering ground\n* Once you are done writing kernels for 3-4 live kaggle competitions in a way I explained with original ideas you will be familiar of how things work at kaggle and will be confident enough to enter a live kaggle competition . \n* There is a quote which I have formed for me and I thought it would be worth mentining ,"" If you are not learning anything new by writing a kernel or by participating in a competition or by doing a personal project ,then its really not worth it"". \n\n### Pro and Elite\n\nWell it would be completely vague if someone like me gives some advice to people in this category. I will say I am just thankful to these people to keep sharing their knowledge in such a easy way for people to understand . Due to presence of these people kaggle becomes such a great platform to learn and practice data science. I hope I keep learning and one day reach the ELITE stage \n\n\nNow when I was starting kaggle there was no tabular data competition so I had to learn a completely new thing altogether just to participate but luckily for you there is this tabular competition going on and you can get started , Now without further ado , lets get started with the steps for tackling any Live Kaggle Competition"
"**Introduction**\n\nI've always wanted to build an end to end ml solution - starting with model creation and ending with a live web app. Here I've managed to do it. Users are able to submit a picture of a skin lesion and get an instant prediction. This kernel details the process I followed to build the model and then convert it from Keras to Tensorflow.js. The javascript, html and css code for the app is available on github. \n\nWeb App:http://skin.test.woza.work/\nGithub: https://github.com/vbookshelf/Skin-Lesion-Analyzer\n\nThis model classifies skin lesions into seven classes. It is a fine tuned MobileNet CNN. All training was done in this kernel. The main challenges were the unbalanced dataset and the small amount of data.  I used data augmentation to reduce the class imbalance and in so doing get categorical accuracy scores that were not heavily skewed by a single majority class.\n\nMobileNet‚Äôs small size and speed makes it ideal for web deployment. It‚Äôs also a joy to train.\n\nTensorflow.js is a new library that allows machine learning models to run in the browser - without having to download or install any additional software. Because the model is running locally, any data that a user submits never leaves his or her pc or mobile phone. I imagine that privacy is especially important when it comes to medical data.\n\n\n\n**What is the objective?**\n\nI found it very helpful to define a clear objective right at the start. This helps guide the model selection process. For example, if a model has an accuracy of 60% it would usually be seen as a bad model. However, if it also has a top 3 accuracy of 90% and the objective requires that it output 3 predictions then it may actually be quite a good model. \n\n*This is the objective that I defined for this task:*\n\n> Create an online tool that can tell doctors and lab technologists the three highest probability diagnoses for a given skin lesion. This will help them quickly identify high priority patients and speed up their workflow. The app should produce a result in less than 3 seconds. To ensure privacy the images must be pre-processed and analysed locally and never be uploaded to an external server.\n"
"**LABELS**\n\nExcerpts from the paper:\n> The HAM10000 Dataset: A Large Collection of Multi-Source Dermatoscopic Images of Common Pigmented Skin Lesions\nhttps://arxiv.org/abs/1803.10417\n\n\n\n **nv**\n Melanocytic nevi are benign neoplasms of melanocytes and appear in a myriad of variants, which all are included in our series. The variants may differ significantly from a dermatoscopic point of view.\n *[6705 images]*\n \n **mel**\n Melanoma is a malignant neoplasm derived from melanocytes that may appear in different variants. If excised in an early stage it can be cured by simple surgical excision. Melanomas can be invasive or non-invasive (in situ). We included all variants of melanoma including melanoma in situ, but did exclude non-pigmented, subungual, ocular or mucosal melanoma.*[1113 images]*\n \n \n**bkl**\n ""Benign keratosis"" is a generic class that includes seborrheic ker- atoses (""senile wart""), solar lentigo - which can be regarded a flat variant of seborrheic keratosis - and lichen-planus like keratoses (LPLK), which corresponds to a seborrheic keratosis or a solar lentigo with inflammation\nand regression [22]. The three subgroups may look different dermatoscop- ically, but we grouped them together because they are similar biologically and often reported under the same generic term histopathologically. From a dermatoscopic view, lichen planus-like keratoses are especially challeng- ing because they can show morphologic features mimicking melanoma [23] and are often biopsied or excised for diagnostic reasons.\n*[1099 images]*\n\n**bcc**\nBasal cell carcinoma is a common variant of epithelial skin cancer that rarely metastasizes but grows destructively if untreated. It appears in different morphologic variants (flat, nodular, pigmented, cystic, etc) [21], which are all included in this set.\n*[514 images]*\n \n**akiec**\nActinic Keratoses (Solar Keratoses) and intraepithelial Carcinoma (Bowen‚Äôs disease) are common non-invasive, variants of squamous cell car- cinoma that can be treated locally without surgery. Some authors regard them as precursors of squamous cell carcinomas and not as actual carci- nomas. There is, however, agreement that these lesions may progress to invasive squamous cell carcinoma - which is usually not pigmented. Both neoplasms commonly show surface scaling and commonly are devoid of pigment. Actinic keratoses are more common on the face and Bowen‚Äôs disease is more common on other body sites. Because both types are in- duced by UV-light the surrounding skin is usually typified by severe sun damaged except in cases of Bowen‚Äôs disease that are caused by human papilloma virus infection and not by UV. Pigmented variants exists for Bowen‚Äôs disease [19] and for actinic keratoses [20]. Both are included in this set.*[327 images]*\n\n\n**vasc**\nVascular skin lesions in the dataset range from cherry angiomas to angiokeratomas [25] and pyogenic granulomas [26]. Hemorrhage is also included in this category.\n*[142 images]*\n\n**df**\nDermatofibroma is a benign skin lesion regarded as either a benign proliferation or an inflammatory reaction to minimal trauma. It is brown often showing a central zone of fibrosis dermatoscopically [24].*[115 images]*\n\n\n*[Total images = 10015]*"
### Plot the Training Curves
### Create a Confusion Matrix
### Print few random paintings
## Data Augmentation
### Print a random paintings and it's random augmented version
## Build Model
## Confusion Matrix. Look at the style of the artists which the model thinks are almost similar. 
# Evaluate performance by predicting on random images from dataset
# Evaluate performance by predicting on random images from dataset
# This portion is just for fun :) Replace the variable `url` with an image of one of the 11 artists above and run this cell.
# This portion is just for fun :) Replace the variable `url` with an image of one of the 11 artists above and run this cell.
"### Thank you for reading the  notebook! Please share your thoughts and feedback in comments section below.\n### Please ""upvote"" the kernel if you like it :)\n### Please take a look at http://supratimh.github.io for other projects I am working on. I will eagerly look forward to your feedback and suggestions."
\n# Logistic Regression with Python\n\nWe'll be trying to predict a classification- survival or deceased.Let's begin our understanding of implementing Logistic Regression in Python for classification.\n\n## Import Libraries\nLet's import some libraries to get started!
## The Data\n
___\n## Data Cleaning\nWe want to fill in missing age data instead of just dropping the missing age data rows. One way to do this is by filling in the mean age of all the passengers (imputation).\nHowever we can be smarter about this and check the average age by passenger class. For example:\n
"We can see the wealthier passengers in the higher classes tend to be older, which makes sense. We'll use these average age values to impute based on Pclass for Age."
Now let's check that heat map again!
Great! Let's go ahead and drop the Cabin column and the row in Embarked that is NaN.
"## Sources:\n### - Notebooks (kernels) of the Prize Competition Winners\n### - Notebooks (kernels) of Kaggle Grandmasters, Masters or Experts\n### - Detailed tutorials of the leading Python libraries\netc."
"## Thanks to:\n\n\n* @agostontorok,\n* @andradaolteanu,\n* @andresionek, \n* @artvolgin, \n* @dwin183287,\n* @haakakak,\n* @ihelon,\n* @kanncaa1, \n* @katemelianova, \n* @masumrumi,\n* @mtodisco10,\n* @mykolazotko,\n* @nareshbhat,\n* @n1sarg,\n* @parulpandey,\n* @pavansanagapati,\n* @pestipeti,\n* @poonaml,\n* @prashant111,\n* @raenish,\n* @robikscube,\n* @shivamb,\n* @siavrez,\n* @spitfire2nd,\n* @subinium,\n* @theshak64,\n* @tkubacka, \n* @toomuchsauce,\n* @tyagit3, \n* @viveknest,\n* @ykhorramz\n\n### for their wonderful and helpful notebooks (kernels)!"
![image.png](attachment:image.png)
![image.png](attachment:image.png)
## 2.3. Comprehensive Plots \n\n[Back to Table of Contents](#0.1)
![image.png](attachment:image.png)
## 2.4. Tips for making the Good Visualization from Kaggle Masters and Experts\n\n[Back to Table of Contents](#0.1)
"**The kernel [Tips for making the Informative Visualization](https://www.kaggle.com/subinium/tips-for-making-the-informative-visualization)**\n\n**Thanks to @subinium**\n\n#### Tips for making the Informative Visualization\n* Basic concepts and elements of graphics (their names in different libraries).\n* Typical approaches to displaying data of different types\n* Tips on how to choose colors, color palette, color scheme\n* Tips on how to choose arrangement (layout), margin, ratio, grids, axis, and borders in the charts\n* Examples of how not to do\n* Links to other interesting and useful resources\n\nConcisely about the main thing.\n\n\n*Quote from the notebook:*\n#### ""Information visualization is a process to find 4 main things.\n* **Composition**: What does the data consist of?\n* **Distribution**: What distribution does the data have?\n* **Comparison**: What distribution are features in?\n* **Relationship**: What about the relationship between two or more features?"""
## 3. Matplotlib \n\n[Back to Table of Contents](#0.1)
**The kernel 2 [Rare Visualization Tools](https://www.kaggle.com/kanncaa1/rare-visualization-tools)**\n\n**Thanks to @kanncaa1**\n\n#### - Spider(radar) plot\n\nA spider(radar) plot is a graphical method of displaying multivariate data in the form of a two-dimensional chart of three or more quantitative variables.
## 4. Seaborn \n\n[Back to Table of Contents](#0.1)
"**The kernel 2 [Seaborn Tutorial for Beginners](https://www.kaggle.com/kanncaa1/seaborn-tutorial-for-beginners)**\n\n**Thanks to @kanncaa1**\n\nIn addition to kernel 1, this kernel contains outhers examples (on ""Fatal Police Shootings in the US"") and:\n* Heatmap\n* Swarm Plot and etc."
**The kernel 1 [Seaborn tutorial for beginners](https://www.kaggle.com/prashant111/seaborn-tutorial-for-beginners)**\n\n**Thanks to @prashant111**\n\n**References of this kernel:**\n\nSeaborn Official Tutorial: http://seaborn.pydata.org/tutorial.html\n\nSeaborn documentation and API reference:\n\n* http://seaborn.pydata.org/\n* http://seaborn.pydata.org/api.html\n\nUseful Seaborn tutorials\n\n* https://www.datacamp.com/community/tutorials/seaborn-python-tutorial\n* https://elitedatascience.com/python-seaborn-tutorial\n* https://www.tutorialspoint.com/seaborn/index.htm#\n\n\n#### Seaborn tutorial\n\n* Seaborn Kernel Density Estimation (KDE) plot\n* Histograms\n* Visualize distribution of values in Preferred Foot variable with Seaborn countplot() function\n* Seaborn catplot() function\n* Seaborn stripplot() function\n* Seaborn boxplot() function\n* Seaborn violinplot() function\n* Seaborn pointplot() function\n* Seaborn barplot() function\n* Visualizing statistical relationship with Seaborn relplot() function\n* Seaborn scatterplot() function\n* Seaborn lineplot() function\n* Seaborn regplot() function\n* Seaborn lmplot() function\n* Multi-plot grids\n* Seaborn Facetgrid() function\n* Seaborn Pairgrid() function\n* Seaborn Jointgrid() function\n* Controlling the size and shape of the plot\n* Seaborn figure styles
## 5. Plotly \n\n[Back to Table of Contents](#0.1)
## 5. Plotly \n\n[Back to Table of Contents](#0.1)
"**The kernel 2 [Simple EDA-Model](https://www.kaggle.com/siavrez/simple-eda-model)**\n\n**Thanks to @siavrez**\n\nEDA for the prize competition [University of Liverpool - Ion Switching](https://www.kaggle.com/c/liverpool-ion-switching)\n\nVery good visual interactive plots based on plotly: scatter, box, density_heatmap and others."
"**The kernel 1 [Plotly Tutorial for Beginners](https://www.kaggle.com/kanncaa1/plotly-tutorial-for-beginners)**\n\n**Thanks to @kanncaa1**\n\n#### Plotly tutorial\n\n* Line Charts, Scatter Charts, Bar Charts, Pie Charts, Bubble Charts\n* Histogram\n* Word Cloud\n* Box Plot\n* Scatter Plot Matrix\n* Data Visualization\n* Inset Plots\n* 3D Scatter Plot with Colorscaling\n* Multiple Subplots and etc."
"**The kernel [COVID-19 EDA and Forecasting](https://www.kaggle.com/n1sarg/covid-19-eda-and-forecasting)**\n\n#### Research for Competition ""COVID19 Global Forecasting (Week 5)""\nMany interesting plots with PlotLy"
## 6. Interactive plots with Bokeh \n\n[Back to Table of Contents](#0.1)
"**The kernel 1 [EDA using Bokeh Visualisation](https://www.kaggle.com/pavansanagapati/eda-using-bokeh-visualisation)**\n\n**Thanks to @pavansanagapati**\n\nVery good examples!\n\n**Bokeh is an interactive visualization library** that targets modern web browsers for presentation. Its goal is to provide elegant, concise construction of versatile graphics, and to extend this capability with high-performance interactivity over very large or streaming datasets. Bokeh can help anyone who would like to quickly and easily create interactive plots, dashboards, and data applications."
[Gallery of Bokeh - examples](https://docs.bokeh.org/en/latest/docs/gallery.html)
**The kernel 2 https://www.kaggle.com/kanncaa1/visualization-bokeh-tutorial-part-1**\n\n**The kernel 3 https://www.kaggle.com/kanncaa1/interactive-bokeh-tutorial-part-2**\n\n**Thanks to @kanncaa1**
"**The kernel 1 [EDA using Bokeh Visualisation](https://www.kaggle.com/pavansanagapati/eda-using-bokeh-visualisation)**\n\n**Thanks to @pavansanagapati**\n\nVery good examples!\n\n**Bokeh is an interactive visualization library** that targets modern web browsers for presentation. Its goal is to provide elegant, concise construction of versatile graphics, and to extend this capability with high-performance interactivity over very large or streaming datasets. Bokeh can help anyone who would like to quickly and easily create interactive plots, dashboards, and data applications."
## 7. Network Analysis \n\n[Back to Table of Contents](#0.1)
![image.png](attachment:image.png)
## 9. Embedding Analysis \n\n[Back to Table of Contents](#0.1)
## 10.1. 3D Plot - Matplotlib \n\n[Back to Table of Contents](#0.1)
![image.png](attachment:image.png)
![image.png](attachment:image.png)
"**The kernel 3 [Rare Visualization Tools](https://www.kaggle.com/kanncaa1/rare-visualization-tools)**\n\n**Thanks to @kanncaa1**\n\n### Basic 3D Scatter Plot (Plotly)\n\n* import data again to avoid confusion\n* go.Scatter3d: 3D scatter\n\nWe will plot iris setosa and iris virginica classes according to their Sepal Length(x), Sepal Width(y), and Petal Length(z)."
"**The kernel 3 [Rare Visualization Tools](https://www.kaggle.com/kanncaa1/rare-visualization-tools)**\n\n**Thanks to @kanncaa1**\n\n### Basic 3D Scatter Plot (Plotly)\n\n* import data again to avoid confusion\n* go.Scatter3d: 3D scatter\n\nWe will plot iris setosa and iris virginica classes according to their Sepal Length(x), Sepal Width(y), and Petal Length(z)."
![image.png](attachment:image.png)
"## 10.3. Animation Plots - Matplotlib, Plotly \n\n[Back to Table of Contents](#0.1)"
**The kernel 2 [Earthquake Animation with Plotly](https://www.kaggle.com/kanncaa1/earthquake-animation-with-plotly)**\n\n**Thanks to @kanncaa1**\n\nInteractive plot with time scale - optimal for time series
**The kernel 1 [Kiva in 2 minutes Animated Story](https://www.kaggle.com/poonaml/kiva-in-2-minutes-animated-story)**\n\n**Thanks to @poonaml**\n\nInteractive maps (with Folium) with save as gif - by FuncAnimation from matplotlib.animation
![image.png](attachment:image.png)
## 10.4. Bringing Matplotlib to the Browser - MPLD3 \n\n[Back to Table of Contents](#0.1)
![image.png](attachment:image.png)
**The kernel [Geostatistical analysis with SciKit-GStat](https://www.kaggle.com/vbmokin/geostatistical-analysis-with-scikit-gstat)**\n\nIt's my kernel.\n\nThe main application for scikit-gstat is variogram analysis and Kriging (Geostatistical analysis). The basic idea of kriging is to predict the value of a function at a given point by computing a weighted average of the known values of the function in the neighborhood of the point.
**The kernel 1 [Wuhan Coronavirus : A geographical analysis](https://www.kaggle.com/parulpandey/wuhan-coronavirus-a-geographical-analysis/notebook)**\n\n**Thanks to @parulpandey**
![image.png](attachment:image.png)
![image.png](attachment:image.png)
![image.png](attachment:image.png)
![image.png](attachment:image.png)
## 15. Altair \n\n[Back to Table of Contents](#0.1)
**The kernel 1 [Cheatsheet 100+ Altair Plots - Part 1 (Basic)](https://www.kaggle.com/raenish/cheatsheet-100-altair-plots-part-1-basic/)**\n\n**Thanks to @raenish**\n\nWith Altair:\n* Scatter\n* Line\n* Area\n* Bubble\n* Bar\n* Histogram\n* Box\n* Time Series\n* Strip Plot\n* Dot Plot
"**[Altair](https://pypi.org/project/altair/)** is a declarative statistical visualization library for Python. With Altair, you can spend more time understanding your data and its meaning. Altair's API is simple, friendly and consistent and built on top of the powerful Vega-Lite JSON specification. This elegant simplicity produces beautiful and effective visualizations with a minimal amount of code. Altair is developed by Jake Vanderplas and Brian Granger in close collaboration with the UW Interactive Data Lab."
**The kernel 2 [Cheatsheet 100+ Altair Plots - Part 2 (Advanced)](https://www.kaggle.com/raenish/cheatsheet-100-altair-plots-part-2-advanced/)**\n\n**Thanks to @raenish**\n\nWith Altair:\n* Heatmap\n* Error\n* Candlestick\n* Violin\n* Gantt\n* Ridgeline\n* Map\n* Interactive
**The kernel 1 [Cheatsheet 100+ Altair Plots - Part 1 (Basic)](https://www.kaggle.com/raenish/cheatsheet-100-altair-plots-part-1-basic/)**\n\n**Thanks to @raenish**\n\nWith Altair:\n* Scatter\n* Line\n* Area\n* Bubble\n* Bar\n* Histogram\n* Box\n* Time Series\n* Strip Plot\n* Dot Plot
## 16. Interactive Dashboard \n\n[Back to Table of Contents](#0.1)
**Embarked and Sex**
"**Embarked, Pclass and Sex :**\n\n** Practically all women of Pclass 2 that embarked in C and Q survived, also nearly all women of Pclass 1 survived. **\n\n** All men of Pclass 1 and 2 embarked in Q died, survival rate for men in Pclass 2 and 3 is always below 0.2 **\n\n** For the remaining men in Pclass 1 that embarked in S and Q, survival rate is approx. 0.4 **"
**Correlation Matrix**
"Survived and Fare positively correlated, Survived and Sex_male negatively correlated.  \nAlso, Survived and Pclass_3 negatively correlated. SibSp and Parch correlated"
## Load all dependencies you need\n from   coffee   import   ***** 
"Let's start seeding everything to make results somewhat reproducible. Anyway, in keras it is quite hard to get 100% reproducible results."
"In this section you can configure the following:\n* Features used for training\n* Basic training setup: BATCH_SIZE and EPOCHS,\n* Configuration for the loss function\n* Optimizers, Learning-Rate-Schedulers incl. Learning Rate start- & endpoint\n* Custom Logging Callback\n* Checkpoint-Saving Callback\n\nThe Learning-Rate scheduler below is inspired by Chris great [Melanoma-detection notebook](https://www.kaggle.com/cdeotte/triple-stratified-kfold-with-tfrecords).  \nFeel free to experiment with the scheduler and it's max/min and decay values.\n\n**Ever wondered why lr_max is scaled by BATCH_SIZE and therefore bigger for larger batches?** The reason for this is the following: the larger the BATCH_SIZE, the more averaged & smoothened a step of gradient decent is and the bigger our confidence in the *direction* of the step is. As there is less ""randomness"" in a huge averaged batch (compared with for example Stochastic Gradient Decent (=SGD) with batch size = 1) and our confidence in the direction is higher, the learning rate can be bigger to advance fast to the optimum."
### Loss Function
"# Evaluation & submission\n\n### Check loss and accuracy\nOkay, we made it! Let's evaluate our model, check our stats (Out-Of-Fold log-loss) and submit it! Let's start with some plots.\nThose plots can tell us, whether our model training is working as expected, or if it strongly overfits.  \nThe below **EXAMPLE-IMAGE** is an example of a strongly overfitting model:\n![image.png](attachment:image.png)\nLong before we hit the 10th epoch, the validation loss is increasing again, while the training loss keeps decreasing. We can also clearly ovserve that there is no improvement in our accuracy anymore. \n\nWhat can we do against strongly overfitting models?\nWe could do the following:\n\n* Collect more training data or use augmentation to generate new data: Sadly I have no brilliant idea on how to do this for this specific Kaggle competition.\n* Reduce the network‚Äôs size (width andor/ dept) by removing layers or reducing the number of neurons in the hidden layers\n* Use regularization like LASSO (=Least Absolute Shrinkage and Selection Operator; aka L1 regularization) or Ridge (aka L2 regularization) which results in adding a cost-term to the loss function\n* Use higher dropout-rate in the Dropout-Layers, which will randomly remove more connections by setting them to zero and forcing the network to generalize better (=avoid relying on a limitied number of strong-influence neurons).\n\nBut now check our model's result and evaluate it:"
### OOF Evaluation
"## Historical Data Science Trends on Kaggle \n\nA number of trends have changed over the years in the field of Data Science. Kaggle is the largest and the most popular data science community across the globe. In this kernel, I am using Kaggle Meta Data to explore the Data Science trends over the years. \n\n## 1. Linear Vs Logistic Regression\n\nLets look at the comparison of linear regression and logistic regression discussions on forums, kernels and replies on kaggle. "
"> - From the above graph, we can observe that there were always been **more discussions related to logistic regression** than linear regression. The generel trend is that number of discussions are increasing every month. \n> - One indication is that there are more number of classification problems than regression problems on Kaggle including the most popular **Titanic Survival Prediction competition**. This competition has most number of discussions and is one of the longest running compeition on Kaggle. There is a regression competition as well : House Prices advanced regression, but people more often start it after titanic only.     \n> - The number of logistic regression discussions on forums, kernel comments, and replies boomed to high numbers in October 2017 and March 2018. One of the reason is the the **Toxic Comments Classification Competition""** in which a number of authors shared excellent information related to classification models including logistic regression. \n\n## 2. The dominance of xgboost"
"> - Among both the popular techniques on Kaggle - xgboost and deeplearning, xgboost has remained on top because it is faster and requires **less computational infrastructure** than very complex and deeper neural networks. \n\n\n## 6. What Kagglers are using for Data Visualizations ?"
"> - Plotly has gained so much popularity since 2017 and is one of the most used data visualization library among the kernels. The second best is seaborn which is used extensively as well. Some of the high quality visualization kernels by kaggle grandmasters such as SRK and Anistropic are created with plotly. Personally, I am a big fan of plotly as well. :P\n\n## 7. Important Data Science Techniques "
"Let's again imagine we want to do\n- a rolling time series split\n- where we have a gap of 2 days between train and validation sets\n- and we make the maximum size of each train set to be 7 days\n\nHere we specify the number of splits, the maximum number of groups in each train set, and the maximum number of groups in each valdiation set (sklearn has this convention where they call it the ""test"" set; I preserve that in the variable names, but prefer to call it the validation set)."
"# With the Real Competition Data\n\nIn the real competition data, the number of datapoints per day (that is per ""group"") is not constant as it was in the spoofed data. We need to confirm that the time series split respects that there are different counts of samples in the the days.\n\nWe load the data and reduce memory footprint."
"Let's imagine that you want to fit on 15 days of data, leave a gap of 5 days between test and validation, and then validate on 5 days of data."
"![Alt Text](https://media.giphy.com/media/5bGYUuT3VEVLa/giphy.gif)\n\nBoom... there you go. Notice that the sizes of the train, gap, and valid sets respect the different number of samples per day. Now you can split the data on **number of days** in the validation set, **number of days** gap between, and **number of days** in the train set. You can see in the ""group"" bars that the days are different lengths."
## Visualize the CV Results
It seems that the model is not that sensitive to the regularization parameter. I will err on the side of caution and choose a lower one.
"# 2.2.1 Age\n- In this part of the data we have age groups of participants,\n- By looking at recent years trends and Data Science being labelled as one of the best jobs by some news sources I expect high number of young participants.\n- With the all hype around data and data related jobs I expect increased demand from employers and graduates with interest in  kaggle .\n- Let's see if data confirms my hypothesis..."
"- Please don't burn my plot to the ground because of the ""Pie Chart"", I know many people hate them, but I find them useful for small categories like this.\n- Ok with leaving pie chart hate behind let's check the values we got, it seems my first instinct was true; most of the participants are younger than 35 years old.\n- And we can also see that 18-21 age group almost made it to top (0.1% difference), so the trend for data related jobs still going strong among students.\n- By looking at the other side of the chart we see either really experienced users or people got interest in data science in late stages of their career, believe me it's never too late! It's nice to see couple hundred of them hanging around..."
# 2.2.1.1 Survey Duration vs. Age\n- Maybe it's not that important to check this relationship but I got the curiosity to see if there is a meaningful relation between these two...
"- It looks like younger people tend to finish survey a little bit faster, or maybe experienced people are paying attention to details a little bit more, anyways no need to hurry eh? :) Let's get on the next demographic feature...\n"
"# 2.2.2 Gender\n- While Data Science sector growing fast, the male dominance still stands, that would be worrying if the trend keeps going incoming years. For some of the people it might not sound like a obvious problem but incresed bias in data related works might affect the outcomes of machine learning/AI predictions in near future. Let's take a look how're things stand for this subject on  kaggle ..."
"- Uhmm doesn't look much better than last year if I recall correctly, they stand around same like last year (~78%, ~19%). Maybe looking at some other relations with this data might help us to get better insights.\n\n# 2.2.2.1 Age vs Gender"
"- Uhmm doesn't look much better than last year if I recall correctly, they stand around same like last year (~78%, ~19%). Maybe looking at some other relations with this data might help us to get better insights.\n\n# 2.2.2.1 Age vs Gender"
"- Another Pie'ish looking chart right? I'm sorry but this one looks little bit cooler, wouldn't you agree :) Anyways;\n- This one gives us a little bit better insight, by looking at age groups of dominant genders we can see that number of younger women are getting into data science/machine learning area a bit more than their counterparts.\n- 18-21 age group takes 5% more in their gender group than males, again 22-24 is 2% more than men followed by 25-29 which again little bit bigger fraction in women than men.\n- So by looking at percentages we would say younger women are getting into data science more and more, it's nice to have diversity among users in future!"
"- Another Pie'ish looking chart right? I'm sorry but this one looks little bit cooler, wouldn't you agree :) Anyways;\n- This one gives us a little bit better insight, by looking at age groups of dominant genders we can see that number of younger women are getting into data science/machine learning area a bit more than their counterparts.\n- 18-21 age group takes 5% more in their gender group than males, again 22-24 is 2% more than men followed by 25-29 which again little bit bigger fraction in women than men.\n- So by looking at percentages we would say younger women are getting into data science more and more, it's nice to have diversity among users in future!"
# 2.2.3 Countries\n- Another interesting demographic statistic for me is the country where participant currently residing. Again thanks to  kaggle 's ease to access and global availability we have chance to see how's the data science community develops around the world.\n- For me it's important to see equal growth around the world to build stronger community and wealth.\n- First let's take a look top countries with highest number of participants:
- Maybe plotting these values on World Map would be more visually satisfying and allows us to see wider perspective:
"- Looks like most of the participants are from Asia, led by India which keeps top spot in worldwide too.\n- In Africa; Nigeria is the flag bearer of the number of participants, followed by Egypt.\n- North America is led by USA which also holding global second place.\n- In South America Brazil has the biggest community then comes the Colombia.\n- Europe looks much more balanced than rest of the world if we don't count Austuralia (continent) :)"
# 2.2.3.1 Age vs. Country\n- Here we're going to check age group frequencies of the given countries.\n- It can give us insights about where's the data science community grows with lots of young people getting interested in the field.
"- I see that countries like Viet Nam, India, Indonesia, Bangladesh are having high number of young people are interested in field where bigger portion of their participants are younger than 25. \n- Meanwhile countries from Europe, Arabian Peninsula, North East Asia, North America are getting little older in average age distribution. I wouldn't say surely without related data but my inner instincts are telling me that there could be two/three reasons for this:\n * These countries adopted data science education earlier and already have experienced people.\n * People moving abroad to bigger companies clustered in some countries after getting experienced in their home countries.\n * Or some countries having really big population growth as well as interest in data science is rising among the studends.\n- Hmm which one it could be...\n\n"
"# 2.2.3.2 Gender vs. Country\n\n- While it's important to have diversity among the countries it's also important to have similar diversity between genders too, let's see if we can see different results in that regard..."
"- Hmm, looks like general trend with global gender distribution is about the same with countries too.\n- You can also notice that people who prefer not to share their gender also likely to don't share their location than rest too, I respect their privacy :)"
"# 2.2.4.1 Education vs Age\n\n- Let's take a closer look to these two degrees with age involved, if we can see something new."
"- Well it doesn't look like anything to me :) It looks like what it should be looking like, maybe at the age group of 18-21 having some participants master's degree plans, we would assume they are older than 20? Actually it depends on education starting age and education system for specific country so we can't be sure...\n- Anyways let's take look at bigger picture with heatmap:"
"- Well it doesn't look like anything to me :) It looks like what it should be looking like, maybe at the age group of 18-21 having some participants master's degree plans, we would assume they are older than 20? Actually it depends on education starting age and education system for specific country so we can't be sure...\n- Anyways let's take look at bigger picture with heatmap:"
"- We can observe that most popular education level starts with bachelor's degree at the age of 18 till 24,\n- Then master's degree taking the lead for big chunk of age groups untill age bin of 70+ where doctoral degree taking first place at that point.\n- **Special spot for the people who are attending/planning(next 2 years) Doctoral/Professional doctorate degree at the age of 18-21, taking 0.0053% of the age group, you are the real champions :))**"
# 2.2.4.2 Education vs Country
"- Highest master's degree ratio goes to France, which makes sense since 25-29 age group has the biggest percentage in France if you recall from Age chapter and Master's Degree was the most popular one among age 25-29 group globally...\n- Highest doctoral rate goes for Denmark, again their age group for country and education looks correlated. Also number of participants from that country should be taken into account.\n- In general these Age-Country-Education relation seems solid but still hypothetical, because there are some countries where this trend doesn't fit, like Uganda; this might due to low number of participants.\n- Maybe looking at countries where number of participants greater/less than a threshold would give us a cleaner view:"
"# 2.2.4.3 What About Courses?\n\n- As we all know the demand for a data scientist is on the rise, at least that's what media says :/\n- Students with analytical talents are getting more and more attracted to career in a data science field.\n- But usually formal education is not enough or person changing his field without getting another formal education about data science.\n- So you will need some specialized/techical education in order to get more data centric roles.\n- This brings us to the courses, selecting a right course might boost your career a lot!\n- Let's see what course platforms are popular among  kaggle  users."
- Looks interesting... We do have some leading courses like Coursera or  kaggle  but still we don't have a clear winner.\n- Let's see the effects of courses on other features like formal education.
# 2.2.4.3.1 Formal Education vs Courses
"- Well, we can see the importance of online learning/courses from the table above.\n- The higher the person gets formal education there is also higher chance for he/she took one or more courses for data science in past.\n- Of course there is time effect on the hypothesis above but still it shows you something for future: Learning never stops in data field!"
# 2.2.5.1 Role vs. Age
"- We see that most of the younger people are still studens, who would have expected that right!? Haha...\n- Anyways, we can also see that the more participant gets older the more he/she gets a specialized role.\n- You can see what I mean above by looking at ""Other"" role on X axis, percentages almost increasing consistently by age.\n- On the other hand we can also observe that data science is still growing field where fraction of data scientists are going down with the age..."
# 2.2.5.2 Role vs. Country
"- Here we can see roles are mainly occupied by ""Student"" for most countries. It was expected since most of the participants are students globally.\n- Some countries almost having 50% of their participants as students like China and Bangladesh, followed by India and Sri Lanka\n- These numbers are cool, but again we don't have balanced distribution for some countries so what I wonder is how are the roles are spread when we count total number of users from these countries.\n- This might give us insights about how are the data related roles developing across the world."
# 2.2.5.4 Education vs. Role
- Hmm this one seems interesting... It looks like to me some students couldn't decide between if they should choose Bachelor's Degree or Some College Study?
# 2.2.5.5 Courses vs. Role
"- Again like education, it doesn't matter what stage you're in your career, you still could use help of online courses.\n- We can clearly see decent amount of people who already has specialized titles attended an online course in past or still attending one."
# 2.3.1.1 Role vs. Experience
"- Participants who never written code mostly are consists of business analysts, product managers and some various other roles.\n- For under one year experience group, unemployed and students are making a big jump, and some data related roles are somewhat closely distributed like ML Engineer, Data Scientist, Data Engineer etc.\n- Next we have 1-3 years of experience, which is the biggest group. 44% of students are in this group as they represent biggest role group among total participants too.\n    - 1-3 years looks like a balanced group if we don't include the students. They take around ~20-25% of every single role group. There are some roles at higher end of this average like Unemployed and Data Analysts...\n- For the 3-10 years of experience range we can observe the beginning of drastic drops at some groups like Students, Unemployed, I believe that's the part where people start establishing their career path choices.\n- 10+ years of experience groups where we can observe lowest levels of unemployment and students.\n    - Again we can see that Data Scientist, Machine Learning Engineer, Data Analyst roles are not popular among really experienced participants. This migt be due to:\n        - Getting more managerial, advocacy roles with increased experience,\n        - Experienced developers who interested in data science but havent changed their career path yet...\n- Maybe merging this relation with one more layer would give us better insights:"
"# 2.3.2.1 Role vs. Language\n- Here we're going to inspect what languages has been popular with various job titles/roles, so we can get better insights about role demands..."
"- In general -as we seen previous section- Python is popular with most of the roles. Especially popular among Data Scientists, Machine Learning Engineers and Students.\n- Looks like R commonly used by Statisticians,\n- SQL has little bit more general usage but, we can see people who work in data pipelines like Database Engineers or Data Engineers it's much more common.\n- It seems MATLAB mainly used by Research Scientists."
# 2.3.2.2 Education vs. Language
"- We have similar look above, Python is most popular one with all education levels meanwhile SQL takes the second spot,\n- It seems R is getting more popular with higher levels of education, is it due to age of the language? Maybe...\n- I don't see significant differences for the rest of the programming languages maybe with exception: You can notice people where attended/planning doctorate have used MATLAB previously more common then other groups, this might be due to engineering related education in past...\n"
"# 2.3.3.1 Recommending the Language by It's User\n\nBefore we start;\n- Please don't forget this recommendation is for aspiring Data Scientists and it's recommended for their first choice of programming language.\n    - So we are not looking for general purpose language recommendation here,\n    - Nor recommendations for experienced data scientists directly.\n    - So for example, a person who's using C++ and not recommending that actually not recommending his own language for the beginner data scientists, not because he doesn't like it :)\n    - Also people who are using C++ are likely been using other languages like Python too, since it's multi choice question...\n- Well, with leaving the warnings behind let's take a look at the data itself :D"
"- Hmm 84% of the participants who uses Python regularly recommends Python for aspiring data scientist to learn first.\n- That makes Python is the most recommended language by it's regular users by far.\n- Closest one to this one is R, which is around 20%\n- Julia also an interesting one, I think high performance makes it recommendable by it's users. Maybe with some more library support we might see increased numbers incoming years?\n- We can already see Python is popular recommendation with all programming language users, so I just want to open new window for Python itself:\n    - Who doesn't recommend Python?\n    - What other languages are recommended by people who uses Python already?"
"- Hmm 84% of the participants who uses Python regularly recommends Python for aspiring data scientist to learn first.\n- That makes Python is the most recommended language by it's regular users by far.\n- Closest one to this one is R, which is around 20%\n- Julia also an interesting one, I think high performance makes it recommendable by it's users. Maybe with some more library support we might see increased numbers incoming years?\n- We can already see Python is popular recommendation with all programming language users, so I just want to open new window for Python itself:\n    - Who doesn't recommend Python?\n    - What other languages are recommended by people who uses Python already?"
- It looks like love for Julia and R strong against Python.\n- People who uses these two languages I mentioned above more likely to recommend you another beginner data science language than Python :)
- It looks like love for Julia and R strong against Python.\n- People who uses these two languages I mentioned above more likely to recommend you another beginner data science language than Python :)
"- It's really small part of the users recommend other languages in this group (remember they mostly recommended Python), people who uses Python but doesn't recommend python are tend to recommend SQL (around %5) and R (around 5%) for aspiring data scientists."
# 2.4.1.1 IDE vs Language\n\n- What we going to find about in next plot is whetever if there is a correlation with IDE and user's programming language preference:
"- Interesting... Even sometimes it's hard to find meaningful correlations between categorical datas we can see some noticable correlations between some categories.\n- The most obvious one is between R and RStudio,\n- With MATLAB and MATLAB IDE there we can notice similar correlation\n- There's noticeable correlation between Python and Jupyter Notebooks\n- Vim / Emacs has some degree of correlation with Bash \n- While looking at this table we can also see some kind of clusters between some languages like: Java, Javascript, C, C++, it shows when a participant uses one of these languages regularly there is a stronger chance than rest to use one of the other languages from this cluster..."
# 2.4.2 Notebooks
"- Colab and  kaggle  notebooks are the leaders here. \n- We also have significant amount of people who doesn't prefer notebooks at all too.\n- It comes back to topic we discussed before, it's important to decide what tool you going to use depending on your project before you start it..."
# 2.5.1 Visualization Libraries
# 2.5.1.1 Visualization Libraries and Languages
# 2.5.1.1 Visualization Libraries and Languages
"- I see some decent correlations here, most obvious one is Ggplot / ggplot2 and R correlation, well it was expected, right? I know some people are using R just for ggplot...\n- Seaborn & Matplotlib are closely related. Well... One built over the other one anyways...\n- With Python most correlated ones are these two I mentioned above, followed by Plotly which gets pretty popular lately.\n\n*For Robert we can suggest taking a closer look to Seaborn & Matplotlib, since he feels competent with Python. We still don't know if he is going to like data visualization though. But anyways, knowing little bit of visualization might help a lot in his other tasks too. I can also recommend Plotly to Robert if he's interested in visualization more, lately I'm seeing lots of cool plots created with it around here :)*"
# 2.5.1.2 Visualization Libraries and Roles
"- Doesn't matter your job/role is you probably used Matplotlib once or twice. The table above shows about 60% of the participants uses that library regularly.\n- Statisticians are using ggplot more often, we can observe the Statistician, R and ggplot relation here easily.\n- Machine Learning Engineers who uses Matplotlib regularly has bigger percentage than Data Scientists. I believe it's because of matplotlib's easy to use and it's way get results in a fast and simple fashion, meanwhile seaborn and plotly a little bit more popular for Data Scientists; probably for the people who needs to present their findings in better shape :)\n\n*Well Robert, I know you are confused and still not sure what data oriented career path you should choose in future but you must know about at least one visualization library. As you can see it doesn't matter if you going to be Data Analyst or Machine Learning Engineer, you need to visualize your findings and use/present them in your job. We already suggested which ones to start learning first but these findings are strengthening the idea.*"
# 2.5.2 Machine Learning Frameworks
"- Things are getting more complicated here :)\n- With ML libraries/frameworks are entering the picture, even the clear winner barely passes the 50% limit, which is Scikit-learn.\n- Since I use most of these on regular basis I might add some personal notes about them:\n    - Scikit-learn is a library provides many learning algorithms, setting standard for many other ML libraries. It's a great tool for prototyping, I forgot how many times I started prototyping a solution for a problem and at the end found out sklearn is more than enough...\n    - Tensorflow is a library for fast numerical computings which we need them for Deep Learning. It's has low level abilities to customize your workflow as you want.\n    - Keras is higher level API for Tensorflow, it wraps most common deep learning tasks aiming for easier usage. (You can still do low level stuff though using funtional or subclassing API's)\n    - Pytorch is somewhere between these two above, again it's an open source ML library for mainly deep learning tasks. People usually say it has more ""Pythonic"" style...\n     - You can think of PyTorch Lightning like a Keras for Pytorch, not exactly same but it's aiming for similar purposes.\n    - JAX is relatively a young framework, it's designed for high-performance numerical computing. People like to call it ""NumPy on steroids"" :) I find that description pretty accurate too :)\n    - Xgboost, LightGBM and Catboost are similar libraries, aiming to get maximum use of gradient boosting. They do well on tabular data usually...\n    - You can also see some libraries has more specific usage areas like Huggingface for Natural Language Processing tasks or Prophet for time series etc."
# 2.5.2.1 Machine Learning Libraries and Roles
"- Scikit-learn seems to have wide variety users, doesn't matter what role you have it's still has some use cases for you. It has pretty common usage with Data Scientists and Machine Learning Engineers.\n- When we switch to Deep Learning Frameworks like TensorFlow, Keras, PyTorch, we see Machine Learning Engineers are getting one step ahead, while most roles seen decrease from sklearn to these frameworks, one of the most limited decrease is from ML Engineers. Followed by Research Scientists and Sofware Engineers (Interesting one)...\n- Gradient boosting algorithms are more common than other role groups among Data Scientists.\n- It seems statisticians are one of the least interested groups in machine learning (after Developer Relations), I was expecting little big jump at Tidymodels for this group but it seems even ""None"" group had sharper spike.\n\n*So Robert... I heard you getting interested specially in Data Scientist and Machine Learning Engineer roles. You better start learning Scikit-learn then, as you can see it's really popular and in demand. Also, like I mentioned above it has great workflow standard, so after getting familiar with it you can learn other libraries much faster. For example some popular libraries like Xgboost, LightGBM already having sklearn interface too, so after getting familiar with sklearn you can start using these too in no time! Good lad! If you want to advance further I'd suggest taking a look at the TensorFlow and especially Keras, because of it's higher level API and having some similarities with sklearn workflow, it would be good starting point. If I recall correctly Keras has sklearn wrapper too. To get little bit of theory and practical knowledge you can get help of some books. For Scikit-learn I'd recommend ""Introduction to Machine Learning with Python: A Guide for Data Scientists"" from Andreas C. M√ºller, for TensorFlow and Keras you can take a look at ""Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow"" from Aur√©lien G√©ron, in that book you can get some solid knowledge for sklearn too. If you want something Keras specific you can read ""Deep Learning with Python"" from Fran√ßois Chollet. Getting familiar with these going to take some time but after that you can start learning PyTorch if you like to, it's a great framework and lately on the rise, many research codes are published in PyTorch format so it's nice to know. Hey Robert! Are you still there?!*"
"# 2.5.2.2 Machine Learning Libraries and Visualization\n- So, couple sections above we have seen visualization libraries are pretty popular among Machine Learning practicioners, let's see how these relations connects with ML frameworks themselves."
"- Most popular visualization tools (Matplotlib, Seaborn) are highly related with Scikit-learn, followed by Keras/TF and XGboost.\n- ggplot and Shiny are more correlated with Tidymodels and Caret. Again strong R connections :)\n- For the rest of visualization libraries I couldn't find meaningful correlations between them and ML libraries, so let's check relations between ML libraries themselves next.\n- You can notice decent correlation between sklearn and gradient boosting algorithms and TF/Keras, so in some ways that confirms our suggestion to learn Scikit-learn first.\n- For PyTorch strongest ones are Pytorch Lightning (Surprise!) and Huggingface, I think you can use Tensorflow with Huggingface too but PyTorch seems more default setting for it. Again TF/Keras has some correlation with PyTorch too since they both serve for Deep Learning...\n\n*You see Robert? Now we have stronger foundations for the suggestions about your data science roadmap. As you can see the higher correlated libraries are creating a cluster for most of the data scientists: Matplotlib/Seaborn usage correlated with Scikit-learn, Scikitlearn usage correlated with gradient boosting algorithms and these are somewhat correlated with deep learning frameworks like TF/Keras. With these small steps we start getting a broad roadmap for what you need to learn already, how you feel about that?*"
# 2.5.3 Machine Learning Algorithms
"- Regression tasks (either linear or logistic for classification) are most popular ML algorithms by participants.\n- Tree based algorithms taking second place, again pretty common among  kaggle  users.\n- Gradient boosting and CNN's (mostly for computer vision tasks) following these two.\n- It's interesting to see DNN's are less popular than CNN's, probably classical supervised approaches (sklearn and gradient boosting stuff) are still preferable for tabular data...\n- Then comes Bayesian Approaches, RNN's and Transformers... Last two are usually used for Natural Language Processing problems but lately Transformers are getting some hype around computer vision tasks too."
# 2.5.3.1 Machine Learning Algorithms and Roles
"- The heatmap above you can see which role regularly interacts with a certain algorithm. \n- Again we can confirm that Data Scientist is a broad term, this role uses wide range of algorithms regularly.\n- Machine Learning Engineer role uses neural networks more than rest, this includes:\n    - DNN's\n    - CNN's\n    - RNN's\n    - Transformers\n    - GAN's\n- These above are more specialized algorithms for tasks like computer vision, natural language processing, autoencoders etc.\n- Research scientists also having similar algorithm usage patterns with machine learning engineers.\n\n*Hey there Robert! You probably heard enough but I'm going to say again, learn Scikit-learn! You can use most of these algorithms within the sklearn package, even some DNN's like Perceptron or MLP's and as you can see you are going to use some of these algorithms a lot and it doesn't matter what specific title you want to get in future. You were interested in Data Scientist or Machine Learning Engineer roles right? I'd definitely start learning then... I can show it to you in another way, look:*"
"- The heatmap above you can see which role regularly interacts with a certain algorithm. \n- Again we can confirm that Data Scientist is a broad term, this role uses wide range of algorithms regularly.\n- Machine Learning Engineer role uses neural networks more than rest, this includes:\n    - DNN's\n    - CNN's\n    - RNN's\n    - Transformers\n    - GAN's\n- These above are more specialized algorithms for tasks like computer vision, natural language processing, autoencoders etc.\n- Research scientists also having similar algorithm usage patterns with machine learning engineers.\n\n*Hey there Robert! You probably heard enough but I'm going to say again, learn Scikit-learn! You can use most of these algorithms within the sklearn package, even some DNN's like Perceptron or MLP's and as you can see you are going to use some of these algorithms a lot and it doesn't matter what specific title you want to get in future. You were interested in Data Scientist or Machine Learning Engineer roles right? I'd definitely start learning then... I can show it to you in another way, look:*"
As you can see there is a strong path to the Data Scientist and Machine Learning Engineer roles from classical libraries.
# 2.5.3.2 Machine Learning Algorithms and Libraries
"- The plot above confirms that Scikit-learn is correlated with most of the machine learning algorithms.\n- For neural networks TF/Keras, PyTorch is getting more popular.\n- And lastly as expected from previous plots Transformer Networks are highly correlated with Huggingface"
# 2.5.4.1 ML Experience Distribution by Role
"*You see Robert? No need to worry about getting confused with data science concepts at first sight, as you can see most of the  kaggle  users are either under 1 years of ML experience or has no experience at all. It's still young field and there will be many career opportunities if you play your cards right and in time!*"
# 2.5.4.2 ML and Programmin Experience
"- We can see solid linear correlation between years of programming experience and years of ML experience.\n- Usually ML experience of participants are couple years less than their programming experience, which makes sense...\n- I noticed there are **6 people with more than 20 years of Machine Learning experience but less than 1 years of coding experience**. Well, I mean it's important to learn theory behind the ML algorithms before start coding them, right? Haha...\n\n*It's a good thing you start learning how to code with Python before you start doing Machine Learning stuff. As I mentioned above usually people are getting familiar with a programming language before they start learning the algorithms etc. You done well there Robert, good lad!*"
# 2.6.1 Work Activities\n\n- Let's stary by looking what's your daily responsibilities going to be like in future job...
"- Seems like analyzing and understanding the data for stakeholders is still most common task for data related jobs.\n- Second most popular activity seems intriguing, looks like industry is still trying to find use of machine learning techniques to take advantage of. So I believe there will be new areas to explore and conquer :) \n- If we merge these two above we can see that we're still in early parts of machine learning part of the data science, lot to discover, lot to understand...\n- Third one more relevant to data itself, creating and storing useful information is really important.\n- Next two seems like using ML techniques more practically, where participant tries to build/upgrade machine learning methods to improve his workflow.\n- Doing research probably important for research scientists or maybe machine learning engineers, we'll find about it soon.\n- And lastly there is decent portion of participants who doesn't find these activities important for his role..."
# 2.6.1.1 Roles and Responsibilities
"- This one above seems interesting... We can learn a lot about roles and their responsibilities.\n- Analyzing and understanding the data is pretty important for all roles, maybe with the exceptions like other roles or software engineers...\n    - For Business and Data analysts learning the data and influence business decisions are most important thing. (Well, surprise...)\n    - Again for data scientists exploring the data seems pretty important but as you can see it's not the only responsibility they usually take :)\n    - We can also see that understanding the data is important for managerial roles too...\n- Again an obvious one, building and running data infrastructure is pretty common for data and database engineers.\n- Building machine learning prototypes for new areas to exploit seems pretty common among the data&research scientists and ML engineers. An interesting one for me is  the product manager one which more than one third of product managers are prototyping machine learning to new areas.\n- Building and running ML services are less common than I expected, mostly taken by machine learning engineers and data scientists.\n- Experimenting or improving models taking important part of ml engineers and data&research scientists daily duties.\n- For research, research scientists are coming first (Duh!) followed by ml engineers. \n\n*Well Robert, whatever you do for work, to find a place in data field you must understand and process the data. As I told you in previous chapter best way to do this is doing exploratory data analysis and one of the best tools going to help you with this is visualization. That's why it's recommended to learn matplotlib/seaborn for you. When you understand the data you can move to the next steps like asking the questions: ""What can I do with this data, how can I use this information to create something useful, can I train a model to predict stuff with this data"". If you start asking these kind of questions you're on the right track, but again you need to get hang of the data first, then the questions are getting more specialized. After exploring, to answer some of these questions you going to use machine learning. It's going to take lot's of prototyping and as I mentioned in previous chapter, Sckit-learn going to help you a lot with this. You don't want to create complex models before you reach satisfactory results with more traditional models. If you manage to do this then you'll get to next step where you need to improve existing model, maybe doing some research to learn about state of the art stuff then deploy the model etc. Maybe you are little bit unexperienced for some of these but it's good to know and by the way please stop asking me about the wage, we'll get there...*"
# 2.6.1.2 ML Experience and Responsibilities
"- Well... With increasing the years of ml experience almost all responsibilities are getting greater. I thought things will get easier in future, oh... :(\n- So the chart above confirms our assumption that the duties you cover/work increases with your knowledge.\n- It also confirms that you can get new skills while working too."
# 2.6.2.1 Industry and Roles
"- The table above we can see distribution of roles by industry.\n- Most of the roles are employed by computers/technology and acadamics/education industries as we seen from previous plot too. But let's take short look on rest.\n- It seems accounting and finance industry employs more analyst positions.\n- For government and public services, statisticians and database engineers are more popular than others.\n- For some fields I hoped for little bit higher percentages and I believe they could take advantage of data much more:\n    - Insurance / Risk Assesment: Where you could train model with lesser human bias.\n    - Medical / Pharmaceutical: Especially computer vision could be life saver (literally) for early and accurate detections.\n    - Retail / Online sales: Better customer segmentation or maybe stock estimation.\n    - Shipping / Transportation: Where companies can optimize their workflow and increase their efficiency.\n    \n*Things looking good for you Robert, it might your options are limited at first sight but if you look closely future is promising. You might be working for tech startup or academia(if you want to continue after master's) and advance your career there and you might want to move bigger tech companies in future. Also you might get hired by a small company or small department of data science in bigger company. Options are countless as long as you add some skills that make you one step ahead of your competitors. And we discussed what you should do for that in previous chapter right? If you forgot you can just go back there and start reading again. I'll be waiting for you in next part...*\n"
# 2.7.1.1 Country of Residence and Yearly Income
"- Before I plot the results I noticed high standard deviation for these variables so I decided to use median instead of mean. I believe this would give more robust results.\n- As you can see from the table above there are huge gaps between countries, but again every country should be analysed by it's economic standards.\n- The chart looks better than I expected at first, if you are little bit familiar with world economies and demographics you can notice there are similarities between them and this chart.\n- You can see some countries are clustering toghether and they usually have similar economic parameters like money zone, gdp/c etc.\n- I'm not going to analyse median income country by country since it would take another chapter itself, but feel free to do so;\n\n*And lucky you Robert, you already residing in top pretty high median income country!*"
# 2.7.1.2 Education and Yearly Income
- Although there are some outliers we can see a clear pattern here: With increasing level of education median income by year increases too.
# 2.7.1.3 Role and Yearly Income
"- Wait what?! Machine Learning Engineer has one of the lowest median income?\n- I thought they earning little bit more than data scientists generally. There must be something wrong there, we need to dig that; but for now let's look at the rest...\n- Seems like managerial roles has the highest income although having some large gaps between quantiles.\n- Some roles like statisticians or data analysts are earning a little bit less than general averages.\n\n*Hey Robert are you there? Sorry what?! You changed your mind over machine learning engineer after this chart? Hold on mate, as I said there must be something affecting this statistic, let me show you; hey listen listen... Go check the next chart!*"
"- Wait what?! Machine Learning Engineer has one of the lowest median income?\n- I thought they earning little bit more than data scientists generally. There must be something wrong there, we need to dig that; but for now let's look at the rest...\n- Seems like managerial roles has the highest income although having some large gaps between quantiles.\n- Some roles like statisticians or data analysts are earning a little bit less than general averages.\n\n*Hey Robert are you there? Sorry what?! You changed your mind over machine learning engineer after this chart? Hold on mate, as I said there must be something affecting this statistic, let me show you; hey listen listen... Go check the next chart!*"
"- That's a relief, I think we found the reason behind the low median income for machine learning engineers globally.\n- If you look at the countries where having the highest number of machine learning engineers you see most of them are placed lower ranks on the list in terms of median USD income.\n- So high number of machine learning engineers from these countries are lowering the global median income statistics.\n- You can see two exceptions on this list: USA and Japan, when you look closely their income values for data scientists and machine learning engineers you can see they're almost identical, meanwhile in countries like India data scientists almost making two times more than machine learning engineers, interesting...\n- Is machine learning engineer title over-hyped in some countries and demand is low? That might be the case...\n\n*You see Robert, there's no need to worry about that, you are interested in machine learning engineer role again? Ah nice, good lad!*"
# 2.7.1.4 Industry and Yearly Income
"- A saddening chart... While Military/Security/Defense Industry giving highest median wages the lowest median one is Academics/Education (If we don't count non-profit ones), I mean security is important in it's own way but I believe Academics should get more love for the future.\n    - Personally I check high paying countries individually and the trend is similar, academics are at bottom in yearly compensation charts...\n- The rest of the Industries seems balanced in their own clusters, especially when you include the country effect into the equation. But I'm not going to inspect all of these: Firsly I'm not that familiar with all of these industries to make assumptions and secondly that would take many charts to analyze.\n- So I'd like to contine by taking picture of the industry in regards to mean yearly compensation, so let's continue with that."
"- A saddening chart... While Military/Security/Defense Industry giving highest median wages the lowest median one is Academics/Education (If we don't count non-profit ones), I mean security is important in it's own way but I believe Academics should get more love for the future.\n    - Personally I check high paying countries individually and the trend is similar, academics are at bottom in yearly compensation charts...\n- The rest of the Industries seems balanced in their own clusters, especially when you include the country effect into the equation. But I'm not going to inspect all of these: Firsly I'm not that familiar with all of these industries to make assumptions and secondly that would take many charts to analyze.\n- So I'd like to contine by taking picture of the industry in regards to mean yearly compensation, so let's continue with that."
"- The trend seems obvious, the bigger the company gets the more wage they pay.\n- You could interpret the size of a company with level of corporateness where corporate firms having higher benefits, more standardized wages etc."
"- The trend seems obvious, the bigger the company gets the more wage they pay.\n- You could interpret the size of a company with level of corporateness where corporate firms having higher benefits, more standardized wages etc."
"- Again a clear trend...\n- So, the companies where incorporated ML methods into their business are paying more for the data workers. This could be due to some reasons, like:\n    - The company understands the value of data and willing to pay even more for right person.\n    - After using ML methods for a while the requirements are getting greater and complex so companies trying to get more skillful employees.\n    - The added-value generated by ML is significant so data department getting higher budgets inside the company.\n    - The demand is high for people who can work with ML stuff but the qualified workforce is low.\n    "
"- Again a clear trend...\n- So, the companies where incorporated ML methods into their business are paying more for the data workers. This could be due to some reasons, like:\n    - The company understands the value of data and willing to pay even more for right person.\n    - After using ML methods for a while the requirements are getting greater and complex so companies trying to get more skillful employees.\n    - The added-value generated by ML is significant so data department getting higher budgets inside the company.\n    - The demand is high for people who can work with ML stuff but the qualified workforce is low.\n    "
- This one confirms our assumptions in previous charts:\n    - The need for people who can work with data increases how much the company is willing to pay for it's employees.
"- Here I'm going to reduce dimension of the data and try to find meaningful clusters, it'a unsupervised learning method might help us to understand more about the data itself..."
"- I'm going to use some of the data we analysed visually before while excluding the ones we didn't analyse properly, but firstly we going to reduce dimension of that data to 2D space using ""Principal Component Analysis"" so we can plot it easily and make it human readable :)\n- Then we're going to try various number of clusters to see intertia values (basically mean distance of instances to the cluster centroid), of course increasing k naturally decreases the inertia, so we looking for the ""Elbow"" where inertia decrease slows down sharply."
"- I'm going to use some of the data we analysed visually before while excluding the ones we didn't analyse properly, but firstly we going to reduce dimension of that data to 2D space using ""Principal Component Analysis"" so we can plot it easily and make it human readable :)\n- Then we're going to try various number of clusters to see intertia values (basically mean distance of instances to the cluster centroid), of course increasing k naturally decreases the inertia, so we looking for the ""Elbow"" where inertia decrease slows down sharply."
- So our k can be 2 or 3 but 3 seems little bit healthier. That means we're going to cluster all instances(participants) into 3 groups.\n- Let's plot that to see if it makes any sense...
- So our k can be 2 or 3 but 3 seems little bit healthier. That means we're going to cluster all instances(participants) into 3 groups.\n- Let's plot that to see if it makes any sense...
"- Ok... Not great, not terrible, I can see one cluster diverges a little bit more than others meanwhile cluster 0 and 2 is just one big blob :)\n- When we plot same data with coloring instances based on their role we can see two roles are fitting pretty good on cluster 1 on left:\n    - Students and Currently Unemployed ones are mainly sitting in the cluster 1,\n    - Other clusters are just chaotic :) It might be due to data itself or 2D might not be enough to plot that for human eyes...\n    \n*Hey Robert! You see yourself there? Wave at us! Here you are! So you are fine fit for cluster 1, this is your new team, meet your friends! Cluster 1 mainly consists of students and unemployed people so cluster 1 is for people who starting their data science career recently. If you want to compare yourself with other people, using this tecnique can help you with reducing your scope so you can make fine grained analyses. Anyways as you can see you are not alone at all and with the takeaways from this notebook you can make rapid advances to other clusters you want to be in :)*"
- Since we reduced the dimensions let's check what's the percentage of variance explained by principal components:
- Oh this explains a lot... Well I mean actually it doesn't explain the variance a lot :) We used first two principal components to visualize the and it only explains 40% of the variance.\n- Let's take a look one more thing about dimension reduction then...
- Oh this explains a lot... Well I mean actually it doesn't explain the variance a lot :) We used first two principal components to visualize the and it only explains 40% of the variance.\n- Let's take a look one more thing about dimension reduction then...
"- Well well, take a look at this one...\n- Looks like 4th principal component represents the Role feature really well, look how clustered the instances are! They look like cute rainbows :)\n\n*Isn't it fun to find hidden stuff inside the data Robert? You having fun there? As I told you before, with little bit knowledge of Sklearn and visualization tools you can find new ways to gain insights. Wait you already clustered Iris dataset? Nice one Robert! It's a great choice to learn about clustering, well done!*"
"# Something Personal...\n\nUntill now I used the actual data itself. I wonder how's the text written by me along this notebook, what points I kept dwell on, what words I over used :/ When I turn back and check the notebook I found out I wrote loads of things, I just crawled them and extracted it to a single .txt file, I think its worth to visualize it :) Let's see:"
"Ok, maybe I over used some words like ""see, one etc."" usually they coming from the parts where I point out a plot or finding like I just did above (facepalm)... If we count these as stopwords then we can see some more important words like:\n- ""Data"",\n- ""Data Science"",\n- ""Data Scientist"",\n- ""Machine Learning"" or ""ML"",\n- ""Machine Learning Engineer"",\n- And many more related stuff...\n\nAnd of course the **Robert**, we couldn't done it without him :)\n\nAnyways it's time to wrap up things..."
## Seaborn Pairplot
### Seaborn PairGrid
### Seaborn PairGrid
"\nWow this figure has a lot going on! But thats okay, we only need to look at it peice by piece do derive some meaningful understanding.\n\nI like the PairGrid better because it provides more customizable options compared to the Pairplot. As you can tell I can have two different kinds of graphs on the upper and lower halves of the grid. In the PairGrid above we see 3 types of graphs:\n\n> 1. Regression scatter plot in the upper right\n> 2. Density Plot down the diagonal\n> 3. Bivariate Denisty plot with contour levels \n\nIn the bottow row, we see 6 empty graphs. The bivariate desity plots cannot plot due to the different scale of values. Win Percentage values range [0,1] while the independent vairables (HP, Attack,...) range from [0,200+].\n\nThe diagonal shows the density plots. For most of the independent variables look realtively normal distribution with a right skew. This means a majority of the pokemon stats are on average higher than the median value for the set. However, the density for the win percentage is different. Comparing the density plot with the frequency plot in the pairplot for 'Win Percentage' we see a more uniformly distribution of the rate at which pokemon win with a slight decrease of the frequency at higher levels. \n\nThe upper right section is the most easy to understand and probably the most useful for our analysis. These are regression plots with a line of best fit. Esentially the slope of the line y=mx+b, where m is the slope is the correlation value between the two variables. Thus we would expect to see a simialr pattern if we were to build a heat map or a correlation table. What I am most interested in is the relationship between each independent variable and the dependent variable (Win Percentage). The greater the slope, m, the more correlated the values are in determining the liklihood of winning. Just by 'eye balling' it (not the most mathmematically correct method), but it appears that **speed** and **attack** have the largest relationship to winning. Lets take a closer look into these two plots given below. \n\nWe begin by replicating the regplot to that we can see it better. To dive deeper, lets take a look at if there any trends when we further break down the data into 'Type 1'.\n\n## Correlation Table"
## Random Forest
## XGBoost
We shall define a simple function which will plot the augmented images for us. 
## **Original Images**\nFirst let us plot the original images so that the augmentations become more clear to us. After this one by one we will implement different augmentations.
**In this section I will demonstrate 15 Torchvision Transforms with demos namely:**\n1. [Center Crop](#2.1)\n2. [Random Crop](#2.2)\n3. [Random Resized Crop](#2.3)\n4. [Color Jitter](#2.4)\n5. [Pad](#2.5)\n6. [Random Affine](#2.6)\n7. [Random Horizontal Flip](#2.7)\n8. [Random Vertical Flip](#2.8)\n9. [Random Perspective](#2.9)\n10. [Random Rotation](#2.10)\n11. [Random Invert](#2.11)\n12. [Random Posterize](#2.12)\n13. [Random Solarize](#2.13)\n14. [Random Autocontrast](#2.14)\n15. [Random Equalize](#2.15)
## **PyTorch Dataset Class**
# Import Libraries 
# Load the Dataset and Add headers
## Ratings
Most of the people has given the rating of 5
"# Home Credit Default Risk - Exploration + Baseline Model\n\nMany people struggle to get loans due to insufficient or non-existent credit histories. And, unfortunately, this population is often taken advantage of by untrustworthy lenders. Home Credit strives to broaden financial inclusion for the unbanked population by providing a positive and safe borrowing experience. In order to make sure this underserved population has a positive loan experience, Home Credit makes use of a variety of alternative data--including telco and transactional information--to predict their clients' repayment abilities.\n\nWhile Home Credit is currently using various statistical and machine learning methods to make these predictions, they're challenging Kagglers to help them unlock the full potential of their data. Doing so will ensure that clients capable of repayment are not rejected and that loans are given with a principal, maturity, and repayment calendar that will empower their clients to be successful.\n\nThis is a simple notebook on exploration and baseline model of home credit default risk data \n\n**Contents**   \n[1. Dataset Preparation](#1)    \n[2. Exploration - Applications Train](#2)  \n¬†¬†¬†¬† [2.1 Snapshot - Application Train](#2.1)    \n¬†¬†¬†¬† [2.2 Distribution of Target Variable](#2.2)    \n¬†¬†¬†¬† [2.3 Applicant's Gender Type](#2.3)    \n¬†¬†¬†¬† [2.4 Family Status of Applicants who takes the loan](#2.4)  \n¬†¬†¬†¬† [2.5 Does applicants own Real Estate or Car](#2.5)    \n¬†¬†¬†¬† [2.6 Suite Type and Income Type of Applicants](#2.6)   \n¬†¬†¬†¬† [2.7 Applicants Contract Type](#2.7)   \n¬†¬†¬†¬† [2.8 Education Type and Occupation Type](#2.8)   \n¬†¬†¬†¬† [2.9 Organization Type and Occupation Type](#2.9)   \n¬†¬†¬†¬† [2.10 Walls Material, Foundation and House Type](#2.10)   \n¬†¬†¬†¬† [2.11 Amount Credit Distribution](#2.11)    \n¬†¬†¬†¬† [2.12 Amount Annuity Distribution - Distribution](#2.12)  \n¬†¬†¬†¬† [2.13 Amount Goods Price - Distribution](#2.13)   \n¬†¬†¬†¬† [2.14 Amount Region Population Relative](#2.14)    \n¬†¬†¬†¬† [2.15 Days Birth - Distribution](#2.15)   \n¬†¬†¬†¬† [2.16 Days Employed - Distribution](#2.16)    \n¬†¬†¬†¬† [2.17 Distribution of Num Days Registration](#2.17)  \n¬†¬†¬†¬† [2.18 Applicants Number of Family Members](#2.18)  \n¬†¬†¬†¬† [2.19 Applicants Number of Children](#2.19)  \n[3. Exploration - Bureau Data](#3)  \n¬†¬†¬†¬† [3.1 Snapshot - Bureau Data](#3)    \n[4. Exploration - Bureau Balance Data](#4)  \n¬†¬†¬†¬† [4.1 Snapshot - Bureau Balance Data](#3)     \n[5. Exploration - Credit Card Balance Data](#5)   \n¬†¬†¬†¬† [5.1 Snapshot - Credit Card Balance Data](#3)   \n[6. Exploration - POS Cash Balance Data](#6)   \n¬†¬†¬†¬† [6.1 Snapshot - POS Cash Balance Data](#3)   \n[7. Exploration - Previous Application Data](#7)   \n¬†¬†¬†¬† [7.1 Snapshot - Previous Application Data](#7.1)  \n¬†¬†¬†¬† [7.2 Contract Status Distribution - Previous Applications](#7.2)  \n¬†¬†¬†¬† [7.3 Suite Type Distribution - Previous Application](#7.3)    \n¬†¬†¬†¬† [7.4 Client Type Distribution  - Previous Application](#7.4)    \n¬†¬†¬†¬† [7.5 Channel Type Distribution - Previous Applications](#7.5)  \n[8. Exploration - Installation Payments](#8)  \n¬†¬†¬†¬† [8.1 Snapshot of Installation Payments](#3)  \n[9. Baseline Model](#9)  \n¬†¬†¬†¬† [9.1 Dataset Preparation](#9.1)  \n¬†¬†¬†¬† [9.2 Handelling Categorical Features](#9.2)     \n¬†¬†¬†¬† [9.3 Create Flat Dataset](#9.3)     \n¬†¬†¬†¬† [9.4 Validation Sets Preparation](#9.4)    \n¬†¬†¬†¬† [9.5 Model Fitting](#9.5)    \n¬†¬†¬†¬† [9.6 Feature Importance](#9.6)    \n¬†¬†¬†¬† [9.7 Prediction](#9.7)   \n\n\n\n## 1. Dataset Preparation "
## 2. Exploration of Applications Data \n\n### 2.1 Snapshot of Application Train\n\nApplication data consists of static data for all applications and every row represents one loan.
"> Married people have applied for a larger number of loan applications about 196K, However, people having Civil Marriage has the highest percentage (about 10%) of loan problems and challenges. \n\n### 2.5. Does applicants own Real Estate or Car ?"
"> About 70% of the applicants own Real Estate, while only 34% of applicants own Car who had applied for the loan in the past years. However, a higher percentage of people having payment difficulties was observed with applicants which did not owned Car or which did not owned Real Estate. \n\n### 2.6 Suite Type and Income Type of Applicants "
"> We see that Applicants having Income Types : Maternity Leaves and UnEmployed has the highest percentage (about 40% and 36% approx) of Target = 1 ie. having more payment problems, while Pensioners have the least (about 5.3%). \n\n### 2.7. Applicant's Contract Type"
> Cash loans with about 278K loans contributes to a majorty of total lonas in this dataset. Revolving loans has significantly lesser number equal to about 29K as compared to Cash loans. \n\n### 2.8 Education Type and Housing Type 
### 2.11. Distribution of Amount Credit 
### 2.12 Distribution of Amount AMT_ANNUITY 
### 2.12 Distribution of Amount AMT_ANNUITY 
### 2.13 Distribution of Amount AMT_GOODS_PRICE 
### 2.13 Distribution of Amount AMT_GOODS_PRICE 
### 2.14 Distribution of Amount REGION_POPULATION_RELATIVE 
### 2.14 Distribution of Amount REGION_POPULATION_RELATIVE 
### 2.15 Distribution of Amount DAYS_BIRTH 
### 2.15 Distribution of Amount DAYS_BIRTH 
### 2.16 Distribution of Amount DAYS_EMPLOYED 
### 2.16 Distribution of Amount DAYS_EMPLOYED 
### 2.17 Distribution of Number of Days for Registration
### 2.17 Distribution of Number of Days for Registration
### 2.18 How many Family Members does the applicants has 
### 2.18 How many Family Members does the applicants has 
> Most of the applicants who applied for loan had 2 family members in total\n\n###  2.19 How many Children does the applicants have 
> Most of the applicants who applied for loan had 2 family members in total\n\n###  2.19 How many Children does the applicants have 
"> A large majority of applicants did not had children when they applied for loan\n\n## 3. Exploration of Bureau Data\n\nAll client's previous credits provided by other financial institutions that were reported to Credit Bureau (for clients who have a loan in our sample). For every loan in our sample, there are as many rows as number of credits the client had in Credit Bureau before the application date.\n\n### 3.1 Snapshot of Bureau Data"
### 7.2 Contract Status Distribution in Previously Filed Applications
"> - A large number of people (about 62%) had their previous applications approved, while about 19% of them had cancelled and other 17% were resued. \n\n### 7.3 Suite Type Distribution of Previous Applications"
"> - A large number of people (about 62%) had their previous applications approved, while about 19% of them had cancelled and other 17% were resued. \n\n### 7.3 Suite Type Distribution of Previous Applications"
> - A majority of applicants had previous applications having Unaccompanied Suite Type (about 60%) followed by Family related suite type (about 25%)\n\n### 7.4 Client Type of Previous Applications
> - A majority of applicants had previous applications having Unaccompanied Suite Type (about 60%) followed by Family related suite type (about 25%)\n\n### 7.4 Client Type of Previous Applications
"> - About 74% of the previous applications were Repeater Clients, while only 18% are new. About 8% are refreshed. \n\n### 7.5 Channel Type - Previous Applications "
"> - About 74% of the previous applications were Repeater Clients, while only 18% are new. About 8% are refreshed. \n\n### 7.5 Channel Type - Previous Applications "
## 8. Exploration of Installation Payments \n### 8.1 Snapshot of Installation Payments 
- Nothing much for the skewness. Quite a normal like distribution for the numerical features.
"- Based on the  matrix, we can observe weak level correlation between the numerical features and the target variable\n- Oldpeak (depression related number) has a positive correlation with the heart disease.\n- Maximum heart rate has negative correlation with the heart disease.\n- interestingly cholesterol has negative correlation with the heart disease.\n"
\nFeature Importance\n\nTable of Contents
\nModel Comparison\n\nTable of Contents
"# Overall IDE Popularity\nTakeaways:\n* Jupyter is a big favorite, with over 70% of repondents using it.\n* RStudio leads up the rest of the pack. It's safe to say that respondents who code esclusively in R would prefer RStudio as their main IDE.\n* Notepad++ is the top pure text editor, above Sublime.\n* Some reponded with *None*.  Is it safe to assume that these people don't code?"
"# Kaggle vs.  StackOverflow IDE Use\nThe 2018 Stack Overflow survey posed a very similar question we can compare the results with to see how kaggle compares to the stack overflow community regarding IDE use. This data was taken from the [stackoverflow developer survey results](https://insights.stackoverflow.com/survey/2018/).\n\nSome observations about the comparison:\n- Notepad ++ is popular for both communities. 40.97% of Kaggle respondents and 34.2% of Stackoverflow respondents have used it.\n- Jupyter/IPython is much more popular among kagglers, as expected. But the difference is staggering, with 73.29% of kaggle respondents using it and only 7.4% of stackoverflow respondents saying they use it.\n- Visual Studio and Visual Studio Code are the most popular of the stackoverflow developers, but relatively less popular for kagglers."
"# Kaggle vs.  StackOverflow IDE Use\nThe 2018 Stack Overflow survey posed a very similar question we can compare the results with to see how kaggle compares to the stack overflow community regarding IDE use. This data was taken from the [stackoverflow developer survey results](https://insights.stackoverflow.com/survey/2018/).\n\nSome observations about the comparison:\n- Notepad ++ is popular for both communities. 40.97% of Kaggle respondents and 34.2% of Stackoverflow respondents have used it.\n- Jupyter/IPython is much more popular among kagglers, as expected. But the difference is staggering, with 73.29% of kaggle respondents using it and only 7.4% of stackoverflow respondents saying they use it.\n- Visual Studio and Visual Studio Code are the most popular of the stackoverflow developers, but relatively less popular for kagglers."
"# Overlap Between IDE Useage\nSeeing which IDEs are popular is one thing, but more importantly we want to know which software users tend to use together. We can start by making venn-diagrams with the most popular IDES. These are interesting because we can not only see how many people have used the IDE, but also comminality in IDE use."
"# Overlap Between IDE Useage\nSeeing which IDEs are popular is one thing, but more importantly we want to know which software users tend to use together. We can start by making venn-diagrams with the most popular IDES. These are interesting because we can not only see how many people have used the IDE, but also comminality in IDE use."
"# Students Love Jupyter!\n\nNearly half of the Kaggle survey respondents answered that they were ""students"". Since jupyter has a large use within academia and teching, it's interesting to see how the venn diagram differs between students and non-student respondents. While Jupyter is still very popular amonst both groups, the overlap between jupyter and the next top two popular IDEs (RStudio and Notepad++) varies much less for students. For non-students there are some exclusive users of other development environments like RStudio and Notepad++, but also every student that uses these IDEs  has also had some interaction with Jupyter."
"# Students Love Jupyter!\n\nNearly half of the Kaggle survey respondents answered that they were ""students"". Since jupyter has a large use within academia and teching, it's interesting to see how the venn diagram differs between students and non-student respondents. While Jupyter is still very popular amonst both groups, the overlap between jupyter and the next top two popular IDEs (RStudio and Notepad++) varies much less for students. For non-students there are some exclusive users of other development environments like RStudio and Notepad++, but also every student that uses these IDEs  has also had some interaction with Jupyter."
"# Notepad++ is the most popular 'text editor' but others are close behind.*\nWe can also gain insights by looking into IDEs that serve the same purpose. Since RStudio is typically used to code in R, and Jupyter is unique in it's interactive cells - what happens if we only look at text editors?\n\nThis is a pretty clean venn diagram. You can see that there are more exclusive Notepad++ users than PyCharm and Sublime Text, but the areas where they overlap are strikingly similar. The 1556 people in the center are possibly just people who enjoy trying out new software.  The question that is being asked states `[which IDE] have you used at work or school in the last 5 years?` This could mean that respondents aren't necessarily stating which IDE they like the most, just the ones they've used.\n\n\* *It's important to note (as pointed out by @sciplic in the comments) Notepad++ is a Windows only text editor. To me this is even understates it's popularity since Mac and Linux users don't have it as an option.*"
"# Notepad++ is the most popular 'text editor' but others are close behind.*\nWe can also gain insights by looking into IDEs that serve the same purpose. Since RStudio is typically used to code in R, and Jupyter is unique in it's interactive cells - what happens if we only look at text editors?\n\nThis is a pretty clean venn diagram. You can see that there are more exclusive Notepad++ users than PyCharm and Sublime Text, but the areas where they overlap are strikingly similar. The 1556 people in the center are possibly just people who enjoy trying out new software.  The question that is being asked states `[which IDE] have you used at work or school in the last 5 years?` This could mean that respondents aren't necessarily stating which IDE they like the most, just the ones they've used.\n\n\* *It's important to note (as pointed out by @sciplic in the comments) Notepad++ is a Windows only text editor. To me this is even understates it's popularity since Mac and Linux users don't have it as an option.*"
"# Job Title and development environments\n\nLet's work off of the hypothesis that the type of work you do will impact the type of tools you use. We compare the IDE use of different job types below. This chart allows us to see what percentage of professional groups use each IDE. \n\nSome interesting insights we can see are:\n1. Jupyter/IPython is popular for pretty much every job title. This is obviously biased because the respondents also use kaggle! Surveying non-kaggler \n2. Statisticians use Jupyter the least (51.9%). Is this because they also tend to be R users (74.68% use RStudio)?\n3. DBA/Database Engineers and Software Engineer are heavy Notepad++ users, but I find it personally suprising that Data Journalists and Developer Advocates also use this IDE.  What drives this similarity among disparate occupations?\n4. Visual Studio and Visual Studio Code are popular amonst Developer Advocates and Software Engineers\n5. Matlab looks to be quite popular within academia. Students, Research Assistants, Research Scientists, and Principal Investigators stand out as Matlab users."
"# Job Title and development environments\n\nLet's work off of the hypothesis that the type of work you do will impact the type of tools you use. We compare the IDE use of different job types below. This chart allows us to see what percentage of professional groups use each IDE. \n\nSome interesting insights we can see are:\n1. Jupyter/IPython is popular for pretty much every job title. This is obviously biased because the respondents also use kaggle! Surveying non-kaggler \n2. Statisticians use Jupyter the least (51.9%). Is this because they also tend to be R users (74.68% use RStudio)?\n3. DBA/Database Engineers and Software Engineer are heavy Notepad++ users, but I find it personally suprising that Data Journalists and Developer Advocates also use this IDE.  What drives this similarity among disparate occupations?\n4. Visual Studio and Visual Studio Code are popular amonst Developer Advocates and Software Engineers\n5. Matlab looks to be quite popular within academia. Students, Research Assistants, Research Scientists, and Principal Investigators stand out as Matlab users."
"# Correlations between IDEs and Salary\n\nIt's always fun to plot the data by salary.   Of course, we shoud be weary of drawing any causal conclusions from this analysis. Correlation does note equal causation! Don't change the IDE you use thinking it will make you more money.\n\n*Note: I ignored NA values and those that answered ""I do not wish to disclose my approximate yearly compensation"" to the salary question. This may bias our results and should be considered.\n\nSome takeaways:\n- Each IDE appears to have it's own unique trend when looking at popularity vs. income which I find unexpected and interesting.\n- RStudio use appears to have a somewhat normal distribution around the mid-salary range.\n- PyCharm and VSCode appear to be fairly flat, with little changes in popularity around income.\n- Notepad++ starts to trend downward as salary goes above \$100k. VIM trends upwards."
"# Correlations between IDEs and Salary\n\nIt's always fun to plot the data by salary.   Of course, we shoud be weary of drawing any causal conclusions from this analysis. Correlation does note equal causation! Don't change the IDE you use thinking it will make you more money.\n\n*Note: I ignored NA values and those that answered ""I do not wish to disclose my approximate yearly compensation"" to the salary question. This may bias our results and should be considered.\n\nSome takeaways:\n- Each IDE appears to have it's own unique trend when looking at popularity vs. income which I find unexpected and interesting.\n- RStudio use appears to have a somewhat normal distribution around the mid-salary range.\n- PyCharm and VSCode appear to be fairly flat, with little changes in popularity around income.\n- Notepad++ starts to trend downward as salary goes above \$100k. VIM trends upwards."
"## Taking a closer look at the relationship between salary and IDE use\nIn the below chart we can see the raw number of respondents from each salary grouping. \nWith a chart like this insights are usually in the abnormalities. For instance:\n- Matlab is popular among those on the low range of salary. This is consistent with the fact that many students use Matlab. \n- PyCharm and Sublime Text catch up to Notepad++ in popularity in the higher salary range (\$100k+). Could this be because this is the salary range of a serious python developer?\n- We can see the proportinal relationship between Jupyter and unix based text editors varies in salary range. We see a Jupyter to Vim ratio of almost `4:1` in the \$0-\$10k salary, but that drops down to a `2:1` ratio in the \$150-\$200k range. Could it be that those that have more job experience also spend more time in the unix shell?"
# IDE Use By Age\nWe can also take a look at how age and IDE use relate. Suprisingly age did not appear to have any overwhelmingly apparent correlations with IDE use. It does appear that RStudio is less popular proportionally amonst the younger respondents (Ages 18-24).
When we plot the age distribution of each IDE out individually we can see that the 22-24 age group stands out as different depending on IDE. Could this be because 22-24 year olds are still deciding on which IDE they prefer- or could it simply be that school requires them to use different IDEs than the older population?
When we plot the age distribution of each IDE out individually we can see that the 22-24 age group stands out as different depending on IDE. Could this be because 22-24 year olds are still deciding on which IDE they prefer- or could it simply be that school requires them to use different IDEs than the older population?
"# Hosted Notebooks and IDEs\nAs we've already seen, jupyter notebooks are quite popular in the kaggle community. But what about jupyter-like hosted notebooks?  We could make the argument that these are IDEs aswell. Lets look at the popularity of hosted notebooks from the kaggle survey.\n- Over 30% of respondents have no experience with hosted notebooks\n- Not suprisingly Kaggle Kernels are the most popular for kaggle respondents with roughtly 25% of respondents saying they've used them. What may be more suprising is how close the next two popular hosted notebooks are. JupyterHub/binder being used by 20% and Google Colab by 14.8%.\n- After that it's a steep dropoff to the next types of hosted kernels."
"# Hosted Notebooks and IDEs\nAs we've already seen, jupyter notebooks are quite popular in the kaggle community. But what about jupyter-like hosted notebooks?  We could make the argument that these are IDEs aswell. Lets look at the popularity of hosted notebooks from the kaggle survey.\n- Over 30% of respondents have no experience with hosted notebooks\n- Not suprisingly Kaggle Kernels are the most popular for kaggle respondents with roughtly 25% of respondents saying they've used them. What may be more suprising is how close the next two popular hosted notebooks are. JupyterHub/binder being used by 20% and Google Colab by 14.8%.\n- After that it's a steep dropoff to the next types of hosted kernels."
Again we can use the venn-diagram to see how users overlap in their use of hosted notebooks. Only 661 respondents have experience with all three of the top 3 most popular hosted notebook platforms.
Again we can use the venn-diagram to see how users overlap in their use of hosted notebooks. Only 661 respondents have experience with all three of the top 3 most popular hosted notebook platforms.
"# Freeform Responses\nThe survey question we are exploring provided a list of options for respondents to select from. There is an obvious selection bias against any IDE not on that list. We can however explore the freeform responses some users chose to provide. The wordcloud below makes some of the most popular IDEs that were not selection options. Eclipse, emacs, netbeans, xcode, octave all are very popular freeform responses. Kaggle should consider including these as options in the 2019 survey."
"# Freeform Responses\nThe survey question we are exploring provided a list of options for respondents to select from. There is an obvious selection bias against any IDE not on that list. We can however explore the freeform responses some users chose to provide. The wordcloud below makes some of the most popular IDEs that were not selection options. Eclipse, emacs, netbeans, xcode, octave all are very popular freeform responses. Kaggle should consider including these as options in the 2019 survey."
# The 4 Types of Kagglers (by IDE use)\n\n\n\n      The Jupyter Lover \n             \n  The Jack of All IDEs \n             \n  The RStudio + Jupyter \n            \n  The Anti-Jupyter \n                 \n\n
"### 1. Anti-Jupyters are more diverse in their use of programming languages\nWhile Python is the clear favorite amonst all kagglers, and R is a close second. We find the the non-jupyter cluster has consistantly more percentage of their group that chooses other programming languages as their language of choice. It stands out that those that don't use jupyter may fall into these two categories:\n- Stictly R Users: who prefer to use RStudio and rather not deal with jupyter's R integration.\n- Database engineers: Who code much in raw SQL\n- Software Developers: who code in languages like C++, C# and Javascript\n- Excel users who code in VBA\n- Specialized programmers who prefer non-open source langagues like SAS and STATA"
### 2. Anti-Jupyters are more likely to code less\nAre you in the non jupyter group? It's more likely you spend less time coding. Use jupyter? You probably spend more time coding. There however is one exception. Anti-Jupyters actually have a higher percentage of their group that codes 100% of the time.
### 2. Anti-Jupyters are more likely to code less\nAre you in the non jupyter group? It's more likely you spend less time coding. Use jupyter? You probably spend more time coding. There however is one exception. Anti-Jupyters actually have a higher percentage of their group that codes 100% of the time.
"### 3. Anti-Jupyters have less expereince with Machine Learning, but they plan to learn it!\nThe percentage of non jupyter users are much higher for those with <1 year of experience with machine learning. They also have a much higher percentage of that plan to learn machine learning methods. These would be the perfect type of user that the kaggle team should make an effort to encourage using kaggle kernels!"
"### 3. Anti-Jupyters have less expereince with Machine Learning, but they plan to learn it!\nThe percentage of non jupyter users are much higher for those with <1 year of experience with machine learning. They also have a much higher percentage of that plan to learn machine learning methods. These would be the perfect type of user that the kaggle team should make an effort to encourage using kaggle kernels!"
"### 4. Those that use Jupyter are much more likely to consider themselves ""Data Scientists""\nWhile around 55% of respondents who use jupyter consider themselves a data scientist. On the other hand 41.3% of the non jupyter user group consider themselves data scientists.\n\nThose who consider themselves ""definately not"" data scientists consist of 13.7% of anti-jupyters while that percentage is only 6.8% of the rest of respondents."
"### 4. Those that use Jupyter are much more likely to consider themselves ""Data Scientists""\nWhile around 55% of respondents who use jupyter consider themselves a data scientist. On the other hand 41.3% of the non jupyter user group consider themselves data scientists.\n\nThose who consider themselves ""definately not"" data scientists consist of 13.7% of anti-jupyters while that percentage is only 6.8% of the rest of respondents."
"### 5. Anti-Jupyters spend less time exploring for model insights\nJupyter is well known for it's use in explorative and iterative scientific programming. As such, those who spend less time exloring for model insights also tend to be in the anti-Jupyter group."
"### 5. Anti-Jupyters spend less time exploring for model insights\nJupyter is well known for it's use in explorative and iterative scientific programming. As such, those who spend less time exloring for model insights also tend to be in the anti-Jupyter group."
"# What can we take away from this analysis?\n\nIn this analysis, we've looked at the responses from the 2018 Kaggle survey. Specifically, we looked at the types of development environments (or IDES) that Kagglers have said they've used over the past 5 years. We found some interesting relationships between the software and tools that kagglers use and how it relates to their job title, salary, and country of origin. We not only looked at what IDEs were popular, but how respondents overlapped in their use of different IDEs. By looking into the freeform responses, we've identified some popular IDEs that the kaggle team might want to include in next year's survey.\n\nFinally, we used unsupervised machine learning techniques to cluster the kaggle respondents into four distinct groups. These groups were each unique, but the cluster that consisted of respondents who had not used Jupyter/IPython stood out in their responses to the other survey questions as being the least like the other groups. The ""Ani-Jupyter"" group tends to be more diverse in their use of programming languages, less likely to consider themselves a Data Scientist, and they spend less time in data projects exploring for model insights.  This is not to say that the anti-Jupyter group does not want to learn new things.  Suprisingly, 23.6% of them say they ""have never used machine learning methods **but plan to**"".\n\nThe anti-Jupyter (or non-jupyter) are an important part of the kaggle community. Whether they are experienced software developers who aren't interested in using jupyter - or if they are new to programming and data science- they are part of the community that Kaggle should focus on reaching. Kaggle Kernels are a great resource and remove many of the barriers that some of the community may have with installing Jupyter on their local computers. Hopefully, more of them will join the community, become involved in using Kaggle kernels, and help us grow and become more diverse.\n\nThanks for reading!"
# **Exploratory Data Analysis**
# **Target Distribution**\n
# **Target Distribution**\n
# **Feature Distribution**\n\nLets analyze the distribution of first nine features starting from 'f_0' to 'f_8'
# **Correlation Heatmap**\n\nLets analyze the correlation of first nine features starting from 'f_0' to 'f_8'
# **Correlation of Target and Features**\n\nLets analyze the correlation of first nine features starting from 'f_0' to 'f_8' with the target
# **Correlation of Target and Features**\n\nLets analyze the correlation of first nine features starting from 'f_0' to 'f_8' with the target
# **Preprocessing**
"# **W & B Artifacts**\n\nAn artifact as a versioned folder of data.Entire datasets can be directly stored as artifacts .\n\nW&B Artifacts are used for dataset versioning, model versioning . They are also used for tracking dependencies and results across machine learning pipelines.Artifact references can be used to point to data in other systems like S3, GCP, or your own system.\n\nYou can learn more about W&B artifacts [here](https://docs.wandb.ai/guides/artifacts)\n\n![](https://drive.google.com/uc?id=1JYSaIMXuEVBheP15xxuaex-32yzxgglV)"
"# **üéØtf.data**\n\n[Source](https://www.tensorflow.org/guide/data)\n\ntf.data API is used for building efficient input pipelines which can handle large amounts of data and perform complex data transformations . tf.data API has provisions for handling different data formats .\n\n\n\n[Image Source](https://www.kaggle.com/jalammar/intro-to-data-input-pipelines-with-tf-data)\n\nData source is essential for building any input pipeline and tf.data.Dataset.from_tensors() or tf.data.Dataset.from_tensor_slices can be used to construct a dataset from data in memory .The recommended format for the iput data stored in file is TFRecord which can be created using TFRecordDataset() .The different data source formats supported are numpy arrays , python generators , csv files ,image , TFRecords , csv and text files. \n\n\n\n[Image Source](https://www.kaggle.com/jalammar/intro-to-data-input-pipelines-with-tf-data)\n\nConstruction of tf.data input pipeline consists of three phases namely Extract , Transform and Load . The extraction involves the loading of data from different file format and converting it in to tf.data.Dataset object .\n\n## **üéØtf.data.Dataset**\n\ntf.data.Dataset is an abstraction introduced by tf.data API and consists of sequence of elements where each element has one or more components . For example , in a tabular data pipeline , an element might be a single training example , with a pair of tensor components representing the input features and its label \n\ntf.data.Dataset can be created using two distinct ways\n\nConstructing a dataset using data stored in memory by a data source\n\nConstructing a dataset from one or more tf.data.Dataset objects by a data transformation\n\n\n\n[Image Source](https://www.kaggle.com/jalammar/intro-to-data-input-pipelines-with-tf-data)\n\n"
0¬†¬†IMPORTS
1¬†¬†BACKGROUND INFORMATION
3¬†¬†HELPER FUNCTIONS
"4¬†¬†TABULAR DATA\n\nRECALL THAT THESE ARE THE TRAIN COLUMNS\n> **`image_id`** - unique image identifier\n**`class_name`** - the name of the class of detected object (or ""No finding"")\n**`class_id`** - the ID of the class of detected object\n**`rad_id`** - the ID of the radiologist that made the observation\n**`x_min`** - minimum X coordinate of the object's bounding box\n**`y_min`** - minimum Y coordinate of the object's bounding box\n**`x_max`** - maximum X coordinate of the object's bounding box\n**`y_max`** - maximum Y coordinate of the object's bounding box"
"4.1  IMAGE_ID COLUMN EXPLORATION\n\n---\n\nThe **`image_id`** column contains a **U**nique **ID**entifier (**UID**) that indicates which patient the respective row (object) relates to.\n\nAs there can be up to three radiologists annotating the same image and potentially multiple objects/bboxes per image, it is possible for a single image UID to occur many times. However, please note that we know from the competition data details that there exists ***only one image for one patient***. This means that if a specific image_id appears 12 times, that there are 4 objects in the image, and each object was annotated by all three radiologists.\n\n*SIDE-NOTE* ‚Äì Due to the ***one image to one patient*** rule, the column name **`image_id`** could be replaced with **`patient_id`** and it would mean exactly the same thing.\n\n\n\nTOTAL OBJECT ANNOTATIONS PER IMAGE\n\nLet's count the distribution of the amount of annotations per unique **`image_id`** value. Note that we use a log-axis for the count axis to handle the large number of values present at 3 annotations (a single object annotated similarily by 3 radiologists)\n\n---\n\n**From the histogram plotted below we can ascertain the following information:**\n* Images contain at least 3 annotations (1 distinct object annotation by 3 radiologists)\n* Images contain at most 57 annotations (19 distinct object annotations by 3 radiologists)\n* The vast majority of images only have 3 annotations (~11,000 out of 15,000 images)\n* The distribution has a heavy skew (**`value=3.8687`** **`# FROM --> scipy.stats.skew(train_df.image_id.value_counts().values)`**). Remember that a perfectly symetrical distribution would have a skew value of **`0`**."
"UNIQUE OBJECT ANNOTATIONS PER IMAGE\n\nLet's count the distribution of **UNIQUE** object-label annotations per unique **`image_id`** value. This means if a radiologist identifies 8 nodules in an image, we count that as 1 unique object annotation. The goal of this is to determine the distributions of different diseases occuring within the same patient.\n\nNote that we use a log-axis for the count axis to handle the large number of values present at 1 unique abnormality\n\n---\n\n**From the histogram plotted below we can ascertain the following information:**\n* Images contain no more than 10 unique abnormalities (out of a possible 14)\n* The more unique abnormalities present in an image, the rarer it is."
"UNIQUE OBJECT ANNOTATIONS PER IMAGE\n\nLet's count the distribution of **UNIQUE** object-label annotations per unique **`image_id`** value. This means if a radiologist identifies 8 nodules in an image, we count that as 1 unique object annotation. The goal of this is to determine the distributions of different diseases occuring within the same patient.\n\nNote that we use a log-axis for the count axis to handle the large number of values present at 1 unique abnormality\n\n---\n\n**From the histogram plotted below we can ascertain the following information:**\n* Images contain no more than 10 unique abnormalities (out of a possible 14)\n* The more unique abnormalities present in an image, the rarer it is."
4.2  CLASS_NAME COLUMN EXPLORATION\n\n---\n\nThe **`class_name`** column indicates the label as a string for the respective object/annotation (each row is for one object/annotation). \n\n\nANNOTATIONS PER CLASS\n\nWe know there are 15 different possible **`class_name`**s (including **`No finding`**). To identify the distribution of counts across the labels we will use a bar-chart.
4.2  CLASS_NAME COLUMN EXPLORATION\n\n---\n\nThe **`class_name`** column indicates the label as a string for the respective object/annotation (each row is for one object/annotation). \n\n\nANNOTATIONS PER CLASS\n\nWe know there are 15 different possible **`class_name`**s (including **`No finding`**). To identify the distribution of counts across the labels we will use a bar-chart.
"4.3  CLASS_ID COLUMN EXPLORATION\n\n---\n\nThe **`class_id`** column indicates the label encoded as a number the respective object/annotation (each row is for one object/annotation). Knowing this, we will remove the previous **class_name** column, as we would rather work with a numeric representation. Prior to removal we will generate a map that will allow us to translate the numeric labels back into their respective string represntations."
"4.4  RAD_ID COLUMN EXPLORATION\n\n---\n\nThe **`rad_id`** column indicates the the ID of the radiologist that made the observation. Remember, three radiologists will annotate a given image out of a pool of seventeen possible radiologists, where the radiologist ID is encoded from R1 to R17.\n\n\nANNOTATIONS PER RADIOLOGIST\n\nWe know there are 17 possible radiologists (**`rad_id`**s). To identify the distribution of annotations performed across the radiologists we will use a historgram.\n\n---\n\n**From the histogram plotted below we can ascertain the following information**\n* 3 of the radiologists (R9, R10, & R8 in that order) are responsible for the vast majority of annotations (~40-50% of all annotations)\n* Among the other 14 radiologists there is some variation around the number of annotations made, however, these 14 radiologists all made between 3121 annotations and 812 annotations with the vast majority annotating 1800-2200 objects."
"ANNOTATIONS PER RADIOLOGIST SEPERATED BY CLASS LABEL\n\nWe have already identified that three of the radiologists are responsible for almost 50% of all of the annotations. We would now like to identify if all of the radiologists were able to see and annotate all 15 classes. If so, can we identify any additional skew or problems that might arise?\n\n---\n\n**From the first histogram plotted below we can ascertain the following information**\n* 3 of the radiologists (R9, R10, & R8 in that order) are responsible for the vast majority of annotations (~40-50% of all annotations)\n* Among the other 11 radiologists there is some variation around the number of annotations made, however, these 11 radiologists all made between 3121 annotations and 812 annotations with the vast majority annotating 1800-2200 objects.\n\n---\n\n**From the second histogram plotted below we can ascertain the following information**\n* Among the other 11 radiologists, 7 of them (R1 through R7) have only ever annotated images as **`No finding`**\n* The other 4 radiologists are also heavily skewed towards the **`No finding`** label when compared to the main 3 radiologists (R8 through R10). This seems to actually be closer to the overall distribution, however it might allow us to estimate that radiologists other than R8, R9, and R10, are much more likely to annotate images as **`No finding`**.\n* The downside to this distribution, is that if we include this information in the model than the model will learn that 7 of the radiologists classify images as **`No finding`** 100% of the time!\n\n¬†¬†¬†¬†¬†¬†¬†¬†Note that this second plot could have been generated by interacting with the first histogram as plotly has this functionality built-in"
"ANNOTATIONS PER RADIOLOGIST SEPERATED BY CLASS LABEL\n\nWe have already identified that three of the radiologists are responsible for almost 50% of all of the annotations. We would now like to identify if all of the radiologists were able to see and annotate all 15 classes. If so, can we identify any additional skew or problems that might arise?\n\n---\n\n**From the first histogram plotted below we can ascertain the following information**\n* 3 of the radiologists (R9, R10, & R8 in that order) are responsible for the vast majority of annotations (~40-50% of all annotations)\n* Among the other 11 radiologists there is some variation around the number of annotations made, however, these 11 radiologists all made between 3121 annotations and 812 annotations with the vast majority annotating 1800-2200 objects.\n\n---\n\n**From the second histogram plotted below we can ascertain the following information**\n* Among the other 11 radiologists, 7 of them (R1 through R7) have only ever annotated images as **`No finding`**\n* The other 4 radiologists are also heavily skewed towards the **`No finding`** label when compared to the main 3 radiologists (R8 through R10). This seems to actually be closer to the overall distribution, however it might allow us to estimate that radiologists other than R8, R9, and R10, are much more likely to annotate images as **`No finding`**.\n* The downside to this distribution, is that if we include this information in the model than the model will learn that 7 of the radiologists classify images as **`No finding`** 100% of the time!\n\n¬†¬†¬†¬†¬†¬†¬†¬†Note that this second plot could have been generated by interacting with the first histogram as plotly has this functionality built-in"
"4.5  EXPLORATION OF BBOX COORDINATE COLUMNS\n\n---\n\nThe **`x_min`**, **`y_min`**, **`x_max`**, and **`y_max`** columns indicate the location of the annotated object bounding box, where the top-left corner is represented by the tuple (**`x_min`**, **`y_min`**) and the bottom-right corner is represented by the tuple (**`x_max`**, **`y_max`**).\n\nA value of **`NaN`** coincides with a label 14 (**`No finding`**) and means that there is nothing to annotate (healthy x-ray).\nFor the purpose of examining these columns we will only be examining rows where the objects have been annotated with a bounding box\n(i.e. All rows with a label of **`No finding`** will be discarded)\n\nPLOT HEATMAP REPRESENTING BOUNDING BOXES FOR VARIOUS CLASSES\n\nThere's a lot to digest within these plots. The important thing to focus on will be identifying for each class the approximate range of locations the annotations are found in and the intensity of the locations within the heatmap.\n\n---\n\n**From the heatmaps plotted below we can ascertain the following information**\n* Regarding Aortic Enlargement (CLASS-ID: 0)\n    * Heatmap distribution is slightly oval (vertical) and is very tight and intense, located in the centre of the image (slight drift to the top-right).\n* Regarding Atelectasis (CLASS-ID: 1)\n    * Heatmap distribution is lung shaped and relatively diffuse with a circular focus on the upper-left part of the left lung.\n* Regarding Calcification (CLASS-ID: 2)\n    * Heatmap distribution is lung shaped and relatively diffuse with a oval (vertical) focus on the top-left edge of the right lung.\n* Regarding Cardiomegaly (CLASS-ID: 3)\n    * Heatmap distribution is rectangular and is very tight and intense, located in the bottom-centre (to bottom-centre-right) of the image.\n* Regarding Consolidation (CLASS-ID: 4)\n    * Heatmap distribution is lung shaped and relatively diffuse, the focus of the distribution covers the entire left lung.\n* Regarding ILD (CLASS-ID: 5)\n    * Heatmap distribution is lung shaped and relatively diffuse, the focus leans a little towards the centre of the lungs.\n* Regarding Infiltration (CLASS-ID: 6)\n    * Heatmap distribution is lung shaped and relatively diffuse, the focus of the distribution covers the entire left lung.\n* Regarding Lung Opacity (CLASS-ID: 7)\n    * Heatmap distribution is lung shaped and relatively diffuse, the focus of the distribution covers the entire left lung.\n* Regarding Nodule/Mass (CLASS-ID: 8)\n    * Heatmap distribution is lung shaped and relatively diffuse, the focus leans a little towards the centre of the lungs. (NOTE: The diffusion pattern looks patchy... probably due to smaller bounding boxes)\n* Regarding Other Lesion (CLASS-ID: 9)\n    * Heatmap distribution is incredibly diffuse and covers most of the image, the focus is towards a vertical-strip in the centre of the image.\n* Regarding Pleural Effusion (CLASS-ID: 10)\n    * Heatmap distribution is lung shaped (slightly more rectangular?) and relatively diffuse, the focus is towards the bottom of the lungs and although both lungs are covered, the left lung has a stronger focus.\n* Regarding Pleural Thickening (CLASS-ID: 11)\n    * Heatmap distribution is vaguely lung shaped (patches near top and focus trails down exterior lung edge fading as it goes), the focus is towards the top of the lungs is oval (horizontal).\n* Regarding Pneumothorax (CLASS-ID: 12)\n    * Heatmap distribution is lung shaped (more rectangular), the focus is on the entire left lung however the right lung has some diffuse coverage.\n* Regarding Pulmonary Fibrosis (CLASS-ID: 13)\n    * Heatmap distribution is vaguely lung shaped (patches near top and focus trails down lung fading as it goes), the focus is towards the top of the lung and it is oval.\n"
"INVESTIGATE THE SIZES OF BOUNDING BOXES AND THE IMPACT OF CLASS\n\nAs we wish to examine the average, as well as the upper and lower limits for various class-based bounding box statistics, we will use a box plot to investigate. To make things easier to understand let us consider the following basic buckets.\n\nBounding Box Area - Median\n* Under   0.01 ‚Äì‚Äì Smallest\n* 0.01 to 0.02 ‚Äì‚Äì Small\n* 0.02 to 0.04 ‚Äì‚Äì¬†Medium\n* 0.04 to 0.06 ‚Äì‚Äì¬†Large\n* Above   0.06 ‚Äì‚Äì Largest\n\nBounding Box Area - Quartile Range\n* Under     0.0075 ‚Äì‚Äì Smallest\n* 0.0075 to 0.0125 ‚Äì‚Äì Small\n* 0.0125 to 0.0250 ‚Äì‚Äì¬†Medium\n* 0.0250 to 0.0500 ‚Äì‚Äì¬†Large\n* Above     0.0500 ‚Äì‚Äì Largest\n\n---\n\n**From the boxplot plotted below we can ascertain the following information**\n* Regarding Aortic Enlargement Box Plot (CLASS-ID: 0)\n    * Median Value is Small  ‚Äì‚Äì‚Äì  Quartile Range is Smallest\n* Regarding Atelectasis Box Plot (CLASS-ID: 1)\n    * Median Value is Medium  ‚Äì‚Äì‚Äì  Quartile Range is Large\n* Regarding Calcification Box Plot (CLASS-ID: 2)\n    * Median Value is Smallest  ‚Äì‚Äì‚Äì  Quartile Range is Medium\n* Regarding Cardiomegaly Box Plot (CLASS-ID: 3)\n    * Median Value is Large  ‚Äì‚Äì‚Äì  Quartile Range is Large\n* Regarding Consolidation Box Plot (CLASS-ID: 4)\n    * Median Value is Medium  ‚Äì‚Äì‚Äì  Quartile Range is Large\n* Regarding ILD Box Plot (CLASS-ID: 5)\n    * Median Value is Largest  ‚Äì‚Äì‚Äì  Quartile Range is Largest\n* Regarding Infiltration Box Plot (CLASS-ID: 6)\n    * Median Value is Medium  ‚Äì‚Äì‚Äì  Quartile Range is Large\n* Regarding Lung Opacity Box Plot (CLASS-ID: 7)\n    * Median Value is Medium  ‚Äì‚Äì‚Äì  Quartile Range is Large\n* Regarding Nodule/Mass Box Plot (CLASS-ID: 8)\n    * Median Value is Smallest  ‚Äì‚Äì‚Äì  Quartile Range is Smallest\n* Regarding Other Lesion Box Plot (CLASS-ID: 9)\n    * Median Value is Small  ‚Äì‚Äì‚Äì  Quartile Range is Large\n* Regarding Pleural Effusion Box Plot (CLASS-ID: 10)\n    * Median Value is Smallest  ‚Äì‚Äì‚Äì  Quartile Range is Large\n* Regarding Pleural Thickening Box Plot (CLASS-ID: 11)\n    * Median Value is Smallest  ‚Äì‚Äì‚Äì  Quartile Range is Smallest\n* Regarding Pneumothorax Box Plot (CLASS-ID: 12)\n    * Median Value is Largest  ‚Äì‚Äì‚Äì  Quartile Range is Largest\n* Regarding Pulmonary Fibrosis Box Plot (CLASS-ID: 13)\n    * Median Value is Small  ‚Äì‚Äì‚Äì  Quartile Range is Medium\n"
"INVESTIGATE THE ASPECT RATIO OF BOUNDING BOXES AND THE IMPACT OF CLASS\n\nWe want to understand the average shape (wide-narrow, square, etc.) of the bouning-boxes associated with each class, and to do this we will use a bar chart with some pre-drawn lines.\n\n---\n\n**From the bar chart plotted below we can ascertain the following information:**\n* The average size of bounding-boxes by class is usually close to square (usually on the horizontal rectangle size of square).\n* Cardiomegaly has, on average, very thin, rectangular, horizontal boxes (mean width is ~2.9x larger than mean height).\n* Pleural Thickening has, on average, thin, rectangular, horizontal boxes (mean width is ~1.9x larger than mean height).\n* ILD has, on average, somewhat thin, rectangular,  vertical boxes (mean height is ~1.6x larger than mean width)\n"
"INVESTIGATE THE ASPECT RATIO OF BOUNDING BOXES AND THE IMPACT OF CLASS\n\nWe want to understand the average shape (wide-narrow, square, etc.) of the bouning-boxes associated with each class, and to do this we will use a bar chart with some pre-drawn lines.\n\n---\n\n**From the bar chart plotted below we can ascertain the following information:**\n* The average size of bounding-boxes by class is usually close to square (usually on the horizontal rectangle size of square).\n* Cardiomegaly has, on average, very thin, rectangular, horizontal boxes (mean width is ~2.9x larger than mean height).\n* Pleural Thickening has, on average, thin, rectangular, horizontal boxes (mean width is ~1.9x larger than mean height).\n* ILD has, on average, somewhat thin, rectangular,  vertical boxes (mean height is ~1.6x larger than mean width)\n"
5¬†¬†IMAGE DATA\n\nRecall that the image data is stored in DICOM format and the annotations are stored in our **`train_df`** Dataframe.
"5.1  CLASS TO HELP PARSE THE DATA \n\n---\n\nClasses are very useful when we need to bind data to functionality. In this case, I have created a class (unwieldy as it may be currently in it's initial version) to help with that called **`TrainData`**.\n\nI will get into the details of how the methods work at a later time... and for today I will simply generate the outputs using each method to show their functionality."
PLOT IMAGES FROM THE CORRESPONDING IMAGE IDS\n\nSummary to be done later\n\n---\n\nMore detail to come later
"# PRINCIPAL COMPONENT ANALYSIS\nPrincipal Component Analysis is a technique used in order to reduce the dimensionality of a dataset while preserving as mush information as possible. Data is reprojected in a lower dimensional space, in particular we need to find a projection that minimizes the squared error in reconstructing the original data. \n\n![Principal Component Analysis](https://sebastianraschka.com/images/faq/lda-vs-pca/pca.png)\n\n\nThere are 3 different technique in order to apply PCA:\n1. **Sequential**  \n2. **Sample Covariance Matrix**\n3. **Singular Value Decomposition (SVD)** \n\nI'll explain the Sample Covariance Matrix technique:\n* The first thing to do is to standardize the data, so for each sample we need to substract the mean of the full dataset and then divide it by the variance, so as having an unitary variance for each istance. This last process is not completly necessary but it is usefull to let the CPU work less.\n$$\n    Z = \frac{X-\mu}{\sigma^2} \n$$\n\n* Then we need to compute Covariance Matrix, given data { $x_1 ,x_2, ..., x_n$ } with $n$ number of samples, covariance matrix is obtained by:\n$$\n\Sigma = \frac {1}{n}\sum_{i=1}^n (x_i - \bar{x})(x - \bar{x})^T $$    $\;\;$  where  $$\bar{x} = \frac {1}{n}\sum_{i=i}^n x_i $$ \nOr simply by multiplying the standardized matrix Z by it self transposed\n$$ COV(X) = Z Z^T $$\n\n* Principal Components will be the eigenvectors of the Covariance Matrix sorted in order of importance by the respective eigenvalues.**Larger eigenvalues $\Rightarrow$ more important eigenvectors.** They represent the most of the useful information on the entire dataset in a single vector"
## PCA EXAMPLE
#### LINEAR SVM
#### SVM + K-FOLD
#### LINEAR SVM + PCA
#### KERNEL SVM + PCA
#### KERNEL SVM + PCA
" \n# K-NEAREST NEIGHBOR\n\nK-NN is a supervised learning method that considers the K closest training examples to the point of interest for predicting its class. The point is assigned to the class that is closest. \nCould be applied different distance metrics such as: Euclidian, Weighted, Gaussian, etc.\nSteps are pretty easy:\n\n* ‚ÄäReceive an unclassified data\n\n*  Measure the distance with choosen metrics from the new data to all others data that are already classified.\n\n* ‚ÄäGets the K smaller distances\n\n* ‚ÄäCheck the list of classes that had the shortest distance and count the amount of each class that appears\n\n*  Takes as correct class the class that appeared the most times\n\n*  Classifies the new data with the class that you took in previous step \n"
"#### COMMENT\n\nFrom this graph is possible to understand how the best value of K is equal to **2**, because the Test Accuracy reaches the best accuracy score and then start decreasing. \n\nTraining accuracy still maintain 100% accuracy starting decreasing for last numbers of K "
"# DECISION TREE\nIn a decision tree each intermediate node of the tree contains splitting attributes used to build different paths, while leaves contains class labels.\n\nThere are differt algorithms to build a decision tree, all are made with a greedy approach, optimal locally.The most famous is **Hunt's algoritm**. \n\n![](https://cdn-images-1.medium.com/max/880/0*QctkHiOX2G2pvfD_.jpg)\n\nStrarting from an empty tree, we need to find iteratively best attribute on which split the data locally at each step. If a subset contains records that belongs to the same class then the leaf containing such class label is created, otherwise if a subset is empty is assigned to default to mayor class.\n\nCritical points of decision trees are test condition, the selection of the best attribute and the splitting condition. \nFor the selection of the best attribute is generally choosen the attribute that generate homogeneus nodes. \nThere are different metrics in order to find the best splitting homogenity, the most common are:\n* **GINI IMPURITY INDEX**: Given **$n$** classes and $p_i$ the fraction of items of class $i$ in a subset p, for $i$‚àà{1,2,...,n}. Then the GINI index is defined as: $$ GINI = 1 ‚àí \sum_{i=1}^n p_i^2 $$\n\n* **INFORMATION GAIN RATIO**: The information gain is based on the decrease of entropy after a data-set is split on an attribute. Constructing a decision tree is all about finding attribute that returns the highest information gain (i.e., the most homogeneous branches).Entropy is defined as $H(i) = -\sum_{i=1}^n p_i\log_2 p_i $. So then Information gain is defined as: \n\n$$ IG = H(p) - H(p,i)  = H(p) - \sum_{i=1}^n \frac{n_i}{n} H(i) $$\n\nwhere p is the parent node.\nAdvantages of Decision Trees are velocity, easy to interpretate and good accuracy, but they could be affected by missing values."
"Accuracy on Training phase increase while the Accuracy on Test phase is decreasing, this means that the model **overfit** increasing the max depth of the tree."
"# MODEL EVALUATION\n\nIn order to find the most suitable algorithm to this dataset, different evaluation methods will be presented: **Accuracy, Confusion Matrix, ROC Curve.**\n\nGiven: \n* TP = #samples for which the prediction is Fruit1 and the true label is Fruit1\n* FP = #samples for which the prediction is Fruit2 but the true label is Fruit1\n* TN = #samples for which the prediction is Fruit2 and the true label is Fruit2\n* FN = #samples for which the prediction is Fruit1 but the true label is Fruit2\n\nWe can define: \n\n* **ACCURACY: $\frac{TP+TN}{TP+FP+TN+FN}$ that is the percentage of samples classified correctly. **\n\n* **CONFUSION MATRIX:  A simple table with previous values used to show performance of a classifier**\n\n* **ROC CURVE: Area Under the Receiver Operating Characteristic curve (AUC)**\nTo introduce this concept, we define the following two metrics:\n\n    * **True positive rate (TPR):**   TPR = recall = $\frac{TP}{FN+TP}$\n    \n    * **False positive rate (FPR):**   FPR = $\frac{FP}{TN+FP}$\n    \nIn order to plot the Receiver Operating Characteristic (ROC) curve we need to compute TPR and FPR and choose a number of thresholds for the classification (AUG). Area under the ROC curve, performed plotting TPR and FPR is used as evaluation matrics for the different classifiers."
"# MODEL EVALUATION\n\nIn order to find the most suitable algorithm to this dataset, different evaluation methods will be presented: **Accuracy, Confusion Matrix, ROC Curve.**\n\nGiven: \n* TP = #samples for which the prediction is Fruit1 and the true label is Fruit1\n* FP = #samples for which the prediction is Fruit2 but the true label is Fruit1\n* TN = #samples for which the prediction is Fruit2 and the true label is Fruit2\n* FN = #samples for which the prediction is Fruit1 but the true label is Fruit2\n\nWe can define: \n\n* **ACCURACY: $\frac{TP+TN}{TP+FP+TN+FN}$ that is the percentage of samples classified correctly. **\n\n* **CONFUSION MATRIX:  A simple table with previous values used to show performance of a classifier**\n\n* **ROC CURVE: Area Under the Receiver Operating Characteristic curve (AUC)**\nTo introduce this concept, we define the following two metrics:\n\n    * **True positive rate (TPR):**   TPR = recall = $\frac{TP}{FN+TP}$\n    \n    * **False positive rate (FPR):**   FPR = $\frac{FP}{TN+FP}$\n    \nIn order to plot the Receiver Operating Characteristic (ROC) curve we need to compute TPR and FPR and choose a number of thresholds for the classification (AUG). Area under the ROC curve, performed plotting TPR and FPR is used as evaluation matrics for the different classifiers."
"### COMMENT:\nSVM and K-NN performs better with this classification with an AUC = 0.99x, instead Decision Tree is worst with an AUC of 0.65 "
#### Distribution of Categorical Features :
- All the categorical features are near about **Normally Distributed**.
### Numerical Features :\n\n#### Distribution of Numerical Features :
- **Oldpeak's** data distribution is rightly skewed.\n- **Cholestrol** has a bidmodal data distribution. 
### Target Variable Visualization (HeartDisease) : 
- The dataset is pretty much **evenly balanced!**
### Categorical Features vs Target Variable (HeartDisease) :
"- **Male** population has more heart disease patients than no heart disease patients. In the case of **Female** population, heart disease patients are less than no heart disease patients. \n- **ASY** type of chest pain boldly points towards major chances of heart disease.\n- **Fasting Blood Sugar** is tricky! Patients diagnosed with Fasting Blood Sugar and no Fasting Blood Sugar have significant heart disease patients. \n- **RestingECG** does not present with a clear cut category that highlights heart disease patients. All the 3 values consist of high number of heart disease patients.\n- **Exercise Induced Engina** definitely bumps the probability of being diagnosed with heart diseases.\n- With the **ST_Slope** values, **flat** slope displays a very high probability of being diagnosed with heart disease. **Down** also shows the same output but in very few data points. "
### Numerical Features vs Target Variable (HeartDisease) :
"- Because of too many unique data points in the above features, it is difficult to gain any type of insight. Thus, we will convert these numerical features,except age, into categorical features for understandable visualization and gaining insights purposes. \n- Thus, we scale the individual values of these features. This brings the varied data points to a constant value that represents a range of values.\n- Here, we divide the data points of the numerical features by 5 or 10 and assign its quotient value as the representative constant for that data point. The scaling constants of 5 & 10 are decided by looking into the data & intuition. "
#### Sex vs Numerical Features :
"- **Male** population displays heart diseases at near about all the values of the numerical features. Above the age of 50, positive old peak values and maximum heart rate below 140, heart diseases in male population become dense.\n- **Female** population data points are very less as compared to **male** population data points. Hence, we cannot point to specific ranges or values that display cases of heart diseases. "
#### ChestPainType vs Numerical Features :
- **ASY** type of chest pain dominates other types of chest pain in all the numerical features by a lot.
#### FastingBS vs Numerical features :
"- Above the **age** 50, heart diseases are found throughout the data irrespective of the patient being diagnosed with Fasting Blood Sugar or not.\n- **Fasting Blood Sugar** with **Resting BP** over 100 has displayed more cases of heart diseases than patients with no fasting blood sugar.\n- **Cholesterol** with **Fasting Blood Sugar** does not seem to have an effect in understanding reason behind heart diseases.\n- Patients that have not been found positive with **Fasting Blood Sugar** but have maximum heart rate below 130 are more prone to heart diseases."
#### RestingECG vs Numerical Features :
"- Heart diseases with **RestingECG** values of **Normal**, **ST** and **LVH** are detected starting from 30,40 & 40 respectively. Patients above the age of 50 are more prone than anyother ages irrespective of **RestingECG** values.\n- Heart diseases are found consistently throughout any values of **RestingBP** and **RestingECG**.\n- **Cholesterol** values between 200 - 300 coupled with **ST** value of **RestingECG** display a patch of patients suffering from heart diseases. \n- For **maximum Heart Rate** values, heart diseases are detected in dense below 140 points and **Normal** RestingECG. **ST** & **LVH** throughout the maximum heart rate values display heart disease cases."
#### ExerciseAngina vs Numerical Features :
- A crsytal clear observation can be made about the relationship between **heart disease** case and **Exercise induced Angina**. A positive correlation between the 2 features can be concluded throughout all the numerical features. 
#### ST_Slope vs Numerical Features :
"- Another crystal clear positive observation can be made about the positive correlation between **ST_Slope** value and **Heart Disease** cases. \n- **Flat**, **Down** and **Up** in that order display high, middle and low probability of being diagnosed with heart diseases respectively."
### Numerical features vs Numerical features w.r.t Target variable(HeartDisease) :
"- For **age** 50+, **RestingBP** between 100 - 175, **Cholesterol** level of 200 - 300,**Max Heart Rate** below 160 and positive **oldpeak** values displays high cases of heart disease.\n- For **RestingBP** values 100 - 175, highlights too many heart disease patients for all the features.\n- **Cholesterol** values 200 - 300 dominates the heart disease cases.\n- Similarly, **Max Heart Rate** values below 140 has high probability of being diagnosed with heart diseases."
### Correlation Matrix :
- It is a huge matrix with too many features. We will check the correlation only with respect to **HeartDisease**. 
- It is a huge matrix with too many features. We will check the correlation only with respect to **HeartDisease**. 
"- Except for **RestingBP** and **RestingECG**, everyone displays a positive or negative relationship with **HeartDisease**."
#### ANOVA Test :
- We will leave out **RestingBP** from the modeling part and take the remaining features.
- Selecting the features from the above conducted tests and splitting the data into **80 - 20 train - test** groups.
#### 1] Logistic Regression :
